<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototype Completion for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Prototype Completion for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-Shot Learning</term>
					<term>Meta-Learning</term>
					<term>Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning aims to recognize novel classes with few examples. Pre-training based methods effectively tackle the problem by pre-training a feature extractor and then fine-tuning it through the nearest centroid based meta-learning. However, results show that the fine-tuning step makes marginal improvements. In this paper, 1) we figure out the reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which implies that fine-tuning feature extractor is less meaningful; 2) instead of fine-tuning feature extractor, we focus on estimating more representative prototypes. Consequently, we propose a novel prototype completion based meta-learning framework. This framework first introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative features for seen attributes as priors. Second, a part/attribute transfer network is designed to learn to infer the representative features for unseen attributes as supplementary priors. Finally, a prototype completion network is devised to learn to complete prototypes with these priors. Moreover, to avoid the prototype completion error, we further develop a Gaussian based prototype fusion strategy that fuses the mean-based and completed prototypes by exploiting the unlabeled samples. Extensive experiments show that our method: (i) obtains more accurate prototypes; (ii) achieves superior performance on both inductive and transductive FSL settings. Our codes are open-sourced at https://github.com/zhangbq-research/Prototype Completion for FSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMANS can adapt to a novel task from only a few observations, because our brains have the excellent capability of learning to learn. In contrast, modern artificial intelligence (AI) systems generally require a large amount of annotated samples to make the adaptations, such as image classification <ref type="bibr" target="#b0">[1]</ref>. However, preparing sufficient annotated samples is often laborious, expensive, or even unrealistic in some applications such as cold-start recommendation <ref type="bibr" target="#b1">[2]</ref> and drug discovery <ref type="bibr" target="#b2">[3]</ref>. To equip the AI systems with such human-like ability, few-shot learning (FSL) becomes an important and widely studied problem. Different from conventional machine learning, FSL aims to learn a classifier from a set of base classes with abundant labeled samples, then adapt to a set of novel classes with few examples <ref type="bibr" target="#b3">[4]</ref>.</p><p>Existing studies on FSL roughly fall into four categories, namely the metric-based methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, optimizationbased methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, graph-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and semantics-based methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Though their methodologies are quite different, almost all methods address the FSL problem by a two-phase meta-learning framework, i.e., (i) a meta-training phase that learns meta-knowledge from a large number of base class tasks, and (ii) a meta-test phase that quickly constructs a model for novel class prediction with the meta-knowledge. Recently, Chen et al. <ref type="bibr" target="#b13">[14]</ref> find that introducing an extra pre-training phase can significantly boost the performance. In this method, a feature extractor first is pre-trained by learning a classifier on the entire base classes. Then, the metric-based meta-learning is adopted to  fine-tune it. In the meta-test phase, the mean-based prototypes are constructed to classify novel classes via the nearest neighbor classifier with cosine distance.</p><formula xml:id="formula_0">? Baoquan</formula><p>Though the pre-training based meta-learning method has achieved promising improvements on FSL, Chen et al. find that the fine-tuning step indeed makes very marginal contributions <ref type="bibr" target="#b13">[14]</ref> during meta-learning. In other words, the power of the pre-trained model is not effectively explored by the meta-learning methods. However, the reason is not revealed in <ref type="bibr" target="#b13">[14]</ref>. To figure out the reason, we visualize the distribution of base and novel class samples of the miniImagenet in the pre-trained feature space, which is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We find that the base class samples form compact clusters while the novel class samples spread as groups with large variances. It means that (i) fine-tuning the feature extractor to gather the base class samples into more compact clusters is less meaningful, because this enlarges the probability to overfit the base tasks; and (ii) the given few labeled samples may be far away from its ground-truth class centers in the case of large variances for novel classes, which poses arXiv:2108.05010v1 [cs.CV] 11 Aug 2021 a great challenge for estimating representative prototypes. Hence, in this paper, instead of fine-tuning feature extractor, we focus on how to estimate representative prototypes from few labeled samples, especially when these samples are far away from their ground-truth class centers.</p><p>Recently, Xue et al. <ref type="bibr" target="#b14">[15]</ref> also attempt to address a similar problem by learning a mapping function from noisy samples to their ground-truth class centers. However, learning to recover representative prototypes from noisy samples without any priors is very difficult. Moreover, the method does not leverage the pre-training strategy. Thus, its performance improvement is limited. In this paper, inspired by the visual attribute learning <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we find that the samples deviated from its ground-truth centers are often incomplete, i.e., missing some representative attribute features. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the meerkat sample nearby the class center contains all the representative features, e.g., the head, body, legs, and tail, while the ones far away may miss some representative features such as legs and tail. This means that the prototypes estimated by the samples deviated from its class centers may be incomplete, which limits the classification performance of FSL.</p><p>Based on this fact, we propose a novel prototype completion framework for FSL. Our framework works in a pretraining manner and introduces some primitive knowledge (i.e., class-level attribute or part annotations), e.g., whether a class object should have ears, legs or eyes, as priors to achieve the prototype completion. Specifically, we first extract the visual features for each seen part/attribute, by aggregating the pre-trained feature representations of all the base class samples that have the corresponding attributes in our primitive knowledge. Second, a Part/Attribute Transfer Network (PATNet) is then designed to infer the visual features for each unseen part/attribute. Third, we mimic the setting of few-shot classification task and construct a set of prototype completion tasks. A Prototype Completion Network (ProtoComNet) is then developed to learn to complete representative prototypes with the primitive knowledge and the obtained visual attribute features. To avoid the prototype completion error caused by primitive knowledge noises or base-novel class differences, we further design a Gaussian-based prototype fusion strategy, which effectively combines the mean-based and completed prototypes by exploiting the unlabeled data. Finally, the few-shot classification is achieved via a nearest neighbor classifier. Our main contributions of this paper can be summarized as follows: <ref type="bibr">?</ref> We reveal the reason why the feature extractor finetuning step contributes very marginally to the pretraining based meta-learning methods, and point out that representative prototype estimation is a more important issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose a novel prototype completion based FSL framework. In the framework, a part/attribute transfer network, a prototype completion network and a Gaussian-based prototype fusion strategy are designed, which offer our framework the excellent ability to construct more representative prototypes, by exploiting the primitive knowledge of both seen and unseen parts/attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In the Gaussian-based prototype fusion strategy, we propose and extend three methods to estimate prototype fusion parameters, i.e., a two-step estimation method, an EM (Expectation Maximization)-based estimation method, and an improved EM-based estimation method, which fully exploit the unlabeled data for more accurate prototypes estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We conduct comprehensive experiments on three real-world data sets. The experimental results demonstrate that our method achieves superior performance in both inductive and transductive FSL settings over state-of-the-art techniques.</p><p>This paper is an extension to our conference version in <ref type="bibr" target="#b17">[18]</ref>. Compared to the conference paper, this version additionally presents (i) a more powerful prototype completion framework for FSL, which introduces a novel part/attribute transfer network for incorporating unseen parts/attributes and develops two new methods (the EM-based and the improve EM-based methods) to estimate fusion parameters for Gaussian-based prototype fusion strategy, and improves the performance significantly; (ii) a unified perspective to understand the mean-based prototype fusion strategy and a theoretical analysis on the Gaussian-based prototype fusion strategy; (iii) more statistical analysis, ablation results, and visualization on miniImagenet, tieredImageNet, and CUB-200-2011, and comparisons with more state-of-the-art methods in both transductive and inductive FSL settings.</p><p>The rest of this work is organized as follows: In Section 2, we briefly review related works on few-shot learning, zeroshot learning, and visual attributes. Section 3 describes our method in details, including the prototype completionbased meta-learning framework and the three key components, i.e., the part/attribute transfer network, prototype completion network and prototype fusion strategy. Section 4 presents and analyzes the experimental results on miniImagenet, tieredImageNet, and CUB-200-2011 data sets. Finally, the conclusion is summarized in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The key idea of the proposed prototype completion-based meta-learning framework is utilizing primitive knowledge to learn to complete prototypes for FSL. Here, the primitive knowledge refers to class-level part or attribute annotations, which can be regarded as external knowledge. Thus, in this section, many relevant studies, including few-shot learning, zero-shot learning, and visual attributes techniques, are reviewed individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-Shot Learning</head><p>In the literature, existing FSL methods can be divided into two groups in terms of their settings, namely the inductive FSL and transductive FSL techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Inductive FSL</head><p>Most existing studies primarily address the FSL problem using the idea of inductive learning, which assumes the information of test samples is not available when performing few-shot classification tasks. Specifically, these approaches can be grouped into three categories. 1) Metricbased approaches. The type of methods aim to learn a good metric space, where novel class samples can be nicely categorized via a nearest neighbor classifier with Euclidean <ref type="bibr" target="#b18">[19]</ref>, cosine distance <ref type="bibr" target="#b19">[20]</ref>, mahalanobis distance <ref type="bibr" target="#b20">[21]</ref>, earth mover's distance <ref type="bibr" target="#b21">[22]</ref>, or learnable distance <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. For example, Chen et al. <ref type="bibr" target="#b24">[25]</ref> proposed a variational method to learn a proper scaling parameter for the Euclidean or cosine based metric, aiming to better fit the metric space to a given data distribution. 2) Optimization-based approaches. The methods follow the idea of modeling an optimization process over few labeled samples under the meta-learning framework, aiming to adapt to novel tasks by a few optimization steps, such as <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. 3) Semantics-based approaches. This line of methods employ the semantic knowledge to enhance the performance of meta-learning on FSL <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. For example, in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, they explored the class correlations, respectively, from the perspectives of the class name, description, and knowledge graph as textual semantic knowledge, aiming to enhance the FSL classifier by the convex combination of visual and semantic modalities. Different from these works, we introduce fine-grained attributes as priors to enable a meta-learner to learn to complete prototypes for FSL, instead of to combine two modalities.</p><p>Recently, some studies turn to pre-training techniques for the FSL problem and achieve promising performance <ref type="bibr" target="#b36">[37]</ref>. Chen et al. <ref type="bibr" target="#b19">[20]</ref> first proposed and investigated the pre-training techniques in FSL, by considering linear-based and cosine distance-based classifiers, respectively. In <ref type="bibr" target="#b13">[14]</ref>, a novel metric-based meta-learning method was developed by incorporating a pre-training phase. These methods, albeit delivering promising performance, do not fully explore the power of pre-training, as results show that the major improvements are made by the pre-training while the metalearning phase contributes very marginally. According to our analysis, this is because novel classes group loosely in the pre-trained feature space. In such case, estimating more accurate and representative prototypes is more important than fine-tuning the projection spaces. Hence, in this paper, we propose a prototype completion framework to address the issue. Recently, there are also other latest pre-training FSL methods such as <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, which focus on developing either a better pre-training strategy or a more powerful parametric classifier. Their strategies are different from our prototype completion framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Transductive FSL</head><p>Different from inductive FSL, transductive FSL assumes that all informtation from test samples can be used for recognizing novel classes. Such approaches have been proved to be more effective than inductive FSL approaches in data-scarce scenario <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. These approaches can be divided into two groups. 1) Graph-based approaches. The type of methods learn how to construct a good graph structure and an effective propagation mechanism from base classes as meta-knowledge, and then apply the meta-knowledge on novel classes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. For instance, Yang et al. <ref type="bibr" target="#b47">[48]</ref> proposed a distribution propagation graph network for transductive FSL, aiming to propagate labels from labeled samples to unlabeled samples with the graph. 2) Pre-training based approaches. The methods also focus on the pre-training feature extractor and attempt to learn a classifier (e.g., SVM) <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> or enhance prototypes by leveraging unlabeled samples <ref type="bibr" target="#b54">[55]</ref>. For example, Liu et al. <ref type="bibr" target="#b54">[55]</ref> developed a label propagation and feature shifting strategy to diminish the intra-class and cross-class prototypes bias in the pre-trained feature space. Different from these studies, we leverage the unlabeled samples to estimate prototype distribution and then leverage it to fuse prototypes. As far as we know, this is the first work to explore unlabeled samples for prototype fusion in FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Zero-Shot Learning</head><p>Zero-shot learning (ZSL) is also closely related to FSL, which aims to address the novel class categorizations without any labeled samples <ref type="bibr" target="#b55">[56]</ref>. The key idea is to learn a mapping function between the semantic and the visual space on the base classes, then apply the mapping to categorize novel classes. The semantic spaces in ZSL are typically attributebased <ref type="bibr" target="#b16">[17]</ref>, text description-based <ref type="bibr" target="#b56">[57]</ref>, and word vectorbased <ref type="bibr" target="#b57">[58]</ref>. For example, in <ref type="bibr" target="#b16">[17]</ref>, the semantic attributes are employed and a structure constraint on visual centers is incorporated for the mapping function learning. Our method differs from these models in two key points: (i) our method is for the FSL problem, where few labeled samples should be effectively utilized; (ii) based on semantic attributes, we propose a novel prototype completion based meta-learning framework, instead of directly learning the map function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Attributes</head><p>Visual attributes refer to the visual features of object components <ref type="bibr" target="#b15">[16]</ref>, which have been successfully utilized in various domains, such as action recognition <ref type="bibr" target="#b58">[59]</ref>, zero-shot learning <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b59">[60]</ref>, person Re-ID <ref type="bibr" target="#b60">[61]</ref>, and image caption <ref type="bibr" target="#b61">[62]</ref>. Recently, several FSL techniques relying on visual attributes have been proposed. In <ref type="bibr" target="#b62">[63]</ref>, an attribute decoupling regularizer was developed based on visual attributes to obtain good representations for images. Hu et al. <ref type="bibr" target="#b63">[64]</ref> proposed a compositional feature aggregation module to explore both spatial and semantic visual attributes for FSL. Zou et al. <ref type="bibr" target="#b64">[65]</ref> explored compositional few-shot recognition by learning a feature representation composed of important visual attributes. All the methods utilize visual attributes for better representations. Different from these studies, we leverage them to learn a prototype completion strategy. As a result, more accurate prototypes can be obtained for FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first present a formal definition of the FSL problem setting. Second, the proposed prototype completion based meta-learning framework is introduced. Finally, the three key components in the framework, namely the parts/attribute transfer network, the prototype completion network, and the prototype fusion strategy are elaborated in the last three subsections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>For N -way K-shot FSL problems, we are given two sets:  consisting of unlabeled samples (called query set). Here x i denotes the image sampled from the set of novel classes C novel , y i ? C novel is the label of x i , N indicates the number of classes in S, K denotes the number of images of each class in S, and M denotes the number of images in Q. Meanwhile, we also have an auxiliary data set with abundant labeled</p><formula xml:id="formula_1">a training set S = {(x i , y i )} N ?K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototypes Fusion Strategy</head><formula xml:id="formula_2">images D base = {(x i , y i )} B i=0 ,</formula><p>where B is the number of images in D base , the image x i is sampled from the set of base classes C base , i.e. y i ? C base , and the sets of class C base and C novel are disjoint. Our goal is to learn a classifier for the query set Q on the support set S and the auxiliary data set D base . We note that the query set Q is available by regarding it as a set of unlabeled samples to transductive FSL. However, it is not accessible for inductive FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>As shown in <ref type="figure">Fig. 2</ref>, the proposed prototype completionbased meta-learning framework consists of four phases, including pre-training, learning to complete prototypes, metatraining, and meta-test. Next, we detail them respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pre-Training</head><p>In this phase, following <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we build and train a convolution neural network (CNN) classifier with the base classes. Then, the last softmax layer is removed and the classifier turns into a feature extractor f ? f (). This produces a good embedding representation for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Learning to Complete Prototypes</head><p>We propose a Prototype Completion Network (ProtoCom-Net) as a meta-learner. It accounts for complementing the missing attributes for incomplete prototypes. The main details of the ProtoComNet will be elaborated in Section 3.4. Here we first give an overview of its workflow depicted in <ref type="figure">Fig. 2</ref>, which includes four steps:</p><p>Step 1. We construct primitive knowledge for all classes. The knowledge is what kinds of attribute feature the class should have, e.g., the leopard has four feet and round spot, and zebra has long face and four feet. We note that such kinds of knowledge is very cheap to obtain, e.g.,</p><formula xml:id="formula_3">from WordNet. Let A = {a i } F ?1</formula><p>i=0 denotes the set of class parts/attributes where F is the number of attributes, and R denotes the association matrix between the attributes and the classes, where R kai = 1 if the attribute a i is associated with the class k; otherwise R kai = 0. Meanwhile, the semantic embeddings of all classes and attributes are calculated by Glove <ref type="bibr" target="#b65">[66]</ref> in an average manner of word embeddings, denoted by</p><formula xml:id="formula_4">H = {h k } |C base |+|C novel |?1 k=0 ? {h ai } F ?1</formula><p>i=0 . In particular, we split the set of class parts/attributes A into two subset: A seen and A unseen (i.e., F = |A seen | + |A unseen |). The former A seen denotes the set of parts/attributes that base classes contains. On the other hand, the latter A unseen refers to the set of parts/attributes that the novel classes contain but does not appear in base classes.</p><p>Step 2. Based on the pre-trained feature extractor f ? f () and the above primitive knowledge, we extract two types of information as priors, namely base class prototypes and seen part/attribute features. Specifically, the base class prototypes p real k can be calculated by averaging the extracted features of all samples in the base class k, that is,</p><formula xml:id="formula_5">p real k = 1 |D k base | (x,y)?D k base f ? f (x),<label>(1)</label></formula><p>where D k base denotes the set of samples from the base class k. As for the feature z ai of each seen part/attribute a i ? A seen , we denote all base class samples that have the corresponding part/attribute a i ? A seen in the primitive knowledge as a set D ai base . Then, we calculate its mean ? ai and diagonal covariance diag(? 2 ai ) as:</p><formula xml:id="formula_6">? ai = 1 |D ai base | (x,y)?D a i base f ? f (x),<label>(2)</label></formula><formula xml:id="formula_7">? ai = 1 |D ai base | (x,y)?D a i base (f ? f (x) ? ? ai ) 2 .<label>(3)</label></formula><p>Here, the mean u ai and the diagonal covariance diag(? 2 ai ) characterize the part/attribute feature distribution of each seen part/attribute a i ? A seen , i.e., z ai ? N (? ai , diag(? 2 ai )), which will be used in Section 3.3 and 3.4.</p><p>Step 3. According to Eqs. 2 and 3, we can estimate the feature distribution of the seen parts/attributes a i ? A seen . However, the method fails to model the unseen parts/attributes a i ? A unseen since it does not appear in base classes. To address the drawback, we design a Part/Attribute Transfer Network (PATNet) f ?p () with parameters ? p , which accounts for inferring the feature distribution of unseen parts/attributes by exploring the semantics relationship between unseen and seen parts/attributes. The intuition behind it is that the similar parts/attributes in semantics should have a similar feature distribution. Its design details will be introduced in Section 3.3. Here, we focus on introducing the overall workflow of the PATNet. Specifically, we take the semantic embedding {h ai }</p><formula xml:id="formula_8">|A seen |?1 i=0</formula><p>of all seen parts/attributes {a i } ? A seen as inputs, and treat the feature distribution N (? ai , diag(? 2 ai )) of the seen parts/attributes a i ? A seen estimated by Eqs. 2 and 3 as prediction targets, to train the proposed PATNet f ?p () by using the Kullback-Leibler (KL) divergence loss. That is,</p><formula xml:id="formula_9">?a i ,?a i = f ?p (ha i ), i = 0, 1, ..., |A seen | ? 1 min ?p Ea i ?A seen KL(N (?a i , diag(? 2 a i )), N (?a i , diag(? 2 a i ))),<label>(4)</label></formula><p>where KL() denotes the Kullback-Leibler (KL) divergence loss. Then, we train the parts/attributes transfer network f ?p () until it converges. The well trained PATNet can infer the feature distribution of each seen and unseen part/attribute through its semantics. As a result, we obtain a new feature distribution? ai ? N (? ai , diag(? 2 ai )) for each seen and unseen parts/attribute by utilizing its semantics as input of PATNet, which will be used in Section 3.4.</p><p>Step 4. Upon the results of the previous steps, we mimic the setting of K-shot tasks and construct a set of prototype completion tasks to train our meta-learner f ?c () (i.e., ProtoComNet) in an episodic manner <ref type="bibr" target="#b66">[67]</ref>. Specifically, in each episode, we first randomly select one class k from base classes C base and K images for the class k from D base as support set S. Then, we average the features of all samples in S as the incomplete prototypes p k . Here, we consider it as incomplete because some representative features may be missing. Even though in some cases this may not be true, regarding them as incomplete ones does no harms to our meta-learner. Finally, we take the incomplete prototypes p k , the primitive knowledge (the classattribute association matrix R and word embedding H), and the parts/attributes features Z = {z ai }</p><formula xml:id="formula_10">|A seen |?1 i=0 and Z = {? ai } |A seen |+|A unseen |?1 i=0</formula><p>as inputs, and treat the base class prototypes p real k as outputs, to train our meta-learner by using the Mean-Square Error (MSE) loss. That is,</p><formula xml:id="formula_11">min ?c E (p k , p real k )?T M SE(f ?c (p k , R, H, Z,?), p real k ), (5)</formula><p>where ? c denotes the parameters of our meta-learner and T denotes the set of prototype completion tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Meta-Training</head><p>To jointly fine-tune the feature extractor f ? f () and the metalearner f ?c (), we construct a number of N -way K-shot tasks from D base following the episodic training manner <ref type="bibr" target="#b66">[67]</ref>.</p><p>Specifically, in each episode, we sample N classes from the base classes C base , K images in each class as the support set S, and M images as the query set Q. Then, f ? f () and f ?c () can be further fine-tuned by maximizing the likelihood estimation on query set Q. That is, <ref type="bibr" target="#b5">(6)</ref> where ? = {? f , ? c } and T denotes the set of N -way Kshot tasks. Specifically, for each episode, we first estimate its class prototype p k by averaging the features of the labeled samples. That is,</p><formula xml:id="formula_12">max ? E (S,Q)?T (x,y)?Q log(P (y|x, S, R, H, Z,?, ?)),</formula><formula xml:id="formula_13">p k = 1 |S k | x?S k f ? f (x),<label>(7)</label></formula><p>where S k is the support set extracted for the class k. Then, the ProtoComNet is applied to complete p k , and we have:</p><formula xml:id="formula_14">p k = f ?c (p k , R, H, Z,?)).<label>(8)</label></formula><p>Moreover, to obtain more reliable prototypes, we further explore unlabeled samples and combine p k andp k by introducing a Gaussian-based prototype fusion strategy (which will be introduced in Section 3.5). As a result, the fused prototypep k is obtained. Finally, the probability of each sample x ? Q to be class k is estimated based on the proximity between its feature f ? f (x) andp k . That is, <ref type="formula">(9)</ref> where d() denotes the cosine similarity of two vectors and ? is a learnable scale parameter.</p><formula xml:id="formula_15">P (y = k|x, S, R, H, Z, ?) = e d(f ? f (x),p k ) ? ? c e d(f ? f (x),p c ) ? ? ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Meta-Test</head><p>Following Eqs. (7) ? (9), we directly perform few-shot classification for novel class prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Part/attribute Transfer Network</head><p>In this subsection, we introduce the first key component of learning to complete prototypes (Step 3 in Section 3.2.2), namely the PATNet f ?p (). Our intuition is that the similar parts/attributes in semantics should have a similar feature distribution. Thus, we directly treat the semantic embeddings {h ai } F ?1 i=0 of part/attribute as input and the parts/attributes distribution N (? ai , diag(? 2 ai )) as output to build the PATNet.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the network consists of an embedding layer f ?pe () and an inference layer f ?pi (), where ? pe and ? pi denote their parameters, respectively. Here, the former aims to map each semantic embeddings to a new embedding space, and then the latter accounts for estimating the feature distribution of each part/attribute. Next, we detail them, respectively.</p><p>Embedding Layer. We take the semantic embedding h ai of each part/attribute as input of the embedding layer f ?pe (), and then project the semantic embedding h ai to a new embedding space. As a result, the new embedding h ai can be obtained. That is, Inference Layer. Based on the new embedding h ai , we employ an inference layer consisting of a mean module and a diagonal covariance module to predict the distribution of each seen and unseen part/attribute, which is characterized by a multivariate normal distribution parameterized with its mean? ai and diagonal convariance diag(? 2 ai ). That is,</p><formula xml:id="formula_16">h ai = f ?pe (h ai ).<label>(10)</label></formula><formula xml:id="formula_17">? ai ,? ai = f ?pi (h ai ), z ai ? N (? ai , diag(? 2 ai )).<label>(11)</label></formula><p>Note that ? p contains the two parameters ? pe and ? pi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prototype Completion Network</head><p>In this subsection, we introduce how the ProtoComNet f ?c () are designed, which is the second key component for learning to complete prototypes (Step 4 in Section 3.2.2). Our intuition is that the parts/atributes feature can be transfered from base classes to novel classes for prototype completion. For example, even if human haven't seen "zebra", they can also imagine its visual features of "long face" once they learn this knowledge from "kangaroo" and "horse". Thus, we treat the primitive knowledge (R and H), part/attribute features Z and? and the incomplete prototypes p k as input and the completed prototypesp k as output, and then build an encoder-aggregator-decoder network, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Here, the encoder aims to form a low-dimensional representation for prototypes and part/attributes. Then, the aggregator accounts for evaluating the importance of different parts/attributes and combining them with a weighted sum. Finally, the decoder is in charge of the prediction of complete prototypesp k . Next, we detail them, respectively. The Encoder. In the training part, the encoding process involves a sampling step of an attribute feature z ai from its distribution N (? ai , diag(? 2 ai )) or N (? ai , diag(? 2 ai )), followed by an encoder g ?ce () that encodes the attribute feature z ai and the estimated prototypes p k to a latent code z ai and z k , respectively. To enhance the generalization of the model for seen and unseen parts/attributes, we adopt a randomized manner with a probability ? = 0.5 to sample the attribute feature z ai from seen part/attribute distribution N (? ai , diag(? 2 ai )) and unseen part/attribute distribution N (? ai , diag(? 2 ai )). The overall encoding process is formally expressed as: where ? ce denotes the parameters of the encoder and r is a random number from 0 to 1. Note that, in the meta-test phase, we regard N (? ai , diag(? 2 ai )) as the feature distribution of seen parts/attributes and N (? ai , diag(? 2 ai )) as the ones of unseen parts/attributes; and we remove the sampling step and use the mean ? ai and? ai to replace z ai .</p><formula xml:id="formula_18">z ai ? N (? ai , diag(? 2 ai )), a i ? A seen &amp; r &lt; ? N (? ai , diag(? 2 ai )), otherwise , z ai = g ?ce (z ai ), z k = p k , z k = g ?ce (z k ),<label>(12)</label></formula><p>The Aggregator. Intuitively, different parts/attributes make varying contributions to distinct classes, for example, the "nose" is more representive for elephants than tigers to complete their prototypes. Hence, differentiating their contributions in the completion is important. To this end, we employ an attention-based aggregator g ?ca (). Here, we calculate the attention weights ? kai by using the semantic embeddings h k and h ai of the class k and the attribute a i , and the incomplete prototypes p k . Then, we apply them to combine the latent codes z k and z ai , and obtain the aggregated result g k as follows:</p><formula xml:id="formula_19">? kai = R kai g ?ca (p k ||h k ||h ai ), g k = ai ? kai z ai + z k ,<label>(13)</label></formula><p>where ? ca is the parameters of the aggregator and || is a concatenation operation. The Decoder. Finally, we use the aggregated result g k to decode the complete prototypesp k for each class k by the decoder module g ? cd (). That is,</p><formula xml:id="formula_20">p k = g ? cd (g k ),<label>(14)</label></formula><p>where ? cd denotes the parameters of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prototype Fusion Strategy</head><p>Till now, we have two prototype estimations, i.e., the meanbased prototypes p k and the completed prototypesp k . Next, we will discuss why and how to fuse these two estimations from the perspective of Bayesian estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Why do we fuse prototypes?</head><p>Actually, both the estimates p k andp k have their own biases. The former is mainly due to the scarcity or incompleteness of labeled samples in novel classes, which produces biased means; while the latter is brought by the primitive knowledge noises and the base-novel class differences. The fact implies that the two estimates can remedy each other. When the labeled samples are very scarce and incomplete, the completed prototypesp k are more reliable because the completion is learned from a great number of base class tasks. As more and more labeled samples become available, the mean-based prototypes are more representative because the ProtoComNet may result in prototype completion error problem under the effects of primitive knowledge noises or class differences. <ref type="figure" target="#fig_4">Fig. 5(a)</ref> shows an example to demonstrate this. We observe that the completed prototypes are more accurate on 1/2-shot tasks while the mean-based ones are better on 3/4/5-shot tasks. Thus, a prototype fusion strategy is desired to combine their advantages and form more representative prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">How to fuse prototypes?</head><p>We apply the Bayesian estimation to fuse the two kinds of prototypes. Specifically, we assume that the estimated prototypes follow a Multivariate Gaussian Distribution (MGD), as the samples in the pre-trained space are continuous and clustered together (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>). Based on this assumption, p k can be regarded as a sample from the MGD with mean ? k and diagonal covariance diag(? 2 k ), i.e., N (? k , diag(? 2 k )). Likewise,p k is a sample from N (? k , diag(? 2 k )) with mean? k and diagonal covariance diag(? 2 k ). As shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, from the view of Bayesian estimation, we regard the distribution N (? k , diag(? 2 k )) as a prior, and treat the distribution N (? k , diag(? 2 k )) as the conditional likelihood of observed few labeled samples. Then, the Beyesian estimation of fused prototypes can be expressed as their product, i.e., a posterior MGD</p><formula xml:id="formula_21">N (? k , diag(? 2 k )) with mean? k = ? 2 k ? k +? 2 k ? k ? 2 k +? 2 k and diagonal covariance diag(? 2 k ) = diag( ? 2 k ? 2 k ? 2 k +? 2 k ),</formula><p>where is element-wise product (Please refer to Appendix A for its derivations). Finally, we take the mean? k as the fused prototypesp k to solve the few-shot tasks (Please refer to Section 3.5.4 for its theoretic analysis).</p><p>In this paper, we term the overall Bayesian estimation procedure as Gaussian-based prototype fusion strategy (GaussFusion). We can see that? k is determined by four unknown variables ? k , ? k , ? k , and ? k . Next, we introduce four types of methods to estimate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5.3</head><p>How to estimate ? k , ? k , ? k , and ? k ? In this part, we discuss four methods to estimate the four unknown variables ? k , ? k , ? k , and ? k , including (i) assumption-based estimation method, (ii) two-step estimation method, (iii) EM-based estimation method, and (iv) improved EM-based estimation method. Among them, the methods (i) and (ii) belong to non-iterative approaches, where the former follows the estimate strategy proposed in <ref type="bibr" target="#b14">[15]</ref> and the latter is our conference strategy <ref type="bibr" target="#b17">[18]</ref>. The rest of these methods (i.e., the methods (iii) and (iv)) all are iterative approaches, which are newly-developed in this paper.</p><p>Assumption-based Estimation method. The Meanbased Prototype Fusion (MeanFusion) strategy proposed in <ref type="bibr" target="#b14">[15]</ref> regards the averaged prototypes as the fused prototype? p k = 0.5(p k +p k ). This strategy can be considered as a special case of our GaussFusion, where we assume that the two means satify ? k = p k and? k =p k , and the two diagonal covariance is also equal, i.e., ? k =? k . However, the assumption is too strong to fit the real prototype distribution. Thus, the performance improvement of the MeanFusion is limited for FSL.</p><p>Two-Step Estimation Method. Inspired by transductive FSL <ref type="bibr" target="#b54">[55]</ref>, we propose to estimate the four variables by leveraging the unlabeled samples in a two-step manner:</p><p>Step 1) we calculate the probability of each sample x ? S?Q belonging to class k by regarding p k andp k as the prototypes, respectively. For example, when we take p k as the prototypes, the probability of each unlabeled sample x ? Q can be computed as:</p><formula xml:id="formula_22">P (y = k|x) = e d(f ? f (x), p k ) ? ? c e d(f ? f (x), pc) ? ? ,<label>(15)</label></formula><p>where d() indicates the cosine similarity of two vectors and ? is a hyper-parameter. Following <ref type="bibr" target="#b19">[20]</ref>, ? = 10 is used. As for each labeled sample x ? S, the probability turns into a one-hot vector by its labels.P (y = k|x) can be computed in a similar manner by using prototypesp k .</p><p>Step 2) we take P (y = k|x) as sample weights and estimate the mean ? k and the diagonal covariance diag(? 2 k ) of each prototype distribution in a weighted average manner. That is,</p><formula xml:id="formula_23">? k = 1 x?S?Q P (k|x) x?S?Q P (k|x)f ? f (x),<label>(16)</label></formula><formula xml:id="formula_24">? k = 1 x?S?Q P (k|x) x?S?Q P (k|x)(f ? f (x) ? ? k ) 2 .<label>(17)</label></formula><p>Similarly, the mean? k and the diagonal covariance diag(? 2 k ) can be calculated in a similar manner by regardingP (y = k|x) as sample weights. The two step prediction strategy is the method proposed in our conference version <ref type="bibr" target="#b17">[18]</ref>.</p><p>EM-based Estimation Method. The EM (Expectation-Maximization) algorithm <ref type="bibr" target="#b67">[68]</ref> is a widely used parameter estimation method, which adopts an iterative strategy to polish the parameter estimation. Thus, we attempt to estimate the above four variables by employing the EM algorithm. Specifically, we regard the support and query samples x ? S ? Q as the observation data from Gaussian mixture distribution with unknown mean ? k or? k and diagonal covariance diag(? k ) or diag(? k ) (k = 0, 1, ..., N ? 1), and regard the prototypes p k orp k as the initial mean of the k-th Gaussian distribution. Our goal is to fit the mean and diagonal covariance to the observation data x ? S ? Q. That is, maximizing the likelihood estimate for ? k and ? k (Note that? k and? k are similar) as:</p><formula xml:id="formula_25">l({? k , ? k } N ?1 k=0 ) = x?S?Q N ?1 k=0 z ? N (x; ? k , diag(? 2 k )),<label>(18)</label></formula><p>where z is a hidden variable denoting the posterior probability that x belongs to class k. We adopt EM algorithm to optimize Eq. 18, which includes following three steps: 1) initializing the mean ? k or ? k by using the prototypes p k orp k and diagonal covariance ? k or? k in a constant (We empirically find that our method can obtain high classification peformance when it is set as 35); 2) Performing E step to estimate the posterior probability z that a given observation x belongs to a given class k by using the probability density function N (x; ? k , diag(? 2 k )) or N (x;? k , diag(? 2 k )). Note that we estimate the probability of each support sample x ? S by a one-hot vector of its label since its label is known; 3) Performing M step to maximize the posterior probability and find the optimal mean ? k or? k and diagonal covariance ? k or? k ; 4) Repeatedly carrying out these two steps (i.e., E step and M step) until convergence. Finally, we take the resulting ? k or? k and ? k or? k as our estimation.</p><p>Improved EM-based Estimation method. In the above EM-based method, the posterior probability z is estimated by using the Gaussian probability density. Its calculation is similar to the Mahalanobis distance. However, recent studies <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref> found that the cosine distance-based classifier show better performance on the estimation of posterior probability z for FSL. Inspired by this fact, we estimate it by leveraging the cosine-based classifier (i.e., Eq. <ref type="formula" target="#formula_5">(15)</ref>). In particular, the improved EM-based method can be regarded as an extension of the above Two-Step Method by using the EM algorithm. Specifically, we first initialize the mean ? k or? k by using the prototypes p k orp k . Second, the step 1) (described in Two-Step Estimation Method) can be regarded as an E-Step, i.e., regarding the mean ? k or? k as the prototypes of cosine classifier and then estimating the posterior probability z that a given observation x belongs to a given class k. This is done by using Eq. <ref type="bibr" target="#b14">(15)</ref>. Third, the step 2) can be regarded as an M-Step, i.e., maximizing the posterior probability to find the optimal mean ? k or? k and diagonal covariance diag(? k ) or diag(? k ). This is done according to Eqs. <ref type="bibr" target="#b15">(16)</ref> and <ref type="bibr" target="#b16">(17)</ref>. Finally, the above two steps are repeated until convergence. Here, we denote the number of iteration as a hyper-parameter n iter and empirically find that setting it to 6 is sufficient to converge. For clarity, we summarize the improved EM-based method in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Theoretic Analysis</head><p>Here, we provide a brief theoretic analysis on the Gaussianbased prototype fusion strategy described in Section 3.5.2. By the strategy, we can obatin five estimations, i.e.,? k , ? k , ? k , p k , andp k . Next, we analyze why the prototypes? k produced by the prototype fusion strategy are better. Proposition 1. ? k (? k ) is more representative than p k (p k ). Proof. We take ? k and p k as an example to prove the Proposition 1. The proof for? k andp k is similar. Let us first revisit how are the variables ? k and ? k estimated. In these EM-based fusion parameter estimation methods, the estimation of ? k and ? k is regarded as a fitting problem of observation data x ? S ? Q with a N -components Gaussian mixture model. Thus, our goal is to optimize the N -components parameters</p><formula xml:id="formula_26">? t+1 = {? t+1 k , ? t+1 k } N ?1 k=0</formula><p>iteratively by maximizing the log-likelihood L(? t ):</p><formula xml:id="formula_27">max ? t L(? t ) = log x?S?Q P (x|? t ) = x?S?Q log N ?1 k=0 P (x, k|? t ),<label>(19)</label></formula><p>where k is the label of k-th Gaussian components. As our solution follows the EM optimization, we have L(? t+1 ) ? L(? t ). This means that each iteration of the improved EMbased algorithm increases the log likelihood L(? t ), i.e., the parameters ? t+1 is more effective than ? t for fitting observation data x ? S ? Q. Thus, the variable ? k obtained by the improved EM-based methods is more representative than the initial variable p k . Proposition 2.? k is more representative than ? k , and? k .</p><p>Proof. Let us revisit the fused prototype distribution, i.e., the posterior MGD N (? k , diag(? 2 k )). Here,</p><formula xml:id="formula_28">? 2 k = ? 2 k ? 2 k ? 2 k +? 2 k</formula><p>denotes the estimation variance of prototypes? k (Note that we assume the covariance is diagonal). Then, we have the two inequalities since these terms</p><formula xml:id="formula_29">? 4 k ? 2 k +? 2 k and? 4 k ? 2 k +? 2</formula><p>k are always greater than or equal to 0:</p><formula xml:id="formula_30">? 2 k = ? 2 k ? 2 k ? 2 k + ? 2 k = ? 2 k ? ? 4 k ? 2 k + ? 2 k ? ? 2 k ,<label>(20)</label></formula><formula xml:id="formula_31">? 2 k = ? 2 k ? 2 k ? 2 k + ? 2 k =? 2 k ?? 4 k ? 2 k + ? 2 k ?? 2 k ,<label>(21)</label></formula><p>where the right equation is satisfied only when ? 2 k or? 2 k is zero. The Eqs. 20 and 21 imply that the variance of prototypes? k decreases for each class k after fusing ? k and ? k . Thus,? k is more representative than ? k and? k . Based on the above propositions 1 and 2, we know that ? k is more representative than ? k ,? k , p k , andp k . Hence, we take the mean? k as the final fused prototypep k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE EVALUATION</head><p>In this section, we evaluate the proposed framework on general and fine-grained few-shot classification tasks, and then discuss the experiment results and present our statistical analysis, ablation study, and visualization in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>MiniImagenet. The data set is a subset of ImageNet, which includes 100 classes and each class consists of 600 images. Following <ref type="bibr" target="#b14">[15]</ref>, we split the data set into 64 classes for training, 16 classes for validation, and 20 classes for test, respectively. The class parts/attributes are extracted from WordNet by using the relation of "part holonyms()". TieredImagenet. The data set is another subset of ImageNet, which includes 608 classes and each class contains about 1200 images <ref type="bibr" target="#b68">[69]</ref>. It is first partitioned into 34 high-level classes, and then split into 20 classes for training, 6 classes for validation, and 8 classes for test, respectively. Similarly, the class parts/attributes are also extracted from WordNet by using the relation of "part holonyms()". CUB-200-2011. The data set is a fine-grained classification data set, which includes 200 classes and contains about 11,788 images. Following <ref type="bibr" target="#b64">[65]</ref>, we split the data set into 100 classes for training, 50 classes for validation, and 50 classes for test, respectively. Different from miniImagenet and tieredImagenet, its class parts/attributes have been manually labeled and made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Architecture. Following <ref type="bibr" target="#b13">[14]</ref>, we employ ResNet12 as the feature extractor. In PATNet, we use a single-layer MLP with 512 units for the embedding layer, and a two-layer MLP with 512-dimensional hidden units for the mean module and diagonal covariance module, respectively. In ProtoCom-Net, we use a single-layer MLP with 256 units for the encoder, a two-layer MLP with a 300-dimensional hidden layer for the aggregator, and a two-layer MLP with 512dimensional hidden layers for the decoder. Here, ReLU is used as the activation function for all network. The number of iteration, namely n iter , is set to 6 for GaussFusion. Training Details. We first pre-train the feature extractor with 100 epochs on base classes via an SGD with momentum of 0.9 and weight decay of 0.0005. The learning rate is initially set to 0.1, and then decayed by 0.1 at epochs 60, 80, and 90, respectively. Second, we train the PATNet with 20000 epochs by using an Adam with weight decay of 0.0005. The learning rate is initially set to 0.001, and then decayed by 0.1 at 10000 epochs. Third, we train the ProtoComNet with 100 epochs in an episodic manner by using an SGD with momentum of 0.9 and weight decay of 0.0005. The learning rate is initially set to 0.1, and then changed at epochs 15, 40, and 80. Finally, we fine-tune all modules with 40 epochs in an episodic manner. The learning rate is initially set to 0.01, and then decayed by 0.1 at epochs 15, 25, and 30. Evaluation. We conduct few-shot classification on 600 randomly sampled episodes from the test set and report the mean accuracy together with the 95% confidence interval. In each episode, we randomly sample 15 query images per class for evaluation in 5-way 1-shot/5-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion of Results</head><p>For a comparison, some state-of-the-art approaches are also applied to the few-shot classification and few-shot finegrained classification tasks as baselines. These methods can be roughly from six types, i.e., metric-based, optimizationbased, semantics-based, attribute-based, graph-based, and pre-training based approaches. For a fair comparison, we employ the MeanFusion and GaussFusion strategy to evaluate the performance of our framework on inductive and transductive FSL seting, respectively.</p><p>1) In few-shot classification. <ref type="table" target="#tab_2">Table 1</ref> shows the results of our method and the baseline methods on miniImagenet and tieredImagenet. It can be found that our method achieves superior performance on both inductive and transductive FSL settings. Specifically, in inductive FSL, compared with the metric-based approaches, our method better exploits the power of pre-training by learning to complete prototypes. The results show our method is more effective, with an improvement of 4% ? 10%. It is worth noting that our method also beats RestoreNet, which also adopts the strategy of prototype learning. This demonstrates the proposed prototype completion is more effective. Compared with the optimization-based methods (e.g., ALFA), our method achieves 3% ? 9% higher accuracy. Different from these methods, we focus on metric-based FSL framework, but targets at learning representative prototypes. As for the semantics and attribute-based approaches, they also leverage the external knowledge. However, our method utilizes the knowledge to learn to complete prototypes, instead of to combine modality or to learn the feature extractor. The result validates the superiority of our manner to incorporate the external knowledge. Note that our method achieves competitive performance with the MultiSem method on 5shot tasks on miniImagenet. We would like to emphasize that this is because MultiSem leverages a more complex backbone, namely the Dense-121 with 121 layers, instead of ResetNet12 in our model.</p><p>Finally, from the results of the pre-training based apporaches, we have the following observations. (i) Our method exceeds the MetaBaseline method by a large margin, around 3%?7% (1-shot) and 2% ? 4% (5-shot). This verifies our motivation that estimating more accurate prototypes is more effective than fine-tuning feature extractor during metalearning. Besides, the improvement of performance on 1shot tasks is more obvious than on 5-shot tasks. This is reasonable because the problem of inaccurate estimation of prototypes on 1-shot is more remarkable than 5-shot tasks.</p><p>(ii) Our method outperforms Neg-Cosine and CentAlign, by around 1% ? 5%. This is because our method focuses on estimating more representative prototypes, instead of pretraining strategy or generating more training samples. (iii) Our method exceeds DC method by around 1% ? 3% on miniImagenet, while performs slightly worse than DC on tieredImagenet. The reason is that the DC method leverags a deeper backbone WRN-28-10 instead of ResNet12 and a complex power transformations for image representation.</p><p>In transductive FSL setting, SRestoreNet is very related with our method, which also explores the query samples to restore prototypes. However, different from it, we leverage the query samples to estimate the prototype distribution and then to fuse prototypes. The result validates the superiority of our method. Compared with the graph-based approaches, our method obtains competitive classification performance, especially in 1-shot tasks. This is because our method exploits unlabeled data to combine mean-based and completed prototypes, instead of propagate embedding or labels. Finally, from the results of the pre-training based apporaches, we have the following observations. (i) Compared with the best results of pre-training based methods (TIM-GD, SIB, LaplacianShot, and ICI), our method obtains 1% ? 6% higher accuracy, which further validates the superiority of learning representative prototypes. (ii) Our method outperforms BD-CSPN, by around 5% ? 14%. The DB-SCPN method also leverages unlabeled samples, but they only focus on pre-training and ignore the advantange of meta-learning. Different from it, we introduce a metalearner, learning to complete prototypes, to explore the power of pre-training further. Besides, the improvement of performance on 1-shot tasks is more obvious than on 5-shot tasks. This is reasonable because the problem of inaccurate estimation of prototypes on 1-shot is more remarkable than 5-shot tasks. (iii) Compared with the conference version <ref type="bibr" target="#b17">[18]</ref>, the extended version (EM-based and Improved EM-based) exceeds it by 1% ? 6%. The main reason is that we explore  RestoreNet <ref type="bibr" target="#b14">[15]</ref> 74.32 ? 0.91% ? ? ?% RAP-ProtoNet <ref type="bibr" target="#b53">[54]</ref> 75.17 ? 0.63% 88.29 ? 0.34% MAML <ref type="bibr" target="#b7">[8]</ref> 55.92 ? 0.95% 72.09 ? 0.76% MultiSem <ref type="bibr" target="#b35">[36]</ref> 76.1% 82.9% CPDE <ref type="bibr" target="#b64">[65]</ref> 80.11 ? 0.34 % 89.28 ? 0.33% CFA <ref type="bibr" target="#b63">[64]</ref> 73.90 ? 0.80% 86.80 ? 0.50% Neg-Cosine <ref type="bibr" target="#b37">[38]</ref> 72.66 ? 0.85% 89.40 ? 0.43% CentAlign <ref type="bibr" target="#b38">[39]</ref> 74.22 ? 1.09% 88.65 ? 0.55% DC <ref type="bibr" target="#b63">[64]</ref> 77.22 ? 0.14% 89.58 ? 0.27% Our Method 88.99 ? 0.58% 94.05 ? 0.34%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trans.</head><p>SRestoreNet <ref type="bibr" target="#b14">[15]</ref> 76.85 ? 0.95% ? ? ?% EPNet <ref type="bibr" target="#b10">[11]</ref> 82.85 ? 0.81% 91.32 ? 0.41% ICI <ref type="bibr" target="#b51">[52]</ref> 87.87% 92.38% TIM-GD <ref type="bibr" target="#b49">[50]</ref> 82.2 % 90.8 % LaplacianShot <ref type="bibr" target="#b50">[51]</ref> 80.96% 88.68% RAP-LaplacianShot <ref type="bibr" target="#b53">[54]</ref> 83.59 ? 0.18% 90.77 ? 0.10% BD-CSPN <ref type="bibr" target="#b54">[55]</ref> 84.90 % 90.22% Conference Version <ref type="bibr" target="#b17">[18]</ref> 93. <ref type="bibr" target="#b19">20</ref>  unseen parts/attributes and enhance the GaussFusion by introducing an iterable parameter estimation algorithm. (iv) our improved EM-based method perform best in all extended methods, thus it is used in subsequent discussion.</p><p>2) In few-shot fine-grained classification. <ref type="table" target="#tab_3">Table 2</ref> summarizes the results on CUB-200-2011, which lead to similar observations as those in <ref type="table" target="#tab_2">Table 1</ref>. We observe that our method (i) also achieves superior performance over state-of-the-art methods with an improvement of 4% ? 5% (inductive FSL)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-5 nearest samples</head><p>Top-5 farthest samples and 4% ? 6% (transductive FSL); (ii) exceeds the conference version around 1%; (iii) obtains almost consistent performance on 1-shot and 5-shot tasks, while the improvements on 1-shot task over baselines are more significant than on 5shot. The results on few-shot fine-grained classification tasks further verify the effectiveness of the proposed method, especially for 1-shot classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Statistical Analysis</head><p>In this subsection, we conduct additional statistical experiments to answer the following four questions: 1) Is our idea reasonable on realistic data? We randomly select five classes from the novel classes of miniImageNet and retrieve top-5 nearest and farthest samples from its ground-truth class center in the feature space. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the nearest images are more complete; however, the  farthest samples are missing partial parts/attributes due to its incompleteness, noise background, or obscured details.</p><p>2) Does our method obtain more accurate prototypes? We calculate the average cosine similarity between the estimated prototypes and the real prototypes on 1000 episodes (5-way 1-shot) on miniImagenet, tieredImagenet, and CUB-200-2011. Three results including the mean-based (p k ), the restored/completed (p k ) and the fused prototypes (p k ) are reported. For a fair comparison, we report the results of SRe-storeNet, FSLKT, and BD-CSPN as the baselines. As shown in <ref type="table" target="#tab_5">Table 3</ref>, the results show that our method obtains more accurate prototypes than these baselines and the conference version <ref type="bibr" target="#b17">[18]</ref>. Note that the prototypesp k from SRestoreNet is better than our method. This is reasonable because they  leverage unlabeled samples before restoring prototypes. However, we exploit them after completing prototypes.</p><p>3) Is our method effective for the samples far away from its class center? On the novel classes of miniImageNet, tieredImagenet, and CUB-200-2011, we calculate the cosine similarity between each noise image and its class center and sort them in descending order (i.e., the larger the sample number is, the farther away it is from the class center). Then, we take the noise images as inputs to predict the prototypes by using our method and RestoreNet, respectively. The cosine similarity between predicted prototypes and real class centers is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. Note that (i) we smoothen the curve through moving average with 50 samples; (ii) we show the average results for all novel classes. From the results of the above three datasets, we observe our method achieves more accurate prototypes than RestoreNet and the improvement becomes larger as the samples are farther away from its center. This means that our method can recover representative prototypes, especially when they are far away from their ground-truth centers.</p><p>4) How set the number of iterations n iter for GaussFusion with improved EM-based method? To find the optimal n iter , we conduct experiments on 5-way 1-shot and 5-shot tasks of miniImagenet, tieredImagenet, and CUB-200-2011, respectively, and report the test accuracy of the proposed method with different n iter . The results are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. We observe that the iteration process is very important and Ablation study on miniImagenet, tieredImagenet and CUB-200-2011. LCP: Learning to complete prototypes. GF, MF: Gaussian, mean-based prototype fusion. CV, EV: conference version <ref type="bibr" target="#b17">[18]</ref>, extended version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We conduct an ablation study on miniImagenet, tieredImagenet, and CUB-200-2011, respectively, to assess the effects of the two components, i.e., learning to complete prototypes and Gaussian-based prototype fusion strategy. Specifically, in <ref type="table" target="#tab_6">Table 4</ref>, (i) we remove all components, i.e., classifying each sample by the mean-based prototypes; (ii) we add the ProtoComNet proposed in the conference version <ref type="bibr" target="#b17">[18]</ref> (i.e., removing unseen parts/attributes) on (i) and classify each sample by the completed prototypes; (iii) we extend <ref type="bibr" target="#b17">[18]</ref> by introducing the PATNet on (ii) to explore the unseen parts/attributes for ProtoComNet and classify each sample by the completed prototypes; (iv) we fuse the mean-based and completed prototypes by MeanFusion; (v) we replace the MeanFusion of (iv) by our two-step estimation methodbased GaussFusion, i.e., the conference version <ref type="bibr" target="#b17">[18]</ref>; (vi) we replace the two-step estimation method by the improved EM-based estimation method on (v), where we don't use the EM-based estimation method because we have proved that the improved EM-based methods is more effective than EM-based methods in <ref type="table" target="#tab_2">Tables 1 and 2.</ref> 1) Learning to Complete Prototypes. From the results of (i) and (ii) in <ref type="table" target="#tab_6">Table 4</ref>, we observe that 1) the latter exceeds the former in 1-shot tasks, by around 4%, which means that learning to complete prototypes is effective; 2) the latter obtains poor performance in 5-shot tasks. As our analysis in Section 3.4, the phenomenon results from the bias of ProtoComNet, namely the primitive knowledge noises or base-novel class differences. Besides, comparing the results of (ii) and (iii), we find that the latter achieves superior performance with an improvement of 1% ? 2%. This implies that exploiting unseen parts/attributes is effective and beneficial for estimating representative prototypes. 2) Gaussian-based Prototype Fusion Strategy. According to the result in (iv) and (v) of <ref type="table" target="#tab_6">Table 4</ref>, we find that 1) the problem of ProtoComNet with poor performance on 5shot tasks is effectively solved after we use the MeanFusion strategy (i.e., the assumption-based distribution estimation method); 2) the performance of the ProtoComNet can be further improved when it is combined with the Gauss-Fusion with the two-step distribution estimation method, which is our conference strategy, by around 3%. The result suggests that the two-step method is more effective than the assumption-based method. The key reason is the twostep method effectively estimates prototype distribution by exploiting the unlabelled samples. Besides, from the results of (v) and (vi), we observe that the latter achieve 1% ? 2% higher classification accuracy. This is because the improved EM-based estimation method estimates more accurate prototype distribution for GaussFusion in an iterative manner.</p><p>Finally, to further verify that GaussFusion is able to alleviate the prototype completion error problem, we analyze the impacts of primitive knowledge with different noise levels ? on classification performance. We report the results of miniImagenet, tieredImagenet, and CUB-200-2011 datasets in <ref type="table" target="#tab_7">Table 5</ref>. Here, we introduce noises by randomly adding or removing class parts/attributes with probability ?. It can be observed that our method is more robust to primitive knowledge noises when GaussFusion is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization</head><p>In this subsection, we conduct visualization analysis on feature space to answer the following two questions:</p><p>How are the part/attribute distributed in the feature space? To understand how our method complete prototypes by using extracted part/attribute features, we randomly select two part/attribute from miniImagenet, i.e., "paw" and "tail". We visualize of all classes by t-SNE in the feature space, where the classes with the part/attribute "paw" or "tail" are marked in color "red", otherwise in color "blue". As shown in <ref type="figure" target="#fig_9">Fig. 9</ref>, we find these classes that have the same attributes are clustered together, which is beneficial to learn to complete prototypes.</p><p>How does our method work? To understand how does the proposed method work, we select a 5-way 1-shot and (a) 5-way 1-shot task (b) 5-way 5-shot task <ref type="figure" target="#fig_0">Fig. 10</ref>. Visualization of a 5-way 1/5-shot task sampled from the metatest set of miniImageNet. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot classification task from the meta-test set of mini-</head><p>ImageNet to visualize the prototypes and samples by t-SNE. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, after completing and fusing the class prototypes, the fused prototypes (marked in squares) become closer to real prototypes (marked in stars).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>For few-shot learning, a simple pre-training on base classes can obtain a good feature extractor, where the novel class samples can be well clustered together. The key challenge is how to obtain more representative prototypes because the novel class samples spread as groups with large variances. To solve the issue, we introduce primitive knowledge and extract representative feature for seen attribues as priors. Then we propose a part/attribute transfer network to infer the visual features for unseen parts/attributes as supplementary priors, a prototype completion network to complete prototypes via primitive knowledge and these priors, and a Gaussian-based prototype fusion strategy to alleviate the prototype completion error problem. Particularly, in the fusion strategy, we develop three methods to estimate fusion parameters, i.e., two-step method, EM (Expectation Maximization)-based method, and improve EM-based estimation method. Experiments show that our method obtains superior performance on three benchmark data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DERIVATION OF GAUSSFUSION</head><p>Proposition. Let f (x) and g(x) be a Multivariate Gaussian Distributions with diagonal covariance, i.e., f (x) = N (? k , diag(? 2 k )) and g(x) = N (? k , diag(? 2 k )) where x is a d-dimension random vector,? k and ? k denote d-dimension mean vector, and? 2 k and ? 2 k are d-dimension variance vector. Then, their product obeys a new Multivariate Gaussian Distributions N (? k , diag(? k 2 )) with ? k = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B WORKFLOW OF IMPROVED EM-BASED ESTIMATION METHOD</head><p>In the section, we provide implementation details of the improved EM-based estimation method for reproducibility. The overall workflow is summarized in Algorithm 1. Specifically, given the support set S, the query set Q, the meanbased prototypes p k , and the completed prototypesp k , we perform the following four steps to estimate the prototype fusion parameters for GaussFusion: (1) initilizing the mean ? k or? k by using the prototypes p k orp k (Line 1); <ref type="bibr" target="#b1">(2)</ref> performing the E-Step to compute the posterior probability that a given sample x ? Q belongs to a given class k by following Eq. (15) (Line 3). (3) performing the M-Step to obtain the optimal mean and diagonal covariance ? k and ? k or? k and? k by Eqs. <ref type="bibr" target="#b15">(16)</ref> and <ref type="formula" target="#formula_5">(17)</ref> (Line 4); (4) Repeatly performing the step <ref type="formula" target="#formula_5">(1)</ref> and <ref type="formula" target="#formula_6">(2)</ref> until the maximum number of iterations n iter is reached (Lines 2 -6).</p><p>Algorithm 1 Improved EM-based estimation method Input:</p><p>A support set S = {(x i , y i )} N ?K i=0 , a query set Q = {(x i , y i )} M i=0 from novel classes, the feature extractor f ? f (), and the prototypes p k orp k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The estimated mean and diagonal covariance of prototype distribution, i.e., ? k and ? k , or ? k and ? k . Inference: <ref type="bibr">1:</ref> Initilizing ? k or? k with p k orp k and regarding them as initial prototypes of cosine classifier; 2: for t = 0, 1, ..., n iter ? 1 do 3:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-</head><p>Step. Estimating the posterior probability that a given sample x ? S/Q belongs to a given class k with Eq. (15) or one-hot vector of its labels; 4:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-</head><p>Step. Estimating the mean and diagonal covariance ? k and ? k or? k and? k with Eqs. <ref type="bibr" target="#b15">(16)</ref> and <ref type="formula" target="#formula_5">(17)</ref> to maximize the posterior probability; <ref type="bibr">5:</ref> Replacing the above prototypes by mean ? k or ? k ; 6: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C DETAILED INFORMATION OF THREE DATASETS</head><p>We summarize the necessary information about the three data sets in <ref type="table" target="#tab_9">Table 6</ref>. Note that different from our conference version <ref type="bibr" target="#b17">[18]</ref>, the extended method can exploit unseen parts/attributes of novel classes for prototype completion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D ADDITIONAL VISUALIZATION</head><p>Is reasonable our motivation on other data sets? To further verify the reasonability of our motivation (i.e., estimating more accurate prototypes is more effective than fine-tuning feature extractor during meta-learning), we additionally visualize the distribution of base and novel class samples of the tieredImagenet and CUB-200-2011 data sets in the pre-trained feature space in <ref type="figure" target="#fig_0">Fig. 11 and 12</ref>. Note that we randomly select 15% of the classes from the base and novel classes on tieredImagenet for clarity. We have the similar observations as those in <ref type="figure" target="#fig_0">Fig. 1</ref> of Section 1, that is, the base class samples form compact clusters while the novel class samples spread as groups with large variances. This means that our motivation is reasonable and the problem of inaccurate estimation of prototypes widely exists in the pre-trained feature space for the real-world data sets.</p><p>(a) Base Classes (? 2 = 0.36) (b) Novel Classes (? 2 = 0.41) <ref type="figure" target="#fig_0">Fig. 11</ref>. The distribution of base and novel class samples in the pretrained feature space on tieredImagenet data set. "? 2 " denotes the averaged variance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Base Classes (? 2 = 0.086) Complete Incomplete (b) Novel Classes (? 2 = 0.099) The distribution of base and novel class samples of miniImagenet in the pre-trained feature space. "? 2 " denotes the averaged variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i=0</head><label></label><figDesc>with a few of labeled samples (called support set) and a test set Q = {(x i , y i )}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the proposed parts/attributes transfer networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the encoder-aggregator-decoder networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Test accuracy of p k andp k on 5-way K-shot tasks of miniImagenet (a) and illustration of prototype fusion strategy (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Top-5 nearest and farthest samples from centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Performance analysis of ProtoComNet on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Performance of GaussFusion with different iterations on 5-way 1/5-shot tasks of miniImagenet, tieredImagenet and CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>(a) miniImagenet (part: "paw") (b) miniImagenet (part: "tail") Visualization of part/attribute feature on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>? 2 k ? k +? 2 k ? k ? 2 k +? 2 k and ? k 2 = ? 2 k ? 2 k ? 2 k +? 2 k,)) with mean ? k = ? 2 k ? k +? 2 k ? k ? 2 k +? 2 k</head><label>22</label><figDesc>where denotes the element-wise product. Derivation. Considering that the covariances of f (x) and g(x) are simplified as diagonal covariances. This means that the variables of the random vector x are uncorrelated. In this case, f (x) and g(x) can be simplified as the expression below: . Thus, h(x) is also a multivariate Gaussian distribution, i.e., N (? k , diag(? k 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>(a) Base Classes (? 2 = 0.054) (b) Novel Classes (? 2 = 0.063) The distribution of base and novel class samples in the pretrained feature space on CUB-200-2011 data set. "? 2 " denotes the averaged variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>CE Loss 1. Pre-Training 3. Meta-Training 4. Meta-Test Prototypes Fusion Strategy</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Part/Attribute Features</cell><cell>Base</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Class Prototypes</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>seen seen</cell><cell>unseen unseen</cell><cell>Step 2</cell><cell>real k p</cell><cell cols="2">( ) f x f q?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KL Loss</cell><cell>Step 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>seen seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word Net</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>seen seen</cell><cell>unseen unseen</cell><cell>Rb</cell><cell></cell><cell></cell></row><row><cell>Step 1 Glove</cell><cell>unseen unseen round spot</cell><cell>R</cell><cell>base base</cell><cell>novel novel</cell><cell>Step 3 PATNet</cell><cell></cell><cell>novel ase base</cell><cell>k Incomplete Prototypes p</cell><cell>( ) f x f q</cell><cell>Training Test Extracting Priors</cell></row><row><cell cols="11">Fig. 2. The prototype completion based meta-learning framework, including four phases: (1) Pre-Training phase that learns a feature extractor</cell></row><row><cell cols="11">by using all base classes (Section 3.2.1); (2) Learning to Complete Prototypes phase that constructs primitive knowledge, extracts base class</cell></row><row><cell cols="11">prototypes and part/attribute distribution for seen attributes, tansfers part/attributes distribution from seen parts/attributes to unseen parts/attributes,</cell></row><row><cell cols="11">and then trains the ProtoComNet to complete prototypes (Section 3.2.2); (3) Meta-Training phase that jointly fine-tunes the feature extractor and</cell></row><row><cell cols="11">ProtoComNet in an episodic training manner (Section 3.2.3); and (4) Meta-Test phase that performs novel class prediction (Section 3.2.4).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Experiment results on the miniImagenet and tieredImagenet data sets. The best results are highlighted in bold. In. and Tran. indicate inductive and transductive FSL setting, respectively. ' ' denotes the absent results in original paper.</figDesc><table><row><cell cols="2">Setting Method</cell><cell>Type</cell><cell>Backbone</cell><cell cols="2">miniImagenet 5-way 1-shot 5-way 5-shot</cell><cell cols="2">tieredImagenet 5-way 1-shot 5-way 5-shot</cell></row><row><cell></cell><cell>RestoreNet [15]</cell><cell>Metric</cell><cell>ResNet18</cell><cell>59.28 ? 0.20%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>ConstellationNet [70]</cell><cell>Metric</cell><cell>ResNet12</cell><cell>64.89 ? 0.23%</cell><cell>79.95 ? 0.17%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>RAP-ProtoNet [54]</cell><cell>Metric</cell><cell>ResNet10</cell><cell>53.64 ? 0.60%</cell><cell>74.54 ? 0.45%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>MAML [8]</cell><cell>Optimization</cell><cell>ResNet12</cell><cell>58.37 ? 0.49%</cell><cell cols="2">69.76 ? 0.46% 58.58 ? 0.49%</cell><cell>71.24 ? 0.43%</cell></row><row><cell></cell><cell>MetaOptNet [26]</cell><cell>Optimization</cell><cell>ResNet12</cell><cell>62.64 ? 0.61%</cell><cell cols="2">78.63 ? 0.46% 65.99 ? 0.72%</cell><cell>81.56 ? 0.53%</cell></row><row><cell></cell><cell>ALFA [71]</cell><cell>Optimization</cell><cell>ResNet12</cell><cell>59.74 ? 0.49%</cell><cell cols="2">77.96 ? 0.41% 64.62 ? 0.49%</cell><cell>82.48 ? 0.38%</cell></row><row><cell></cell><cell>AM3-TRAML [33]</cell><cell>Semantics</cell><cell>ResNet12</cell><cell>67.10 ? 0.52 %</cell><cell>79.54 ? 0.60%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>MultiSem [36]</cell><cell>Semantics</cell><cell>Dense-121</cell><cell>67.3%</cell><cell>82.1%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>FSLKT [35]</cell><cell>Semantics</cell><cell>ConvNet128</cell><cell>64.42 ? 0.72%</cell><cell>74.16 ? 0.56%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>CPDE [65]</cell><cell>Attribute</cell><cell>ResNet12</cell><cell>63.21 ? 0.78%</cell><cell>79.68 ? 0.82%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell>In.</cell><cell>CFA [64]</cell><cell>Attribute</cell><cell>ResNet18</cell><cell>58.50 ? 0.80%</cell><cell>76.60 ? 0.60%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>MetaBaseline [14]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>63.17 ? 0.23%</cell><cell>79.26 ? 0.17%</cell><cell>68.62 ? 0.27%</cell><cell>83.29 ? 0.18%</cell></row><row><cell></cell><cell>Neg-Cosine [38]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>63.85 ? 0.81%</cell><cell>81.57 ? 0.56%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>CentAlign [39]</cell><cell>Pre-training</cell><cell>ResNet18</cell><cell>59.88 ? 0.67%</cell><cell cols="2">80.35 ? 0.73% 69.29 ? 0.56%</cell><cell>85.97 ? 0.49%</cell></row><row><cell></cell><cell>DC [40]</cell><cell>Pre-training</cell><cell>WRN-28-10</cell><cell>66.91 ? 0.17%</cell><cell>80.74 ? 0.48%</cell><cell>75.92 ? 0.60%</cell><cell>87.84 ? 0.65%</cell></row><row><cell></cell><cell>Our Method (MeanFusion)</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>69.68 ? 0.76%</cell><cell>81.65 ? 0.54%</cell><cell>74.19 ? 0.90%</cell><cell>86.09 ? 0.60%</cell></row><row><cell></cell><cell>SRestoreNet [15]</cell><cell>Metric</cell><cell>ResNet18</cell><cell>61.14 ? 0.22%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>DPGN [48]</cell><cell>Graph</cell><cell>ResNet12</cell><cell>67.77 ? 0.32%</cell><cell>84.60 ? 0.43%</cell><cell>72.45 ? 0.51%</cell><cell>87.24 ? 0.39%</cell></row><row><cell></cell><cell>EPNet [11]</cell><cell>Graph</cell><cell>ResNet12</cell><cell>66.50 ? 0.89%</cell><cell cols="2">81.06 ? 0.60% 76.53 ? 0.87%</cell><cell>87.32 ? 0.64%</cell></row><row><cell></cell><cell>MCGN [47]</cell><cell>Graph</cell><cell>ConvNet256</cell><cell>67.32 ? 0.43%</cell><cell cols="2">83.03 ? 0.54% 71.21 ? 0.85%</cell><cell>85.98 ? 0.98%</cell></row><row><cell></cell><cell>TIM-GD [50]</cell><cell>Pre-training</cell><cell>ResNet18</cell><cell>73.9 ? ?%</cell><cell>85.0 ? ?%</cell><cell>79.9 ? ?%</cell><cell>88.5 ? ?%</cell></row><row><cell></cell><cell>TFT [49]</cell><cell>Pre-training</cell><cell>WRN-28-10</cell><cell>65.73 ? 0.68%</cell><cell cols="2">78.40 ? 0.52% 73.34 ? 0.71%</cell><cell>85.50 ? 0.50%</cell></row><row><cell>Trans.</cell><cell>SIB [72] LaplacianShot [51]</cell><cell>Pre-training Pre-training</cell><cell>WRN-28-10 ResNet18</cell><cell>70.0 ? 0.6% 72.11 ? 0.19%</cell><cell cols="2">79.2 ? 0.4% 82.31 ? 0.14% 78.98 ? 0.21% ? ? ?%</cell><cell>? ? ?% 86.39 ? 0.16%</cell></row><row><cell></cell><cell>RAP-LaplacianShot [54]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>74.29 ? 0.20%</cell><cell>84.51 ? 0.13%</cell><cell>? ? ?%</cell><cell>? ? ?%</cell></row><row><cell></cell><cell>ICI [52]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>65.77 ? ?%</cell><cell>78.94 ? ?%</cell><cell>80.56 ? ?%</cell><cell>87.93 ? ?%</cell></row><row><cell></cell><cell>BD-CSPN [55]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>65.94%</cell><cell>79.23%</cell><cell>76.17%</cell><cell>85.70%</cell></row><row><cell></cell><cell>Conference Version [18]</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>73.13 ? 0.85%</cell><cell>82.06 ? 0.54%</cell><cell>81.04 ? 0.89%</cell><cell>87.42 ? 0.57%</cell></row><row><cell></cell><cell>Our Method (EM)</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>75.35? 0.87%</cell><cell>83.46 ? 0.58%</cell><cell>81.40 ? 0.96%</cell><cell>88.15 ? 0.59%</cell></row><row><cell></cell><cell>Our Method (Improved EM)</cell><cell>Pre-training</cell><cell>ResNet12</cell><cell>79.01 ? 0.89%</cell><cell>84.18 ? 0.56%</cell><cell>83.06 ? 1.00%</cell><cell>88.60 ? 0.57%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Experiment results on the CUB-200-2011 data set. The best results are highlighted in bold. In. and Tran. indicate inductive and transductive FSL setting, respectively.</figDesc><table><row><cell>Setting Method</cell><cell>CUB-200-2011 5-way 1-shot 5-way 5-shot</cell></row><row><cell>In.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>The cosine similarity between the estimated and real prototypes on 1000 episodes (5-way 1-shot) of miniImagenet, tieredImagenet, and CUB-200-2011. d(x, y) denotes the cosine simiarity of vectors x and y.</figDesc><table><row><cell cols="3">Methods</cell><cell></cell><cell cols="3">d(p k , p real k</cell><cell>)</cell><cell>d(p k , p real k</cell><cell>) d(p k , p real k</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">miniImagenet</cell></row><row><cell cols="4">SRestoreNet</cell><cell></cell><cell>0.55</cell><cell></cell><cell>0.78</cell><cell>0.79</cell></row><row><cell cols="3">BD-CSPN</cell><cell></cell><cell></cell><cell>0.55</cell><cell></cell><cell>-</cell><cell>0.67</cell></row><row><cell cols="5">Conference Version [18]</cell><cell>0.55</cell><cell></cell><cell>0.71</cell><cell>0.90</cell></row><row><cell cols="4">Our Method</cell><cell></cell><cell>0.55</cell><cell></cell><cell>0.77</cell><cell>0.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">tieredImagenet</cell></row><row><cell cols="4">SRestoreNet</cell><cell></cell><cell>0.72</cell><cell></cell><cell>0.86</cell><cell>0.91</cell></row><row><cell cols="3">BD-CSPN</cell><cell></cell><cell></cell><cell>0.72</cell><cell></cell><cell>-</cell><cell>0.83</cell></row><row><cell cols="5">Conference Version [18]</cell><cell>0.72</cell><cell></cell><cell>0.84</cell><cell>0.95</cell></row><row><cell cols="4">Our Method</cell><cell></cell><cell>0.72</cell><cell></cell><cell>0.85</cell><cell>0.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">CUB-200-2011</cell></row><row><cell cols="4">SRestoreNet</cell><cell></cell><cell>0.68</cell><cell></cell><cell>0.83</cell><cell>0.89</cell></row><row><cell cols="3">BD-CSPN</cell><cell></cell><cell></cell><cell>0.68</cell><cell></cell><cell>-</cell><cell>0.79</cell></row><row><cell cols="5">Conference Version [18]</cell><cell>0.68</cell><cell></cell><cell>0.77</cell><cell>0.95</cell></row><row><cell cols="4">Our Method</cell><cell></cell><cell>0.68</cell><cell></cell><cell>0.80</cell><cell>0.98</cell></row><row><cell cols="2">Cosine Similarity</cell><cell cols="2">0.4 0.5 0.6 0.7 0.8 0.9</cell><cell cols="2">Noise Samples Our Method RestoreNet</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell></cell><cell>300</cell><cell>400</cell><cell>500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Sample Points</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) miniImagenet</cell></row><row><cell>Cosine Similarity</cell><cell cols="2">0.70 0.75 0.80 0.85 0.90 0.95</cell><cell>0</cell><cell>300 Noise Samples Our Method RestoreNet</cell><cell cols="3">600 Sample Points</cell><cell>900</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) tieredImagenet</cell></row><row><cell>Cosine Similarity</cell><cell cols="2">0.70 0.75 0.80 0.85 0.90</cell><cell>0</cell><cell cols="2">10 Noise Samples Our Method RestoreNet</cell><cell cols="2">20</cell><cell>30</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Sample Points</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>The performance analysis of primitive knowledge with different noise level ? on 5-way 1-shot tasks of miniImagenet, tieredImagenet, and CUB-200-2011. Fusion 65.99 % 52.87 % 46.68 % 42.53 % w/ MeanFusion 69.64 % 64.93 % 60.28 % 57.20 % w/ GaussFusion 79.01 % 77.89 % 77.57 % 77.24 % tieredImagenet w/o Fusion 72.35 % 40.78 % 32.37 % 29.49 % w/ MeanFusion 74.19 % 69.06 % 62.93 % 57.77 % w/ GaussFusion 83.06 % 81.60 % 81.53 % 81.51 % CUB-200-2011 w/o Fusion 85.03 % 82.24 % 78.18 % 74.17 % w/ MeanFusion 85.34 % 85.02 % 84.66 % 84.18 % w/ GaussFusion 93.78 % 93.65 % 93.56 % 93.28 %</figDesc><table><row><cell>Methods</cell><cell>? = 0.0</cell><cell>? = 0.1</cell><cell>? = 0.2</cell><cell>? = 0.3</cell></row><row><cell></cell><cell cols="2">miniImagenet</cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Baoquan Zhang is currently pursuing the Ph.D. degree with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. His current research interests include meta learning, few-shot learning, and machine learning. Li is currently an Associate Professor with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. His research interests include data mining, machine learning, graph mining, and social network analysis, especially tensor-based learning, and mining algorithms. Yunming Ye is currently a Professor with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. His research interests include data mining, text mining, and ensemble learning algorithms.</figDesc><table><row><cell>Xutao</cell></row></table><note>Shanshan Feng is currently an Associate Professor with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. His research interests include sequential data mining and social network analysis.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>The basic statistics of the MiniImagenet, TieredImagenet and CUB-200-2011 dataset.</figDesc><table><row><cell>Datasets</cell><cell cols="6">Number of Class train val test seen unseen Number of Part/attribute all</cell></row><row><cell>MiniImagenet</cell><cell>64</cell><cell>16</cell><cell>20</cell><cell>168</cell><cell>122</cell><cell>290</cell></row><row><cell>TieredImagenet</cell><cell>351</cell><cell>97</cell><cell>160</cell><cell>411</cell><cell>165</cell><cell>576</cell></row><row><cell>CUB-200-2011</cell><cell>100</cell><cell>50</cell><cell>50</cell><cell>171</cell><cell>141</cell><cell>312</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>our method converges within 6 iterations, and obtains the best performance on all datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A meta-learning perspective on cold-start recommendations for items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bratman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low data drug discovery with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">TADAM: task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<editor>NeurIPS, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9714" to="9723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchically structured meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7045" to="7054" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embedding propagation: Smoother manifold for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drouin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">12371</biblScope>
			<biblScope unit="page" from="121" to="138" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4848" to="4858" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learn to abstract via concept graph for weakly-supervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page">107946</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new metabaseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One-shot image classification by learning to restore prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="6558" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-label object attribute classification using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<idno>abs/1811.04309</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transductive zero-shot learning with visual structure constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="9972" to="9982" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prototype completion with primitive knowledge for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">490</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">210</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1208" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lgm-net: Learning to generate matching networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3825" to="3834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational metric scaling for metric-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3478" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-transfer learning through hard tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metalearning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Meta-learning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-level semantic feature augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4594" to="4605" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting few-shot learning with adaptive margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge-guided multi-label few-shot learning for general image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot image recognition with knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="441" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Baby steps towards few-shot learning with multiple semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A two-stage approach to few-shot learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3336" to="3350" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12349</biblScope>
			<biblScope unit="page" from="438" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Associative alignment for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Afrasiyabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gagn?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="18" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Free lunch for few-shot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring complementary strengths of invariant and equivariant representations for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3602" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4005" to="4016" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mutual crf-gnn for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2329" to="2339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DPGN: distribution propagation graph network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transductive information maximization for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">670</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Instance credibility inference for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="833" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How to trust unlabeled data instance credibility inference for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reinforced attention for few-shot learning and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="913" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Zero and few shot learning with semantic feature synthesis and competitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2129" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">DAAL: deep activationbased attribute learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Zero-shot learning -A comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Show, observe and tell: Attribute-driven attention model for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="606" to="612" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6372" to="6381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Weakly-supervised compositional feature aggregation for few-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Compositional few-shot recognition with primitive discovery and enhancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3630" to="3638" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Meta-learning for semisupervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Attentional constellation nets for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Meta-learning with adaptive hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
