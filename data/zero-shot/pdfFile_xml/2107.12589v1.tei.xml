<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Event, China Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent PCG</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent PCG</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">Guangdong Key Laboratory of Information Security Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>danxu@cse.ust.hk</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<region>HK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<email>yingsshan@tencent.com</email>
							<affiliation key="aff2">
								<orgName type="department">Applied Research Center (ARC)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent PCG</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<email>wszheng@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Machine Intelligence and Advanced Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">Virtual Event, China Localization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM International Conference on Multimedia (MM &apos;21)						</meeting>
						<imprint>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475298</idno>
					<note>* Equal Contribution. ? Corresponding author. ACM Reference Format: Event, China. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Weakly supervised learning</term>
					<term>Temporal action localization</term>
					<term>Feature re-calibration</term>
					<term>Mutual learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised temporal action localization (WS-TAL) is a challenging task that aims to localize action instances in the given video with video-level categorical supervision. Previous works use the appearance and motion features extracted from pre-trained feature encoder directly, e.g., feature concatenation or score-level fusion. In this work, we argue that the features extracted from the pre-trained extractors, e.g., I3D, which are trained for trimmed video action classification, but not specific for WS-TAL task, leading to inevitable redundancy and sub-optimization . Therefore, the feature re-calibration is needed for reducing the task-irrelevant information redundancy. Here, we propose a cross-modal consensus network (CO 2 -Net) to tackle this problem. In CO 2 -Net, we mainly introduce two identical proposed cross-modal consensus modules (CCM) that design a cross-modal attention mechanism to filter out the task-irrelevant information redundancy using the global information from the main modality and the cross-modal local information from the auxiliary modality. Moreover, we further explore inter-modality consistency, where we treat the attention weights derived from each CCM as the pseudo targets of the attention weights derived from another CCM to maintain the consistency between the predictions derived from two CCMs, forming a mutual learning manner. Finally, we conduct extensive experiments on two commonly used temporal action localization datasets, THUMOS14 and ActivityNet1.2, to verify our method, which we achieve the stateof-the-art results. The experimental results show that our proposed cross-modal consensus module can produce more representative features for temporal action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consensus</head><p>Figure 1: Our proposed cross-modal consensus module first encodes modal-specific global context from the main modality and the cross-modal local-focused information from the current snippet of the auxiliary modality, which then cooperate to achieve a consensus in modeling channel-wise feature responses and enhance the features via information redundancy filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Activity recognition and understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Temporal action localization is a task to localize the start and end timestamps of action instances and recognize their categories. In recent years, many works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref> put effort into the fully arXiv:2107.12589v1 [cs.CV] 27 Jul 2021 supervised manner and gain great achievements. However, these fully supervised methods require extensive manual frame/snippet level annotations. To address this problem, many weakly supervised temporal action localization (WS-TAL) methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref> are proposed to explore an efficient way to detect the action instances in the given videos with only video-level supervision which is more easily obtained by the annotator.</p><p>As other weakly supervised video understanding tasks likes video anomaly detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref> and video highlight detection <ref type="bibr" target="#b9">[10]</ref>, most existing WS-TAL methods develop their framework based on the multiple-instance learning (MIL) manner <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50]</ref>. These methods firstly predict the categorical probabilities for each snippet and then aggregate them as the video-level prediction. Finally, they perform the optimization procedure using the given video-level labels. Among them, some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref> introduce an attention module to improve the ability to recognize the foreground by suppressing the background parts. For action completeness modeling, Islam et al. <ref type="bibr" target="#b11">[12]</ref> utilize an attention module to drop the most discriminative parts of the video but focus on the less discriminative ones. With regards to feature learning, most of WS-TAL methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> mainly apply a contrastive learning loss on their intermediate features. <ref type="bibr">Lee et al. [20]</ref> proposed to distinguish foreground from background via the inconsistency of their feature magnitudes.</p><p>The aforementioned methods use the original extracted features that contain the task-irrelevant information redundancy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> to produce predictions directly for each snippet. However, as the features extracted from trained for another task, i.e., trimmed video action classification, which introduces redundancy inevitably, their performances are restricted to the quality of extracted features and only acquire sub-optimization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. Intuitively, performing feature re-calibration for task-specific features is a way to tackle this problem. Instead of finetuning the feature extractor <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b47">48]</ref> with high time and computation cost, we explore to re-calibrate the features in a more efficient manner. In this work, our intuition is simple: the RGB and FLOW features contain modal-specific information (i.e., appearance and motion information) from different perspectives of the given data. Therefore, we can filter out the redundancy contained in a certain modality with the help of global context information from itself and the local context information from different perspectives of different modalities ( <ref type="figure">Figure 1</ref>).</p><p>As discussed above, the inconsistency between pre-trained task with the target one leads to inevitable task-irrelevant information in the extracted features denoted as redundancy, which restricts the optimization, especially under weak supervision. Previous works pay less attention to this problem but use the features directly. Here, we aim to re-calibrate the features in the very beginning by leveraging information from two different modalities (i.e., RGB and FLOW features). In this work, we develop a CrOss-modal cOnsensus NETwork (CO 2 -Net) to re-calibrate the representations of each modality for each snippet in the video. CO 2 -Net contains two identical cross-modal consensus modules (CCM). Specifically, two types of modal features are fed into both CCMs, one of them acts as the main modality and the other one serves as the auxiliary modality. In CCM, we obtain the modality-specific global context information from the main modality and the cross-modal localfocused descriptor from the auxiliary modality. Then we aggregate them to produce a channel-wise descriptor that can be used to filter out the task-irrelevant information redundancy. Intuitively, with the global information of the main modality, CCM can use the information from different perspectives of the auxiliary modality to determine whether a certain part of the main modality is task-irrelevant information redundancy. Thus we obtain the RGBenhanced features and FLOW-enhanced features from two CCMs after filtering the redundancy in original RGB features and FLOW features, respectively. Then we utilize these two enhanced features to estimate the modality-specific attention weights, respectively, and apply mutual learning loss on these two estimated attention weights for mutual promotion. In addition, we also apply the top-k multiple-instance learning loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref> that is widely used to learn the temporal class activation map (T-CAM) for each video.</p><p>Finally, we conduct extensive experiments on two public temporal action localization benchmarks, i.e., THUMOS14 dataset <ref type="bibr" target="#b14">[15]</ref> and Activity1.2 dataset <ref type="bibr" target="#b6">[7]</ref>. In our experiments, we investigate and discuss the effect of our proposed cross-modal consensus module with other feature fusion manners (e.g., additive and concatenate function). The experimental results show that our CO 2 -Net achieves the state-of-the-art performance on two public datasets, which verify its efficacy for temporal action localization. To summarize, our contribution is three-fold:</p><p>? As far as we know, it is the first work to investigate multimodal feature re-calibration and modal-wise consistency via mutual learning for temporal action localization. ? We propose a framework, i.e., CO 2 -Net, for temporal action localization to explore a novel cross-modal attention mechanism to re-calibrate the representation for each modality. ? We conduct extensive experiments on two public benchmarks, where our proposed method achieves the state-ofthe-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Weakly Supervised Temporal Action Localization. Weakly supervised temporal action localization provides an efficient way to detect the action instances without overload annotations. Many works mainly tackle this problem using the multiple-instance learning (MIL) framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. Several works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> mainly aggregate snippet-level class scores to produce video-level predictions and learn from video-level action labels. In this formulation, background frames are forced to be mis-classified as action classes to predict video-level labels accurately. To address such a problem, many works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> apply an attention module in their framework to suppress the activation of background frames to improve localization performance. Lee et al. <ref type="bibr" target="#b18">[19]</ref> introduces an auxiliary class for background and proposes a two-branch weight-sharing architecture with an asymmetrical training strategy. Besides, MIL-based methods only focus on optimizing the most discriminative snippets in the video <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. For action completeness modeling, some works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> adopt the complementary learning scheme that drops the most discriminative parts of the video but focuses on the complementary parts. Also, several works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52]</ref> attempt to optimize their framework under a self-training regime.  <ref type="figure" target="#fig_2">Figure 2</ref>: An overview of the proposed cross-modal consensus network (CO 2 -Net) with two identical CCMs. CCM would filter out the task-irrelevant redundancy of the main modality and generate the enhanced features for the main modality by the consensus of both global-context information of itself and the local information from the auxiliary modality. The enhanced features would be fed into the attention unit to estimate the modality-specific attention weights. On the one hand, we aggregate two attention weights to generate final attention weights A, while these two modality-specific attention weights are optimized by mutual learning loss L for mutual promotion. On the other hand, we first fuse the two enhanced features as fused features and then feed them into a classifier to predict a temporal class activation map (T-CAM). Finally, we apply the top-k multipleinstance learning loss (i.e., L and L ) and co-activity similarity loss (i.e., L ) to optimize the whole framework.</p><p>Zhai et al. <ref type="bibr" target="#b51">[52]</ref> treats the outputs in the last epoch as pseudo labels and refines the network using these pseudo labels. Different from aforementioned methods, this work is the first one that considers filtering out the task-irrelevant information redundancy from each modality with the help of the consensus of different modalities. Our method aims to re-calibrate the representation, so that each modality has less information redundancy, which can produce more accurate predictions. Modalities Fusion. Recently, deep neural networks have been exploited in multi-modal clustering issue due to powerful feature transformation ability. Many computer vision models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref> adopt multiple modalities in their framework to obtain performance gains. Different modalities can help to complement each other in a proper way. In the early stage, Ngiam et al. <ref type="bibr" target="#b30">[31]</ref> take deep auto-encoder network architecture to learn the common representations of multi-modal data and achieves significant performance in speech and vision tasks. Several works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> combine the visual modality and audio modality to tackle a specific task. In general, the video and audio contain different modal information but can enhance each other because visual and audio events tend to occur together. Hong et al. <ref type="bibr" target="#b9">[10]</ref> utilize audio modality in a multiple-head structure to assist vision modality in localizing the video highlights.</p><p>In this work, instead of feature extractor finetuning, we attempt to filter out the task-irrelevant information redundancy from the specific modality via a novel re-calibration way, which we make a consensus between the global context from itself and the local context information from another modality, while the aforementioned works treat the multiple modalities information equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Video is a typical type of multimedia that can be translated into multiple modalities that represent the information from different perspectives. In this work, we propose a cross-modal consensus network (CO 2 -Net) to re-calibrate the representations of each modality using the information from different perspectives of different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We first formulate the WS-TAL problem as follows: suppose V denotes a batch of data with |V| videos and corresponding video-</p><formula xml:id="formula_0">level categorical labels are Y, where Y = { (1) , ..., ( |V |) } and ( ) = { ( ) 1 , ..., ( ) } = {0, 1} for -th video,</formula><p>where means the number of category. The goal of WS-TAL is to learn a function that simultaneously detects and classifies all action instances temporally with precise timestamps as ( , , , ) for each video, where , , , denote the start time, the end time, the predicted category and the confidence score for corresponding action proposal, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pipeline</head><p>Feature Extraction. Following recent WS-TAL methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, we construct CO 2 -Net upon snippet-level feature sequences extracted from non-overlapping video volumes, where each volume contains 16 frames. The features for appearance modality (RGB)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Element-wise multiplication</head><p>Sigmoid function</p><formula xml:id="formula_1">AvgPool Global-Context-Aware Unit</formula><p>Cross-Modal-Aware Unit <ref type="figure">Figure 3</ref>: An overview of the proposed cross-modal consensus module. The module contains a Global-Context-Aware unit and Cross-Modal-Aware unit to distinguish the information redundancy and re-calibrate the features. In this module, the main modality cooperates with the auxiliary modality to generate channel-wise descriptors to govern the excitation of each channel to filter out the information redundancy. Thus the main modality features are then enhanced by channel-wise attention mechanism as . The workflow is same when the roles of these modalities are exchanged. and motion modality (optical flows) are both extracted from pretrained extractors, i.e., I3D <ref type="bibr" target="#b2">[3]</ref>. The features for appearance and motion modality are 1024-dimension for each snippet. For -th video with snippets, we use matrix tensors ? R ? and ? R ? to represent the RGB and FLOW features of the whole video, respectively, where D means the dimension of the feature vector. Structure Overview. <ref type="figure" target="#fig_2">Figure 2</ref> shows the whole pipeline of our proposed CO 2 -Net. Both RGB and FLOW features are fed in two identical cross modal consensus modules. In each CCM, we select one of the two modalities as the main modality that will be enhanced by removing the task-irrelevant information redundancy with the help of the global context of itself and cross-modal localfocused information from another (auxiliary) modality. Thus we can obtain the more task-specific representation for each modality. Then, the enhanced representation is utilized to produce attention weights that indicate the probabilities of each snippet being foreground through an attention unit that consists of two convolution layers. We aggregate two attention weights generated by enhanced features from two CCMs respectively to produce final attention weights that can be used in the testing stage. And we also fuse the two enhanced features and feed them into a classifier to predict the categorical probabilities for each snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-modal Consensus Module</head><p>In this work, we employ a cross-modal consensus module to filter out the task-irrelevant information redundancy for each modality before the process of downstream learning task. The proposed crossmodal consensus module is constructed by a global-context-aware unit and a cross-modal-aware unit to distinguish the information redundancy and filter out them via a channel-wise suppression on the features. As shown in <ref type="figure">Figure 3</ref>, we treat the appearance modality (RGB features) as the main modality and the motion modality (FLOW features) as the auxiliary to feed in our proposed crossmodal consensus module, while the same workflow is performed when the roles of the two modality are exchanged. For the convenience of expression, we take RGB features as the main modality features as an example in the rest of the article.</p><p>As the features are extracted from a encoder that pretrained on some large datasets not related to WS-TAL task, thus the features may contain some task-irrelevant misleading redundancy that restricts the localization performance. Given the main modality and the auxiliary modality, instead of directly concatenating them, we aim to design a mechanism to filter out the task-irrelevant information redundancy in the main modality. Motivated by the self-attention mechanism <ref type="bibr" target="#b41">[42]</ref> and squeeze-and-excitation block <ref type="bibr" target="#b10">[11]</ref>, we develop a similar manner, named cross-modal attention mechanism, to distinguish the information redundancy and filter out them.</p><p>In the global-context-aware unit, we first squeeze modalityspecific global context information into a video-level feature ? R , which is aggregated from the main modality , using an average pooling operator (?) on temporal dimension. Then, we adopt a convolution layer to fully capture channel-wise dependencies and produce modality-specific global-aware descriptor . The process is formulated below:</p><formula xml:id="formula_2">= ( ), = ( ).<label>(1)</label></formula><p>As multiple modalities provide information from different perspectives, we can leverage the information from the auxiliary modality to detect the task-irrelevant information redundancy in the main modality. Thus, in the cross-modal-aware unit, we aim to capture the cross-modal local-specific information from the auxiliary modality features . Here, we introduce a convolution layer that embed the features of the auxiliary modality to produce a crossmodal local-focused descriptor as follows:</p><formula xml:id="formula_3">= ( ).<label>(2)</label></formula><p>Here, we obtain channel-wise descriptor for feature recalibration by multiplying modality-specific global-aware descriptor with cross-modal local-focused descriptor . Finally, the task-irrelevant information redundancy is filtered out via a crossmodal attention mechanism as follows:</p><formula xml:id="formula_4">= ? , = ( ) ? ,<label>(3)</label></formula><p>where (?) is a Sigmoid function, while the "?" means element-wise multiplication operator. Remarkably, and can be treated as "Query" and "Key" in the self-attention module <ref type="bibr" target="#b41">[42]</ref>. Instead of using a softmax operator, we apply a Sigmoid function to produce channel-wise re-calibration weights to enhance the original main modality features .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dual Modal-specific Attention Units</head><p>After obtaining the enhanced features, we attempt to produce modality-specific temporal attention weights that indicate the snippet-level probabilities of being foreground. Here, following previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, we feed the enhanced features into the attention unit for modality-specific attention weights:</p><formula xml:id="formula_5">A = ( ),<label>(4)</label></formula><p>where (?) is the attention unit for RGB with three convolution layers, which is same with the attention unit for FLOW . As we have two CCMs in the proposed CO 2 -Net, we obtain the RBG-enhanced features and modality-specific attention weights A from one CCM that treats the appearance modality as the main modality and motion modality as the auxiliary modality, while we also gain the FLOW-enhanced features and modality-specific attention weights A from another CCM, in which the roles of two modalities are opposite to the former CCM.</p><p>After obtaining the enhanced features (i.e., and ) and modality-specific attention weights (i.e., A and A ). We first fuse two attention weights:</p><formula xml:id="formula_6">A = A + A 2 .<label>(5)</label></formula><p>We think that the two modality-specific attention weights produced by two enhanced features respectively have different emphasis on the video, while the fused attention weights A can better represent the probability of snippet being foreground because it made a trade-off between the two modality-specific attention weights. Finally, We concatenate two types of enhanced features, i.e., and , to form and feed it into a classifier that contains three convolution layers to produce the temporal class activation map (T-CAM) S ? R ?( +1) for the given video, where the ( + 1)-th class is the background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimizing Process</head><p>Constraints on Attention Weights. Here, we have obtained two modality-specific attention weights (i.e., A and A ) and a fused attention weights A. Then we first apply mutual learning scheme on two modality-specific attention weights:</p><formula xml:id="formula_7">L = (A , (A )) + (1 ? ) (A , (A )),<label>(6)</label></formula><p>where (?) represents a function that truncates the gradient of input, while (?) means a similarity metric function and is a hyperparameter. In Eq. 6, we treat A and A as pseudo-labels of each other (as shown in <ref type="figure" target="#fig_1">Figure 4</ref>), so that they can learn from each other and align the attention weights. Here, we adopt mean square error (MSE) as function (?) in Eq. 6. Besides MSE, we also discuss others similarity metric functions (i.e., Jensen-Shannon (JS) divergence, Kullback-Leibler (KL) divergence and mean absolute error (MAE) ) that is applied in Eq. 6 in Section 4.4. In addition, we can find that the distribution of attention weights should be opposite to the probability distribution of the background class in S:</p><formula xml:id="formula_8">L = 1 3 (|A + +1 ? 1| + |A + +1 ? 1| + |A + +1 ? 1|),<label>(7)</label></formula><p>where | ? | is a absolute value function, and +1 is the last column in the T-CAM S that represents the probabilities of each snippet being background. And we also utilize a normalization loss L to make the attention weights more polarized:</p><formula xml:id="formula_9">L = 1 3 (||A || 1 + ||A || 1 + ||A|| 1 ),<label>(8)</label></formula><p>where || ? || 1 is a L1-norm function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constraints on T-CAMs and Features.</head><p>In order to better recognize the background activity, we apply the attention weights A to suppress the background snippets in T-CAM S and obtain suppressed T-CAM S:</p><formula xml:id="formula_10">S = A ? S.<label>(9)</label></formula><p>In this work, we apply the widely used top-k multiple-instance learning loss <ref type="bibr" target="#b34">[35]</ref> on T-CAM S and S, denoted as L = L + L</p><p>. Also, we apply the co-activity similarity loss L 1 <ref type="bibr" target="#b34">[35]</ref> on fused features and suppressed T-CAM S to learn better representations and T-CAM. Because we utilize the suppressed T-CAM in the testing stage in Section 3.6, we only apply L on suppressed T-CAM. Final Objective Function. Finally, we aggregate all aforementioned objective functions to form the final objective function for whole framework optimization:</p><formula xml:id="formula_11">L = L + L + L + 1 L + 2 L ,<label>(10)</label></formula><p>here, the 1 and 2 are hyperparameters. Our framework can learn more robust representation to produce more accurate T-CAM by optimizing that final objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Temporal Action Localization</head><p>At the testing stage, we follow the process of <ref type="bibr" target="#b11">[12]</ref>. Firstly, we calculate the video-level categorical probabilities that indicate the possibility of each action class happened in the given video. Then we set a threshold to determine the action classes that would be localized in the video. For the selected action class, we threshold the attention weights A to drop the background snippets and obtain the class-agnostic action proposals by selecting the continuous components of the remaining snippets. As we said in Section 3.1, a candidate action proposal is a four-tuple: ( , , , ). After obtaining the action proposals, we utilize the suppressed T-CAM S to calculate the class-specific score for each proposal using Outer-Inter Score <ref type="bibr" target="#b37">[38]</ref>. Moreover, we use multiple thresholds to threshold the attention weights to enrich the proposal set with proposals in different levels of scale. Further, we remove the overlapping proposals using soft non-maximum suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive experiments on two public temporal action localization benchmarks, i.e., THUMOS14 <ref type="bibr" target="#b14">[15]</ref> and ActivityNet1.2 dataset <ref type="bibr" target="#b6">[7]</ref>, to investigate the effectiveness of our proposed framework. In addition, we conduct ablation studies to discuss each component in CO 2 -Net and visualize some results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We evaluate our proposed approach on two public benchmark datasets, i.e., THUMOS14 dataset <ref type="bibr" target="#b14">[15]</ref> and ActivityNet1.2 dataset <ref type="bibr" target="#b6">[7]</ref>, for temporal action localization. THUMOS14. There are 200 validation videos and 213 test videos of 20 action classes in THUMOS14 dataset. These videos have diverse length and those actions frequently occur in the videos. Following the previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, we use 200 validation videos to train our framework and 213 test videos for testing. ActivityNet1.2. ActivityNet1.2 dataset is a large temporal action localization dataset with coarser annotations. It is composed of 4,819 training videos, 2,383 validation videos and 2,489 test videos of 100 action classes. We cannot obtain the ground-truth annotations for the test video, because they are withheld for the challenge. Therefore, we utilize validation videos for testing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Evaluation Metrics. In this work, we evaluate our method with mean average precision (mAP) under several different intersections of union (IoU) thresholds, which are the standard evaluation metrics for temporal action localization <ref type="bibr" target="#b34">[35]</ref>. Moreover, we utilize the official released evaluation code 2 to measure our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In this work, we implement our method in PyTorch <ref type="bibr" target="#b33">[34]</ref>. In the very beginning, we apply I3D networks <ref type="bibr" target="#b2">[3]</ref> pretrained on Kinetics-400 <ref type="bibr" target="#b16">[17]</ref> to extract both RGB and FLOW features for each video, following previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>. We sample continuous nonoverlapping 16 frames from video as a snippet, where the features for each modal of each snippet are 1024-dimension. In the training stage, we randomly sample 500 snippets for THUMOS14 dataset and 60 snippets for ActivityNet1.2 dataset, while all snippets are taken during testing. For fair comparisons, we do not finetune the feature extractor, i.e., I3D. The attention unit is constructed with 3 convolution layers, whose output dimensions are 512, 512 and 1 while the kernel sizes are 3, 3 and 1. The classification module contains 3 temporal convolution layers. Between each convolution layer, we use Dropout regularization with possibility as 0.7.</p><p>For each hyperparameters, we set 1 = 2 = 0.8 for the last two terms of regularization in the final objective function, and = 0.5 to obtain the best performance for both two datasets. In the training process, we sample 10 videos in a batch, in which there are 3 pairs 2 http://github.com/activitynet/ActivityNet of videos and each pair contains the same categorical tags for coactivity similarity loss L . We deploy Adam optimizer <ref type="bibr" target="#b17">[18]</ref> for optimizing, in which the learning rate is 5e-5 and weight decay rate is 0.001 for THUMOS14, while 3e-5 and 5e-4 for ActivityNet1.2 dataset. All experiments are run on a single NVIDIA GTX TITAN (Pascal) GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With State-of-the-art Methods</head><p>We first compare our proposed CO 2 -Net with current weakly supervised state-of-the-art methods and several fully supervised methods. We report the results in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>. From <ref type="table">Table  1</ref>, we can find that our method outperforms all weakly supervised methods in all IoU metrics on the THUMOS14 dataset, while even comparable with fully supervised methods at low IoU region. Compared with those native early fusion methods (e.g., HAM-Net <ref type="bibr" target="#b11">[12]</ref> and UM <ref type="bibr" target="#b19">[20]</ref>) and late fusion methods (e.g., TSCN <ref type="bibr" target="#b51">[52]</ref>), our method gains a significant improvement. For example, The results on "AVG mAP (0.1:07)" of CO 2 -Net vs. that of UM is 44.6% vs. 41.9%. These results show that using the information from different modalities to reduce the task-irrelevant information redundancy can benefit the temporal action localization. In addition to this, we also compare our method with several fully supervised, we can find that the results produced by our CO 2 -Net are even comparable with those fully supervised methods in terms of metrics with low IoU, i.e., mAP@IoU0.1 and mAP@IoU0.2. Moreover, our method even outperforms some fully supervised methods, e.g., S-CNN <ref type="bibr" target="#b38">[39]</ref> and BSN <ref type="bibr" target="#b21">[22]</ref>. These results validate the effectiveness of our proposed method.</p><p>With regard to the results of ActivityNet1.2 dataset reported in <ref type="table" target="#tab_1">Table 2</ref>, we can find that our method is still better than the current SOTA methods on the whole. However, We can not obtain the same impressive improvement on ActivityNet1.2 dataset as it we do in the THUMOS14 dataset, because ActivityNet1.2 dataset has only 1.5 action instances per video, compared with THUMOS14 dataset which has around 15 action instances per video. Additionally, we find that the annotations of ActivityNet1.2 dataset are coarser than those in THUMOS14 dataset. Taking all these into account, we recognize that the THUMOS14 dataset is more suitable for temporal action localization task than ActivityNet1.2 dataset (as discussed in <ref type="bibr" target="#b12">[13]</ref>). Therefore, we mainly use the former to verify our method in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>In this work, we propose a cross-modal consensus module to re-calibrate the representations and produce the enhanced features, and a mutual learning loss to enable two CCMs can learn from each other. Also, our final objective function consists of several components. Here, we first conduct the ablation studies to investigate the effect of each object functions. Then we discuss different kinds of combination of main and auxiliary modalities in the cross-modal consensus module. Finally, we also illustrate the results of different multi-modal fusion methods as well as SE-attention <ref type="bibr" target="#b10">[11]</ref> that replace the CCM in   objective function, we conduct related ablation studies and report results in <ref type="table" target="#tab_3">Table 3</ref>. We can find that each objective function makes contributions to the final performance. We treat the "Exp 1" as our baseline that only uses multiple-instance learning loss L . It is notable that our baseline is similar to the BaS-Net <ref type="bibr" target="#b18">[19]</ref> but our baseline outperforms the latter by 2.8%. Because our baseline contains the cross-modal consensus module to filter out the task-irrelevant information redundancy from two modalities and uses the concatenation of two enhanced features as the representation of each snippet. The results in <ref type="table" target="#tab_3">Table 3</ref>    <ref type="table">Table 4</ref>: Ablation studies of different types of mutual learning loss in term of average mAP under multiple IoU thresholds from 0.1 to 0.7 with interval as 0.1.</p><p>also evaluate the effect of the different types of mutual learning loss and report the results in <ref type="table">Table 4</ref>. The results of all types of mutual learning loss can outperform the current state-of-the-art results shown in <ref type="table">Table 1</ref>. These results indicate that it is necessary for the two CCMs to learn from each other and MSE is more suitable. Effect on different kinds of combination for two modalities. In our proposed CCM, we treat one modality as the main modality and another as the auxiliary modality. In Section 3.3, we utilize the  <ref type="table">Table 5</ref>: Comparisons of different kinds of combination for the main modality and the auxiliary modality in our crossmodal consensus module. "Global" means that a convolution layer after global pooling is adopted to capture modalspecific global context, while "Local" means a convolution layer without global pooling but local-focused. main modality to generate the modality-specific global-aware descriptor , while the auxiliary modality derives the cross-modal local-focused descriptor via a convolution layer. Here, we evaluate the different kinds of combination of the main and auxiliary modality in our cross-modal consensus module. The results are reported in <ref type="table">Table 5</ref>. We can find that obtaining the global context information from the main modality or auxiliary can obtain stable improvement compared with the results of the first row in <ref type="table">Table  5</ref>. e.g., the results of third row outperforms that of the first row by 1.6% in AVG result. It verifies that obtaining global context information benefit to guide the recognition of information redundancy. In addition, when we obtain the global context information from the main modality, we can get the best results. Because we aims to remove the task-irrelevant information redundancy from the main modality instead of the auxiliary modality, and obtaining the global context information from the main modality can handle the overall information of the main modality. Compare with other fusion methods. To verify that our proposed cross-modal consensus module is more suitable than other fusion methods for WS-TAL, we compare with other fusion methods and report results in <ref type="table">Table 6</ref>, in which SSMA is the fusion method in <ref type="bibr" target="#b40">[41]</ref>. We can find that our proposed CCM gains the best results compared with other fusion methods, e.g., our CO 2 -Net with CCM  <ref type="table">Table 6</ref>: Comparisons with other multi-modal early fusion methods (i.e., addition and concatenation), SSMA <ref type="bibr" target="#b40">[41]</ref> and SE-attention <ref type="bibr" target="#b10">[11]</ref> in CO 2 -Net in term of average mAP under multiple IoU thresholds {0.1:0.7:0.1}.</p><p>outperforms that with "Concate" by 5.1%. We can also find that the SSMA even underperforms the "Add" and "Concat", because it contains a specific structure that does not suitable for temporal action localization. The SE-attention mechanism <ref type="bibr" target="#b10">[11]</ref> can also gain an improvement compared with those early fusion methods, but results of our method still outperform that of SE by 1.6%. The results in <ref type="table">Table 6</ref> verify that our proposed cross-modal consensus module can better fuse two modalities to boost the performance than those fusion methods. Moreover, Though CO 2 -Net also concatenate two types of features after filtering information redundancy with CCM, the results of "Concat" and "CCM" shown in <ref type="table">Table 6</ref> indicate that our method with CCM performs much better than the method with "Concat" on the original features, showing the significance of feature re-calibration for more representative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visual Results</head><p>To better illustrate our method, we also illustrate the detected results of several samples in <ref type="figure" target="#fig_3">Figure 5</ref> using the methods in <ref type="table">Table 6</ref>. It is obvious that our method with CCM can predict more accurate localization against than the other fusion methods, showing the significance of removing the task-irrelevant information redundancy and the efficacy of our CCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we explore feature re-calibration for action localization to reduce the redundancy. A cross-modal consensus network is proposed to tackle this problem. We utilize a cross-modal consensus module to filter out the information redundancy in the main modality with the help of information from different perspectives of the auxiliary modality. Also, we apply a mutual learning loss to enable two cross-modal consensus modules to learn from each other for mutual promotion. Finally, we conduct extensive experiments to verify the effectiveness of our CO 2 -Net and the results on ablation studies show that our proposed cross-modal consensus module can help to produce more representative features that would boost the performance of WS-TAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>This work was supported partially by the NSFC (U1911401, U1811461), Guangdong NSF Project </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the workflow of the mutual learning process. The two temporal attention weights generated from dual model-specific attention units are learning from each other by treating the other as pseudo labels and stopping the gradients backwards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CO 2 -</head><label>2</label><figDesc>Net to verify the effectiveness of CCM. Effect of each component of final objective function. Each component in the final objective function (Eq. 10) performs important role in our framework to help to learn the feature representations and final predictions. To verify the effectiveness of each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The illustration of the action localization results predicted by our full method and several variant methods on several video samples. Action proposals are represented by green boxes. The horizontal and vertical axes are time and intensity of attention, respectively. The method "Ours + CCM" means our full method CO 2 -Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(No.2020B1515120085, 2018B030312002), the Key-Area Research and Development Program of Guangzhou (202007030004), the Early Career Scheme of the Research Grants Council (RGC) of the Hong Kong SAR under grant No. 26202321 and HKUST Startup Fund No. R9253. Work was partilly done during Fa-Ting's and Jia-Chang's internship in ARC, PCG Tencent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>; while ? means additional information is adopted in this method, such as action frequency or human pose. * indicates that the results are obtained by contacting the corresponding authors via email.</figDesc><table><row><cell></cell><cell>Supervision</cell><cell>Method</cell><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="3">mAP@IoU (%) 0.4 0.5 0.6</cell><cell>0.7</cell><cell cols="3">AVG mAP (%) 0.8 0.9 0.1:0.5 0.1:0.7 0.1:0.9</cell></row><row><cell></cell><cell></cell><cell cols="2">S-CNN [39] (2016)</cell><cell cols="6">47.7 43.5 36.3 28.7 19.0 10.3</cell><cell>5.3</cell><cell>-</cell><cell>-</cell><cell>35.0</cell><cell>24.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">SSN[54] (2017)</cell><cell cols="5">60.3 56.2 50.6 40.8 29.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Fully</cell><cell cols="2">BSN [22] (2018)</cell><cell>-</cell><cell>-</cell><cell cols="5">53.5 45.0 36.9 28.4 20.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">TAL-Net [4] (2018)</cell><cell cols="7">59.8 57.1 53.2 48.5 42.8 33.8 20.8</cell><cell>-</cell><cell>-</cell><cell>52.3</cell><cell>45.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">P-GCN[51] (2019)</cell><cell cols="5">69.5 67.5 63.6 57.8 49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">CMCS[23] (2019)</cell><cell cols="6">57.4 50.8 41.2 32.1 23.1 15.0</cell><cell>7.0</cell><cell>-</cell><cell>-</cell><cell>40.9</cell><cell>32.4</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">STAR [49] (2019)</cell><cell cols="5">68.8 60.0 48.7 34.7 23.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.4</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Weakly ?</cell><cell cols="2">3C-Net [29] (2019)</cell><cell cols="5">59.1 53.5 44.2 34.1 26.6</cell><cell>-</cell><cell>8.1</cell><cell>-</cell><cell>-</cell><cell>43.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="8">PreTrimNet [53] (2020) 57.5 50.7 41.4 32.1 23.1 14.2</cell><cell>7.7</cell><cell>-</cell><cell>-</cell><cell>41.0</cell><cell>23.7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">SF-Net [26] (2020)</cell><cell cols="6">71.0 63.4 53.2 40.7 29.3 18.4</cell><cell>9.6</cell><cell>-</cell><cell>-</cell><cell>51.5</cell><cell>40.8</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">BaS-Net [19] (2020)</cell><cell cols="9">58.2 52.3 44.6 36.0 27.0 18.6 10.4 3.3 0.4</cell><cell>43.6</cell><cell>35.3</cell><cell>27.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Gong et al. [9] (2020)</cell><cell>-</cell><cell>-</cell><cell cols="5">46.9 38.9 30.1 19.8 10.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">DML [13] (2020)</cell><cell>62.3</cell><cell>-</cell><cell>46.8</cell><cell>-</cell><cell>29.6</cell><cell>-</cell><cell>9.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">A2CL-PT [27] (2020)</cell><cell cols="9">61.2 56.1 48.1 39.0 30.1 19.2 10.6 4.8 1.0</cell><cell>46.9</cell><cell>37.8</cell><cell>30.0</cell></row><row><cell></cell><cell>Weakly</cell><cell cols="2">TSCN [52] (2020)</cell><cell cols="9">63.4 57.6 47.8 37.7 28.7 19.4 10.2 3.9 0.7</cell><cell>47.0</cell><cell>37.8</cell><cell>29.9</cell></row><row><cell></cell><cell></cell><cell cols="2">ACSNet [24] (2021)</cell><cell>-</cell><cell>-</cell><cell cols="5">51.4 42.7 32.4 22.0 11.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">HAM-Net [12] (2021)</cell><cell cols="9">65.9 59.6 52.2 43.1 32.6 21.9 12.5 4.4* 0.7*</cell><cell>50.7</cell><cell>39.8</cell><cell>32.5</cell></row><row><cell></cell><cell></cell><cell cols="2">UM [20] (2021)</cell><cell cols="9">67.5 61.2 52.3 43.4 33.7 22.9 12.1 3.9* 0.4*</cell><cell>51.6</cell><cell>41.9</cell><cell>33.0</cell></row><row><cell></cell><cell></cell><cell>CO 2 -Net</cell><cell></cell><cell cols="9">70.1 63.6 54.5 45.7 38.3 26.4 13.4 6.9 2.0</cell><cell>54.4</cell><cell>44.6</cell><cell>35.7</cell></row><row><cell cols="14">Table 1: Comparisons of CO 2 -Net with other methods on the THUMOS14 dataset. AVG is the average mAP under multiple</cell></row><row><cell cols="7">thresholds, namely, 0.1:0.5:0.1, 0.1:0.7:0.1 and 0.1:0.9:0.1Supervision mAP@IoU (%) Method 0.5 0.75 0.95 AVG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully</cell><cell cols="2">SSN[54] (2017)</cell><cell cols="4">41.3 27.0 6.1 26.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weakly ?</cell><cell cols="2">3C-Net [29] (2019) CMCS [23] (2019)</cell><cell cols="4">35.4 22.9 8.5 21.1 36.8 22.0 5.6 22.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BaSNet [19] (2020)</cell><cell cols="4">38.5 24.2 5.6 24.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">ActionBytes [14] (2020) 39.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DGAM [37] (2020)</cell><cell cols="4">41.0 23.5 5.3 24.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Gong et al. [9] (2020)</cell><cell cols="4">40.0 25.0 4.6 24.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Weakly</cell><cell cols="2">TSCN [52] (2020) RefineLoc [33] (2021)</cell><cell cols="4">37.6 23.7 5.7 23.6 38.7 22.6 5.5 23.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">HAM-Net [12] (2021)</cell><cell cols="4">41.0 24.8 5.3 25.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">UM [20] (2021)</cell><cell cols="4">41.2 25.6 6.0 25.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ACSNet [24] (2021)</cell><cell cols="4">40.1 26.1 6.8 26.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CO 2 -Net</cell><cell cols="4">43.3 26.3 5.2 26.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our algorithm with other methods on the ActivityNet1.2 dataset. AVG means average mAP from IoU 0.5 to 0.95 with 0.05 increment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of our algorithm in term of average mAP under multiple IoU thresholds as {0.1:0.7:0.1}. MAE 69.2 63.0 53.8 45.2 38.6 26.2 13.9 44.3 KL 67.7 62.2 53.9 44.8 37.3 25.7 14.7 43.8 JS 69.2 63.3 54.5 46.0 38.3 26.5 14.1 44.5 MSE 70.1 63.6 54.5 45.7 38.3 26.4 13.4 44.6</figDesc><table><row><cell>L</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>mAP@IoU 0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>AVG</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both the top-k multiple-instance learning loss and co-activity similarity are widely used in current WS-TAL methods. They are not the main contributions in this work, so that we do not detail them in our paper. More details of them can refer to<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joon Son Chung, and Andrew Zisserman. 2020. Self-supervised learning of audio-visual objects from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tsp: Temporallysensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11479</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Triplet-based deep hashing network for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bernard Ghanem Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Temporal Co-Attention Models for Unsupervised Video Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly Supervised Temporal Action Localization Using Deep Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ActionBytes: Learning from trimmed videos to localize actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-Modal Cross-Domain Moment Alignment Network for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Background Suppression Network for Weakly-Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilhyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
		<title level="m">Weakly-supervised Temporal Action Localization by Uncertainty Modeling. arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ACSNet: Action-Context Separation Network for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zheng Nanning, and Gang Hua. 2021</title>
		<imprint/>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<title level="m">Fang Wan, Trevor Darrell, and Huijuan Xu. 2020. Weakly-Supervised Action Localization with Expectation-Maximization Multi-Instance Learning. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SF-Net: Single-frame supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxin</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourab</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial background-aware loss for weaklysupervised temporal activity localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3c-net: Category count and center loss for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Activity Graph Transformer for Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised action localization by sparse temporal pooling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Refineloc: Iterative refinement for weakly-supervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">W-talc: Weaklysupervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujoy</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly-supervised action localization by generative attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Autoloc: Weakly-supervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning compact hash codes for multimodal representations using orthogonal deep structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pruning 3D Filters For Accelerating 3D ConvNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Cross-Modal Deep Representations for Robust Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PAD-Net: Multi-Tasks Guided Prediciton-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Boundary-sensitive pre-training for temporal localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10830</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segregated temporal assembly recurrent networks for weakly supervised multiple action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-An</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Two-stream consensus network for weakly-supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label action recognition and localization based on spatio-temporal pretrimming for untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
