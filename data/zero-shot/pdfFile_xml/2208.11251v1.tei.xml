<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnable Human Mesh Triangulation for 3D Human Pose and Shape Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Chun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept of ECE</orgName>
								<orgName type="institution">Kwangwoon University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbum</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NCSOFT</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
							<email>jychang@kw.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of ECE</orgName>
								<orgName type="institution">Kwangwoon University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Human Mesh Triangulation for 3D Human Pose and Shape Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared to joint position, the accuracy of joint rotation and shape estimation has received relatively little attention in the skinned multi-person linear model (SMPL)based human mesh reconstruction from multi-view images. The work in this field is broadly classified into two categories. The first approach performs joint estimation and then produces SMPL parameters by fitting SMPL to resultant joints. The second approach regresses SMPL parameters directly from the input images through a convolutional neural network (CNN)-based model. However, these approaches suffer from the lack of information for resolving the ambiguity of joint rotation and shape reconstruction and the difficulty of network learning. To solve the aforementioned problems, we propose a two-stage method. The proposed method first estimates the coordinates of mesh vertices through a CNN-based model from input images, and acquires SMPL parameters by fitting the SMPL model to the estimated vertices. Estimated mesh vertices provide sufficient information for determining joint rotation and shape, and are easier to learn than SMPL parameters. According to experiments using Human3.6M and MPI-INF-3DHP datasets, the proposed method significantly outperforms the previous works in terms of joint rotation and shape estimation, and achieves competitive performance in terms of joint location estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation from single or multi-view images is a long-standing computer vision problem. In many studies <ref type="bibr" target="#b2">[9,</ref><ref type="bibr" target="#b4">11,</ref><ref type="bibr" target="#b23">30,</ref><ref type="bibr" target="#b28">35]</ref>, the human pose is simply represented as a set of 3D coordinates of the body joints. Compared to joint coordinate, human joint rotation and shape estimation has not received much attention. However, when 3D joint coordinates as well as joint rotations and human shape information are available together, the body of a person can be better described, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref> and (c). The es- timated joint and shape information can also be used for human part segmentation <ref type="bibr" target="#b6">[13]</ref> and detailed human mesh reconstruction <ref type="bibr" target="#b38">[45,</ref><ref type="bibr" target="#b39">46]</ref>. The skinned multi-person linear model (SMPL) <ref type="bibr" target="#b15">[22]</ref> is frequently used for multi-view human mesh reconstruction methods <ref type="bibr" target="#b25">[32,</ref><ref type="bibr" target="#b26">33,</ref><ref type="bibr" target="#b31">38,</ref><ref type="bibr" target="#b35">42]</ref>, which can acquire joint rotations and human shape as well as joint coordinates. Among the methods, the most similar to our proposed method is <ref type="bibr" target="#b35">[42]</ref>. This method first estimates 3D joints from multiview images and then additionally computes joint rotation and shape information by fitting the SMPL-X <ref type="bibr" target="#b21">[28]</ref> model to the 3D joints. However, this fitting framework heavily relies on regularization because joint coordinates do not provide enough information to resolve the ambiguity in the estimation of joint rotation and shape information. Nevertheless, the lack of such information can degrade joint rotation and human shape estimation performance, as shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. The convolutional neural network (CNN)-based model proposed in <ref type="bibr" target="#b26">[33]</ref> directly regresses SMPL pose and shape parameters from input multi-view images. However, the mapping function from the input image to the SMPL parameter is highly non-linear <ref type="bibr" target="#b20">[27]</ref>, which makes learning the model difficult.</p><p>In this paper, we propose a Learnable human Mesh Triangulation (LMT) method for SMPL-based human mesh reconstruction from sparse multi-view images. The proposed method can solve the above two problems. LMT first estimates human surface vertex coordinates, not human joints, from the input multi-view images, and then fits the SMPL model to the resultant vertices. Such surface vertex coordinates provide strong constraints on joint rotation and human shape, which can help resolve the ambiguity problem. Also, many previous works <ref type="bibr">[3,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b17">24,</ref><ref type="bibr" target="#b27">34,</ref><ref type="bibr" target="#b30">37,</ref><ref type="bibr" target="#b32">39]</ref> verified that heatmap-based keypoint estimation can be easily learned through CNNs, especially fully convolutional networks. Our basic idea is to extend this heatmap-based keypoint estimation framework to SMPL mesh vertex estimation, which can solve the non-linearity problem in direct SMPL parameter regression.</p><p>To reconstruct SMPL-based human mesh vertices, we extend Learnable Triangulation of human pose (LT) <ref type="bibr" target="#b4">[11]</ref>, the heatmap-based method for estimating sparse joints to dense vertices. However, the application of LT to mesh vertices is non-trivial and raises two issues to be overcome. The first is high computational complexity. LT generates 3D heatmaps to estimate body joints. No problem is observed in the case of sparse joints (e.g., ?20 for Human3.6M <ref type="bibr" target="#b3">[10]</ref> and MPI-3DHP-INF <ref type="bibr" target="#b19">[26]</ref>). In contrast, the use of a 3D heatmap may cause excessive GPU memory usage in the case of dense mesh vertices (e.g., 6890 for SMPL). However, the optimization process used to obtain SMPL parameters in the proposed method does not require full-vertices. Rather, estimating appropriately sampled sub-vertices can improve the performance of the model while solving the computational issue, which is proven through our experiments.</p><p>The second issue is the inconsistency between multiview features. In our method, multi-view features are aggregated in each voxel after being unprojected into 3D space. In the case of voxel on the human surface, multi-view features aggregated into the voxel must be consistent. However, occlusion can lead to inconsistency between aggregated multi-view features, which makes vertex coordinate estimation difficult. To alleviate this problem, we propose to utilize the visibility information obtained from the singleview mesh reconstruction method. The basic idea is to use visibility information to increase the dependence of a certain voxel on features obtained from visible views and reduce the dependence on features obtained from invisible views. We experimentally show that utilizing visibility information alleviates the multi-view inconsistency problem and improves mesh reconstruction performance.</p><p>The contributions of this paper can be summarized as follows:</p><p>? We quantitatively and qualitatively prove that fitting the SMPL model to human surface vertices rather than human body joints leads to better mesh reconstruction results in terms of joint rotation and human shape.</p><p>? We show that the computational issue that makes it difficult to extend the heatmap-based framework to SMPL mesh vertices can be resolved through subvertices estimation, which also brings additional performance gain.</p><p>? Per-vertex visibility information is utilized to consider the consistency of multi-view features. Moreover, cross-dataset experiments show that the use of visibility improves the generalization performance of our model.</p><p>? Extensive experiments using Human3.6M and MPI-INF-3DHP datasets prove that the ideas of sub-vertices estimation and per-vertex visibility are effective. Consequently, the proposed framework outperforms previous methods in terms of joint rotation and human shape while showing competitive results in terms of 3D joint coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-view Joint Estimation</head><p>Many methods <ref type="bibr" target="#b2">[9,</ref><ref type="bibr" target="#b4">11,</ref><ref type="bibr" target="#b5">12,</ref><ref type="bibr" target="#b22">29,</ref><ref type="bibr" target="#b23">30,</ref><ref type="bibr" target="#b29">36,</ref><ref type="bibr" target="#b33">40]</ref> have been proposed to estimate the 3D human pose in the form of joint coordinates from the input multi-view images. Among the methods for estimating the pose of a single person, the one most similar to our work is LT <ref type="bibr" target="#b4">[11]</ref>. LT aggregates 2D features extracted from multi-view images in 3D voxel space and then applies the 3D convolution to the aggregated feature to estimate 3D pose. However, the final LT output is the 3D joint locations without joint rotation information. In contrast, in our method, the SMPL parameters are estimated, which enables a richer reconstruction of the human body, including joint rotations and human shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-view Joint and Shape Estimation</head><p>Many studies <ref type="bibr" target="#b10">[17,</ref><ref type="bibr" target="#b25">32,</ref><ref type="bibr" target="#b26">33,</ref><ref type="bibr" target="#b31">38,</ref><ref type="bibr" target="#b35">42,</ref><ref type="bibr" target="#b36">43]</ref> have been conducted to estimate joint rotations or human shape as well as joint coordinates from input multi-view images. For SMPL and SMPL-X parameter estimation, the model is fitted to the predicted 3D joints in <ref type="bibr" target="#b35">[42]</ref>, and the 3D joints are fed into the feedforward network in <ref type="bibr" target="#b31">[38]</ref>. In contrast, our method estimates SMPL parameters using 3D mesh vertices rather than 3D joints. Since the human surface provides richer information than joint coordinates for joint rotation and human shape estimation, our method can reconstruct rotation and shape more accurately than joint-based methods <ref type="bibr" target="#b31">[38,</ref><ref type="bibr" target="#b35">42]</ref>. In <ref type="bibr" target="#b10">[17]</ref>, the Mannequin dataset <ref type="bibr" target="#b11">[18]</ref> is used to train a model that robustly predicts SMPL parameters in an in-the-wild environment. The dataset provides videos of static humans captured by a dynamic camera. The method in <ref type="bibr" target="#b10">[17]</ref> performs 3D joint estimation by applying the structure-frommotion (SfM) algorithm to the input video. However, the SfM method is generally difficult to apply to sparse multiview environments, e.g., Human3.6M and MPI-INF-3DHP datasets, which are the focus of this work. The geometry of a clothed human is reconstructed in <ref type="bibr" target="#b36">[43]</ref> and multiple images obtained from a dynamic camera are used in <ref type="bibr" target="#b25">[32]</ref>. Their goals and settings are different from our work. In <ref type="bibr" target="#b26">[33]</ref>, an existing work with the same goal as ours, SMPL parameters are directly regressed from multi-view images through a CNN model. However, learning the network in this method is difficult due to the high nonlinearity of the regression function [4, <ref type="bibr" target="#b6">13,</ref><ref type="bibr" target="#b8">15,</ref><ref type="bibr" target="#b9">16,</ref><ref type="bibr" target="#b12">19,</ref><ref type="bibr" target="#b13">20,</ref><ref type="bibr" target="#b20">27]</ref>. Therefore, our method learns a keypoint estimation network based on heatmap regression rather than parameter regression, and then obtains SMPL parameters by fitting SMPL to human mesh vertices predicted by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of the Proposed Method</head><p>We propose a method (i.e., LMT) to estimate the SMPLbased 3D mesh of a single person from multi-view images obtained by C calibrated cameras. of sub-sampled mesh from the aggregated feature V agg using 3D convolution and soft-argmax operation <ref type="bibr" target="#b28">[35]</ref>. The fitting module outputs the final joint coordinates, rotations, and shape information by fitting the SMPL model to the 3D vertex coordinates M from the vertex regression module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visibility Module</head><p>The visibility module calculates the per-vertex visibility map v c from the single-view image I c . We implement the visibility module using the I2L-MeshNet <ref type="bibr" target="#b20">[27]</ref>, one of the state-of-the-art single-view human mesh reconstruction methods, and the general visibility computation algorithm 1 . The detailed procedure is as follows. We first feed a single image I c into the I2L-MeshNet and obtain the human mesh defined in the human-centered coordinate system of which the origin is defined as the pelvis joint. However, the visibility computation algorithm requires camera coordinates of the human mesh. Therefore, the algebraic triangulation method <ref type="bibr" target="#b4">[11]</ref> is used to estimate the pelvis joint. The camera coordinates of the estimated pelvis joint are used to transform the human mesh obtained by I2L-MeshNet into the camera coordinate system. The visibility computation algorithm is then used to obtain the visibility map for fullvertices v f ull c ? R 6890 . To prevent overfitting of the proposed model, we apply additional mesh subsampling <ref type="bibr" target="#b24">[31]</ref> to v f ull c and use the resultant per-vertex visibility map v c of sub-vertices for subsequent processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backbone</head><p>The CNN backbone outputs the visibility augmented image features {F 2D c } C c=1 from input multi-view images {I c } C c=1 and per-vertex visibility {v c } C c=1 . To construct the proposed backbone, according to <ref type="bibr" target="#b4">[11]</ref>, we remove the last classification and pooling layers of ResNet-152 [8] pretrained on COCO <ref type="bibr" target="#b14">[21]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>, and then add three deconvolution layers and a 1 ? 1 convolution layer to the back of the network. The last deconvolution layer of the backbone creates an intermediate feature </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Aggregation Module</head><p>In the feature aggregation module, the 2D feature F 2D c from the backbone is unprojected into a cuboid defined in 3D world space to create a volumetric unprojected feature V unproj c . The volumetric aggregated feature V agg is then calculated through the aggregation of the unprojected features {V unproj c } C c=1 . In the proposed method, the estimation of the vertex coordinates of the subsampled mesh M depends on the unprojected 3D features in the cuboid. Therefore, the location and size of the cuboid should be set so that the cuboid contains the target human subject. Consequently, a cuboid with a side length of 2.0 m, centering on the pelvis of the target subject, is created.</p><p>The construction process of the unprojected feature V unproj c through the unprojection of F 2D c is as follows. We first project the 3D coordinates of the cuboid voxels V coords ? R 64?64?64?3 into the 2D image plane of each view using the camera projection matrix and obtain the 2D image coordinates V proj c ? R 64?64?64?2 . Next, bilinear sampling is used to extract 2D features corresponding to each location of V proj c from F 2D c , and, consequently, V unproj c is obtained:</p><formula xml:id="formula_0">V unproj c = F 2D c {V proj c },<label>(1)</label></formula><p>where {?} denotes bilinear sampling. Then C unprojected features in 3D world space are aggregated using 3D softmax operation <ref type="bibr" target="#b4">[11]</ref>. This can be written as:</p><formula xml:id="formula_1">V agg = C c=1 (d c V unproj c ),<label>(2)</label></formula><formula xml:id="formula_2">d c = exp(V unproj c ) C c=1 exp(V unproj c ) ,<label>(3)</label></formula><p>where d c ? R 64?64?64?K and denote the confidence weight and element-wise multiplication, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Vertex Regression Module</head><p>The vertex regression module with encoder-decoder structure composed of 3D convolution generates the vertex coordinates of the subsampled mesh M from the input aggregated feature V agg . The encoder first computes a 3D feature with 2 ? 2 ? 2 resolution and 128 channel dimension from V agg , which is fed into the decoder to output a volumetric feature V ? R 64?64?64?32 . Next, a 1 ? 1 ? 1 3D convolution is applied to V to produce 3D heatmaps H 3D ? R 64?64?64?N for the subsampled vertices. Details of the proposed encoder-decoder are presented in the Supplementary material.</p><p>A 3D soft-argmax operation is used to obtain vertex coordinates M from the 3D heatmaps H 3D :</p><formula xml:id="formula_3">H 3D n = exp(H 3D n ) i,j,k exp(H 3D n (i, j, k)) ,<label>(4)</label></formula><formula xml:id="formula_4">M n = i,j,k r ?H 3D n (i, j, k),<label>(5)</label></formula><p>where r = [r i , r j , r k ] denotes the world coordinate vector of the voxel with indices (i, j, k) in the 3D heatmap. H 3D n , H 3D , and M n denote the n-th channel of the 3D heatmap, the normalized 3D heatmap, and the n-th row vector of M , respectively.</p><p>To train the proposed network, an L1 loss is applied to the vertices generated by the vertex regression module:</p><formula xml:id="formula_5">L M = 1 N N n=1 M n ? M * n 1 ,<label>(6)</label></formula><p>where M * denotes the ground-truth mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Fitting Module</head><p>The fitting module is used to acquire the SMPL parameters corresponding to the vertex coordinates M generated by the vertex regression module. Fitting module is based on optimization according to the existing works <ref type="bibr" target="#b16">[23,</ref><ref type="bibr" target="#b18">25,</ref><ref type="bibr" target="#b21">28,</ref><ref type="bibr" target="#b34">41,</ref><ref type="bibr" target="#b35">42]</ref> and optimization parameters ? = {z ? R 32 , R ? R 6 , ? ? R 10 , t ? R 3 } contains VPoser's latent code z, global rotation with continuous representation <ref type="bibr" target="#b37">[44]</ref> R, shape parameter ?, and global translation t. From the latent code, VPoser V(?) calculates the SMPL pose parameter ? = V(z) ? R 69 , which is fed into the SMPL decoder M(?) together with R, ?, and t to produce the SMPL mesh</p><formula xml:id="formula_6">M f it = M(?, R, ?, t) ? R 6890?3 . The SMPL mesh is transformed into sub-vertices M f it sub = sub(M f it ) ? R N ?3</formula><p>by the mesh coarsening function <ref type="bibr" target="#b24">[31]</ref> sub(?). The fitting module updates ? iteratively to reduce the difference between the sub-vertices of the fitted mesh M f it sub and the regressed vertices M .</p><p>The cost function for fitting is defined as follows:</p><formula xml:id="formula_7">E f it = E data + E reg ,<label>(7)</label></formula><formula xml:id="formula_8">E data = 1 N N n=1 M f it sub,n ? M n 2 2 ,<label>(8)</label></formula><formula xml:id="formula_9">E reg = ? z E z + ? ? E ? + ? w E ?w + ? ? E ? ,<label>(9)</label></formula><p>where M f it sub,n and ? w ? R 6 denote the n-th row vector of M f it sub and the axis-angle representation of both wrist joints. E z , E ? , and E ?w are the L2 regularization terms for z, ?, and ? w , respectively. And E ? is the exponential regularization term for preventing unnatural bending of the elbows and knees <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">28]</ref>. Each ? represents regularization weight.</p><p>Joint coordinates J = GM f it ? R 17?3 can be obtained from the fitted mesh M f it using a pretrained joint regression matrix G ? R 17?6890 . The obtained J is used to evaluate the joint coordinate estimation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The spatial sizes of the input image I c and 2D feature F 2D c are set to (H 0 , W 0 ) = (384, 384) and (H, W ) = (96, 96), respectively. The bounding box provided in the datasets is used to crop the human region from the input image. Random rotation is applied to the cuboid <ref type="bibr" target="#b4">[11]</ref> along the vertical axis of the ground, and other augmentation is not used. Except for the fitting module, our network is trained end-to-end. Learnable parameters are included in the backbone and vertex regression module, and their learning rates are set to 1e-4 and 1e-3, respectively. The minibatch size, number of epochs, number of sub-vertices N , and channel of the feature map K is set to 3, 15, 108, and 32, respectively. The Adam optimizer <ref type="bibr" target="#b7">[14]</ref> is used to train our network, which takes about 3.5 days using a single RTX 3090 GPU. Mesh sub-sampling algorithms <ref type="bibr" target="#b24">[31]</ref> are applied to ground-truth human mesh vertices to obtain sub-sampled vertices, which are used for network training. The Adam optimizer is also used to update the optimization parameter ? in the fitting module. The fitting module learning rate, number of iterations for fitting, ? w , ? z , ? ? , and ? ? are set to 6e-2, 500, 6e-2, 2e-6, 5e-6, and 5e-5, respectively. All regularization weights are simply determined through greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Human3.6M <ref type="bibr" target="#b3">[10]</ref> is a large-scale dataset for 3D human pose estimation, including 3.6M video frames and 3D body joint annotations acquired from four synchronized cameras. It includes 11 human subjects (five females and six males), and according to previous works <ref type="bibr" target="#b4">[11,</ref><ref type="bibr" target="#b26">33]</ref>, S1, S5, S6, S7, and S8 are used for training and S9, and S11 for testing. The SMPL mesh obtained by applying MoSh <ref type="bibr" target="#b16">[23]</ref> to Hu-man3.6M is used for training and testing as ground-truth. The input image is undistorted before training and testing.</p><p>MPI-INF-3DHP <ref type="bibr" target="#b19">[26]</ref> is a dataset for 3D human pose estimation and is obtained through the multi-camera markerless MoCap system. Since its test data includes single-view images, only train data composed of multi-view (i.e., <ref type="bibr" target="#b7">14)</ref> images are used in our experiments. Train data includes eight subjects. For a fair comparison, according to previous work <ref type="bibr" target="#b26">[33]</ref>, S1-S7 are used for training, S8 is used for testing, and views 0, 2, 7, and 8 are used among all cameras. MPI-INF-3DHP provides ground-truth 3D human joints, but does not provide ground-truth 3D human meshes, so pseudo ground-truth meshes are used to train the model. The pseudo ground-truth SMPL parameters are obtained by fitting the SMPL model to ground-truth 3D joints <ref type="bibr" target="#b21">[28]</ref>, but the pseudo parameters are not used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Mean-per-joint-position-error (MPJPE) is a metric that evaluates the performance of 3D human pose estimation based on the L2 distance between the predicted and groundtruth body joints. For LMT, joint coordinates in the world coordinate system can be estimated. Thus, following existing works <ref type="bibr" target="#b2">[9,</ref><ref type="bibr" target="#b4">11]</ref>, the L2 distance between the two joint sets is computed without aligning the predicted and groundtruth pelvis joints [5, <ref type="bibr" target="#b6">13,</ref><ref type="bibr" target="#b8">15,</ref><ref type="bibr" target="#b9">16,</ref><ref type="bibr" target="#b12">19,</ref><ref type="bibr" target="#b13">20,</ref><ref type="bibr" target="#b20">27]</ref>.</p><p>Mean-per-vertex-error (MPVE) is a metric that evaluates the performance of human mesh reconstruction based on the L2 distance between predicted and ground-truth mesh vertices. The proposed method is evaluated through MPVE only for the Human3.6M dataset on which ground-truth human meshes are available.</p><p>MPJPE and MPVE are used for the evaluation of human mesh reconstruction methods in most existing works. However, because MPJPE and MPVE measure the position errors for joints and vertices, they do not provide information on whether the rotation of the body part is accurately estimated. Therefore, the angular distance d ang [7] is used between the estimated and ground-truth joint rotations for evaluating the proposed method:</p><formula xml:id="formula_10">d ang = 2 sin ?1 R ? R * F 2 ? 2 ,<label>(10)</label></formula><p>where, R, R * , and ? F denote the predicted rotation matrix, ground-truth rotation matrix, and Frobenius norm, respectively. The joint rotation is defined relative to its parent joint. The rotation of the root joint (i.e., pelvis) denotes the global orientation of the entire body. All angular distances described in this paper are in degree units. MPJPE averages the 3D position errors of all joints, so it cannot provide information about the case where only a specific joint has a large error. Therefore, 3DPCK <ref type="bibr" target="#b19">[26]</ref> that computes the proportion of 3D joints with errors below a certain threshold is used. The AUC <ref type="bibr" target="#b19">[26]</ref> is also presented for threshold-independent evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>The number of sub-vertices. The main problem of 3D  heatmap-based prediction for SMPL mesh vertices is excessive GPU memory allocation for 3D heatmaps. This problem can be solved by estimating fewer sub-vertices. For example, if 108 sub-vertices are used instead of 6890 SMPL vertices, the size of GPU memory for the 3D heatmap is reduced by about 6890/108 ? 63.8 times.</p><p>To investigate the effect of using sub-vertices on mesh reconstruction performance, MPJPE, MPVE, and angular distance results are presented according to the number of vertices in <ref type="table" target="#tab_0">Table 1</ref>. To compare the full-vertices model and all sub-vertices models under the same condition, 16 ? 16 ? 16 heatmap resolution is used. It is the maximum resolution at which a full-vertices model under our computing resources can be trained. <ref type="table" target="#tab_0">Table 1</ref> shows that better quantitative results are obtained in most cases using sub-vertices than when using full-vertices. Only for the 54 sub-vertices, MPVE and angular distance performances deteriorate compared to fullvertices. This degraded performance is due to the fact that 54 sub-vertices do not provide sufficient information for joint rotation and shape reconstruction, given the supplementary material. We adopt the 108 vertices model that shows the best MPJPE and angular distance performance and requires a relatively smaller heatmap size.</p><p>Heatmap resolution. Experiments using various heatmap resolutions are conducted to investigate a model that can accurately estimate 108 sub-vertices. <ref type="table" target="#tab_1">Table 2</ref> shows the performance for the cases in which the heatmap resolution is set to 16?16?16, 32?32?32, and 64?64?64. The proposed method shows the best performance at 64?64?64 heatmap resolution. In this case, the memory allocation for the heatmap is 64 ? 64 ? 64 ? 108 ? 4byte = 113.2M B, which is similar to the memory allocation of 16 ? 16 ? 16 ? 6890 ? 4byte = 112.9M B for the maximum heatmap resolution allowed by full-vertices. According to the perfor- mance comparison of the full-vertices model in <ref type="table" target="#tab_0">Table 1</ref> and the sub-vertices model in <ref type="table" target="#tab_1">Table 2</ref>, the 108 vertices model with 64?64?64 heatmap resolution achieves better performance for all evaluation metrics than the full-vertices model without additional memory cost.</p><p>Multi-view inconsistency. To prove that using visibility helps feature aggregation, we implement the softmax baseline that does not use visibility, and compare it to the LMT. The Softmax baseline generates the image feature F 2D c by directly feeding F deconv c into the 1 ? 1 convolution layer without concatenation with visibility v c . When aggregating multi-view features, using only features obtained from views in which the human body surface is visible is desirable, because this leads to the consistency of aggregated multi-view features. However, in the softmax baseline, the multi-view inconsistency problem may arise, which can be mitigated through the use of visibility.</p><p>In the first and fourth views of <ref type="figure" target="#fig_4">Fig. 3</ref>, the right foot is clearly visible. In the third view, the right foot is not visible, but it can be contextually inferred that it is behind the left foot. However, in the second view, severe occlusion prevents the right foot from being estimated. Therefore, relying on the features obtained from the remaining views rather than the second view is preferable for estimating the posi-    tion of the right foot. However, the softmax baseline shows a higher dependence on the second view than on the third view, which causes the model to incorrectly estimate the right foot mesh. On the other hand, LMT uses visibility to reduce the dependence on the second view and increase the dependence on the remaining views. Consequently, LMT successfully reconstructs the right foot mesh. Effect of using visibility. We investigate the quantitative results of using per-vertex visibility in terms of joint coordinates, rotations, and shape estimation. <ref type="table" target="#tab_3">Table 3</ref> shows the results when the softmax baseline and LMT are trained on Human3.6M train data and evaluated on Human3.6M test data. The second column in <ref type="table" target="#tab_3">Table 3</ref> shows the MPVE for sub-vertices estimated by the vertex regression module, which proves that using visibility helps the network to estimate the human surface accurately. Columns 3-5 of <ref type="table" target="#tab_3">Table 3</ref> show that the use of visibility helps to improve MPJPE, MPVE, and angular distance results even after fitting.</p><p>Generalization. The proposed method exploits geometry information (i.e., visibility) obtained from a single-view model for feature aggregation. This single-view model can be trained using more various datasets than the multi-view model. Therefore, the use of geometry information from the single-view model causes the effect of implicit learning through such various datasets and helps to improve the generalization performance of the proposed method. To prove this quantitatively, we train the softmax baseline and LMT using Human3.6M train data and evaluate them for all subjects of MPI-INF-3DHP. Despite the differences between the two datasets, <ref type="table" target="#tab_4">Table 4</ref> shows that the LMT significantly outperforms the softmax baseline in all metrics evaluating the joint coordinate estimation performance.</p><p>Comparison with joint fitting. We demonstrate that fitting on the human surface brings more benefits than fitting on the human joint <ref type="bibr" target="#b35">[42]</ref>. However, the method of <ref type="bibr" target="#b35">[42]</ref> cannot be directly compared with LMT because it is for multiperson mesh reconstruction. Therefore, we design the LTfitting baseline using the state-of-the-art multi-view joint estimation method LT <ref type="bibr" target="#b4">[11]</ref>. LT-fitting modifies E data to minimize the difference between the predicted and ground-truth joints, and uses the same regularization term as LMT. <ref type="table" target="#tab_5">Table 5</ref> shows MPJPE, MPVE, and angular distance results of LT-fitting and LMT evaluated on Human3.6M. In both cases of LT-fitting and LMT, the use of regularization results in better joint rotation and shape estimation. And LT-fitting relies more heavily on regularization than LMT. However, LT-fitting with regularization shows worse MPVE and angular distance results than LMT without regularization. <ref type="table" target="#tab_7">Table 6</ref> shows the rotation errors of LT-fitting and LMT for each joint. For most joints, the rotation prediction performance of LMT is significantly better than that of LT-fitting. <ref type="figure" target="#fig_5">Fig. 4</ref> shows the human mesh reconstruction by LT-fitting and LMT. LT-fitting cannot describe the subject's body shape well because it cannot resolve the ambiguity of the human shape. On the other hand, LMT shows a visually satisfactory result. All these results show that using the human surface rather than the human joint is beneficial for human pose and shape estimation. Angular ? pelvis L-hip R-hip torso L-knee R-knee spine L-ankl R-ankl chest neck L-thrx R-thrx head L-shld R-shld L-elbw R-elbw L-wrst R-wrst LT-fitting <ref type="bibr" target="#b4">[11,</ref><ref type="bibr" target="#b35">42]</ref> 8. <ref type="bibr" target="#b11">18</ref>      <ref type="table" target="#tab_8">Table 7</ref> shows the results of previous multi-view human mesh reconstruction methods and LMT trained and evaluated on Human3.6M. The same input image size and the same backbone are used for a fair comparison with the parameter regression method of <ref type="bibr" target="#b26">[33]</ref>. Since <ref type="bibr" target="#b26">[33]</ref> does not provide MPVE and angular distance results, MPJPE is used for comparison, which shows that LMT significantly outperforms the method of <ref type="bibr" target="#b26">[33]</ref>. These results show that the combination of heatmap-based vertex regression and subsequent SMPL fitting brings more accurate results than the method of directly regressing the SMPL parameters from input images. LT-fitting is different from LMT in that SMPL is fitted to the human joint rather than the human surface. Due to this difference, LT-fitting does not obtain enough information to resolve the ambiguity for joint rotation and human shape determination, and as a result achieves significantly lower MPVE and angular distance performance than LMT. <ref type="table" target="#tab_9">Table 8</ref> shows the results of previous multi-view human mesh reconstruction methods and LMT trained and evaluated on MPI-INF-3DHP. For a fair comparison with <ref type="bibr" target="#b26">[33]</ref>, the LMT model is pretrained on Human3.6M and then fine-tuned on MPI-INF-3DHP. For 3DPCK with a threshold of 150 mm, <ref type="bibr" target="#b26">[33]</ref> shows better results than LMT, but for threshold-independent AUC, LMT shows better results. Also, as in Human3.6M, LMT shows better MPJPE performance. The LMT model shows a competitive joint coordinate estimation result with LT-fitting. In the case of MPI-INF-3DHP, ground-truth SMPL parameters are not provided, so joint rotation and shape estimation results are not presented. However, LMT gives qualitatively better mesh </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison on Human3.6M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Comparison on MPI-INF-3DHP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a two-stage method consisting of visibilitybased sub-vertices estimation and surface fitting is proposed to reconstruct a single human mesh from multi-view images. The estimation of sub-vertices rather than full-vertices solves the problem of excessive GPU memory usage. In addition, the use of per-vertex visibility improves the mesh vertices estimation performance by alleviating the multiview inconsistency problem. Surface fitting is also demonstrated to help estimate joint rotations and human shape compared to joint fitting. According to the experimental results, the proposed LMT significantly outperforms the existing multi-view human mesh reconstruction methods on the Human3.6M and MPI-INF-3DHP datasets. However, since using a single-view mesh reconstruction model to acquire visibility complicates the proposed model, additional studies are needed for a more efficient method to obtain visibility information. In addition, the investigation of more diverse viewpoints and in-the-wild input images is another future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In the supplementary material, we first provide the detailed architecture of the vertex regression module. We then give the additional results of the 54 vertices model, singleview method in the visibility module, and multi-view inconsistency. Also, we present qualitative comparison of surface fitting and joint fitting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Vertex Regression Module</head><p>The vertex regression module consists of basic convolution blocks, residual blocks, downsample blocks, upsample blocks, and a 1 ? 1 ? 1 convolution layer. The basic convolution block consists of a 3D convolution layer, a batch normalization layer, and a ReLU activation function. The residual block contains two 3D convolution layers, two batch normalization layers, two ReLU activation functions, and a residual connection. The downsample block consists of a 3D max pooling layer with a stride of 2. The upsample block consists of a 3D deconvolution layer with a stride of 2, a batch normalization layer, and a ReLU activation function. The vertex regression module is constructed using 3 basic convolution blocks, 20 residual blocks, 5 downsample blocks, 5 upsample blocks, and a 1 ? 1 ? 1 convolution. <ref type="figure" target="#fig_0">Fig. S1</ref> shows the detailed structure of the vertex regression module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. 54 Vertices Model</head><p>The 54 vertices model shows worse MPVE and angular distance performance compared to other sub-vertices models because the number of vertices on the arms and hands is relatively small. Too few vertices do not provide enough information to resolve ambiguity in joint rotation and shape estimation. Consequently, the 54 vertices model results in higher wrist rotation errors than the 108 vertices model, which is presented in <ref type="table" target="#tab_0">Table S1</ref>. A visualization of the vertex positions of the 54 vertices model and other sub-vertices models is presented in <ref type="figure" target="#fig_1">Fig. S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Single-view Method for Visibility</head><p>In this section, we present justification for the use of I2L-MeshNet <ref type="bibr" target="#b20">[27]</ref> in the proposed visibility module. To this end, we construct three visibility modules by combining three state-of-the-art methods for single-view human mesh reconstruction (i.e., I2L-MeshNet, METRO <ref type="bibr" target="#b12">[19]</ref>, and Graphormer <ref type="bibr" target="#b13">[20]</ref>) and a visibility computation algorithm 2 . A detailed procedure for visibility estimation based on single-view mesh reconstruction is presented in Sec. 3.2 of the main paper. <ref type="table" target="#tab_1">Table S2</ref> shows the performance comparison for the cases in which three visibility modules are used in the proposed method. We found that using I2L-MeshNet 2 https://github.com/MPI-IS/mesh produces better results than using other methods. Based on this result, we adopt I2L-MeshNet in our visibility module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4. Multi-view Inconsistency</head><p>This section presents additional examples on multi-view inconsistency in the ablation experiments of the main paper. <ref type="figure" target="#fig_4">Fig. S3</ref> gives a scenario where the left hand is invisible due to occlusion. In the second view, it is difficult to determine the exact position of the left hand because the subject's left hand is not visible. However, in the remaining views, the position of the subject's left hand can be easily found. Therefore, in order to reconstruct the left-hand mesh, the model is desirable to have a higher dependence on the features obtained from the remaining views other than the second view. However, the softmax baseline has a relatively high dependence on the features obtained from the second view. As a result, the softmax baseline incorrectly reconstructs the left hand. However, LMT reduces the dependence on the features obtained from the second view and successfully reconstructs the human mesh. <ref type="figure" target="#fig_5">Fig. S4</ref> shows a scenario where the subject's right foot cannot be seen well in the second view. According to the results, the softmax baseline fails to reconstruct the mesh, but LMT reconstructs it successfully. Similar to the case of <ref type="figure" target="#fig_4">Fig. S3</ref>, it can be seen that the use of visibility reduces the dependence on the feature obtained from the second view where occlusion occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5. Qualitative Results</head><p>This section shows that our surface fitting produces qualitatively better results in terms of joint rotation and shape compared to joint fitting <ref type="bibr" target="#b4">[11,</ref><ref type="bibr" target="#b35">42]</ref>. <ref type="figure" target="#fig_6">Fig. S5</ref> shows the human meshes reconstructed by LT-fitting and LMT on the Human3.6M <ref type="bibr" target="#b3">[10]</ref> dataset. The second row of <ref type="figure" target="#fig_6">Fig. S5</ref> shows that LT-fitting incorrectly predicts the rotations of the left ankle, elbows, and wrists. The fifth row of <ref type="figure" target="#fig_6">Fig. S5</ref> shows that LT-fitting incorrectly reconstructs the right knee rotation and human shape. However, in both cases, LMT accurately predicts joint rotation and human shape. <ref type="figure">Fig. S6</ref> shows the human meshes reconstructed by LTfitting and LMT on the MPI-INF-3DHP <ref type="bibr" target="#b19">[26]</ref> dataset. The second row of <ref type="figure">Fig. S6</ref> shows that LT-fitting incorrectly predicts the rotations of the right shoulder, elbows, wrists, knees, and ankles. The fifth row of <ref type="figure">Fig. S6</ref> shows that LTfitting incorrectly predicts the rotations of the neck, wrists, elbows, and right knee. However, similar to the results on Human3.6M, LMT accurately predicts joint rotations in both cases. As can be seen from Figs. S5 and S6, the human mesh reconstructed with accurate joint rotation and shape information can explain the human body more naturally, and we qualitatively prove the superiority of surface fitting based on these results.    <ref type="table" target="#tab_1">Table S2</ref>: Ablation results on the single-view mesh reconstruction method in the visibility module.  <ref type="figure" target="#fig_5">Figure S4</ref>: The first row visualizes the input multi-view images. The second and third rows show the reconstructed meshes generated from the softmax baseline and LMT, respectively. <ref type="figure" target="#fig_6">Figure S5</ref>: Qualitative results on Human3.6M. The first and fourth rows show the input images. The second and fifth rows visualize the human meshes reconstructed by LT-fitting. And the third and sixth rows visualize the human meshes reconstructed by LMT. <ref type="figure">Figure S6</ref>: Qualitative results on MPI-INF-3DHP. The first and fourth rows show the input images. The second and fifth rows visualize the human meshes reconstructed by LT-fitting. And the third and sixth rows visualize the human meshes reconstructed by LMT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Results for (a) joint position estimation, (b) joint fitting, and (c) surface fitting are visualized. Joint fitting and surface fitting indicate that SMPL is fitted to the estimated joint set and vertex set, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall pipeline of the proposed method. Visible vertices in the visibility map are colored in gold. ? denotes concatenate operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the overall pipeline of the proposed method, which consists of the visibility module, CNN backbone, feature aggregation module, vertex regression module, and fitting module. The visibility module estimates per-vertex visibility v c ? R N for subsampled mesh from every single image I c ? R H0?W0?3 , where N denotes the number of subsampled vertices. The CNN backbone computes visibility augmented image features F 2D c ? R H?W ?K from the input multi-view image I c and per-vertex visibility v c . The feature aggregation module unprojects the input image feature F 2D c into the 3D global voxel space to generate C volumetric unprojected features V unproj c ? R 64?64?64?K , then aggregates the unprojected features {V unproj c } C c=1 to produce the volumetric aggregated feature V agg ? R 64?64?64?K . The vertex regres-sion module generates 3D vertex coordinates M ? R N ?3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>F deconv c ? R H?W ?256 . After v c is extended to the spatial axis, it is concatenated with the intermediate feature F deconv c . An additional 1 ? 1 convolution is applied to the concatenated feature to generate the visibility augmented image feature F 2D c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The first row visualizes the input multi-view images. The second and third rows show the reconstructed meshes generated from the softmax baseline and LMT, respectively.d c is obtained by averaging the confidence weight d c corresponding to the red pixel on the multi-view image along the channel axis. The red pixels are obtained by projecting the voxel including a ground-truth vertex on the right foot to each image plane. Therefore,d c indicates how much the model depends on the image feature obtained from each view c to construct the aggregated feature of the voxel containing the right foot vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison with joint fitting on Human3.6M. The first column shows the input images. The second and third columns visualize the meshes reconstructed by LT-fitting and LMT, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison with joint fitting on MPI-INF-3DHP. The first column shows the input images. The second and third columns visualize the meshes reconstructed by LT-fitting and LMT, respectively. reconstruction results than LT-fitting, as shown in Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S1 :Figure S2 :</head><label>S1S2</label><figDesc>The architecture of the vertex regression module. (a) Pipeline of the vertex regression module. (b) Basic convolution block. (c) Residual block. (d) Upsample block. (e) Downsample block. Visualization of the vertex positions of sub-vertices models. Green and red point sets denote full-vertices and sub-vertices, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>11 Figure S3 :</head><label>11S3</label><figDesc>The first row visualizes the input multi-view images. The second and third rows show the reconstructed meshes generated from the softmax baseline and LMT, respectively.Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In ECCV, pages 561-578, 2016. [3] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). In IJCV, 2017. [4] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose. In ECCV, 2020. [5] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose. In ECCV, 2020. [6] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto, and Rita Cucchiara. Compressed volumetric heatmaps for multi-person 3d pose estimation. In CVPR, 2020. [7] Richard I. Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong Li. Rotation averaging. IJCV, 103(3):267-305, 2013. [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation results for the number of estimated vertices on Human3.6M. 3D heatmaps with 16 ? 16 ? 16 resolution are used in all experiments in this table.</figDesc><table><row><cell>Number of vertices</cell><cell>MPJPE ?</cell><cell>MPVE ?</cell><cell>Angular ?</cell></row><row><cell>6890</cell><cell>19.85</cell><cell>25.21</cell><cell>11.98</cell></row><row><cell>431</cell><cell>18.40</cell><cell>24.15</cell><cell>11.60</cell></row><row><cell>216</cell><cell>18.97</cell><cell>25.10</cell><cell>11.75</cell></row><row><cell>108</cell><cell>18.10</cell><cell>24.88</cell><cell>11.54</cell></row><row><cell>54</cell><cell>18.35</cell><cell>26.47</cell><cell>12.00</cell></row><row><cell cols="2">Heatmap resolution MPJPE ?</cell><cell>MPVE ?</cell><cell>Angular ?</cell></row><row><cell>16 ? 16 ? 16</cell><cell>18.10</cell><cell>24.88</cell><cell>11.54</cell></row><row><cell>32 ? 32 ? 32</cell><cell>18.02</cell><cell>24.19</cell><cell>11.45</cell></row><row><cell>64 ? 64 ? 64</cell><cell>17.59</cell><cell>23.70</cell><cell>11.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation results for the resolution of 3D heatmaps on Human3.6M. 108 vertices are estimated in all experiments in this table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the softmax baseline on Hu-man3.6M. ? means that the regressed vertices M from the vertex regression module are evaluated.</figDesc><table><row><cell>Model</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>S6</cell><cell>S7</cell><cell>S8</cell><cell>Avg</cell></row><row><cell>MPJPE ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax</cell><cell cols="9">85.83 64.03 63.42 82.55 65.18 71.68 66.02 70.74 70.66</cell></row><row><cell>LMT</cell><cell cols="9">81.32 61.49 60.50 78.62 62.47 71.39 65.77 66.78 68.02</cell></row><row><cell>3DPCK ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax</cell><cell cols="9">89.16 96.68 95.01 88.67 94.37 93.42 94.05 94.04 93.32</cell></row><row><cell>LMT</cell><cell cols="9">90.28 97.80 95.85 89.81 95.21 93.52 94.91 95.22 94.07</cell></row><row><cell>AUC ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Softmax</cell><cell cols="9">53.00 59.97 62.10 58.01 60.11 58.06 60.44 57.53 58.91</cell></row><row><cell>LMT</cell><cell cols="9">53.94 60.47 62.54 58.66 60.32 58.12 60.53 58.33 59.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Cross-dataset evaluation of the softmax baseline and LMT. The two models are trained on Human3.6m and evaluated on MPI-INF-3DHP. S1-S8 denote the subjects in MPI-INF-3DHP.</figDesc><table><row><cell>Model</cell><cell>MPJPE ?</cell><cell>MPVE ?</cell><cell>Angular ?</cell></row><row><cell>LT-fitting [11, 42]</cell><cell>16.21</cell><cell>35.20</cell><cell>15.73</cell></row><row><cell>LT-fitting [11, 42] (w/o reg)</cell><cell>16.40</cell><cell>42.99</cell><cell>22.94</cell></row><row><cell>LMT</cell><cell>17.59</cell><cell>23.70</cell><cell>11.33</cell></row><row><cell>LMT (w/o reg)</cell><cell>17.48</cell><cell>25.30</cell><cell>13.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison with joint fitting on Human3.6M.</figDesc><table /><note>"w/o reg" means that no regularization term E reg is used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>10.10 9.37 10.75 9.17 9.21 7.8 17.31 16.86 5.88 12.07 10.72 11.64 12.52 11.65 14.18 20.24 16.14 43.00 43.20 LMT 4.77 5.69 5.79 6.40 5.80 5.38 5.68 8.58 9.85 4.48 12.32 9.39 10.22 10.69 11.86 14.06 13.45 11.50 19.53 20.22</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Per-joint rotation error comparison with joint fitting on Human3.6M.</figDesc><table><row><cell>Model</cell><cell>MPJPE ?</cell><cell>MPVE ?</cell><cell>Angular ?</cell></row><row><cell>(R50-224) Parameter regr. [33]</cell><cell>46.90</cell><cell>-</cell><cell>-</cell></row><row><cell>(R50-224) LMT</cell><cell>30.56</cell><cell>42.28</cell><cell>14.61</cell></row><row><cell>(R152-384) LT-fitting [11, 42]</cell><cell>16.21</cell><cell>35.20</cell><cell>15.73</cell></row><row><cell>(R152-384) LMT</cell><cell>17.59</cell><cell>23.70</cell><cell>11.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison results on Human3.6M.</figDesc><table><row><cell>"R50-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison results on MPI-INF-3DHP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S1 :</head><label>S1</label><figDesc>Per-joint rotation error comparison of the 108-vertices and 54-vertices models. 3D heatmaps with 16?16?16 resolution are used in both experiments in this table.</figDesc><table><row><cell>Single-view method</cell><cell>MPJPE ?</cell><cell>MPVE ?</cell><cell>Angular ?</cell></row><row><cell>I2L-MeshNet [27]</cell><cell>17.59</cell><cell>23.70</cell><cell>11.33</cell></row><row><cell>METRO [19]</cell><cell>18.15</cell><cell>23.98</cell><cell>11.55</cell></row><row><cell>Graphormer [20]</cell><cell>17.77</cell><cell>24.23</cell><cell>11.52</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Peter Angular ? pelvis L-hip R-hip torso L-knee R-knee spine L-ankl R-ankl chest neck L-thrx R-thrx head L-shld R-shld L-elbw R-elbw L-wrst</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<idno>R-wrst 108 5.09 5.75 5.89 5.80 5.71 5.75 5.55 8.32 9.88 4.59 13.31 9.71 10.49 11.11 12.69 14.66 13.75 11.72 19.82 20.94 54 5.04 6.21 6.30 6.23 6.09 5.57 5.85 8.47 9.69 4.60 12.89 10.17 10.59 10.70 12.10 15.10 13.58 12.57 25.61 24.22</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Epipolar transformers. In CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolrahim</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10462</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Smply benchmarking 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Br?gier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Combaluzier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?gory</forename><surname>Rogez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning the depths of moving people by watching frozen people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mesh graphormer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Larry Zitnick. Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1-248:16</idno>
		<imprint>
			<date type="published" when="2015-10" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<idno>220:1-220:13</idno>
		<imprint>
			<date type="published" when="2014-11" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the heatmap regression for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="13264" to="13273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating 3D faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="725" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Airpose: Multi-view fusion network for aerial 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamir</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08093</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-view human pose and shape estimation using learnable volumetric aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eni</forename><surname>Halilaj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13427</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Direct multi-view multi-person 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-based 3d multi-person pose estimation using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Size</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="11148" to="11157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">We are more than our joints: Predicting how 3D bodies move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Light-weight multi-person total capture using sparse multi-view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepmulticap: Performance capture of multiple characters using sparse multiview cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Detailed avatar recovery from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
