<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">Shankar</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Serra</surname></persName>
							<email>edoardoserra@boisestate.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Boise State University</orgName>
								<address>
									<settlement>Boise</settlement>
									<region>ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahantesh</forename><surname>Halappanavar</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Pacific Northwest National Lab Richland</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pothen</surname></persName>
							<email>apothen@purdue.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Purdue University West Lafayette</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehab</forename><surname>Al-Shaer</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weaknesses in computer systems such as faults, bugs and errors in the architecture, design or implementation of software provide vulnerabilities that can be exploited by attackers to compromise the security of a system. Common Weakness Enumerations (CWE) are a hierarchically designed dictionary of software weaknesses that provide a means to understand software flaws, potential impact of their exploitation, and means to mitigate these flaws. Common Vulnerabilities and Exposures (CVE) are brief low-level descriptions that uniquely identify vulnerabilities in a specific product or protocol. Classifying or mapping of CVEs to CWEs provides a means to understand the impact and mitigate the vulnerabilities. Since manual mapping of CVEs is not a viable option, automated approaches are desirable but challenging.</p><p>We present a novel Transformer-based learning framework (V2W-BERT) in this paper. By using ideas from natural language processing, link prediction and transfer learning, our method outperforms previous approaches not only for CWE instances with abundant data to train, but also rare CWE classes with little or no data to train. Our approach also shows significant improvements in using historical data to predict links for future instances of CVEs, and therefore, provides a viable approach for practical applications. Using data from MITRE and National Vulnerability Database, we achieve up to 97% prediction accuracy for randomly partitioned data and up to 94% prediction accuracy in temporally partitioned data. We believe that our work will influence the design of better methods and training models, as well as applications to solve increasingly harder problems in cybersecurity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In order to understand and mitigate specific vulnerabilities in software products and protocols, one needs to accurately map them to hierarchically designed security dictionaries that provide insight on attack mechanisms, and thereby, means to mitigate weaknesses. Automating the mapping of vulnerabilities to weaknesses is a hard problem with significant challenges. In the paper, we present a novel Transformer-based framework to exploit recent developments in natural language processing, link prediction and transfer learning to accurately map vulnerabilities to hierarchically structured weaknesses, even when little or no prior information exists.  <ref type="bibr" target="#b11">(12)</ref> 101-500 (15) 1-100 (84) 0 <ref type="bibr" target="#b12">(13)</ref> Cumulative number of CVEs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1999-2017</head><p>2018-2020 <ref type="figure">Figure 1</ref>: Distribution of the number of CVEs per CWE in the National Vulnerability Database, bucketed into four categories: <ref type="bibr" target="#b11">12</ref> CWEs with 500 or more CVEs per CWE, <ref type="bibr" target="#b14">15</ref> CWEs with 100 to 500 CVEs per CWE, 84 CWEs with 1 to 100 CVEs per CWE, and 13 CWEs with zero CVE. We partition the data into two time periods to simulate testing for CVEs observed in the future: 1999-2017 (used for training) and 2018-2020 (used for testing). Cumulative numbers of CVEs are plotted on the Y-axis. The proposed framework (V2W-BERT) targets efficient mapping of rare instances that have not been addressed in earlier studies. Common Weakness Enumerations (CWE) 1 provide a blueprint for understanding software flaws and their impacts through a hierarchically designed dictionary of software weaknesses. Weaknesses are bugs, errors and faults that occur in different aspects of software such as architecture, design, or implementation that lead to exploitable vulnerabilities. Non-disjoint classes of CWEs are organized in a tree structure, where higher level classes provide general definitions of weaknesses, and lower level classes inherit the characteristics of the parent classes and add further details. Thus, analyzing the correct path from a root to lower level nodes provides valuable insight and functional directions to learn a weakness. For example, tracing the path from the root node, CWE-707, to a node CWE-89 2 , reveals that SQL injection (CWE-89) is caused by improper neutralization of special elements in data query logic (CWE-943), which in turn is caused by injection (CWE-74) or sent to a downstream component . This insight provides a means to design countermeasures even when a specific CWE node is not available <ref type="bibr" target="#b0">[1]</ref>.</p><p>In contrast, Common Vulnerabilities and Exposures (CVE) <ref type="bibr" target="#b2">3</ref> reports are uniquely identified computer security vulnerabilities, where a vulnerability is defined as a set of one or more weaknesses in a specific product or protocol that allows an attacker to exploit the behaviors or resources to compromise the system. CVEs are brief and low-level descriptions that provide a means to publicly share information on vulnerabilities. For example, CVE-2004-0366 provides specific description of an attack action through the execution of arbitrary SQL statement for a specific product, libpam-pgsql library, producing the specific consequence of SQL injection, which can then be used to compromise a system. However, and more importantly, the CVE report does not specify the characteristics of the SQL injection that are necessary to detect and mitigate the attack <ref type="bibr" target="#b0">[1]</ref>. This information comes from the corresponding CWE; CWE-89: SQL Injection, mentioned earlier.</p><p>Accurate mapping of CVEs to CWEs will enable the study of the means, impact and ways to mitigate attacks; hence it is an important problem in cyber-security <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. However, the problem is riddled with several challenges. A CVE can be mapped to multiple and interdependent CWEs that belong to the same path, which leads to ambiguity. CVEs are manually mapped to CWEs, which is neither scalable nor reliable. Consequently, there is a lack of high-quality mapping information. Only about 2% of CVEs are mapped in the MITRE database. Although NVD provides a higher percentage of mapping, about 71%, the number of CWEs that are mapped is considerably small (about 32%). As of February 2021, there are a total of 157, 325 CVEs registered in the NIST National Vulnerability Database (NVD), and 916 CWEs in the MITRE CWE database. Since new CVEs are created at a fast pace, manual mapping of CVEs is not a viable approach. Therefore efficient methods to automate the mapping of CVEs to CWEs are critical to address the ever increasing cybersecurity threats. We propose a novel method in this paper to address this challenging problem.</p><p>Automated mapping is limited by several challenges such as lack of sufficient training data, semantic gaps in the language of CVEs and CWEs, and non-disjoint hierarchy of CWEs classes. Our work focuses on one of the hardest problems in mapping CVEsrare CWE classes that do not have any CVEs mapped to them. As illustrated in <ref type="figure">Figure 1</ref>, a significant number of CVEs are currently mapped to a small set of CWE classes. Currently, about 70% of the CWE classes have fewer than 100 CVEs for training, about 10% have no CVEs mapped to them, and only 10% have more than 500 CVEs. The current approaches of classification work well only when a sufficient amount of data is available to train <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Although recent efforts using neural networks and word embedding based methods to process CVE reports have showed better performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>, they fail when little or no training data exists. Consequently, a large set of rare CWEs are completely ignored in literature. A second challenge that we address in this work is the practical scenario of classifying the vulnerabilities based on past data (1999 ? 2017) to predict future data (2018 ? 2020). Furthermore, rare CWE cases have been appearing more frequently in recent years, thus making the task even harder.</p><p>In this paper, we present a novel Transformer-based <ref type="bibr" target="#b18">[19]</ref> learning framework, V2W-BERT, that outperforms existing approaches 3 https://cve.mitre.org/cve/ for mapping CVEs to the CWE hierarchy at finer granularities. In particular, V2W-BERT is especially effective for rare instances. The Bidirectional Encoder Representations from Transformers (BERT) is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both the left and right sides of the context of a text token during the training phase <ref type="bibr" target="#b6">[7]</ref>. BERT is trained on a large text corpus, learning a deeper and intimate understanding of how language works, which is useful for downstream language processing tasks. Pre-trained BERT models can be enhanced with additional custom layers to customize for a wide range of Natural Language Processing (NLP) tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. We exploit this feature to transfer knowledge to the security domain and use it for mapping CVEs.</p><p>The second aspect of novelty in our work comes from the formulation of the problems as a link prediction problem that is different from previous formulation. In particular, we use the Siamese model <ref type="bibr" target="#b4">[5]</ref> to embed semantically different text forms in CVEs and CWEs into the same space for mapping through link predictionassociate the best link from a CVE to a CWE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>The key contributions of our work are as follows: <ref type="bibr" target="#b0">(1)</ref> We present a novel Transformer-based learning framework, V2W-BERT, to classify CVEs into CWEs ( ?3), including a detailed ablation study ( ?4.2). Our framework exploits both labeled and unlabeled CVEs, and uses pre-trained BERT models in a Siamese <ref type="bibr" target="#b4">[5]</ref> architecture to predict links between CVEs and CWEs ( ?3.1). Rare and unseen vulnerabilities are classified using a transfer learning procedure. The Reconstruction Decoder ( ?3.3) ensures that the pre-trained model on unsupervised data is not compromised by overfitting the link prediction task. (2) This is the first work to formalize the problem of mapping multiple-weakness definitions into a single vulnerability (multiclass) as a link prediction task. The hierarchical relationships among the CWEs are also incorporated during training and prediction phases. Unlike the traditional classification based methods, adding new CWE definitions to the corpus does not require changes to the model architecture. (3) V2W-BERT outperforms related approaches for all types of CWEs (both rare and frequently occurring) ( ?4.3). We simulate a challenging real-world scenario in our experiments where future mappings (2018-2020) are predicted based on past years' (1999-2017) data. We predict the CWEs of a vulnerability to finergranularities (root to the leaf node), and the user can control the precision. (4) For frequently occurring cases, V2W-BERT predicts immediate future (2018) mappings with 89%-98% accuracy for precise and relaxed prediction (definitions of these modes of prediction are provided in ?4). For rarely occurring CVEs, the proposed method achieves 48%-76% prediction accuracy, which is 10% to 15% higher than the existing approach. Additionally, the proposed method can classify completely unseen types of CWEs with up to 61% accuracy. We believe that this feature enables us to detect if and when a new CWE definition becomes necessary.</p><p>To the best of our knowledge, this is the first work to propose a novel Transformer-based framework that builds on link prediction to efficiently map CVEs to hierarchically-structured CWE descriptions. The framework not only performs well for CWE classes with abundant data, but also for rare CWE classes with little or no data to train, along with the power to map as yet unseen CVEs to existing or new CWEs. Therefore, we believe that our work will motivate the development of new methods as well as practical applications of the framework to solve increasingly challenging problems in automated organization of shared cyber-threat intelligence <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries &amp; Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem formulation</head><p>The Common Vulnerabilities and Exposures (CVEs) reports comprise the input text data, and the Common Weakness Enumerations (CWEs) are the target classes. The CWEs have textual details (Name, Description, Extended Description, Consequences, etc.), which are ignored in classification based methods. To utilize CWE descriptions and make the model flexible, we convert this multi-class multi-label problem into a binary link prediction problem. We propose a function, , that takes a CVE-CWE description pair ( , ) and returns a confidence value measuring their association:</p><formula xml:id="formula_0">= ( , ).<label>(1)</label></formula><p>Here, is a learnable function and the vector denotes learnable parameters. If a particular CVE ( ) is associated to a CWE ( ), then the function returns a value ? 1; and, ? 0 otherwise. To learn , both positive and negative links from the known associations are used. If a CVE has a known mapping to some CWE in the hierarchy, we consider all associations between them and their ancestors as positive links. The rest of the CVE-CWE associations are negative links. To predict the CWEs to be associated with a CVE report, we find the link with the highest confidence value in the hierarchy, from the root to a leaf node, using . The function also helps to easily incorporate new CWE definitions into the classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Brief Overview of BERT</head><p>BERT <ref type="bibr" target="#b6">[7]</ref> stands for Bidirectional Encoder Representations from Transformers. Transformers are attention-based Neural Networks that can effectively handle sequential data like texts by learning the relevance to the far away tokens concerning the current token <ref type="bibr" target="#b18">[19]</ref>. Unlike directional models, which read the text input sequentially (left-to-right or right-to-left), BERT is a bidirectional model that learns the context of a word based on its surroundings. Training on large unlabeled text corpus helps BERT learn how the underlying languages work. Devlin et al. <ref type="bibr" target="#b6">[7]</ref> reported two BERT models, BERT BASE ( = 12, = 768, = 12, Total parameters=110M), BERT LARGE ( = 24, = 1024, = 16, Total parameters=340M) where , , stand for number of layers (Transformer blocks), hidden size, and number of self-attention heads, respectively.</p><p>The original BERT models are pre-trained considering two tasks: ( ) Masked Language Model (LM), and ( ) Next Sentence Prediction (NSP). In the Masked LM task, 15% of random tokens are masked in each text sequence. Among those masked tokens, 80% are replaced with token [MASK], 10% are replaced with random tokens, and 10% are kept the same. These masked inputs are fed through the BERT encoder model, and the hidden states are passed to a decoder containing a linear transformation layer with softmax activation over the vocabulary. The model is optimized using cross entropy loss.</p><p>As for Next Sentence Prediction (NSP) task, a pre-training batch consists of pairs of sentences , where 50% of the time , the sentence next to , appears in the training samples, and for the remainder they do not. NSP helps downstream Question Answering (QA) and Natural Language Inference (NLI) tasks by directly learning the relationship between sentences. The pre-trained BERT models (BERT BASE , BERT LARGE ) are trained over BooksCorpus (800 words) and the English Wikipedia (2500 words) dataset, considering both MLM and NSP tasks together.</p><p>BERT BASE uses WordPiece embeddings with 30,522 vocabulary tokens to convert text sequences to vector forms. The first token is always [CLS] and end of a sentence is represented with [SEP]. The final hidden state corresponding to this [CLS] token usually represent the whole sequence as an aggregated representation. In this work, BERT BASE is used, and other variants of sequence representation are considered through different pooling operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related Work</head><p>Several studies have investigated the CVE to CWE classification problem. However, V2W-BERT is the first approach that formulates the problem as a link prediction problem using Transformers. Recent work by Aota et al. <ref type="bibr" target="#b2">[3]</ref> uses Random Forest and a new feature selection based method to classify CVEs to CWEs. This work only uses the 19 most frequent CWE definitions and ignores CWEs with fewer than 100 instances. It achieves 1 -Score of 92.93% for classification. Further, it does not support multi-label classification and does not consider the hierarchical relationships within CWEs. All these limitations are addressed in our work.</p><p>Na et al. <ref type="bibr" target="#b12">[13]</ref> predict CWEs from CVE descriptions using a Na?ve Bayes classifier. They focused only on the most frequent 2-10 CWEs without considering the hierarchy. When the number of CWEs considered increases from 2 to 10, their accuracy drops from 99.8% to 75.5%. Rahman et al. <ref type="bibr" target="#b15">[16]</ref> use Term Frequency-Inverse Document Frequency (TF-IDF) based feature vector and Support Vector Machine (SVM) technique to map CVEs to CWEs. They use only 6 CWE classes and 427 CVEs without considering hierarchy.</p><p>Recent work by Aghaei et al. <ref type="bibr" target="#b0">[1]</ref> uses TF-IDF weights of the vulnerabilities to initialize single layer Neural Networks (NNs). They use CWE hierarchy to predict classes iteratively. However, this is a shallow NN with only one layer, and comparative performance with more complex networks is not discussed in their work. Further, they consider all classes with scores higher than a given threshold as a prediction. This approach decreases the precision of prediction and is less desirable when precise predictions are needed, a limitation that is addressed in our work. Depending on the level of hierarchy, they achieve 92% and 94% accuracy for a random partition of the dataset. In contrast, we study a more representative partition of data based on time.</p><p>We note that each study uses different sets of CVEs for learning and testing. The choice of the number of CWEs used and evaluation methods are also different. Therefore, there is no consistent way to compare the accuracy numbers presented by different authors. Some studies use CVE descriptions to perform fundamentally different tasks than mapping to CWEs. For example, Han et el. <ref type="bibr" target="#b7">[8]</ref> and Nakagawa et al. <ref type="bibr" target="#b13">[14]</ref> use word2vec for word embedding and Convolutional Neural Network (CNN) to predict the severity of a vulnerability (score from 0 to 10). Neuhaus et al. <ref type="bibr" target="#b14">[15]</ref> use Latent Dirichlet Allocation (LDA) to analyze the CVE descriptions and assign reports on 28 topics.</p><p>To the best of our knowledge, V2W-BERT is the first BERT [7] based method to classify CVEs into CWEs. We fine-tune the pretrained BERT model with CVE and CWE descriptions, and then learn (Equation 1), using a Siamese network of BERT. A Siamese network shares weights while working in tandem on two different inputs to compute comparable outputs. A few recent studies have used the Siamese BERT architecture for information retrieval and sentence embedding tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. Reimers et al. <ref type="bibr" target="#b16">[17]</ref> proposed Sentence-BERT (SBERT), which uses Siamese and triplet network for sentence pair regression and achieves the state-of-the-art performance in Semantic Textual Similarity (STS) <ref type="bibr" target="#b1">[2]</ref>. V2W-BERT is conceptually similar to SBERT, but with notable differences. V2W-BERT has a different architecture where Reconstruction Decoder is coupled with the Siamese network to preserve context to improve performance in classifying rare and unseen vulnerabilities. Further, V2W-BERT is designed to classify CVEs into CWEs hierarchically, and therefore, has significantly different training and optimization processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Novel Framework: V2W-BERT</head><p>In this section, we present a novel framework V2W-BERT to classify CVEs to CWEs hierarchically. V2W-BERT optimizes the learnable parameters of ( ?2.1) in two steps. In the first step, the pre-trained BERT language model is further fine-tuned with CVE/CWE descriptions specific to cyber security. In the second step, the trained BERT model is employed in a Siamese network architecture to establish links between CVEs and CWEs. The architecture takes a specific CVE-CWE pair as input, and predicts whether the CVE belongs to the CWE or not, with a confidence value. V2W-BERT includes a Mask Language Model (LM) based Reconstruction Decoder to ensure that the descriptions' contexts are not changed too much during the training process. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall architecture of the V2W-BERT framework. V2W-BERT contains two primary components: ( ) Link Prediction (LP), and ( ) Reconstruction Decoder (RD). The LP module's primary purpose is to map CVEs with CWEs while the RD module preserves the context of the descriptions of CVEs and CWEs. During the backpropagation step, the trainable BERT layers are updated while optimizing LP and RD loss simultaneously. <ref type="figure" target="#fig_0">Figure 2</ref> shows a simplified architecture where the attention, fully connected, dropout, and layer-normalization layers have been omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Pre-training of BERT</head><p>Specific downstream inference tasks benefit from pre-training BERT with language associated with the domain-specific unlabeled data and the addition of custom Neural Network layers to the base model. To incorporate the cyber-security specific data on top of the base model, we pre-train BERT further with CVE and CWE descriptions. This is useful as a significant amount of CVE descriptions are not labeled and thus do not help with supervised learning. Since the pre-training process does not require CWE class labels, we utilize both labeled and unlabeled CVE descriptions to learn the cyber-security context. The original BERT model is trained considering Masked Language Model (LM) and Next Sentence Prediction (NSP) tasks. Like NSP, CVE and CWE are linked using the Link Prediction (LP) component as the second step of the V2W-BERT algorithm. Therefore, the BERT encoder is tuned on the Masked LM task only over available CVE and CWE descriptions. All layers of BERT are allowed to be updated in the pre-training step incorporating the cyber-security context. Section A.1 in the Appendix shows the architecture of the Masked Language Model in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Link Prediction Component</head><p>In the original problem, = ( , ), both CVE and CWE descriptions need to be processed together to establish links between them. There are many ways to tackle this. For example, TF-IDF or word embeddings (word2vec, glove, etc.) could be used to get vector representations of CVEs and CWEs, and these representations could be combined and classified with any learnable method that returns confidence about the association. However, the pre-trained BERT model knows the context of this problem domain, and can map relevant descriptions to similar vector spaces better than word embeddings <ref type="bibr" target="#b16">[17]</ref>. Furthermore, we need BERT to be tuned for the function , and the multi-layer Neural Network is the most compatible classification approach.</p><p>Therefore, in the Link Prediction (LP) component of V2W-BERT, the pre-trained BERT model is used to transform the CVE/CWE description. We fix the parameters of first out of layers ( = 12 in BERT BASE ) to allow minimal changes to the model to preserve previously learned context <ref type="bibr" target="#b17">[18]</ref>. We used = 9 in this study. LP adds a pooling layer on top of the pre-trained BERT encoder model to get a vector representation of the input sequence. These individual representations are then combined and passed through a classification layer with the softmax activation function. The output values create the relationship between a CVE and a CWE description with a degree of confidence.</p><p>Pooling: By default, the hidden state corresponding to the [CLS] token from the BERT encoder is considered as a pooled vector representation. However, recent work <ref type="bibr" target="#b17">[18]</ref> has shown that other pooling operations can perform better depending on the problem. Two additional pooling methods MAX-pooling (it takes MAX of the representation vectors of all tokens), and MEAN-pooling (which takes the MEAN of the vectors) are considered in ouor work. The pooled representations are passed through another transformation layer to get the final vector representation. In the CVE classification task, we found MEAN-pooling to be the best performing. The pooled vector representations are denoted as x for a CVE and y for a CWE. Combination: The pooled representations of input sequence pair can be combined in different ways <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. Some common operations are: Concatenation, multiplication, addition, set-operations, or combinations of these. In the current problem, concatenation of absolute difference and multiplication (|x ? y |, x ? y ) operation has shown best performance. Appendix A.2 shows that there are significant differences in the results from these choices.</p><p>Link Classification: The combined representations are classified into the link and unlink confidence values using the linear output layer with two neurons and softmax activation function. The softmax value ranges between [0, 1] and represents the confidence value of associating a CVE to a CWE. <ref type="bibr">CWE</ref>   For a specific CVE-CWE pair, if the link value is higher than the unlink value, then the CVE is associated with that CWE. A single neuron can also classify a link/unlink when the value is close to 1.0, indicating a high link association. However, experiments show that an output layer with two neurons outperforms a single neuron classifier. The cross-entropy loss is used to optimize link prediction:</p><formula xml:id="formula_1">( , ) = ( ( , ), ( , )),<label>(2)</label></formula><p>where, ( , ) is the link classification loss between predicted and real values of the CVE-CWE relation.</p><p>( , ) generates a 2-dimensional vector where first and second indices represent unlink and link association confidence values, respectively. If belongs to , ideally these values should be ? 0 for first index, and ? 1 for the second index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reconstruction Decoder Component</head><p>The classification challenge comes from three types of CVEs associated with rare CWEs classes: ( ) The CVEs belonging to a CWE class with few training instances, ( ) the CVEs of a particular CWE that appear in the test set but not in the training set, and ( ) CVEs with description styles that differ from the training set, or instances where the labels are erroneous.</p><p>The advantage of transfer learning is that it helps classify cases with few training instances <ref type="bibr" target="#b17">[18]</ref> as pre-trained BERT can produce correlated transformed vector representations from similar input sequences. The Link Prediction (LP) component learns to relate a CVE with the available CWEs by establishing links even when the training instances are few or do not exist.</p><p>For a new CVE type, we expect to have a low link association value with CWEs that exist in the training set (due to negative training links), and a high value for CWEs not included in the training set with similar text descriptions. However, due to learning bias towards available CWEs in Link Prediction (LP), we will have a higher link association to existing CWEs compared to new CWEs. Therefore, if we could preserve the original context that BERT learned during the pre-training phase while changing the LP model, it could improve the performance for rare CVE cases, and for completely unseen CWE classes. Note that for unseen cases this approach would work only if the corresponding CVE and CWE descriptions have some textual similarity. Preserving context can also be useful for detecting unusual or differently styled CVE descriptions during the test as they may not create any links with the available CWEs.</p><p>To preserve context while updating LP, we add a Reconstruction Decoder (RD) component ( <ref type="figure" target="#fig_0">Figure 2</ref>). When the BERT encoder transforms a CVE/CWE description, the last hidden state is passed to the Masked Language Model (LM) and optimized for Masked tokens. LP and RD share BERTs' hidden states, and the trainable layers are updated considering both link classification loss and reconstruction loss simultaneously. In this way, V2W-BERT trains for link classification while preserving context. Cross-Entropy loss is used to optimize the difference between original input and reconstructed tokens.</p><p>Let ( ) denote the reconstruction loss of an input sequence ; and ( ( )) be a reconstruction decoder that takes the last hidden state of BERT and reconstructs masked tokens. We can express the reconstruction loss as follows:</p><formula xml:id="formula_2">( ) = ( ( ( )), ( )), ( ) = ( ( ( )), ( )).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Details</head><p>To learn the parameters of the model , we have to train V2W-BERT with positive and negative link mappings between CVEs and CWEs. A single CVE can belong to multiple CWEs at different levels of the hierarchy. According to the MITRE classification, a CWE can have multiple parents and multiple children. When a CVE belongs to a CWE, that CVE-CWE pair is considered a positive link, and all ancestor CWEs of that weakness are also considered as positive links. The remaining CWEs available during training are used for negative links ( unlinks).</p><p>Let be a mini-batch of CVEs selected randomly. The set CWE( ) denotes the CWEs associated with a vulnerability , and ( ) is the set of all ancestors of the weakness . Similarly, is a set of CWEs available only to the training data. The positive and negative links ( , ) for training are generated as follows:</p><formula xml:id="formula_3">= ? ?CWE ( ) {( , ) ? {( , ) : ? ( )}}; (4) = ? {( , ) : ? randomly in{ ? CWE( )}}. (5)</formula><p>Using Equations 2 and 3, the losses for the ( , ) links from the LP and RD components can be expressed as,</p><formula xml:id="formula_4">= ?? ( , ) ? ( , ) + ( ) + ( ); (6) = ?? ( , ) ? ( , ) + ( ) + ( ).<label>(7)</label></formula><p>Here, and refer to link classification and reconstruction loss, respectively. Since a single CVE can belong only to a few CWEs, only a few positive link pairs are present in a batch compared to the possible negative links. In the loss function, it is necessary to balance and to prevent bias, and this can be prevented either by repeating positive links in a batch or putting more weight on positive links . The total loss, , in a mini-batch of CVEs is given by:</p><formula xml:id="formula_5">= 1 ? + 2 ? .<label>(8)</label></formula><p>The parameters of the model are updated after processing the links from each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CVE to CWE Prediction using V2W-BERT</head><p>V2W-BERT considers the same CWE hierarchy during learning and prediction. CVE data in NVD use only a subset of the CWEs from MITRE, and the hierarchical CWE relations available in NVD omit some of the parent-child relations available in MITRE. Therefore, we use the same 124 CWEs used in NVD, but their hierarchical relationships are enriched using the data from MITRE 4 .</p><p>These 124 CWEs are distributed in three levels in the hierarchy, with 34 in the first level, 78 in the second level, and 16 in the third level. Some CWEs have multiple parents in different levels and are counted twice. At the first level, there are 34 CWEs, and the prediction is made among these 34 CWEs initially. For a single CVE, we create 34 CVE-CWE pairs and get the predicted link values from the Link Prediction (LP) component. The link value with the highest confidence is considered as the CWE prediction. Next, we consider the children of the predicted CWE, and continue until we reach a leaf node.</p><p>To illustrate, <ref type="figure" target="#fig_1">Figure 3</ref> shows a partial hierarchy of CWEs extracted from MITRE. At the first level, there are three CWEs ('CWE-668', 'CWE-404', 'CWE-20'), and prediction will be made among <ref type="bibr" target="#b3">4</ref> Partial CWE hierarchy extracted from MITRE these three at first. If 'CWE-668' is predicted, we predict the next weakness among its three children ('CWE-200', 'CWE-426', 'CWE-427'), and continue until it reaches a leaf node.</p><p>Based on the user preference it is useful to have precise or relaxed prediction. For a precise prediction, we can select the best ( 1 = 1) from first level, the best ( 2 = 1) from second level (if exists), and the best ( 3 = 1) from the third level (if exists). For a relaxed prediction, we can select the top 1 ? 5 confident CWEs from the first level, the top 2 ? 2 from each of their children in the second level, and the best 3 ? 2 from the third level. This type of user-controlled precision is useful to get better confidence about the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We begin by discussing experimental settings for CVE to CWE classification, and then in an ablation study, we evaluate each component of the V2W-BERT framework to investigate how the best performance may be obtained. Finally, we compare the V2W-BERT framework with related approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description</head><p>The Common Vulnerabilities and Exposure (CVE) dataset is collected from the NVD 5 website. After processing and filtering, we get 137, 101 usable CVE entries dating from 1999 to 2020 (May). Among these 82, 382 CVE entries are classified into CWEs. MITRE categorizes CWEs based on Software Development, Hardware Design, and Research Concepts. Research Concepts cover all of the 916 weaknesses, but NVD uses only 124 of these CWEs. We use the same 124 CWEs used in NVD, but also include their hierarchical relations from MITRE.</p><p>We simulate real-world CVE-CWE classification scenarios by temporally partitioning the dataset by years. CVEs from the year 1999-2017 are included in the training set, CVEs of the year 2018 are used as Test Set 1, and CVEs of 2019-2020 are used as Test Set 2. Test Set 1 and Test Set 2 act as a near-future and far-future test cases, respectively. There are 46, 003 instances in training, 14, 176 instances in Test Set 1, and 22, 203 instances in Test Set 2. This temporal split creates a forecasting scenario when future CVEs need to be classified using currently available data, but it makes accurate CVE classification more difficult as CVE description styles change with time, and new CVEs occur in more recent years. We also report results from a random partition of the data (stratified k-fold cross-validation), where we randomly take 70% of the data from each category for training, 10% for validation of early stopping criteria and for hyperparameter settings, and 20% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V2W-BERT Settings</head><p>In the pre-training phase of V2W-BERT, we allow weights of all BERT 6 layers to be updated. The model is trained for 25 epochs with a mini-batch size of 32. In the CVE to CWE association phase, we freeze the first nine out of twelve layers of BERT and allow the last three layers to be updated. The model is trained for 20 epochs with a mini-batch size of 32. The number of random negative links for a CVE is set to 32, and positive links are repeated (or can be weighted) to match the number of negative links to prevent bias. The Adamw <ref type="bibr" target="#b9">[10]</ref> optimizer is used with a learning rate of 2 ?5 , and with warm-up steps of 10% of the total training instances. For training the V2W-BERT algorithm, we used two Tesla P100-PCIE-16GB GPUs and 20 CPUs. V2W-BERT processes about 5 links for a mini-batch of 32 CVEs. For optimization, we compute the pooled representation of the CVE and CWE mini-batches separately, and combine them later as per training links ( , ). For each configuration, the experiments were repeated five times and the results were averaged. The method with the best performance is highlighted in bold in the Tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Process</head><p>The 124 CWEs are distributed in three levels in the MITRE hierarchy, and the CWEs that each CVE belongs to are predicted at each level down the hierarchy. There are 34 first-level CWEs, and each class has three child CWEs on an average, with a maximum of nine. At the second level, each CWE has an average of three child CWEs and a maximum of five. A few examples are provided in <ref type="figure" target="#fig_1">Figure 3</ref>. When reporting performance, we take different top values of CWEs from each level. The choice ( 1 = 1, 2 = 1, 3 = 1) gives precise prediction with only one path in the hierarchy. With moderate precision ( 1 = 3, 2 = 2, 3 = 1), there are at most six possible paths. Finally, a more relaxed prediction can be obtained with ( 1 = 5, 2 = 2, 3 = 2), with at most twenty paths. If the true CWE(s) are present along the predicted paths, the prediction is considered to be accurate. Additionally we use the 1 -score of correctly classified links to evaluate the link prediction performance. <ref type="table" target="#tab_2">Table 1</ref> lists the key notations used in the section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We evaluate each component of the V2W-BERT framework to find the best configuration for solving the problem. Additionally, we show how preserving the pre-trained BERT context using Reconstruction Decoder (RD) improves classification performance in rare and unseen cases. The temporal partition of the dataset is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling and Combine Operations</head><p>Experimental results show that MEAN-Pooling works best among the CLS, MEAN, and MAX pooling operations. When combining the vector representations of a CVE and CWE, concatenation of the absolute difference and multiplication (|x ? y |, x ? y ) performs best, and these two operations are used for further experimentation. Due to page limitations, comparative details of different combination and pooling operations are given in Appendix A.2 and A.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Pre-training and Reconstruction Decoder</head><p>To highlight the contribution of each component, we train V2W-BERT using only Link Prediction (LP) module with BERT BASE as a pre-trained model. This establishes our baseline for comparing the performance of additional pre-training and Reconstruction Decoder (RD). Next, we fine-tune BERT BASE with all labeled and unlabeled CVE/CWE descriptions in the training years and train LP using this updated model. We refer this updated BERT model as BERT CVE . Finally, we have a third experiment that uses LP and RD together using BERT CVE as a pre-trained model. <ref type="figure" target="#fig_3">Fig 4 shows</ref> precise and relaxed prediction accuracy of cases mentioned above. The use of BERT CVE outperforms BERT BASE in both the near and far future as learned cyber-security contexts help to transfer domain knowledge better. The addition of the Reconstruction Decoder (RD) component helps preserve the context of BERT CVE , which improves performance in classifying CVEs of rare and unknown CWE classes, thus improving overall performance. Test 2 has a lower accuracy than Test 1 as we predict two years into the future, containing different descriptions' style. Appendix A.4 shows the quantitative details of these experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction Decoder for Few/Zero-shot Learning</head><p>The Reconstruction Decoder (RD) component helps preserve the context of BERT CVE , which improves performance in classifying CVEs of rare and unknown CWE classes. We evaluate LP with and without the RD to highlight the improvement. We consider the CVEs of CWEs that appear in the test set but not in the training set or have few instances. We call these two cases zero-shot and few-shot, respectively. We use BERT CVE as the pre-trained model for experimentation. Zero-shot Performance: We removed all CVEs of the descendants and ancestors of these unseen CWEs from the training process to avoid any bias for zero-shot evaluation. <ref type="table" target="#tab_3">Table 2</ref> shows that the addition of Reconstruction Decoder (RD) improves the accuracy for unseen cases. The precise and relaxed prediction accuracies are evaluated for the CWEs that were absent during training. Here, "Test 1 ( 1 , 2 , 3 ), 89" refers to 89 CVEs instances in year 2018 whose corresponding CWEs were unavailable during training. The precise accuracy is relatively low but significantly higher than random prediction. For relaxed prediction, we get about (86% accuracy for Test 1 and (61% for Test 2 (illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>). The performance of predicting unseen CVEs completely depends on inherent textual similarities between a CVE and CWE description. Few-shot Performance: <ref type="table" target="#tab_4">Table 3</ref> shows the performance of CVEs where the corresponding CWEs have total training instances between ([ 1 , 2 ]). The "Test 1, = [1, 50], 1057" refers to 1057 test CVE instances from 2018 whose corresponding CWEs had training examples between 1 to 50. With addition of RD, the model achieves significantly higher precise-prediction accuracy than Link Prediction (LP) alone. The model achieves 71%-84% prediction accuracy in 2018 when we have only 51 ? 100 training instances in the past <ref type="bibr">(1999)</ref><ref type="bibr">(2000)</ref><ref type="bibr">(2001)</ref><ref type="bibr">(2002)</ref><ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref><ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref><ref type="bibr">(2010)</ref><ref type="bibr">(2011)</ref><ref type="bibr">(2012)</ref><ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref><ref type="bibr">(2015)</ref><ref type="bibr">(2016)</ref><ref type="bibr">(2017)</ref>. This improvement in rare cases is significant compared to related work, as detailed in ?4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Related Approaches</head><p>We compare the performance of the V2W-BERT framework (using settings from ?4.2) with related work. V2W-BERT is compared against two classification methods and a link association approach similar to ours. We compare with two classification approaches, a TF-IDF based Neural Network (NN) <ref type="bibr" target="#b0">[1]</ref> and a fine-tuned BERT classifier (this work). While fine-tuning the BERT classifier, we use the same pre-trained BERT CVE algorithm and MEAN -Pooling as with V2W-BERT. Custom layers with dropout and fully connected Neural Networks are added on top of the pooling layer to predict all usable CWEs. Additionally, we implement a TF-IDF feature-based link association method to train the model . We use the TF-IDF feature directly and use the same (|x ? y |, x ? y ) combination operation and classification layer as we did in V2W-BERT. The training links are also kept same as V2W-BERT. We highlight the classification and link prediction based method with prefix 'Class' and 'Link' in the table.</p><p>Performance in the random partition of the dataset <ref type="table" target="#tab_5">Table 4</ref> shows the comparative performance of the related methods. We take 70% of the data for training from each category, 10% for validation for hyper-parameter settings, and 20% for testing. With more training data and examples overlapping all years, V2W-BERT and achieves 89% ? 97% precise and relaxed prediction accuracies. Performance in the temporal partition of the dataset Unlike random partition, where we have taken training examples from each category, temporal partition is more challenging and reflective of the application. <ref type="table" target="#tab_6">Table 5</ref> compares the accuracy of V2W-BERT trained with data from 1999-2017, and tested for 2018 (Test 1) and 2019-2020 (Test 2). Key results are illustrated in <ref type="figure">Figure 5</ref>. To highlight the performance of CVEs of rare and frequently occurring CWEs, we split the test sets by CWEs having 1 ? 100 training examples, and by CWEs with more than a hundred training examples. The V2W-BERT outperforms the competing approaches in both precise and relaxed predictions, overall as well as in rare and frequently occurring cases. For CWEs with ? 100 training instances, V2W-BERT achieves 89% ? 98% precise and relaxed prediction accuracy in Test 1 (2018). The performance on Test 2 data is lower than that of Test 1, since the former is further into the future. To demonstrate sustainability of V2W-BERT, we experimented by adding recent data (from 2018) for training, and it improves the performance on Test 2 data (Appendix A.5).  <ref type="figure">Figure 5</ref>: A summary of the key results for Test 1 (T1) showing superior performance of V2W-BERT with respect to other approaches, especially for rare CWEs classes. Details are provided in <ref type="table" target="#tab_6">Table 5</ref>. F 1 -Score of predicted links: We evaluate both link and unlink pairs that are correctly classified. Only the two link-based methods (V2W-BERT and Link, TF-IDF NN) predict links. V2W-BERT achieves 1 -Scores of 0.93 for Test 1, and 0.92 for Test 2, where as TF-IDF NN achieves 0.91 and 0.88 respectively ( ?A.6). Performance of predicting links is higher than the precise CWE predictions since predicting a CWE accurately down to the leaf node requires all links to the ancestor to be correctly predicted.</p><p>Zero-shot performance of link methods: <ref type="table" target="#tab_7">Table 6</ref> captures classification performance of CVEs associated with CWEs not seen in training. Only the link-based methods are compared since classificationbased approaches do not support this task. The link-based TF-IDF NN performs worse than random choice since it is over-fitted to the available training CWEs. Predicting a new CWE definition For a given CVE, V2W-BERT gives link and unlink values to all available CWEs. If the link value is higher than unlink, we consider the CVE to be associated with that CWE. The link value represents the confidence about the association of a vulnerability to a weakness. We can push this confidence boundary for a more robust prediction and consider the link only if the value is greater than a threshold . For a CVE description, if all link values to the available CWEs are less than , then the CVE description has a different style, or we need a new CWE definition. Appendix A.7 shows experimental evidence where we get most occurrences of all unlinks in the case of unseen CWEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Future Work</head><p>We presented a Transformer-based framework (V2W-BERT) to efficiently map CVEs (specific vulnerability reports) to hierarchically structured CWEs (weakness descriptions). Using data from standard sources, we demonstrated high quality results that outperform previous efforts. We also demonstrated that our approach not only performs well for CWE classes with abundant data, but also for rare CWE classes with little or no data to train. Since classifying rare CWEs has been an explored problem in literature, our framework provides a promising novel approach towards a viable practical solution to efficiently classify increasing more and diverse software vulnerabilities. We also demonstrated that our framework can learn from historic data and predict new information that has not been seen before. Our future work will focus on scaling larger pre-trained BERT models with high-performance computing platforms to further enhance the classification performance, and automated suggestions for defining new weaknesses to match novel vulnerabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In the appendix, we discuss in more detail some components of the V2W BERT framework. We allow all layers of BERT to update in this step as we are learning the relevant cyber-security context. A custom Language Model (LM) layer is added on top of the BERT encoder, which takes the last hidden state tensor from the BERT encoder and then passes that to a linear layer of input-output size ( , ). Then layer normalization is performed, and values are passed to a linear layer with an input-output feature size ( , vocab ) to predict masked tokens. The cross-entropy loss is used on the predicted masked tokens to optimize the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Masked Language Model for Pre-training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Link Prediction (LP) with Different Combination Operations</head><p>Following recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, the V2W BERT is evaluated by different combination operations. For simplicity, only the Link Prediction (LP) component is used with CLS-pooling. The BERT BASE is used as the pre-trained model for experimentation, and experiments are run for ten epochs only. <ref type="table" target="#tab_8">Table 7</ref> shows comparative performance of some combination operations. The concatenation operation (x, y) does not achieve good performance, but multiplication, (x ? y), performs better than absolute difference, (|x?y|). Their combination (|x?y|, x?y) shows the overall best performance, and is used for further experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Link Prediction (LP) with different Pooling operations</head><p>Reimers et. al. <ref type="bibr" target="#b16">[17]</ref> have shown that other pooling operations can outperform CLS-Pooling. In this work, we have investigated V2W BERT with three pooling operations, CLS-pooling, MAX-pooling, and MEAN-pooling. <ref type="table" target="#tab_9">Table 8</ref> shows comparative performance of different BERT poolers with (|x ? y |, x ? y ) as the combination operation. BERT BASE is used as the pre-trained model and the experiments are run for ten epochs only. MEAN-pooling has shown marginally better performance than CLS-Pooling, and is used for V2W BERT . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Link Prediction (LP) and Reconstruction</head><p>Decoder (RD) with different pre-trained models. <ref type="table" target="#tab_10">Table 9</ref> shows precise and relaxed prediction accuracy of the three scenarios of V2W-BERT: 1) Link Prediction (LP) component with BERT BASE as pre-trained model, 2) LP with fine tuned BERT using with CVE/CWE descriptions (BERT CVE ), 3) LP with Reconstruction Decoder (RD) using BERT CVE as pre-trained model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Training on 1999-2018</head><p>We have performed additional training with CVEs from the year 2018 to predict Test 2 (2019-2020). As expected, recent data improves the performance of the immediate future predictions. <ref type="figure" target="#fig_7">Fig 7 shows</ref> prediction accuracy improvement of V2W-BERT in Test 2 (2019-2020) with additional training data from 2018 and <ref type="table" target="#tab_2">Table 10</ref> shows comparative details.  A.6 1 -Scores of predicted links <ref type="table" target="#tab_2">Table 11</ref> shows the link prediction performance of the V2W-BERT algorithm and the TF-IDF based link prediction method.  Here "Test 1 (1-100)" refers to CVEs associated with CWEs in Test Set 1 with total training instances between 1-100. As expected, CVEs of unseen CWEs have the highest fraction of occurrences, because these CVEs have different styles not seen by training method. Also, the rare type CVEs have higher unlinks to links ratio than frequent ones. Therefore, if we see only high unlink values to CWEs for some CVE description, we could suggest that experts take a closer look at the description, and if needed provide a new CWE.  <ref type="table" target="#tab_2">Table 12</ref> shows how many times we get all link values less than = 0.90, and the fraction of such instances. We partition the Test sets based on the number of CVEs per CWE class in training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Data Augmentation to handle Class Imbalance</head><p>We experimented with data augmentation <ref type="bibr" target="#b20">[21]</ref> techniques to handle class imbalance during training. New CVE descriptions are created from the available training CVE descriptions. For CWEs with less than 500 training instances, we gather all text descriptions of the associated CVEs to create a pool of CVE sentences. We take random sentences from the pool of sentences, replace some words with synonyms, and create augmented CVEs description. <ref type="table" target="#tab_2">Table 13</ref> shows performance comparison before and after the augmentation. Augmentation makes overall convergence faster but achieves similar performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the architecture of V2W-BERT framework with the Link Prediction module (shown in the middle) and the Reconstruction Decoder modules (shown on the left and right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Partial hierarchy of CWE extracted from MITRE to demonstrate how precise and relaxed prediction is performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Precise and relaxed prediction accuracy for different components of V2W-BERT. Left: All data. Right: CVEs with unseen (zero-shot) CWEs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 6</head><label>6</label><figDesc>shows a simplistic view of fine-tuning BERT with Masked LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>CVE and CWE descriptions, , BERT, Layer 1 to , Trainable Linear, (input, output) ( , ) Layer Normalization Linear, (input, output) ( , ) CE Loss on masked tokens Last Hidden State Tensor, ( , ) Architecture of Masked Language Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Accuracy of Test 2 before and after adding data from the year 2018 in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 8</head><label>8</label><figDesc>shows fraction of instances we get all link values less than = 0.90.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The fraction of occurrences of all unlinks with link threshold set to = 0.90 in different scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Key notations used in the section</figDesc><table><row><cell>Notation</cell><cell>Meaning</cell></row><row><cell>BERT BASE</cell><cell>Original pre-trained BERT model [7]</cell></row><row><cell>BERT CVE</cell><cell>Additional pre-training with CVE/CWE descriptions</cell></row><row><cell>LP</cell><cell>Link Prediction component only</cell></row><row><cell>LP+RD</cell><cell>Link Prediction coupled with Reconstruction Decoder</cell></row><row><cell cols="2">V2W-BERT LP+RD, with BERT CVE</cell></row><row><cell>&gt;</cell><cell>CVEs from CWEs with more than training instances</cell></row><row><cell>[ 1 , 2 ]</cell><cell>CVEs from CWEs with training instances between 1 to 2</cell></row><row><cell>( 1 , 2 , 3 )</cell><cell>Top 1 , 2 , 3 predictions for the -th level in the hierarchy</cell></row><row><cell>Test 1</cell><cell>Test instances from 2018 (near-future)</cell></row><row><cell>Test 2</cell><cell>Test instances from 2019-2020 (far-future)</cell></row><row><cell>Link</cell><cell>Formulated as link prediction problem</cell></row><row><cell>Class</cell><cell>Formulated as classification problem</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Zero-shot accuracy with and without RD</figDesc><table><row><cell>Model</cell><cell>Test 1 ( 1 , 2 , 3 ), 89</cell><cell>Test 2 ( 1 , 2 , 3 ), 247</cell></row><row><cell></cell><cell cols="2">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell cols="3">Random 0.0032 0.0196 0.0653 0.0032 0.0196 0.0653</cell></row><row><cell>LP</cell><cell cols="2">0.1263 0.5454 0.8483 0.0273 0.2568 0.5902</cell></row><row><cell>LP+RD</cell><cell cols="2">0.2809 0.6954 0.8558 0.1012 0.3475 0.6104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Few-shot accuracy evaluated for rare CWE classes with different training instances between [ 1 , 2 ]</figDesc><table><row><cell>Model</cell><cell cols="2">Test 1, n=[1, 50], 1057</cell><cell cols="2">Test 2, n=[1, 50], 2632</cell></row><row><cell cols="4">( 1 , 2 , 3 ) (1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1)</cell><cell>(5,2,2)</cell></row><row><cell>LP</cell><cell>0.2142 0.4991</cell><cell>0.671</cell><cell>0.2462 0.5151</cell><cell>0.6306</cell></row><row><cell>LP+RD</cell><cell cols="4">0.3199 0.6176 0.705 0.2474 0.5569 0.6736</cell></row><row><cell></cell><cell cols="2">Test 1, n=[51, 100], 800</cell><cell cols="2">Test 2, n=[51, 100], 1221</cell></row><row><cell>LP</cell><cell cols="3">0.5687 0.8075 0.8400 0.5652 0.7771</cell><cell>0.8054</cell></row><row><cell>LP+RD</cell><cell cols="3">0.7087 0.8087 0.8375 0.6457 0.7870</cell><cell>0.8035</cell></row><row><cell></cell><cell cols="4">Test 1, n=[101, 150], 690 Test 2, n=[101, 150], 1643</cell></row><row><cell>LP</cell><cell cols="3">0.6645 0.8373 0.9097 0.4221 0.6605</cell><cell>0.7639</cell></row><row><cell>LP+RD</cell><cell cols="4">0.7238 0.8475 0.9222 0.5091 0.6648 0.7849</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance with randomly partitioned dataset</figDesc><table><row><cell>Model</cell><cell>Test Set ( 1 , 2 , 3 )</cell></row><row><cell></cell><cell>(1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell cols="2">Class, TF-IDF NN 0.8606 0.9464 0.9668</cell></row><row><cell cols="2">Link, TF-IDF NN 0.8642 0.9502 0.9693</cell></row><row><cell>Class, BERT CVE</cell><cell>0.8812 0.9503 0.9689</cell></row><row><cell cols="2">Link, V2W-BERT 0.8916 0.9523 0.9723</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of V2W-BERT Model Test 1 ( 1 , 2 , 3 ) Test 2 ( 1 , 2 , 3 )</figDesc><table><row><cell></cell><cell cols="3">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell></cell><cell cols="3">Class, TF-IDF NN 0.2631 0.5656 0.6537 0.2519 0.4838 0.5739</cell></row><row><cell>1-100</cell><cell cols="3">Link, TF-IDF NN 0.3626 0.5998 0.6791 0.3395 0.564 Class, BERT CVE 0.4138 0.6602 0.7466 0.2914 0.6105 0.6902 0.659</cell></row><row><cell></cell><cell cols="3">Link, V2W-BERT 0.4765 0.6933 0.7564 0.4072 0.6293 0.7179</cell></row><row><cell></cell><cell cols="3">Class, TF-IDF NN 0.8524 0.9425 0.9616 0.7815 0.8953 0.9404</cell></row><row><cell>&gt;100</cell><cell cols="3">Link, TF-IDF NN 0.8463 0.9227 0.9485 0.7604 0.8738 0.9153 Class, BERT CVE 0.8852 0.9479 0.9649 0.8067 0.9064 0.9414</cell></row><row><cell></cell><cell cols="2">Link, V2W-BERT 0.8905 0.947</cell><cell>0.9763 0.8113 0.9123 0.9492</cell></row><row><cell></cell><cell>Class, TF-IDF NN 0.775</cell><cell>0.893</cell><cell>0.9298 0.6886 0.8231 0.8761</cell></row><row><cell>All</cell><cell cols="3">Link, TF-IDF NN 0.7828 0.8803 0.9132 0.6863 0.8196 0.8706 Class, BERT CVE 0.8232 0.9101 0.9363 0.7163 0.8578 0.9038</cell></row><row><cell></cell><cell cols="2">Link, V2W-BERT 0.8362 0.914</cell><cell>0.9442 0.7345 0.8594 0.9151</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Zero-shot accuracy of link-based methods</figDesc><table><row><cell>Model</cell><cell>Test 1 ( 1 , 2 , 3 ), 89</cell><cell>Test 2 ( 1 , 2 , 3 ), 247</cell></row><row><cell></cell><cell cols="2">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell>Random</cell><cell cols="2">0.0032 0.0196 0.0653 0.0032 0.0196 0.0653</cell></row><row><cell cols="3">Link, TF-IDF NN 0.0000 0.1158 0.4875 0.0000 0.0562 0.1717</cell></row><row><cell cols="3">Link, V2W-BERT 0.2809 0.6954 0.8558 0.1012 0.3475 0.6104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Accuracy of Link Prediction (LP) component over different combination operations.</figDesc><table><row><cell>Combination</cell><cell>Test 1 ( 1 , 2 , 3 )</cell><cell cols="3">Test 2 ( 1 , 2 , 3 )</cell></row><row><cell></cell><cell cols="4">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell>(x, y)</cell><cell cols="4">0.2631 0.5401 0.6517 0.2237 0.5063 0.6288</cell></row><row><cell>(|x ? y|)</cell><cell>0.6471 0.8816 0.9175</cell><cell>0.544</cell><cell cols="2">0.8237 0.8742</cell></row><row><cell>(x ? y)</cell><cell cols="4">0.7657 0.8885 0.9279 0.6897 0.8395 0.8953</cell></row><row><cell>(|x ? y|, x ? y)</cell><cell cols="4">0.7829 0.8794 0.9209 0.6995 0.8337 0.8879</cell></row><row><cell>(x, y, x ? y)</cell><cell cols="4">0.7628 0.8846 0.9225 0.6915 0.8411 0.8880</cell></row><row><cell>(x, y, |x ? y|)</cell><cell cols="2">0.7769 0.8828 0.9233 0.6839</cell><cell>0.822</cell><cell>0.8823</cell></row><row><cell cols="5">(x, y, |x ? y|, x ? y) 0.7815 0.8827 0.9211 0.6833 0.8203 0.8766</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Accuracy of Link Prediction (LP) component over different pooling approaches.</figDesc><table><row><cell>Pooling</cell><cell cols="3">Test 1 ( 1 , 2 , 3 )</cell><cell cols="3">Test 2 ( 1 , 2 , 3 )</cell></row><row><cell></cell><cell cols="6">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell>CLS-Pooling</cell><cell cols="6">0.7829 0.8794 0.9209 0.6995 0.8337 0.8879</cell></row><row><cell>MAX-Pooling</cell><cell>0.7592</cell><cell>0.872</cell><cell cols="2">0.9175 0.6705</cell><cell>0.818</cell><cell>0.8748</cell></row><row><cell cols="7">MEAN-Pooling 0.782 0.8886 0.9244 0.6874 0.8364 0.8897</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Prediciton accuracy of LP and RD components with different pre-trained models. BERT CVE 0.8310 0.9144 0.9425 0.7274 0.8592 0.9051</figDesc><table><row><cell>Model</cell><cell>Test 1 ( 1 , 2 , 3 )</cell><cell>Test 2 ( 1 , 2 , 3 )</cell></row><row><cell></cell><cell cols="2">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell>LP, BERT BASE</cell><cell cols="2">0.7829 0.8794 0.9209 0.6995 0.8337 0.8879</cell></row><row><cell>LP, BERT CVE</cell><cell cols="2">0.8169 0.9137 0.9429 0.7132 0.8505 0.9049</cell></row><row><cell>LP+RD,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Accuracy of Test 2 including 2018 in the training. Model Test 2 ( 1 , 2 , 3 ) (1,1,1) (3,2,1) (5,2,2) Class, TF-IDF NN 0.7109 0.8444 0.8962 Link, TF-IDF NN 0.7302 0.8636 0.9162 Class, BERT CVE 0.7527 0.8683 0.9090 Link, V2W-BERT 0.7666 0.8901 0.9273</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>1 -score of correctly predicted links.</figDesc><table><row><cell>Model</cell><cell>1 -score</cell><cell></cell></row><row><cell></cell><cell cols="2">Test 1 (2018) Test 2 (2019-2020)</cell></row><row><cell>Link, TF-IDF NN</cell><cell>0.9095</cell><cell>0.8816</cell></row><row><cell>Link, V2W-BERT</cell><cell>0.9343</cell><cell>0.9156</cell></row><row><cell cols="3">A.7 Predicting a new CWE definition</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Count of how many times all link values of a CVE to available CWEs are less than = 0.90 in different scenarios.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Count #Instances Fraction of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Occurrences</cell></row><row><cell>Test 1, 1-100</cell><cell>189</cell><cell>1,851</cell><cell>0.1021</cell></row><row><cell>Test 1, &gt;100</cell><cell>372</cell><cell>12,236</cell><cell>0.0304</cell></row><row><cell>Test 2, 1-100</cell><cell>357</cell><cell>3,851</cell><cell>0.0927</cell></row><row><cell>Test 2, &gt;100</cell><cell>823</cell><cell>18,105</cell><cell>0.0454</cell></row><row><cell>Unseen CWEs</cell><cell>117</cell><cell>377</cell><cell>0.3103</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Performance of V2W BERT before and after data augmentation. Aug500 0.8299 0.9138 0.9425 0.7374 0.8584 0.9107</figDesc><table><row><cell>Model</cell><cell>Test 1 ( 1 , 2 , 3 )</cell><cell>Test 2 ( 1 , 2 , 3 )</cell></row><row><cell></cell><cell cols="2">(1,1,1) (3,2,1) (5,2,2) (1,1,1) (3,2,1) (5,2,2)</cell></row><row><cell>V2W BERT</cell><cell cols="2">0.8362 0.914 0.9442 0.7345 0.8594 0.9151</cell></row><row><cell>V2W BERT ,</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://cwe.mitre.org 2 https://cwe.mitre.org/data/definitions/89.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://nvd.nist.gov/ 6 https://huggingface.co/bert-base-uncased</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ThreatZoom: neural network for automated vulnerability mitigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Aghaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehab</forename><surname>Al-Shaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Annual Symposium on Hot Topics in the Science of Security</title>
		<meeting>the 6th Annual Symposium on Hot Topics in the Science of Security</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automation of Vulnerability Classification from its Description using Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Aota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Kanehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Symposium on Computers and Communications (ISCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Universal Sentence Encoder</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Chicco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="73" to="94" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How is Security Testing Done in Agile Teams? A Cross-Case Analysis of Four Software Teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Soares Cruzes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felderer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XP (Lecture Notes in Business Information Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">283</biblScope>
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Tosin Daniel Oyetoyan, Matthias Gander, and Irdin Pekaric</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to predict severity of software vulnerability using only vulnerability description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuobing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenchang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Vulnerabilities in OpenSSL and the Linux Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Traon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd Asia-Pacific Software Engineering Conference (APSEC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TwinBERT: Distilling knowledge to twin-structured BERT models for efficient retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06275</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Common Weakness Enumeration (CWE) Status Update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Barnum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ada Lett</title>
		<imprint>
			<biblScope unit="page" from="88" to="91" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study on the classification of common vulnerabilities and exposures using na?ve bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarang</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwankuk</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Broadband and Wireless Computing, Communication and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="657" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-level convolutional neural network for predicting severity of software vulnerability from vulnerability description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Kanehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Furumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Takita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakatu</forename><surname>Morii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="1679" to="1682" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Security trend analysis with cve topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE 21st International Symposium on Software Reliability Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software design level vulnerability classification model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shabana</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Mustafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Security (IJCSS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">238</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cyber threat intelligence sharing: Survey and research directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">E</forename><surname>Abdallah</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cose.2019.101589</idno>
		<ptr target="https://doi.org/10.1016/j.cose.2019.101589" />
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">101589</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<title level="m">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
