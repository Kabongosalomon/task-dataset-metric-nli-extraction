<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COMISR: Compression-Informed Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
							<email>yinxiao@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
							<email>pengchong@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
							<email>fengyang@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
							<email>celiu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>minghsuan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
							<email>milanfar@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COMISR: Compression-Informed Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compressioninformed video super-resolution model to restore highresolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. The source codes and trained models are available at https://github.com/google-research/googleresearch/tree/master/comisr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-resolution is a fundamental research problem in computer vision with numerous applications. It aims to reconstruct detailed high-resolution (HR) image(s) from lowresolution (LR) input(s). When the input is one single image, the reconstruction process usually uses learned image priors to recover high-resolution details of the given image, which is called single-image super-resolution (SISR) <ref type="bibr" target="#b55">[56]</ref>. When numerous frames in a video are available, the re- COMISR <ref type="figure">Figure 1</ref>. Video super-resolution results (4?, RGB-channels) on compressed Vid4 and REDS datasets. Here we show the results using the most widely adopted compression rate (CRF 23 <ref type="bibr" target="#b9">[10]</ref>).</p><p>construction process uses both image priors and inter-frame information to generate temporally smooth high-resolution results, which is known as video super-resolution (VSR).</p><p>Although great progress has been made, existing SISR and VSR methods rarely take compressed images as input. We note that the uncompressed videos used in prior work in fact are high-quality image sequences with low compression rate. As such, these SR methods tend to generate significant artifacts when operating on heavily compressed images or videos. However, most videos on the web or mobile devices are stored and streamed with images compressed at different levels. For example, a wide-used compression rate (Constant Rate Factor (CRF)) for H.264 encoding is 23 as a trade-off between visual quality and file size. We note the state-of-the-art VSR algorithms do not perform well when the input videos are compressed.</p><p>To handle compressed videos, one potential solution is to first denoise images and remove compression artifacts in images <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b57">58]</ref> before applying one of the state-of-theart VSR models. At first glance, this is appealing since a VSR model is fed with high-quality frames, similar to directly using the evaluation data, such as Vid4 <ref type="bibr" target="#b31">[32]</ref>. However, our experiments in Section 4.3 show that this approach would not improve SR results and instead negatively affect the visual quality. With pre-processing, it is likely that the denoising model in the first step will be significantly differ-ent from the degradation kernel used implicitly during the VSR training process. After the denoising process, the VSR models effectively need to handle more challenging images.</p><p>Another possible solution is to train the existing state-ofthe-art VSR models on the compressed images. This will enforce the VSR models to account for compression artifacts during the training process. However, our experiments described in Section 4.5 show that simply using compressed frames in model training brings only modest improvement. In fact, without specific changes to the designs of network modules, such training data may even negatively affect the overall performance.</p><p>To address the above-mentioned issues, we propose a compression-informed (i.e., compression-aware) superresolution model that can perform well on real-world videos with different levels of compression. Specifically, we design three modules to robustly restore the missing information caused by video compression. First, a bi-directional recurrent module is developed to reduce the accumulated warping errors from the random locations of the intra-frame from compressed video frames <ref type="bibr" target="#b45">[46]</ref>. Second, a detail-aware flow estimation module is introduced to recover HR flow from compressed LR frames. Finally, a Laplacian enhancement module is adopted to add high-frequency information to the warped HR frames washed out by video encoding. We refer to this proposed model as COMpression-Informed video Super-Resolution (COMISR).</p><p>With the proposed COMISR model, we demonstrate the effectiveness of these modules with ablation studies. We conduct extensive experiments on several VSR benchmark datasets, including Vid4 <ref type="bibr" target="#b31">[32]</ref> and REDS4 <ref type="bibr" target="#b40">[41]</ref>, using videos compressed with different CRF values. We show that the COMISR model achieves significant performance gain on compressed videos (e.g., CRF23), as shown in <ref type="figure">Figure 1</ref>, and meanwhile maintains competitive performance on uncompressed videos. In addition, we present evaluation results based on different combinations of a state-of-the-art VSR model and an off-the-shelf video denoiser. Finally, we validate the robustness of the COMISR model on YouTube videos, which are compressed with proprietary encoders.</p><p>The contributions of this paper can be summarized as:</p><p>? We introduce a compression-informed model for super-resolving real-world compressed videos and achieve state-of-the-art performance. ? We incorporate three modules that are novel to VSR to effectively improve critical components for video super-resolution on compressed frames. ? We conduct extensive experiments of state-of-the-art VSR models on compressed benchmark datasets. We also present a new setting for evaluating VSR models on YouTube transcoded videos, which is a real-world application scenario that existing evaluation methods do not consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A plethora of super-resolution methods have been developed in the literature based on variational formulations <ref type="bibr" target="#b60">[61]</ref> or deep neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62]</ref>. In this section, we discuss recent deep models closely related to our work for super-resolution.  <ref type="bibr" target="#b46">[47]</ref> model with an efficient subpixel convolution layer at the end of the network. In the LatticeNet method <ref type="bibr" target="#b37">[38]</ref>, a light-weighted model is developed by using a lattice block, which reduces half amount of the parameters while maintaining similar SR performance. To learn SR models at multiple scales efficiently, Lai et al. <ref type="bibr" target="#b26">[27]</ref> develop the LapSRN model which progressively recovers the sub-band residuals of high-resolution images. Instead of relying on deeper models, the Mem-Net <ref type="bibr" target="#b47">[48]</ref> introduce memory block to exploit long-term dependency for effective SR models. On the other hand, the SRDenseNet <ref type="bibr" target="#b49">[50]</ref> and RDN <ref type="bibr" target="#b67">[68]</ref> are proposed for SISR based on the DenseNet <ref type="bibr" target="#b18">[19]</ref> model with dense connections. Haris et al. <ref type="bibr" target="#b14">[15]</ref> design a deep back-projection network for super-resolution by exploiting iterative up-sampling and down-sampling layers. In <ref type="bibr" target="#b13">[14]</ref>, the DSRN introduces a dual-state recurrent network model to reduce memory consumption for SISR. The MSRN <ref type="bibr" target="#b28">[29]</ref> and RFA <ref type="bibr" target="#b32">[33]</ref> models use different blocks to efficiently exploit image features. Recently, attention mechanisms have also been used to improve the super-resolution image quality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-image Super-resolution</head><p>Aside from deep neural network models, generative adversarial networks (GANs) have been adopted for SISR, including SRGAN <ref type="bibr" target="#b27">[28]</ref>, EnhanceNet <ref type="bibr" target="#b43">[44]</ref>, ESRGAN <ref type="bibr" target="#b54">[55]</ref>, SPSR <ref type="bibr" target="#b38">[39]</ref> and SRFlow <ref type="bibr" target="#b36">[37]</ref>. These methods typically generate visual pleasing results by using adversarial losses <ref type="bibr" target="#b11">[12]</ref> or normalizing flows <ref type="bibr" target="#b42">[43]</ref>. In addition, several models have been developed for SISR based on degrated closer to the real-world scenarios <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Super-resolution</head><p>Video super-resolution is a more challenging problem than SISR as both content and motion need to be effectively predicted. The motion information provides additional cues in restoring high-resolution frames from multiple low-resolution images.  Sliding-window methods. Multi-frame super-resolution methods potentially can restore more high-resolution details of target frames as more visual information is available. On the other hand, these methods need to account for motion content between frames for high quality SR results. A number of models compute optical flows between multi-frames to aggregate visual information. Xue et al. <ref type="bibr" target="#b59">[60]</ref> introduce a task-oriented flow estimation method together with a video processing network for denoising and super-resolution. Haris et al. <ref type="bibr" target="#b15">[16]</ref> use multiple backprojected features for iterative refinement rather than explicitly aligning frames. Recently, deformable convolution networks <ref type="bibr" target="#b3">[4]</ref> have been developed to tackle feature misalignment in dense prediction tasks. Both EDVR <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> and TDAN <ref type="bibr" target="#b48">[49]</ref> use deformable convolution models to align features from video frames for video super-resolution.</p><p>Haris et al. <ref type="bibr" target="#b16">[17]</ref> design a model that leverages mutually informative relationships between time and space to increase spatial resolution of video frames and interpolate frames to increase the frame rate. In <ref type="bibr" target="#b62">[63]</ref>, Yi et al. propose a model that use non-local blocks to fuse spatial-temporal information from multiple frames. Recenlty, Li et al. <ref type="bibr" target="#b29">[30]</ref> present a mutli-correspondence network model to exploit spatial and temporal correlation between frames to fuse intra-frame as well as iner-frame information for video SR.</p><p>Recurrent models. Recurrent neural networks have been widely used for numerous vision tasks, such as classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>, detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref>, and segmentation <ref type="bibr" target="#b51">[52]</ref>. Such network models can process inputs of any length by sharing model weights across time. In addition, recurrent models can account for long-range dependence among pixels. A number of VSR models have been developed based on recurrent neural networks in recent years. The FRVSR <ref type="bibr" target="#b44">[45]</ref> model stores the previous information in a HR frame for restoring the current frame in a sequence. Fuoli <ref type="bibr" target="#b10">[11]</ref> use a recurrent latent space to encode and propagate temporal information among frames for video super-resolution. Most recently, the RSDN model <ref type="bibr" target="#b21">[22]</ref> incorporates a structurepreserving module into a recurrent network and achieves state-of-the-art performance for restoring details from LR frames without relying on motion compensation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The COMISR model is designed based on a recurrent formulation. Similar to the state-of-the-art video SR methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref>, it feeds visual information from the previous frames to the current one. The recurrent models usually entail low memory consumption, and can be applied to numerous inference tasks in videos. <ref type="figure" target="#fig_2">Figure 2</ref> shows an overview of the COMISR model. We develop three modules, i.e., bi-directional recurrent warping, detail-aware flow estimation, and Laplacian enhancement modules, to effectively super-resolve compressed videos. Given the LR ground truth frames, we use the forward and backward recurrent modules to generate the HR frame predictions, and compute content losses against HR ground truth frames in both directions. In the recurrent module, we predict flows and generate warped frames in both LR and HR, and train the network end to end using the LR and HR ground truth frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bi-directional Recurrent Module</head><p>One common approach for video compression is to apply different algorithms to compress and encode frames at different positions in the video stream. Typically, a codec randomly selects several reference frames, known as the intra-frames, and compresses them independently without using information from other frames. It then compresses the other frames by exploiting consistency and encoding differences from the intra-frames. As a result, the intraframes usually require more bits to encode and have less compression artifacts than the other frames. Since the locations of intra-frames is not known in advance, to effectively reduce the accumulated errors from the unknown locations of intra-frames for video super-resolution, we propose a bi-directional recurrent network to enforce the forward and backward consistency of the LR warped inputs and HR predicted frames.</p><p>Specifically, the bi-directional recurrent network consists of symmetric modules for forward and backward directions. In the forward direction, we first estimate both the LR flow F LR t?1?t and HR one F HR t?1?t using the LR frames I LR t?1 and I LR t (described in Section 3.2). We then apply different operations separately in LR and HR streams. In the LR stream, we warp the previous LR frame I LR t?1 to time t using F LR t?1?t to obtain the warped LR frame? LR t , which will be used at later stages:</p><formula xml:id="formula_0">I LR t = W arp(I LR t?1 , F LR t?1?t ).<label>(1)</label></formula><p>In the HR stream, we warp the previous predicted frame? I HR t?1 to time t using F HR t?1?t to obtain the warped HR fram? I HR t , followed by a Laplacian Enhancement Module to generate accurate HR warped frame:</p><formula xml:id="formula_1">I HR,W arp t = W arp(? HR t?1 , F HR t?1?t ),<label>(2)</label></formula><formula xml:id="formula_2">I HR t = Laplacian(? HR,W arp t ) +? HR,W arp t .<label>(3)</label></formula><p>We then apply a space-to-depth operation on? HR t to shrink back its resolution while expanding its channel, fuse it with the LR input I LR t and pass the concatenated frame to the HR frame generator to predict the final HR image? HR t . We compare? HR t with the ground truth HR I HR t to measure the loss.</p><p>Similarly, we apply the symmetric operations in the backward direction to obtain the warped LR frame and the predicted HR frame. In this case, the detail-aware flow estimation module generates the backward flow from time t to t ? 1, and images are warped by applying the backward flow to the frame at time t for estimating the frame at time t ? 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Detail-aware Flow Estimation</head><p>In our recurrent module, we explicitly estimate both the LR and HR flows between neighboring frames and pass this information in forward and backward directions.</p><p>Here we take the forward direction for illustration. The operations in the backward direction are similarly applied. We first concatenate two neighboring LR frames I LR t?1 and I LR t and pass it through the LR flow estimation network to estimate the LR flow F LR t?1?t . Instead of directly upsampling the LR flow F LR t?1?t , we add a few additional deconvolution layers on top of the bilinearly upsampled LR flow. Thus, a detailed residual map is learned during the end-toend training, and we can better preserve high-frequency details in the predicted HR flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Laplacian Enhancement Module</head><p>The Laplacian residual has been widely used in numerous vision tasks, including image blending, superresolution, and restoration. It is particularly useful at finding fine details from a video frame, where such details could be smoothed out during video compression. In our recurrent VSR model, the warped predicted HR frame retains detailed texture information learned from the previous frames. Such details can be easily missing from the up-scaling network, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. As such, we add a Laplacian residual to a predicted HR frame to enhance details.</p><p>An image is enhanced by Laplacian residuals using a Gaussian kernel blur G(?, ?) with the width of ?:</p><formula xml:id="formula_3">I HR t =? HR t + ?(? HR t ? G(? HR t , ? = 1.5)),<label>(4)</label></formula><p>where? HR t is an intermediate results of the predicted HR frame and ? is weighted factor for the residuals. We present more ablation studies in Section 4 to demonstrate the effectiveness of Laplacian residuals for enhancing image details.</p><p>By exploiting the Laplacian, we add details back to the warped HR frame. This is followed by a space-to-depth operation, which rearranges blocks of spatial data into depth dimension, and then concatenation with the LR input frame. We pass it through the HR frame generator to obtain the final HR prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>During training, the losses are computed from two streams for HR and LR frames. For loss on HR frames, the L 2 distance is computed between the final outputs and the HR frames. In Section 3.1, we describe our bi-directional recurrent module for improving the model quality. Here, I t denotes the ground truth frame and? t denotes the generated frame at time t. For each of the recurrent steps, the predicted HR frames are used to compute losses. The L 2 losses are combined as:</p><formula xml:id="formula_4">L HR content = 1 2N ( N t=1 ||I HR t ?? HR t || 2 forward + 1 t=N ||I HR t ?? HR t || 2 backward ).<label>(5)</label></formula><p>Each of the warped LR frames from t?1 to t is penalized by the L 2 distance with respect to the current LR frame,</p><formula xml:id="formula_5">L LR warp = 1 2N ( N t=1 ||I LR t ?? W arp t?1 || 2 forward + 1 t=N ||I LR t ?? W arp t?1 || 2 backward</formula><p>).</p><p>The total loss is the sum of the HR and LR losses,</p><formula xml:id="formula_7">L total = ?L HR content + ?L LR warp ,<label>(7)</label></formula><p>where ? and ? are weights for each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Analysis</head><p>In this section, we first introduce our implementation details and evaluation metrics. We then evaluate our method against the state-of-the-art VSR models on benchmark datasets. In addition, we demonstrate that our method performs better than a baseline method based on a denoiser and a VSR model. We also evaluate the COMISR model on real-world compressed YouTube videos. Finally, we show ablation on the three novel modules with analysis, and user study results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Datasets. We use the REDS <ref type="bibr" target="#b40">[41]</ref> and Vimeo <ref type="bibr" target="#b59">[60]</ref> datasets for training. The REDS dataset contains more than 200 video sequences for training, each of which has 100 frames with 1280 ? 720 resolution. The Vimeo-90K dataset contains about 65k video sequences for training, each of which has 7 frames with 448 ? 256 resolution. One main difference between these two datasets is the REDS dataset contains images with much larger motion captured from a handheld device. To train and evaluate the COMISR model, the frames are first smoothed by a Gaussian kernel with the width of 1.5 and downsampled by a factor of 4.</p><p>We evaluate the COMISR model on the Vid4 <ref type="bibr" target="#b31">[32]</ref> and REDS4 <ref type="bibr" target="#b40">[41]</ref> datasets (clip# 000, 011, 015, 020). All the testing sequences contain more than 30 frames. In the following experiments, the COMISR model evaluated on the REDS4 dataset is trained with the REDS dataset using the same setting described in <ref type="bibr" target="#b52">[53]</ref>. The COMISR model in all the other experiments is trained using the Viemo-90K. Compression methods. We use the most common setting for the H.264 codec at different compression rates (i.e., different CRF values). The recommended CRF value is between 18 and 28, and the default is 23 (although the CRF value ranges between 0 and 51). In our experiments, we use CRF of 15, 25, and 35 to evaluate video super-resolution with a wide range of compression rates. For fair comparisons, when evaluating other methods, we use the same degradation method to generate the LR sequences before compression. Finally, these compressed LR sequences are fed into the VSR models for inference.</p><p>Training process. For each video frame, we randomly crop 128 ? 128 patches from a mini-batch as input. Each mini-batch consists of 16 samples. The ?, ?, and ? parameters described in Section 3 are set to 1, 20, 1, respectively. The model trained with the loss functions described in the Section 3.4. We use the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> with ? 1 = 0.9 and ? 2 = 0.999. The learning rate is set to 5 ? 10 ?5 . While we aim to train the COMISR model for VSR with compressed videos as input, we first feed uncompressed images to the model, and during the last 20% of the training epochs, we randomly add compressed images in the training process with a probability of 50%. The FFmpeg codec is employed for compression with a CRF value randomly selected between 15 and 25. All the models were trained on 8 NVidia Tesla V100 GPUs. More details can be found on the project website.</p><p>Evaluation metrics. We use PSNR, SSIM, and LPIPS <ref type="bibr" target="#b65">[66]</ref> for quantitative evaluation of video superresolution results. For the experiments on YouTube videos, we only present video SR results for evaluation since the ground-truth frames are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation against the State-of-the-Arts</head><p>We evaluate the COMISR model against state-of-theart VSR methods, including FRVSR <ref type="bibr" target="#b44">[45]</ref>, DUF <ref type="bibr" target="#b22">[23]</ref>, EDVR <ref type="bibr" target="#b52">[53]</ref>, TecoGan <ref type="bibr" target="#b2">[3]</ref>, MuCAN <ref type="bibr" target="#b29">[30]</ref>, and RSDN <ref type="bibr" target="#b21">[22]</ref>. Three of the evaluated methods are based on recurrent models, whereas the other three use temporal sliding windows (between 5 and 7 frames). When available, we use the original code and trained models, and otherwise implement these methods. For fair comparisons, the LR frames have been generated the same as described in the published work. These LR frames are then compressed and fed into the super-resolution networks for performance evaluation.</p><p>For the Vid4 dataset <ref type="bibr" target="#b31">[32]</ref>, the PSNR and SSIM metrics are measured on both the Y-channel and RGB-channels, as shown in <ref type="table" target="#tab_1">Table 1</ref>. We present the averaged performance on uncompressed videos (original sequences), and videos compressed at different levels (CRF15, <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35)</ref>. We also report the individual sequence performance under CRF25. More results on other CRF factors are presented in the supplementary material. Overall, the COMISR method outperforms all the other methods on videos with medium to high compression rates by 0.5-1.0db in terms of PSNR. Meanwhile, our method performs well (2nd or 3rd place) in less compressed videos. <ref type="figure" target="#fig_3">Figure 3</ref> shows some results by the evaluated methods from two sequences. The COMISR model can recover more details from the LR frames with fewer compression artifacts. Both quantitative and visual results  show that the COMISR method achieves the state-of-the-art results on compressed videos. We also evaluate the COMISR model against the stateof-the-art methods on the REDS4 dataset <ref type="bibr" target="#b40">[41]</ref>. Unlike the Vid4 dataset, the sequences in this set are longer (100 frames) and more challenging with larger movements between frames. <ref type="table" target="#tab_2">Table 2</ref> shows the COMISR model achieves the best performance on the compressed videos from the REDS4 dataset. <ref type="figure" target="#fig_4">Figure 4</ref> shows that our method is able to recover more details such as textures from the bricks on the sidewalk and windows on the buildings.</p><p>It is known that low-level structure accuracy (e.g., PSNR or SSIM) does not necessarily correlate well with high-level perceptual quality. In other words, perceptual distortion cannot be well characterized by such low-level structure accuracy <ref type="bibr" target="#b1">[2]</ref>. We also use the LPIPS <ref type="bibr" target="#b65">[66]</ref> for performance evaluation. <ref type="table">Table 3</ref> shows the evaluation results using the LPIPS metric on both Vid4 and REDS4 datasets. Overall, the COMISR model performs well against the state-of-theart methods on both datasets using the LPIPS metric.</p><p>We show video super-resolution results on the project website. Although the compression artifacts are not easily observable in the LR frames, such artifacts are amplified and easily observed after super-resolution. For the compressed videos, the COMISR model effectively recovers more details from the input videos with fewer artifacts.   <ref type="table">Table 3</ref>. Performance evaluation using the LPIPS [66] metric (lower is better). Our method performs well, especially on the more challenging REDS4 dataset.</p><p>The COMISR model does not perform well on highly compressed (e.g., CRF35) videos. Some failure cases are due to heavy compression so that necessary details are missing for super-resolving frames. Other failure cases are caused by extremely large movements in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VSR on Denoised Videos</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, the COMISR model generates high-quality frames with fewer artifacts from compressed videos. An interesting question is whether the state-of-the-art methods can achieve better results if the compressed videos are first denoised. As such, we use the state-of-the-art compressed video quality method, STDF <ref type="bibr" target="#b5">[6]</ref>, for evaluation.</p><p>Using the settings described in Section 4.2, we compress video frames with CRF25. The STDF method is then used to remove the compression artifacts and generate enhanced LR frames as inputs for the state-of-the-art VSR methods.  <ref type="table" target="#tab_3">Table 4</ref>. Ablation study on applying a video denoiser to the compressed frames before the VSR models using the Vid4 dataset. Each entry shows the PSNR/SSIM results on the Y or RGB channel. The COMISR model outperforms the state-of-the-art VSR methods with the STDF <ref type="bibr" target="#b5">[6]</ref> denoiser.</p><p>the COMISR model and the state-of-the-art VSR methods on videos denosied by the STDF scheme. We note that the performance of all of the evaluated method drops on the denoised LR frames. This can be attributed to that a separate denoising step is not compatible with the learned degradation kernel from the VSR methods. In addition, as discussed in Section 4.5, simply using compressed images for model training does not lead to good VSR performance. These results show that the COMISR model is able to efficiently recover more details from compressed videos, and outperforms state-of-the-art models on denoised videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Real-World Compressed Videos</head><p>Most videos on the web are compressed where frames can be preprocessed by a combination of proprietary meth- ods. We use the videos from the REDS4 testing dataset for experiments as the image resolution is higher. We first generate uncompressed videos out of the raw frames, and then upload them to YouTube. These videos are encoded and compressed at different resolutions for downloading. In our setting, the uploaded videos are of 1280 ? 720 pixels. The resolutions that are available for downloading on YouTube are 480p, 360p, 240p, and 144p. In the following experiments, we download the videos at 360p using the YouTube-dl <ref type="bibr" target="#b63">[64]</ref>. We evaluate three state-ofthe-art methods, including MuCAN <ref type="bibr" target="#b29">[30]</ref>, RSDN <ref type="bibr" target="#b21">[22]</ref>, and TecoGan <ref type="bibr" target="#b2">[3]</ref> on these videos that are compressed by proprietary methods by YouTube. <ref type="figure" target="#fig_5">Figure 5</ref> shows the VSR results by the evaluated methods, where the COMISR model produces better visual results with less artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We analyze the contribution of each module in the COMISR model. We start with the recurrent module described in Section 3 as the baseline model. Similar to FRVSR <ref type="bibr" target="#b44">[45]</ref>, the recurrent model computes the flow between consecutive frames, warps the previous frame to the current, and upscales the frames. We carry out two sets of ablation studies, with or without using compressed images, to show the effectiveness of each module (see Section 4.1). <ref type="table">Table 5</ref> shows the ablation studies where we incrementally add each module to the basic recurrent model. For each setting, the model is trained with and without compressed images, and then evaluated on original and compressed frames. The results show that each module helps achieve additional performance gain, in both training process with only compressed images or a combination of compressed and uncompressed images. We note it is important to add some uncompressed images in the training process to achieve best results on compressed videos. The full COMISR model performs best among all settings. For example, the fourth row in <ref type="table">Table 5</ref>, the uncompressed PSNR on Vid4 drops 0.17 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">User Study</head><p>To better evaluate the visual quality of the generated HR videos, we conduct a user study using Amazon MTurk <ref type="bibr" target="#b20">[21]</ref> No  <ref type="figure">Figure 6</ref>. Aggregated user study results on Vid4 and Reds4. Results show that users favored COMISR against all other compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a compression-informed video super-resolution model which is robust and effective on compressed videos. Within an efficient recurrent network framework, we design three modules to effectively recover more details from the compressed frames. We conduct extensive experiments on challenging video with a wide range of compression factors. The proposed COMISR model achieves the state-of-the-art performance on compressed videos qualitatively and quantitatively, while performing well on uncompressed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Model Details</head><p>The overall COMISR model is shown in the <ref type="figure" target="#fig_2">Figure 2</ref> in the paper manuscript. Here we present more details of the two modules: detail-preserving flow estimation and HR frame generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Detail-Preserving Flow Estimation</head><p>The flow is estimated via a network architecture as is shown in <ref type="figure" target="#fig_6">Figure 7</ref>. At each training mini-batch, a short video clip (e.g. 7 frames) is used for training. The flow estimation can be divided into two parts. The first part is to learn the motion discontinuities between the consecutive frames. The second part is for upscaling the estimated flow, which will be then used on the HR frames. The upscaling process is designed by a learn residual added to a 4? bilinear upsampling. Such residual is implemented by repeating a 2? transpose convolutional layer twice. All the convolutional kernels are 3?3. The number of filters in each layer are marked in the <ref type="figure" target="#fig_6">Figure 7</ref>. The output flow estimation is used to warp the t?1 generated HR frame to the t timestamp, and then used for generating the HR frame in the t timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">HR Frame Generator</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref> of the manuscript, the input of the HR frame generator is the concatenation of a spaceto-depth results of the current estimated HR frame and the current input LR frame. In the HR frame generator, 10 repeated residual blocks are first employed to extract highlevel features. Then a upscaling module, similar to Detail-Preserving Flow Estimation in Section 3.2 is used to create estimated HR frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experimental Results</head><p>We evaluate the COMISR model against the state-of-theart VSR methods on the Vimeo-90K-T dataset <ref type="bibr" target="#b59">[60]</ref>, includ- ing FRVSR <ref type="bibr" target="#b44">[45]</ref>, EDVR <ref type="bibr" target="#b52">[53]</ref>, TecoGan <ref type="bibr" target="#b2">[3]</ref>, and RSDN <ref type="bibr" target="#b21">[22]</ref>. The Vimeo-90K-T dataset contains 7824 short video clips, where each clip only has 7 frames. Similar to the observation in <ref type="bibr" target="#b21">[22]</ref>, the recurrent-based method may not take full advantages due to very short video clips, the COMISR model can still outperform others on the compressed videos. We show quantitative result below.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Forward recurrent model used in both training and inference (b) Backward recurrent model only used in training (c) Detailed illustration of the forward recurrent module Concatenation c Element-wise addition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the COMISR model. The forward and backward recurrent modules are symmetric and share the weights. In the figure, red rectangles represent the LR input frames and green dash-lined rectangles represent the HR predicted frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative evaluation on the Vid4 dataset for 4? VSR. The COMISR model can recover more structure details such as faces and boundaries, with much fewer artifacts. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on videos from the REDS4 dataset 4? VSR. The COMISR model achieves much better quality on detailed textures, with much fewer artifacts. The brightness of the images is adjusted for viewing purposes. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>4? VSR results on REDS4 videos downloaded from YouTube with resolution of 360 pixels. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Detail-Preserving Flow Estimation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>HR Frame Generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Visual example of the Vid4 dataset. All the LR input frames are compressed with CRF25. Zoom in for best view. Visual example of the REDS4 dataset. All the LR input frames are compressed with CRF25. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dong et al. [8] propose the SRCNN model based on convolutional neural networks for single image superresolution. Based on the residual learning framework [18], Kim et al. propose the VDSR [24] and DRCN [25] models for more effective image super-resolution. To learn more efficient SR models, Dong et al. [9] use a deconvolution layer at the end of the network to directly learn the mapping from low-resolution to high-resolution images. Similarly, Shi et al. introduce the ESPCN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance evaluation on compressed Vid4 videos. For each entry, the first row is PSNR/SSIM on Y channel, and the second row is PSNR/SSIM on RGB channels. The best method on the Y channel for each column is highlighted in bold and shade. The FLOPs are reported based on the Vid4 4? VSR. The FLOPs and #Param of FRVSR is based on our implementation.</figDesc><table><row><cell></cell><cell>FLOPs</cell><cell></cell><cell cols="2">CRF 25</cell><cell></cell><cell>No compression</cell><cell></cell><cell>Compressed Results</cell><cell></cell></row><row><cell>Model</cell><cell>#Param.</cell><cell>calendar</cell><cell>city</cell><cell>foliage</cell><cell>walk</cell><cell>-</cell><cell>CRF15</cell><cell>CRF25</cell><cell>CRF35</cell></row><row><cell>FRVSR [45]</cell><cell>0.05T 2.53M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CRF 25</cell><cell></cell><cell>No compression</cell><cell></cell><cell>Compressed Results</cell><cell></cell></row><row><cell>Model</cell><cell>#Frame</cell><cell>clip 000</cell><cell>clip 011</cell><cell>clip 015</cell><cell>clip 020</cell><cell>-</cell><cell>CRF15</cell><cell>CRF25</cell><cell>CRF35</cell></row><row><cell>FRVSR [45]</cell><cell>recur(2)</cell><cell>24.25 / 0.631</cell><cell>25.65 / 0.687</cell><cell>28.17 / 0.770</cell><cell>24.79 / 0.694</cell><cell>28.55 / 0.838</cell><cell>27.61 / 0.784</cell><cell>25.72 / 0.696</cell><cell>23.22 / 0.579</cell></row><row><cell>DUF [23]</cell><cell>7</cell><cell>23.46 / 0.622</cell><cell>24.02 / 0.686</cell><cell>25.76 / 0.773</cell><cell>23.54 / 0.689</cell><cell>28.63 / 0.825</cell><cell>25.61 / 0.775</cell><cell>24.19 / 0.692</cell><cell>22.17 / 0.588</cell></row><row><cell>EDVR [53]</cell><cell>7</cell><cell>24.38 / 0.629</cell><cell>26.01 / 0.702</cell><cell>28.30 / 0.783</cell><cell>25.21 / 0.708</cell><cell>31.08 / 0.880</cell><cell>28.72 / 0.805</cell><cell>25.98 / 0.706</cell><cell>23.36 / 0.600</cell></row><row><cell>TecoGan [3]</cell><cell>recur(2)</cell><cell>24.01 / 0.624</cell><cell>25.39 / 0.682</cell><cell>27.95 / 0.768</cell><cell>24.48 / 0.686</cell><cell>27.63 / 0.815</cell><cell>26.93 / 0.768</cell><cell>25.46 / 0.690</cell><cell>22.95 / 0.589</cell></row><row><cell>MuCAN [30]</cell><cell>5</cell><cell>24.39 / 0.628</cell><cell>26.02 / 0.702</cell><cell>28.25 / 0.781</cell><cell>25.17 / 0.707</cell><cell>30.88 / 0.875</cell><cell>28.67 / 0.804</cell><cell>25.96 / 0.705</cell><cell>23.55 / 0.600</cell></row><row><cell>RSDN [22]</cell><cell>recur(2)</cell><cell>24.04 / 0.602</cell><cell>25.40 / 0.673</cell><cell>27.93 / 0.766</cell><cell>24.54 / 0.676</cell><cell>29.11 / 0.837</cell><cell>27.66 / 0.768</cell><cell>25.48 / 0.679</cell><cell>23.03 / 0.579</cell></row><row><cell>COMISR</cell><cell>recur(2)</cell><cell>24.76 / 0.660</cell><cell>26.54 / 0.722</cell><cell>29.14 / 0.805</cell><cell>25.44 / 0.724</cell><cell>29.68 / 0.868</cell><cell>28.40 / 0.809</cell><cell>26.47 / 0.728</cell><cell>23.56 / 0.599</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance evaluation on compressed the REDS4 dataset. Each entry shows the PSNR/SSIM on RGB channels. The best method for each column is highlighted in bold and shade, and recur(2) indicates a recurrent network using 2 frames.</figDesc><table><row><cell></cell><cell cols="6">FRVSR TecoGan DUF EDVR MuCAN RSDN COMISR</cell></row><row><cell>Vid4</cell><cell>4.105</cell><cell>3.245</cell><cell>4.010 4.396</cell><cell>3.985</cell><cell>4.292</cell><cell>3.689</cell></row><row><cell>REDS4</cell><cell>4.188</cell><cell>3.643</cell><cell>4.223 4.075</cell><cell>4.085</cell><cell>4.423</cell><cell>3.384</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows the quantitative results by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>In each experiment, two videos generated by the COMISR model and other methods are presented side by side and each user is asked "which video looks better?" For the Vid4 and REDS4 datasets, all the test videos are used for the user study. For each of the video pairs, we assign to 20 different raters. The aggregated results are shown inFigure 6.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">compression Aug</cell><cell></cell><cell></cell><cell>Aug CRF15-25</cell></row><row><cell cols="3">Components Uncompressed</cell><cell>CRF25</cell><cell></cell><cell cols="2">Uncompressed</cell><cell>CRF25</cell></row><row><cell>Recur</cell><cell cols="2">26.61 / 0.808</cell><cell cols="2">23.97 / 0.634</cell><cell cols="2">26.53 / 0.815</cell><cell>24.23 / 0.648</cell></row><row><cell>Recur + a</cell><cell cols="2">27.16 / 0.837</cell><cell cols="2">24.24 / 0.650</cell><cell cols="2">26.64 / 0.818</cell><cell>24.74 / 0.686</cell></row><row><cell>Recur + ab</cell><cell cols="2">27.45 / 0.844</cell><cell cols="2">24.27 / 0.649</cell><cell cols="2">27.27 / 0.838</cell><cell>24.92 / 0.696</cell></row><row><cell>Recur + abc</cell><cell cols="2">27.48 / 0.845</cell><cell cols="2">24.31 / 0.650</cell><cell cols="2">27.31 / 0.840</cell><cell>24.97 / 0.701</cell></row><row><cell cols="7">Table 5. Ablations on three modules of the COMISR model on</cell></row><row><cell cols="7">Vid4: (a) bi-directional recurrent module, (b) detail-aware flow</cell></row><row><cell cols="7">estimation, and (c) Laplacian enhancement module. Each entry</cell></row><row><cell cols="7">shows the PSNR/SSIM values on the Y-channel.</cell></row><row><cell cols="7">on the Vid4 [32] and REDS4 [41] datasets. We evaluate</cell></row><row><cell cols="7">the COMISR model against all other methods using videos</cell></row><row><cell cols="4">25% 50% 75% compressed with CRF25. 0% 100%</cell><cell></cell><cell></cell></row><row><cell cols="2">FRVSR</cell><cell>DUF</cell><cell>TecoGan</cell><cell cols="2">EDVR</cell><cell>MuCAN</cell><cell>RSDN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance evaluation of Y-channel on the Vimeo90K testing set.</figDesc><table><row><cell></cell><cell>Uncompressed</cell><cell>CRF25</cell></row><row><cell>FRVSR [45]</cell><cell>35.64 / 0.932</cell><cell>30.07 / 0.788</cell></row><row><cell>TecoGan [3]</cell><cell>34.07 / 0.909</cell><cell>29.84 / 0.784</cell></row><row><cell>EDVR [53]</cell><cell>37.61 / 0.949</cell><cell>30.53 / 0.844</cell></row><row><cell>RSDN [22]</cell><cell>37.23 / 0.947</cell><cell>29.63 / 0.815</cell></row><row><cell>OURS</cell><cell>35.71 / 0.926</cell><cell>31.05 / 0.816</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A deep journey into super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning temporal coherence via selfsupervision for gan-based video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal deformable convolution for compressed video quality enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">FFmpeg h.264 video encoding guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffmpeg</surname></persName>
		</author>
		<ptr target="https://trac.ffmpeg.org/wiki/Encode/H.264.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient video super-resolution through recurrent latent space propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Fuoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Closedloop matters: Dual regression networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshuai</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent back-projection network for video superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Space-time-aware multi-resolution video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Correction filter for single image super-resolution: Robustifying off-theshelf deep super-resolvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Shady Abu Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Tirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<ptr target="https://www.mturk.com/,2021.8" />
	</analytic>
	<monogr>
		<title level="j">Amazon Inc. Amazon mturk</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video super-resolution with recurrent structure-detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Isobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangfu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual feature aggregation network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Looking fast and slow: Memoryguided mobile video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep kalman filtering network for video compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ting</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep non-local kalman network for video compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1725" to="1737" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latticenet: Towards lightweight image super-resolution with lattice block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structure-preserving super resolution with gradient guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yean</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image super-resolution with cross-scale non-local attention and exhaustive selfexemplars mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on video deblurring and superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Frame-Recurrent Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiejie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naifan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<ptr target="https://github.com/xinntao/BasicSR" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep learning for image super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Component divide-and-conquer for real-world image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-local convlstm for video compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longwen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huyang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unified dynamic convolutional network for super-resolution with variational degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Syuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-Yao Roy</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Kai</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Min</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Singleimage super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Youtube-downloader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Youtube-Dl</surname></persName>
		</author>
		<ptr target="https://github.com/ytdl-org/youtube-dl" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep unfolding network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
