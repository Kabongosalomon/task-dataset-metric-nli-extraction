<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Alignment Constraint for Continuous Sign Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Hao</surname></persName>
							<email>aiming.hao@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
							<email>chaixiujuan@caas.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Agricultural Information Institute</orgName>
								<orgName type="institution">Chinese Academy of Agricultural Sciences</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Alignment Constraint for Continuous Sign Language Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented signs from image streams. Overfitting is one of the most critical problems in CSLR training, and previous works show that the iterative training scheme can partially solve this problem while also costing more training time. In this study, we revisit the iterative training scheme in recent CSLR works and realize that sufficient training of the feature extractor is critical to solving the overfitting problem. Therefore, we propose a Visual Alignment Constraint (VAC) to enhance the feature extractor with alignment supervision. Specifically, the proposed VAC comprises two auxiliary losses: one focuses on visual features only, and the other enforces prediction alignment between the feature extractor and the alignment module. Moreover, we propose two metrics to reflect overfitting by measuring the prediction inconsistency between the feature extractor and the alignment module. Experimental results on two challenging CSLR datasets show that the proposed VAC makes CSLR networks end-to-end trainable and achieves competitive performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sign Language is a complete and natural language that conveys information through both manual components (hand/arm gestures) and non-manual components (facial expressions, head movements, and body postures) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> with its own grammar and lexicon <ref type="bibr" target="#b40">[41]</ref>. Vision-based Continuous Sign Language Recognition (CSLR) aims to automatically recognize signs from image streams, which can bridge the communication gap between the Deaf and hearing people. It also provides more non-intrusive communication channel for sign language users. Different from speech recognition, the data collection and annotation of sign language are costly, which poses a  significant problem for recognition <ref type="bibr" target="#b1">[2]</ref>. Therefore, most recent CSLR works solve this problem in a weakly supervised manner and adopt network architectures composed of the feature extractor and the alignment module. The feature extractor abstracts visual information from each frame, and the alignment module searches the possible alignments between visual features and the corresponding labeling. Different to those works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> adopt HMMs to update frame-wise state labels for the feature extractor, Graves et al. <ref type="bibr" target="#b14">[15]</ref> provide a more elegant solution so-called Connectionist Temporal Classification (CTC) to align the prediction and labeling by maximizing the sum of probability of all feasible alignments, which is adopted by following works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Although CTC-based CSLR methods provide convenience in training, previous studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39]</ref> show that endto-end training limits the discriminative power of the feature extractor. They leverage the iterative training scheme to enhance the feature extractor, which significantly improves the performance. Nevertheless, it requires an additional fine-tuning process besides the end-to-end training and increases the training time. Several recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> try to accelerate this training scheme by adopting fully convolutional networks and fine-grained labels.</p><p>In this study, we revisit CTC-based CSLR model at dif-ferent iterations and observe that only a few frames play key roles in training. The feature extractor abstracts visual information and provides initial localizations of key frames for the alignment module. The alignment module further refines the recognition results from the feature extractor and learns long-term relationships with its powerful temporal modeling ability. Due to the spike phenomenon of CTC <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, the alignment module converges much faster than the feature extractor on CSLR datasets with limited samples and cannot provide enough feedback to the feature extractor. The overfitting of the alignment module leads to insufficient training of the feature extractor and deteriorates the generalization ability of the trained model. The iterative training scheme tries to solve this problem by enhancing the feature extractor with iteratively refined pseudo labels. Based on above observations, we conclude that constraining the feature space is critical to efficiently train CSLR models. To solve this problem, we propose a Visual Alignment Constraint (VAC) to make CSLR networks endto-end trainable. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the proposed VAC is composed of two auxiliary losses which provide extra supervision for the feature extractor. The visual enhancement loss enforces the feature extractor to make predictions based on visual features only and the visual alignment loss aligns the short-term visual predictions to long-term contextual predictions. With the combination of the two losses, the proposed method achieves competitive performance to the latest methods on PHOENIX14 <ref type="bibr" target="#b27">[28]</ref> and CSL <ref type="bibr" target="#b22">[23]</ref> datasets.</p><p>To better understand the performance gains, we present two metrics named Word Deterioration Rate (WDR) and Word Amelioration Rate (WAR) to evaluate the contributions of the feature extractor and the alignment module, which can also be used as indicators of overfitting. Comparing to the iterative training procedure, experimental results show that the proposed method can obtain a more powerful feature extractor and make better use of visual features.</p><p>The major contributions are summarized as follows:</p><p>? Revisiting the iterative training scheme in CSLR and showing that the overfitting of the alignment module leads to insufficient training of the feature extractor.</p><p>? Proposing a visual alignment constraint to make the network end-to-end trainable by enhancing the feature extractor and aligning visual and contextual features.</p><p>? Presenting two metrics to evaluate the contributions of the feature extractor and the alignment module, which verifies the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Continuous Sign Language Recognition</head><p>Sign Language Recognition (SLR) methods can be roughly categorized into isolated SLR <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> and con-tinuous SLR <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. Different to isolated SLR, most CSLR approaches model sequence recognition in a weakly supervised manner: only sentence-level labeling is provided. Some early CSLR methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref> adopt a divide-andconquer paradigm that splits sign video into several subunits with HMM-based recognition systems to work with limited data. Hand-crafted features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43]</ref> are carefully selected to provide better visual information.</p><p>The recent successes of CNNs in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> provide powerful tools for visual features representation. However, CNNs need frame-wise annotations contrary to the weakly supervised nature of CSLR. To solve this problem, Koller et al. <ref type="bibr" target="#b28">[29]</ref> propose an iterative expectationmaximization approach by adding a hand shape classifier to the GMM-HMM model as an intermediate task to provide frame-level supervision. A few studies extend this work by proposing CNN+LSTM+HMM framework <ref type="bibr" target="#b29">[30]</ref>, incorporating more clues <ref type="bibr" target="#b26">[27]</ref> and improving the iterative alignment approach <ref type="bibr" target="#b30">[31]</ref>. This iterative CNN-LSTM-HMM setup provides robust visual features that are adopted by many recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Although the CNN-LSTM-HMM hybrid approaches achieve great results, they still need HMMs to provide frame-wise labels. Graves et al. <ref type="bibr" target="#b14">[15]</ref> propose the CTC loss to maximize probabilities of all feasible alignments, which is widely used in many sequence problems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>. Several recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> use CTC loss to achieve the end-to-end training of CSLR. However, some works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref> find that such an end-to-end approach cannot train feature extractor properly and bring the iterative training back in use. Until very recently, some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> try to solve this problem in an end-to-end way. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> propose a gloss feature enhancement module to learn better visual features. Niu and Mak <ref type="bibr" target="#b35">[36]</ref> propose a multiple states approach and several operations to alleviate the overfitting problem. In this work, we try to explore the nature of iterative training and propose a more efficient method to train CSLR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Auxiliary Learning</head><p>Different from the conventional Multi-Task Learning <ref type="bibr" target="#b4">[5]</ref>, which aims to improve the generalization of all tasks, auxiliary learning chooses proper auxiliary tasks to assist in the generalization of the primary task. One straightforward way is to combine multiple tasks at the output stage. Follow this idea, Kim et al. <ref type="bibr" target="#b25">[26]</ref> use CTC to speed up the training process and provide a monotonic alignment constraint. Pu et al. <ref type="bibr" target="#b38">[39]</ref> propose an iteratively alignment network that jointly optimizes the CTC decoder and the LSTM decoder, additionally with a soft-DTW alignment constraint. Goyal et al. <ref type="bibr" target="#b12">[13]</ref> propose an auxiliary loss to alleviate the posterior collapsing phenomenon in autoregressive decoder <ref type="bibr" target="#b0">[1]</ref>. Another idea is to use different supervision at different stages. Sanabria et al. lower-level tasks, such as phoneme recognition, to constrain intermediate representations for speech recognition. In this study, we adopt the auxiliary learning strategy to provide the visual alignment constraint for the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting the Iterative Training in CSLR</head><p>The CSLR aims to predict the corresponding gloss label sequence l = (l 1 , ? ? ? , l N ) based on a sequence of T frames X = (x 1 , ? ? ? , x T ). The feature extractor plays an important role in CSLR, which extracts visual features V = (v 1 , ? ? ? , v T ) from image sequences. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we choose 2D-CNN to extract frame-wise features and 1D-CNN to extract local posture and motion information from neighboring frames as previous works did <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48]</ref>. The gloss-wise features are fed into a two-layer BiLSTM and the primary classifier F p to combine long-term relationships and provide the predicted logits Z = (z 1 , ? ? ? , z T ). CTC loss is adopted to provide supervision by aligning the predictions and sequence labelings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Spike Phenomenon of CTC</head><p>The Connectionist Temporal Classification <ref type="bibr" target="#b14">[15]</ref> is designed for end-to-end temporal classification tasks with unsegmented data. To provide more effective supervision, CTC introduces a 'blank' to represent unlabeled data (such as movement epenthesis or non-gesture segments in CSLR) and solves the alignment problem with dynamic programming. The blank class and gloss vocabulary G build the final extended gloss vocabulary G = G ? {blank}.</p><p>CTC defines a many-to-one function B : G T ? G ?T to align label sequence referred to as path ? ? G T and labeling l ? G ?T by sequentially removing the repeated labels and the blanks from the path. For example,</p><formula xml:id="formula_0">B(-aaa--aabbb-) = B(-a-ab-) = aab.</formula><p>With the help of this function, CTC can provide supervision for parameters ? of the feature extractor and the alignment module by summing the probabilities of all feasible paths:</p><formula xml:id="formula_1">L CT C = ? log p(l|X; ?) = ? log ??B ?1 (l) p(?|X; ?) .<label>(1)</label></formula><p>The conditional probability p(?|X) can be calculated according to the conditional independence assumption:</p><formula xml:id="formula_2">p(?|X) = T t=1 p(? t |X; ?),<label>(2)</label></formula><p>where the probabilities are calculated by applying softmax fuction to the the network output logits: P ? = softmax(Z). As mentioned above, CTC aligns the path and the labeling by introducing a blank class and removing the repeat labels. When optimizing network with CTC, predictions tend to form a series of spike responses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>. The main reason for this is that predicting a blank label is a much safer choice for CTC when the network cannot confidently distinguish gloss boundaries. For example, both B(aaab) and B(a--b) are corresponding to the same labeling, but B(abab) will bring larger loss even if there is only one mistake. Therefore, the CTC loss mainly focuses on key frames, and the final predictions are composed of a few nonblank key frames and many high-confidence blank frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visualization of LSTM Gates</head><p>Long Short-Term Memory <ref type="bibr" target="#b21">[22]</ref> is widely used in sequence modeling, which excellently models long-term dependencies. The core component of LSTM is its memory design: the input and forget gates control information from current inputs and the past memory to the current memory. The output gate controls what is expected to output from the current memory. The total update mechanism is as follows ( denotes the Hadamard product):</p><formula xml:id="formula_3">i t = ?(U i v t + W i h t?1 + b i ), f t = ?(U f v t + W f h t?1 + b f ), o t = ?(U o v t + W o h t?1 + b o ), c t = ?(U c v t + W c h t?1 + b c ), c t = f t c t?1 + i t c t , h t = o t tanh(c t ).<label>(3)</label></formula><p>Here the i t , f t and o t are corresponding to input, forget and output gates, respectively, the vector h t and c t are hidden and cell states. where U ? and W ? are the input-tohidden and hidden-to-hidden weight matrices, and b ? are bias vectors. Element-wise sigmoid is reprensented by ?.</p><p>Previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref> adopt iterative training to enhance the visual extractor. To explore how iterative training works and how LSTM makes predictions in CSLR, we begin by visualizing the averaged gate values of the last forward-direction LSTM and the network predictions at different iterations in <ref type="figure">Fig. 3</ref>. For the predictions, we only visualize non-blank classes that occur in the labeling. We can make some observations from the comparison of line charts:</p><p>1) The gate values and the predictions have positive correlations on the training set, and they reach the local maximum on similar frame subsets.</p><p>2) The correlations appear to be weakened as the iteration progresses, especially for the input and output gates, which become larger and smoother.</p><p>The above two observations are quite puzzling, as three gates are expected to play different roles in information flow. As shown in Equ. 3, three gates take the same inputs and have independent parameters. Therefore, we pinpoint the problem to the magnitude of input features and further visualize the l 2 norms of the activations before the first and the second BiLSTM layers, which are referred to as the gloss and sequence norms in <ref type="figure">Fig. 3</ref>. <ref type="figure">Fig. 3</ref> presents an interesting observation that the l 2 norms of gloss and sequence features have similar tendencies with gates values and final predictions. Besides, the magnitudes variances of both gloss and sequence become smaller as the iteration progresses. Several recent papers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref> found that well-separated features tend to have larger magnitudes, and we hypothesize the magnitudes variances are relevant to the importance of frames:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Magnitude Hypothesis</head><p>The l 2 norms of the features are effect indicators that reflect frame importance: the optimization algorithm will decrease the magnitudes of activations when suppressing the non-key frames due to the spike phenomenon of CTC.</p><p>With the above hypothesis, it is clear that frames with larger magnitudes in <ref type="figure">Fig. 3</ref> play key roles compared to their neighbors. We further interpret the learning process of CTC-based CSLR model into two stages: 1) the feature extractor provides visual and initial localization information for the alignment module, and 2) the BiLSTM layers refine the localization and learn long-term relationships among key frames. Such a learning scheme can make efficient use of the data and accelerate the training process.</p><p>However, current CSLR datasets contain less data than other sequence learning tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, which means the BiLSTM layers can easily overfit the whole training set with partial visual information and other frames are decreasingly involved in the training progress. Although the network can achieve stable convergence, the power of feature extractor is not sufficiently explored. Therefore, the feature extractor cannot provide robust visual features during inference and deteriorate the generalization performance.</p><p>Based on these analyses, we attribute the success of iterative training to the reduction of the overfitting problem. With pseudo labels generated by the alignment module, the fine-tuning stage can enhance the feature extractor to make it generalize better. Although the pseudo labels can relieve the overfitting problem in some sense, it is still not enough. Therefore, we propose the visual alignment constraint on the visual feature space, which enforces the feature extractor to make predictions on its own and adopts the distillation loss to align both visual and contextual spike responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visual Alignment Constraint</head><p>As mentioned above, the BiLSTM layers can easily overfit the training set with partial visual information. In this paper, we propose the Visual Alignment Constraint (VAC) to enhance the feature extractor with more alignment supervision. The proposed VAC is implemented by two simple auxiliary losses: the Visual Enhancement (VE) loss and the Visual Alignment (VA) loss. Besides, we propose two new evaluation metrics, Word Deterioration Rate (WDR) and Word Amelioration Rate (WAR), to evaluate the contributions of the feature extractor and the alignment module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss Design of VAC</head><p>VE Loss. To enhance the feature extractor, we proposed to add an auxiliary classifier F a on visual features V to get the auxiliary logitsZ = (z 1 , ? ? ? ,z T ) = F a (V ) and propose the VE loss that directly provides supervision for the feature extractor. This auxiliary loss enforces the feature extractor to make predictions based on local visual information only. Compared to previous gloss-wise supervision that needs to generate pseudo labels, we propose to add a CTC loss on the auxiliary classifier as the VE loss, which is compatible with the primary CTC loss and flexible to network designs. The VE loss only provides supervision for parameters ? v of the feature extractor and the auxiliary classifier:</p><formula xml:id="formula_4">L V E = L v CT C = ? log p(l|X; ? v ).<label>(4)</label></formula><p>VA Loss. Because the VE loss lacks contextual information and is independent of the primary loss, which may lead to misalignment between two classifiers, we further propose the VA loss. The VA loss is implemented as a knowledge distillation loss <ref type="bibr" target="#b20">[21]</ref>, which regards the entire network and the visual feature extractor as the teacher and student models, respectively. A high temperature ? is adopted to "soften" probability distribution from spike responses. The distillation process is formulated as:</p><formula xml:id="formula_5">L V A = KL softmax( Z ? ), softmax(Z ? ) .<label>(5)</label></formula><p>In summary, to achieve the visual alignment goal, the VE loss enforces the feature extractor to provide more robust visual features for the alignment module, while the VA loss aligns the predictions of two classifiers by providing longterm supervision for the visual extractor. With the help of both losses, the feature extractor obtains more supervision which is compatible with the alignment module. The final objective function is composed of the primary CTC loss, the visual enhancement loss, and the visual alignment loss:</p><formula xml:id="formula_6">L = L CT C + L V E + ?L V A .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prediction Inconsistency Measurement</head><p>Word Error Rate (WER) is a widely-used metric to evaluate the performance of recognition algorithms in CSLR <ref type="bibr" target="#b27">[28]</ref>. It is also referred to as the length normalized edit distance, which first aligns the recognized sequence with the reference sentence and then counts the number of operations, including substitution (sub), deletion (del), and insertion (ins), to transfer from the reference to the recognized sequence: WER = (#sub + #del + #ins) / #reference.</p><p>As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, both of the auxiliary and the primary recognized sentences (HYP a and HYP p ) have the same WER 22.22% (HYP a has two deletion errors, and HYP p has two insertion errors). The primary classifier corrects the misrecognized results of the auxiliary classifier but makes new mistakes, which can not be measured by WER. Therefore, we firstly align sentence triplet (REF * , HYP * a , HYP * p ) and then calculate WDR and WAR: WDR measures the ratio that is correctly recognized by the auxiliary classifier but misrecognized by the primary classifier (two 'SUED' in HYP * p ), and WAR does in the opposite direction ('MEHR' and 'KALT' in HYP * p ). With the proposed metrics, we can connect the WER * 1 performance of two classifiers by:  In Equ. 7, the final result WER * p come from three aspects: how well the visual extractor performs (related to WER * a ), how much visual information is not fully utilized (related to WDR) and how many predictions are made by contextual information only (related to WAR). More details are given in the supplementary material.</p><formula xml:id="formula_7">WER * p = WER * a + WDR ? WAR.<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Datasets. We evaluate the proposed method on two widely used datasets: RWTH-PHOENIX-Weather-2014 (PHOENIX14) <ref type="bibr" target="#b27">[28]</ref> and Chinese Sign Language (CSL) dataset <ref type="bibr" target="#b22">[23]</ref>. All ablations are performed on PHOENIX14. The PHOENIX14 dataset is a widely used CSLR dataset recorded from the German TV weather forecasts and performed by nine hearing SL interpreters. It contains 6841 sentences with 1295 different glosses. The dataset is split into 5672 training sentences, 540 development (Dev) sentences, and 629 test sentences for the multi-signer setup.</p><p>The CSL dataset is collected under laboratory conditions with 100 sign language sentences with a vocabulary size of 178. Fifty signers perform each sentence five times (in 25000 videos with 100+ hours). We follow the previous setting <ref type="bibr" target="#b5">[6]</ref> and split the dataset into training and test sets according to the ratio of 8:2. Implementation Details. ResNet18 <ref type="bibr" target="#b19">[20]</ref> is picked as the frame-wise feature extraction in considering its efficiency on the PHOENIX14 dataset. For the CSL dataset, we adopt VGG11 <ref type="bibr" target="#b41">[42]</ref> as the backbone to reduce side effects of inconsistent statistics under the signer-independent setting. The gloss-wise temporal layer and two BiLSTM layers with 2?512 dimensional hidden states are adopted as the default setting. The weight ? for L V A is set to 25 and its temperature ? is set to 8 by default. We train all the models for 80 epochs for PHOENIX14 and 20 epochs for CSL with a mini-batch size of 2. Adam optimizer is used with an initial learning rate of 10 ?4 , divided by five after 40 and 60 epochs for PHOENIX14 and 10 and 15 for CSL. For iterative training, we reduce the learning rate by a factor of five after each iteration. All frames are resized to 256x256, and the training set is augmented with random crop (224x224), horizontal flip (50%), and random temporal scaling (?20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Results</head><p>Ablation on iterative training and BN. Batch Normalization (BN) <ref type="bibr" target="#b23">[24]</ref> is a widely-used tool to accelerate the training of deep networks by normalizing the activations. Al- though we adopt a small batch size, BN significantly improves the performance. As shown in <ref type="table" target="#tab_0">Table 1</ref>, adding a BN layer after each temporal convolution layer brings 5.5%, 3.4%, and 3.6% performance gains at each iteration on the Dev set, which indicates the existence of insufficient training of the feature extractor. We can also observe that adopting iterative training can lead to noticeable performance gains compared to non-iterative training. Ablation on learning pace. A natural idea to solve the insufficient training problem is adjusting the learning paces of the feature extractor and the alignment module. In Table 2, we compare results under different learning rate ratios. Adopting a smaller learning rate for the feature extractor leads to comparable results with iterative training, which suggests the existence of insufficient training. However, it is hard to find an optimal learning setting. We adopt a noniterative model with BN layers and the normal 1:1 learning rate ratio as our baseline. Ablation on VAC. Ablations on VAC are presented in <ref type="table">Table 3</ref>. Constraining visual features with L V E and L V A improves the recognition results (2.1% and 0.9% on Dev set), which verifies the need to strengthen supervision on the feature extractor. It is also worth noting that although adopting the L V A only leads to smaller gains than the L V E only, adopting both losses can achieve further improvement. It suggests that aligning two spike responses provides more effective supervision than adopting independent supervision or distillation only.</p><p>Obeservations about the overfitting problem. <ref type="figure" target="#fig_7">Fig. 6</ref> visualizes performance comparison with different evaluation metrics and we can draw some interesting observations about overfitting. First, the primary classifier can reach much lower WER on the training set than the auxiliary classifier in <ref type="figure" target="#fig_7">Fig. 6(a)</ref>, which reflects its powerful temporal modeling ability. Second, there exists a significant performance gap between the training and Dev sets on WDR, which indicates that the BiLSTM layers do not fully incorporate the visual information although it successfully overfits the training set. Third, the actual performance gap is much larger than WER shows (?WER * ). For example, the  performance gap between two classifiers of Baseline on Dev set in <ref type="figure" target="#fig_7">Fig. 6(b)</ref> is only 4.9% (=30.4%-25.5%), however, the primary classifier makes 11.3% correct predictions based on contextual information only (WAR) and ignores 6.5% correct visual information (WDR). The proposed inconsistent prediction metrics provide a helpful tool to understand and evaluate the overfitting problem.</p><p>Obeservations about the performance gap. Another interesting observation from <ref type="figure" target="#fig_7">Fig. 6(b)</ref> is that while the iterative training strengthens the visual extractor, it also increases the WDR. We assume that the pseudo-label-based approach is not well compatible with the primary CTC loss (previous work <ref type="bibr" target="#b5">[6]</ref> adopts a balanced ratio to reduce the effects of "blank" labels). Therefore, we adopt an additional CTC loss as our L V E and it significantly improves both WAR and WDR. The proposed L V A has a limited effect on the visual extractor but it can narrow the performance gap between two classifiers. The combined use of both auxil- iary losses achieves better performance with a smaller actual performance gap (WDR and WAR), which verifies the effectiveness of the proposed visual alignment constraint. Ablation on temporal network design. Previous pseudolabel-based methods need to carefully design the temporal receptive field, which is set to approximate the average length of the isolated sign <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. <ref type="table" target="#tab_2">Table 4</ref> presents the performance comparison with different temporal receptive fields ?t to show the effectiveness and flexibility of the proposed VAC. To our surprise, the frame-wise feature extractor still achieves competitive results to other settings, and there is a small performance differences in the temporal layer design. The VAC provides more flexible supervision for the feature extractor and results show that it is superior to iterative training sceme <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results</head><p>Results Visualization. To better understand the learning process, we give some recognized examples in <ref type="figure" target="#fig_12">Fig. 9(a)</ref>. The upper sample from the training set shows that the auxiliary classifier of the baseline does not correctly recognize some glosses (NACHT, loc-SUEDWEST, ORT-PLUSPLUS ), but the primary classifier can still deliver the correct result. Although it is reasonable for the primary classifier to make predictions based on contextual information only, the lack of constraint on the feature space increases the risk of overfitting, which may lead to unpredictable predictions when context changes during inference.</p><p>With the help of the VAC, both auxiliary and primary classifiers are sufficiently trained and make better predictions on the training set. The lower sample from the Dev set shows a failure case of the alignment module. The auxiliary classifier makes the correct predictions (HEUTE, OST and SCHON) based on visual features only. Nevertheless, the primary classifier neglects this information and gives a worse result, which is not mentioned in the WER metric but can be identified by the proposed metrics. More qualitative results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the State-of-the-art.</head><p>We present the comparison results with several state-ofthe-art approaches in <ref type="table" target="#tab_3">Table 5 and Table 6</ref>. From <ref type="table" target="#tab_3">Table 5</ref> we can see that the proposed method with gloss-wise temporal layer and VAC achieves competitive results with previous iteration-based methods. We can also illustrate the success of STMC <ref type="bibr" target="#b47">[48]</ref> and CMA <ref type="bibr" target="#b37">[38]</ref> from the overfitting perspective: the former enforces the feature extractor to extract visual information from extra supervision and the latter weakens the contextual information with the data augmentation.</p><p>To examine the generalization of the proposed method, we also evaluate it on the CSL dataset. As no official split is given, the performance comparison among methods in <ref type="table">Table 6</ref> has limited practical value. The proposed method shows improvement than baseline and achieves better per-formance than recent work <ref type="bibr" target="#b5">[6]</ref> under the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Discussion</head><p>We can roughly divide recent methods into two categories from the overfitting perspective: enhancing the feature extractor <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> and weakening the alignment module <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. The proposed VAC is an attempt to make better use of visual information, which provides a new perspective to solve this problem. How to better use visual features with a more powerful temporal model, which will be easier to overfit but can further improve WAR, is a challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Overfitting is one of the major problems in CTC-based sign language recognition, which leads to insufficient training of the feature extractor. In this study, we propose the visual alignment constraint to make CSLR networks endto-end trainable by enforcing the feature extractor to make predictions with more alignment supervision. Two metrics are proposed to measure the inconsistent predictions of the feature extractor and the alignment module. Experimental results show that the proposed VAC narrows the gap between predictions of the auxiliary and the primary classifiers. The proposed metrics and relevant experiments provide a new perspective on the relationship between visual and alignment modules, and we hope they can inspire future studies on CSLR and other sequence classification tasks.</p><p>Our source codes and trained models are available at https://vipl.ict.ac.cn/resources/codes or https://github.com/ycmin95/VAC_CSLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix provides details that are not shown in the main paper. We first present the training process of the proposed VAC ( ? A.1) and ablations on dataset size ( ? A.2), temporature ( ? A.3), loss weight ( ? A.4) and augmentation ( ? A.5). Then we present the details of the temporal convolution designs ( ? B.1), the proposed metrics ( ? B.2) and the performance gap ( ? B.3). Finally, we visualize the spatial activations ( ? C.1), magnitudes ( ? C.2) and more qualitative results ( ? C.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training process of VAC</head><p>We compare the curves with different constraints in <ref type="figure" target="#fig_8">Fig. 7</ref>. Adopting VAC can significantly accelerate the training process, which achieves better performance than baseline after the first learning rate decay. The L V E can immediately accelerate the training process at the beginning and the L V A takes effect when the alignment model begins to converge, which happens after the first learning rate decay. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Ablation on Dataset Size</head><p>We visualize the recognition results with different sizes of training data in <ref type="figure" target="#fig_9">Fig. 8</ref> below. It can be seen that VAC can steadily improve performance as the training data size increases, while the visual extractor of the baseline (WER a ) shows a saturation trend, which implies the available training data is NOT sufficient for the visual extractor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablation on Temporature ?</head><p>To determine the temperature ? in Equ. 5 of the main paper, we evaluate its effect in <ref type="table" target="#tab_4">Table 7</ref>. Low temperatures leads to spike responces and high temperatures will produce noisy supervision. According to ablation results, ? =8 is a proper choice. Another hyperparameter need to be carefully tuned is the loss weight ? in Equ. 6 of the main paper. We conduct ablation study on it and present results in <ref type="table">Table 8</ref>. As weight of distillation increases, the performance first increases and then decreases after certain value. The optimal weight for distillation loss is 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Ablation on Data Augmentation</head><p>As mentioned in Sect. 5.1, we adopt three kinds of data augmentation strategies (random crop, horizontal flip and random temporal scaling) during training, which is the same as previous work <ref type="bibr" target="#b47">[48]</ref>. In <ref type="table" target="#tab_5">Table 9</ref>, we evaluate the effect of data augmentation. We can observe that adopting data augmentation can significantly improve the performance, especially with random crop. We assume that the network has a tendency to use shortcuts, such as the absolute position of hands in video, and adopting random crop can enforce the network to learn more high-level features and mitigate these shortcuts. It is interesting to see that the horizontal flip can improve the results although all signers in PHOENIX14 use their right hand as the dominant hand when signing, which brings about 0.6% performance gain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Details on Temporal Layer Designs</head><p>As mentioned in Sect. 5.2, we evaluate three kinds of basic temporal convolution layers and present the details in <ref type="table" target="#tab_0">Table 10</ref>. The output dimension C of the ResNet18 <ref type="bibr" target="#b19">[20]</ref> is 512, and the output dimension C of the temporal layer is 1024. Conv1x? (1x? Convolution-BN-ReLU) and Maxpooling 1x? are used to extract different levels of features. The lengths T of output sequences of (Frame-wise Raw, Frame-wise Conv1x3, Subgloss-wise, Gloss-wise) are (T, T ?2, T /2?2, T /4?3). The alignment model contains a two-layer BiLSTM (512 hidden states for each direction) and a fully-connected layer with N output units is adopted to make the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Details on Proposed Metrics</head><p>In Sect. 4.2, we propose two metrics, Word Deterioration Rate (WDR) and Word Amelioration Rate (WAR), to evaluate the performance of the recognition results. To calculate WDR and WAR, we need to align the reference sentence and the recognized sentences from the auxiliary classifier and the primary classifier first. As shown in <ref type="figure" target="#fig_12">Fig. 9(a</ref> With the help of alignment results, we can compare the performance of the two classifiers. As shown in <ref type="figure" target="#fig_12">Fig. 9(a)</ref>, both of the auxiliary and the primary classifiers have the same WER 22.22% (HYP p has two insertion errors, and REF a has two deletion errors). The primary classifier corrects the misrecognized results of the auxiliary classifier but makes new mistakes, which can not be measured by WER. WDR measures the ratio that is correctly recognized by the auxiliary classifier but misrecognized by the primary classifier (two 'SUED' in HYP * p ), and WAR does in the opposite direction ('MEHR' and 'KALT' in HYP * p ). Based   Due to the alignment process and different weights of operations, the proposed three-sentence alignment strategy leads to a little performance degradation than the general WER, as discussed in Sect. 5.2. <ref type="figure" target="#fig_12">Fig. 9(b)</ref> and <ref type="figure" target="#fig_12">Fig. 9(c</ref> the alignment results, which often breaks substitution errors to more deletion and insertion errors. However, only a small ratio of sequences has such a problem, and we believe this problem is acceptable for results analysis. <ref type="figure" target="#fig_7">Figure 6</ref> in Sect. 5.2 visualizes the performance gap with different settings, and we present the detailed results in <ref type="table" target="#tab_0">Table 11</ref>. The conclusions in Sect. 5.2 are consistent on both dev and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details on the Performance Gap</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Visualization of Spatial Activations</head><p>We visualize some recognition results in the animation folder. As shown in <ref type="figure" target="#fig_1">Fig. 10</ref>, the reference and the predictions of baseline and Visual Alignment Constraint (VAC) are presented above the videos. The bottom videos visualize the activation changes during the signing. The activation maps are obtained by calculating the l 2 norm of the 7x7 ResNet18 feature maps. From the animation, we can observe that the baseline mainly focuses on the central area of frames, and the proposed method can dynamically focus on hands and facial expressions, which extracts more discriminative visual features. <ref type="figure" target="#fig_1">Figure 10</ref>. Interface of the recognition result animation. We highlight the wrong recognition glosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Visualizing of Magnitudes</head><p>In Sect. 3.3, we propose a magnitude hypothesis that the l 2 norms of features reflect the importance of frames. Besides, experimental results in Sect. 5.2 verify that the proposed VAC is more compatible with the spiky activations. <ref type="figure" target="#fig_1">Fig. 11</ref> presents the gate values, the l 2 norms of features, and the final predictions on dev and training sets. The baseline shows different behavior on training and dev sets: the norms of gloss and sequence features have consistent tendencies on the training set but the correlations become weakened on the dev set. Baseline+VAC shows consistent behavior on both sets, which indicates the effectiveness of the proposed VAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. More Qualitative Recognition Results</head><p>We visualize more sequences in <ref type="figure" target="#fig_1">Fig. 12</ref>, and we can notice that the prediction results of two classifiers are not always consistent. As shown in <ref type="figure" target="#fig_1">Fig. 12(a)</ref>, the primary classifier can provide better results by incorporating more context information. However, the primary classifier may neglect visual information or predict wrong glosses, which gives worse results in some cases, as shown in <ref type="figure" target="#fig_1">Fig. 12(b)</ref>. The proposed VAC attempts to make better use of visual and context information.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed non-iterative CLSR approach with the visual alignment constraint. To solve the insufficient training of the feature extractor, the proposed VAC enhances the generalization ability of the visual extractor by constraining the feature space with the alignment supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The proposed framework consists of three components: a feature extractor, an alignment module, and an auxiliary classifier Fa. The feature extractor first takes image sequence to abstract frame-wise features, and then applies 1D-CNN to extract the local visual information with ?t temporal receptive field. The outputs of 1D-CNN noted as visual features are sent to the alignment model and the auxiliary classifier. Two auxiliary losses are adopted during training: the visual enhancement loss (LV E ) aligns visual features and the target sequence, and the visual alignment loss (LV A) aligns short-term visual predictions and long-term context predictions through knowledge distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Figure 3 .</head><label>33</label><figDesc>Visualization of the gate values, the l2 norm of features and the final prediction of a training sample among different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>REF:</head><label></label><figDesc>__ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT REF : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUEDOST ABER KALT HYP : __ON__ HEUTE NACHT **** SCHNEE NORD SUEDOST ABER **** REF * : __ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP * : __ON__ HEUTE NACHT **** SCHNEE NORD **** SUEDOST **** ABER **** HYP * : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Alignment results of the proposed metrics. We highlight wrong recognized glosses and the alignment results of the auxiliary classifier and the primary classifier .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison among different settings with examples from training (the upper) and Dev (the lower) sets of PHOENIX14. Wrong recognized glosses (except del) are marked in red. The primary classifier and auxiliary classifier outputs are marked as (P) and (A). ?WER * (a) Results on PHOENIX14 training set. (b) Results on PHOENIX14 Dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Performance comparison with different metrics and settings (?WER * = WER * a ?WER * p = WAR?WDR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Learning curves of WER(%) on PHOENIX14 with different settings. The learning rate is decayed at 40 and 60 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Results on PHOENIX14 with sampled training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>), we first align the reference and the recognized sentences and refer the alignment results as to (REF p , HYP p ) and (REF a , HYP a ) for the primary classifier and the auxiliary classifier, respectively. Then we align REF a and REF p to obtain the aligned reference REF * . The final alignment results (REF * , HYP * a , HYP * p ) are presented in the last row of Fig. 9(a) by aligning (REF * , HYP a ) and (REF * , HYP p ), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>REF:</head><label></label><figDesc>__ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT REF : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUEDOST ABER KALT HYP : __ON__ HEUTE NACHT **** SCHNEE NORD SUEDOST ABER **** REF * : __ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP * : __ON__ HEUTE NACHT **** SCHNEE NORD **** SUEDOST **** ABER **** HYP * : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT Alignment process of three sentences. REF : __ON__ KUEHL KOMMEN TEMPERATUR SAMSTAG SONNTAG IX GLEICH HYP : **** KUEHL WEHEN TEMPERATUR SAMSTAG SONNTAG ** WIE REF : __ON__ KUEHL KOMMEN **** TEMPERATUR SAMSTAG SONNTAG IX GLEICH HYP : __ON__ KUEHL KOMMEN WEHEN TEMPERATUR SAMSTAG SONNTAG ** DASSELBE REF * : __ON__ KUEHL KOMMEN **** TEMPERATUR SAMSTAG SONNTAG IX GLEICH HYP * : **** KUEHL **** WEHEN TEMPERATUR SAMSTAG SONNTAG ** WIE An example of performance deterioration of the primary classifier. REF : MORGEN BESONDERS NORD REGION UND **** WEST **** REGEN KOENNEN HYP : MORGEN BESONDERS NORD REGION UND WEST WEST DANN REGEN KOENNEN REF : MORGEN BESONDERS NORD REGION UND WEST REGEN KOENNEN HYP : MORGEN BESONDERS NORD NACHT WEST WEST REGEN KOENNEN REF * : MORGEN BESONDERS NORD REGION UND **** WEST **** REGEN KOENNEN HYP * : MORGEN BESONDERS NORD **** *** NACHT WEST WEST REGEN KOENNEN An example of performance amelioration of the primary classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Alignment results of the proposed alignment method. We highlight wrong recognition glosses and the alignment results of the auxiliary classifier, the primary classifier .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of the gate values, the l2 norm of features and the final prediction on PHOENIX14. (a) The primary classifier provides better results than the auxiliary.(b) The auxiliary classifier provides better results than the primary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative comparison among different network settings with examples from Dev set on PHOENIX14. Wrong recognition results (except deletion operations) are marked in red. The primary classifier and auxiliary classifier outputs are marked as (P) and (A).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation results (WER, %) of iterative training and BN.</figDesc><table><row><cell>Iterations</cell><cell cols="2">w/o BN Dev Test</cell><cell cols="2">w/ BN Dev Test</cell></row><row><cell>1</cell><cell>32.7</cell><cell>33.0</cell><cell>27.2</cell><cell>28.0</cell></row><row><cell>2</cell><cell>28.9</cell><cell>29.8</cell><cell>25.5</cell><cell>26.3</cell></row><row><cell>3</cell><cell>28.3</cell><cell>28.9</cell><cell>24.7</cell><cell>26.2</cell></row><row><cell>None</cell><cell>30.4</cell><cell>32.1</cell><cell>25.4</cell><cell>26.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation results (WER, %) of Learning Rate (LR) ratios (LR of the feature extractor / LR of the alignment model).</figDesc><table><row><cell>LR Ratio</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>2</cell><cell>10</cell></row><row><cell>Dev</cell><cell>25.0</cell><cell>25.6</cell><cell>25.4</cell><cell>26.9</cell><cell>34.8</cell></row><row><cell>Test</cell><cell>25.6</cell><cell>26.5</cell><cell>26.6</cell><cell>27.5</cell><cell>35.1</cell></row><row><cell cols="6">Table 3. Ablation results (WER,%) of VAC design.</cell></row><row><cell></cell><cell cols="5">L CT C L V E L V A Dev Test</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.4 26.6</cell></row><row><cell>Baseline+VE</cell><cell></cell><cell></cell><cell></cell><cell cols="2">23.3 23.8</cell></row><row><cell>Baseline+VA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">24.5 25.1</cell></row><row><cell cols="2">Baseline+VAC</cell><cell></cell><cell></cell><cell cols="2">21.2 22.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation results (WER, %) of temporal layer design. C? and P? correspond to 1D convolutional layer and max pooling layer with a kernel size of ?, respectively.</figDesc><table><row><cell></cell><cell cols="3">Temporal Layers ?t Dev / Test</cell></row><row><cell>Frame-wise</cell><cell>C1 C3</cell><cell>1 3</cell><cell>25.2 / 26.5 24.4 / 25.4</cell></row><row><cell cols="2">Subgloss-wise C5-P2</cell><cell>6</cell><cell>24.0 / 24.3</cell></row><row><cell>Gloss-wise</cell><cell>C5-P2-C5-P2</cell><cell cols="2">16 21.2 / 22.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison on PHOENIX14 dataset. Results of the proposed method are based on ResNet18 and Gloss-wise temporal layer. The entries denoted by "*" used extra clues (such as keypoints and tracked face regions).</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Iteration</cell><cell>Dev(%) del/ins</cell><cell>WER</cell><cell>Test(%) del/ins</cell><cell>WER</cell></row><row><cell>SubUNet [3]</cell><cell>CaffeNet</cell><cell></cell><cell>14.6/4.0</cell><cell>40.8</cell><cell>14.3/4.0</cell><cell>40.7</cell></row><row><cell>Staged-Opt [8]</cell><cell>VGG-S/GoogLeNet</cell><cell></cell><cell>13.7/7.3</cell><cell>39.4</cell><cell>12.2/7.5</cell><cell>38.7</cell></row><row><cell>Align-iOpt [39]</cell><cell>3D-ResNet</cell><cell></cell><cell>12.6/2.6</cell><cell>37.1</cell><cell>13.0/2.5</cell><cell>36.7</cell></row><row><cell>Re-Sign [31]</cell><cell>GoogLeNet</cell><cell></cell><cell>-</cell><cell>27.1</cell><cell>-</cell><cell>26.8</cell></row><row><cell>SFL [36]</cell><cell>ResNet18</cell><cell></cell><cell>7.9/6.5</cell><cell>26.2</cell><cell>7.5/6.3</cell><cell>26.8</cell></row><row><cell>STMC [48]</cell><cell>VGG11</cell><cell></cell><cell>-</cell><cell>25.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DNF [9]</cell><cell>GoogLeNet</cell><cell></cell><cell>7.8/3.5</cell><cell>23.8</cell><cell>7.8/3.4</cell><cell>24.4</cell></row><row><cell>FCN [6]</cell><cell>Custom</cell><cell></cell><cell>-</cell><cell>23.7</cell><cell>-</cell><cell>23.9</cell></row><row><cell>CMA [38]</cell><cell>GoogLeNet</cell><cell></cell><cell>7.3/2.7</cell><cell>21.3</cell><cell>7.3/2.4</cell><cell>21.9</cell></row><row><cell>CNN+LSTM+HMM [27]*</cell><cell>GoogLeNet</cell><cell></cell><cell>-</cell><cell>26.0</cell><cell>-</cell><cell>26.0</cell></row><row><cell>DNF [9]*</cell><cell>GoogLeNet</cell><cell></cell><cell>7.3/3.3</cell><cell>23.1</cell><cell>6.7/3.3</cell><cell>22.9</cell></row><row><cell>STMC [48]*</cell><cell>VGG11</cell><cell></cell><cell>7.7/3.4</cell><cell>21.1</cell><cell>7.4/2.6</cell><cell>20.7</cell></row><row><cell>Baseline</cell><cell>ResNet18</cell><cell></cell><cell>8.3/3.1</cell><cell>25.4</cell><cell>8.8/3.2</cell><cell>26.6</cell></row><row><cell>Baseline+VAC</cell><cell>ResNet18</cell><cell></cell><cell>7.9/2.5</cell><cell>21.2</cell><cell>8.4/2.6</cell><cell>22.3</cell></row><row><cell cols="2">Table 6. Performance comparison (%) on CSL dataset. The entry</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">denoted by "*" used extra clues (keypoints).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LS-HAN [23]</cell><cell>17.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SubUNet [3]</cell><cell>11.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SF-Net [47]</cell><cell>3.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN [6]</cell><cell>3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STMC [48]*</cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline+VAC</cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation results (WER, %) of temporature ? .</figDesc><table><row><cell>?</cell><cell>1</cell><cell>4</cell><cell>8</cell><cell></cell><cell>12</cell><cell>16</cell></row><row><cell>Dev</cell><cell>22.1</cell><cell>22.0</cell><cell cols="2">21.2</cell><cell>21.7</cell><cell>21.6</cell></row><row><cell>Test</cell><cell>22.8</cell><cell>22.9</cell><cell cols="2">22.3</cell><cell>22.9</cell><cell>22.7</cell></row><row><cell cols="5">A.4. Ablation on Loss Weight ?</cell><cell></cell><cell></cell></row><row><cell cols="7">Table 8. Ablation results (WER, %) of loss weight ?.</cell></row><row><cell>?</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row><row><cell>Dev</cell><cell>22.1</cell><cell>21.9</cell><cell>21.5</cell><cell>21.2</cell><cell>21.5</cell><cell>22.0</cell></row><row><cell>Test</cell><cell>23.0</cell><cell>22.4</cell><cell>22.1</cell><cell>22.3</cell><cell>22.6</cell><cell>23.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Ablation results (WER, %) of augmentation.</figDesc><table><row><cell>Crop</cell><cell>Flip</cell><cell>Temporal Scaling</cell><cell>Dev</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell>28.1</cell><cell>28.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>23.8</cell><cell>24.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>26.1</cell><cell>26.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>27.4</cell><cell>27.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>23.2</cell><cell>23.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>22.1</cell><cell>23.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>More details about the temporal layer design. Conv1x? (1x? Convolution-BN-ReLU) and Max-pooling 1x? are used to extract different levels of features.</figDesc><table><row><cell>Layer</cell><cell>Output Size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>on the proposed metrics, we can calculate that both WAR and WDR are 22.22% and better understand the recognition results: the introduction of the alignment model brings<ref type="bibr" target="#b21">22</ref>.22% gains and extra 22.22% errors, so the total WER remains unchanged.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>) show some examples. Aligning REF a and REF p changesTable 11. Train/Dev/Test performance comparison (%) with different evaluate metrics on PHOENIX14. WER * p and WER * a correspond to the WER * results of primary classifier and auxiliary classifier, respectively, and ?WER * = WER * a ? WER * p = WAR ? WDR. Baseline 12.9 / 30.4 / 29.4 2.5 / 25.5 / 26.9 11.5 / 11.3 / 10.0 1.2 / 6.5 / 7.4 10.4 / 4.9 / 2.5 Baseline + iteration 7.9 / 27.5 / 27.0 1.9 / 25.1 / 26.3 7.0 / 9.8 / 8.8 0.9 / 7.3 / 8.2 6.0 / 2.4 / 0.7</figDesc><table><row><cell cols="5">WER  *  a ?WER  Baseline + VE WER  *  p WAR WDR 3.8 / 26.2 / 26.3 2.5 / 23.4 / 24.0 2.1 / 6.2 / 5.8 0.8 / 3.4 / 3.4 1.3 / 2.8 / 2.3</cell></row><row><cell>Baseline + VA</cell><cell>13.4 / 26.8 / 26.9 2.0 / 24.7 / 25.2</cell><cell>12.0 / 7.7 / 7.8</cell><cell cols="2">0.6 / 5.7 / 6.2 11.4 / 2.1 / 1.7</cell></row><row><cell>Baseline + VAC</cell><cell>3.9 / 25.1 / 25.2 1.9 / 22.2 / 23.0</cell><cell>2.3 / 5.5 / 5.0</cell><cell>0.4 / 2.6 / 2.8</cell><cell>2.0 / 2.9 / 2.2</cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The adopted alignment approach leads to a little performance degradation than the general WER.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This study was partially supported by the Natural Science Foundation of China under contract No. 61976219.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sign language recognition, generation, and translation: An interdisciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Bellard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larwan</forename><surname>Berke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Boudreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annelies</forename><surname>Braffort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><surname>Verhoef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subunets: End-to-end hand shape and continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint end-toend sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10023" to="10033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><surname>Leong Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="697" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7361" to="7369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep neural framework for continuous sign language recognition by iterative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1880" to="1891" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition techniques for a sign language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Zahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orientation histograms for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Automatic Face and Gesture recognition</title>
		<meeting>the International Workshop on Automatic Face and Gesture recognition</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A chinese sign language recognition system based on sofm/srn/hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaolin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2389" to="2402" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised sequence labelling with recurrent neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="61" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine learning</title>
		<meeting>the 23rd International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modelling and segmenting subunits for sign language recognition based on hand motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="623" to="633" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-based sign language recognition without temporal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement</title>
		<meeting>the Association for the Advancement</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ms-asl: A largescale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaezi</forename><surname>Hamid Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint ctcattention based end-to-end speech recognition using multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with multi-stream cnnlstm-hmms to discover sequential parallelism in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihan</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3793" to="3802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep sign: hybrid cnn-hmm for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4297" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6205" to="6214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinterpreting ctc-based training as iterative fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic fine-grained labeling of multi-state sign glosses for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic sign language analysis: A survey and the future beyond lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Sylvie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surendra</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="873" to="891" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting continuous sign language recognition via cross modality augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1497" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative alignment network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4165" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical multitask learning with ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Spoken Language Technology Workshop</title>
		<meeting>the 2018 IEEE Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sign language and linguistic universals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Lillo-Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative exemplar coding for sign language recognition with kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1418" to="1428" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Connectionist temporal fusion for sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Gang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1483" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01341</idno>
		<title level="m">Sf-net: Structured feature network for continuous sign language recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial-temporal multi-cue network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
