<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINE-TUNE THE ENTIRE RAG ARCHITECTURE (INCLUDING DPR RETRIEVER) FOR QUESTION-ANSWERING A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-22">22 Jun 2021 June 23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamane</forename><surname>Siriwardhana</surname></persName>
							<email>shamane@ahlab.org</email>
							<affiliation key="aff0">
								<orgName type="department">Auckland Bioengineering Instutute</orgName>
								<orgName type="institution">The University of Auckland Auckland</orgName>
								<address>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rivindu</forename><surname>Weerasekera</surname></persName>
							<email>rivindu@ahlab.org</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Wen</surname></persName>
							<email>elliott@ahlab.org</email>
							<affiliation key="aff2">
								<orgName type="department">Auckland Bioengineering Instutute</orgName>
								<orgName type="institution">The University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suranga</forename><surname>Nanayakkara</surname></persName>
							<email>suranga@ahlab.org</email>
							<affiliation key="aff3">
								<orgName type="department">Auckland Bioengineering Instutute</orgName>
								<orgName type="institution">The University of Auckland</orgName>
								<address>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FINE-TUNE THE ENTIRE RAG ARCHITECTURE (INCLUDING DPR RETRIEVER) FOR QUESTION-ANSWERING A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-22">22 Jun 2021 June 23, 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>NLP ? Question Answering ? Information Retrieval ? Transformers</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we illustrate how to fine-tune the entire Retrieval Augment Generation (RAG) architecture  in an end-to-end manner. We highlighted main engineering challenges that needed to be addressed to achieve this objective. We also compare how end-to-end RAG architecture outperforms the original RAG architecture for the task of question answering. We have open-sourced our implementation in the HuggingFace Transformers library <ref type="bibr" target="#b1">[Wolf et al., 2019]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In September 2020, Facebook open-sourced a new NLP model called Retrieval Augmented Generation (RAG) on the Hugging Face Transformer library. RAG is capable to use a set of support documents from an external knowledge base as a latent variable to generate the final output. The RAG model consists of an Input Encoder, a Neural Retriever, and an Output Generator. All three components are initialized with pre-trained transformers. However, the original Hugging Face implementation only allowed fine-tuning the Input Encoder and the Output Generator in an end-toend manner, while the Neural Retriever needs to be trained seperately. To the best of our knowledge, an end-to-end RAG implementation that trains all three components does not exist. In this paper, we introduce a novel approach to extending the RAG implementation which fulfills the task of training all three components end-to-end. Although this appears straightforward, there are many engineering challenges that need to be addressed.</p><p>As the name suggests, RAG adds an intermediate information retrieval step before the final generation process. It combines the ideas of neural information retrieval with seq2seq generation in an end-to-end manner. During the training phase, RAG takes an input and encodes it to a high-dimensional vector. This vector then gets used as a query to select the most relevant passages from an external database using Maximum Inner Product Search (MIPS) <ref type="bibr" target="#b2">[Johnson et al., 2017</ref><ref type="bibr" target="#b3">, Guo et al., 2020</ref>. Afterward, The input and the selected set of documents get fed into a generator that produces the final answer. It is important to emphasize that components of RAG are being initialized with pre-trained BERT and BART transformers. Finally, The probability of selecting documents given an input context (p(z|x)) is Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering A PREPRINT used to propagate gradients back to the question encoder. During the process 1 . of making the RAG end-to-end trainable, we mainly work on the Retriever part, which uses a neural retriever model called Dense Passage Retrieval (DPR) .</p><p>Our main contributions can be summarized as follows:</p><p>? Highlighting the importance of making entire RAG architecture end-to-end trainable. ? Exploration of engineering challenges regarding the extension of original RAG. ? Comparing the original RAG with end-to-end RAG on question answering to highlight the effectiveness of making entire RAG end-to-end trainable. ? Open source implementation 2 Rag Retriever</p><p>In our extension, we mainly work with the RAG retriever. The retriever basically consists of a pre-trained DPR model ]. The DPR model consists of two BERT models. One model encodes questions and the other model encodes the documents using the CLS token outputs. The DPR model used in RAG has already been trained with passages and questions extracted from open domain Wikipedia-based datasets. RAG has a neural retriever and a reader combined in an end-to-end manner. In practice, however, we freeze the passage encoder and only train the question encoder. In our approach, the passage encoder is trainable as well. In RAG, prior to fine-tuning, a frozen Passage Encoder is used to encode your external Knowledge Base (KB) and save it to disk. Then we do the dataset indexing. Imagine there are millions of documents that you need to search and retrieve during the training time. Usual vector similarity can be too slow. So we use some approximation methods by clustering the vector space into sub-regions. There are several libraries for this, such as ScANN <ref type="bibr" target="#b3">[Guo et al., 2020]</ref> and FAISS <ref type="bibr" target="#b2">[Johnson et al., 2017]</ref>. In the RAG implementation Huggingface uses FAISS to make the retrieval phase faster. After this step, we start the training process with the indexed dataset where we only update the model parameters of the Question Encoder and Generator networks. We do not use the Passage Encoder or change the encoded embeddings for documents during the training.</p><p>2.1 Why is it important to fine-tune the entire retriever?</p><p>RAG authors illustrated that it is acceptable not to fine-tune the Passage Encoder for tasks like question answering and fact-checking. But the authors have conducted their experiments mainly on open domain Wikipedia-like datasets <ref type="bibr" target="#b5">[Kwiatkowski et al., 2019]</ref>. Since DPR is also initially trained on Wiki-data, this really makes sense. Our exploration is mainly motivated by the following factors.</p><p>? Does training of the entire RAG-retriever help with domain adaptation? ? The authors of similar retrieval augmented models like REALM <ref type="bibr" target="#b6">[Guu et al., 2020</ref><ref type="bibr">, Sachan et al., 2021</ref> mention it would be advantageous to train the entire retriever.</p><p>3 Why is it hard to make the entire RAG end-to-end trainable?</p><p>Theoretically, there are no problems in propagating gradients to both the passage encoder and question encoder BERT model. As described in the RAG loss function , we can compute the document scores for latent documents (p(z|x)) by computing question encoder embeddings and document encoder embeddings during the training time. This has the following two steps,</p><p>? Combined a pre-trained Passage Encoder model to the RAG model prior to the training.</p><p>? During the training, first, retrieve relevant passages given a question by using the indexed dataset.</p><p>? Calculate the document scores by re-computing CLS embeddings for retrieved documents using the initialized passage BERT model and do the same thing for input using the question encoder.</p><p>Although the propagation of the gradients to the passage encoder is straightforward, we need to make sure the updated passage encoder gets used in the overall training architecture. So we have to update the indexed dataset during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Why updating the indexed KB during the training is time-consuming and computationally expensive?</head><p>The main two engineering challenges can be illustrated as follows.</p><p>? During training, we first need to send each passage in the external KB through the updated passage encoder to compute the CLS token. Let's call this step re-encoding the KB.</p><p>? Secondly, with the updated embeddings for each passage, we need to re-index the external KB with FAISS. Let's call this part re-indexing of the KB.</p><p>So, similar to the REALM training process, we can execute external KB re-encoding and re-indexing commands every N-training steps. The authors in REALM have mentioned that if we improve the frequency of KB updates, we can get better results (check section 3.3 in REALM paper <ref type="bibr" target="#b5">[Kwiatkowski et al., 2019]</ref>). Since the re-encoding and re-indexing processes can take large amounts of execution time, we should not practically hold the training process for hours until we finish the above two processes (read about the effectiveness of stale gradients in REALM paper in section 3.3).</p><p>4 Making the re-encoding and re-indexing practical during the training loop</p><p>We achieve this task by using HuggingFace Datasets, Pytorch-Lightning Falcon <ref type="bibr">[2019]</ref>, and Python Multiprocessing libraries. First, we need to make sure the re-encoding step is fast and does not make the entire training loop wait until it finishes the computation of embeddings for every passage in the KB. To achieve this we executed the re-encoding and re-indexing processes parallel to the main training loop.</p><p>To have a detailed explanation about the implementation details and a code walk-through, please refer our medium article 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We conducted a simple experiment to investigate the effectiveness of this end2end training extension using the SQuAD dataset. First we Created a knowledge-base using all the context passages in the SQuAD dataset with their respective titles. It is important to note that we chunk each context in to maximum of 100 words, which creates around 20 000 passages. Then we used the standard training and validation splits to train and evaluate our model. As illustrated in <ref type="table">Table 1</ref>, our results suggest that finetuning the RAG with the entire-DPR can outperform the original RAG by 12%.</p><p>Model Name Exact Match RAG-Original 28.12 RAG -(Ours) 40.02 <ref type="table">Table 1</ref>: Comparison of two RAG models on SQUAD training data, when using contexts as the knowledge base.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please refer to our medium tutorial by clicking this link</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://shamanesiri.medium.com/how-to-finetune-the-entire-rag-architecture-including-dpr-retriever-4b4385322552</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<title level="m">Tim Rockt?schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating largescale inference with anisotropic vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<title level="m">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrieval-augmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end training of neural retrievers for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00408</idno>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning,3" />
	</analytic>
	<monogr>
		<title level="j">WA. Pytorch lightning. GitHub</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. Falcon</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
