<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Cao</surname></persName>
							<email>jinming.ccao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Leng</surname></persName>
							<email>hanchao.leng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Cohen-Or</surname></persName>
							<email>cohenor@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
							<email>changhe.tu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
							<email>yangyan.lee@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding Author !&quot;# $%&amp;&apos;? ? ! ? &quot; ? ! ? &quot; ? # ? $ ? # ? $</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the RGB and depth features, ignoring their intrinsic differences. In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this observation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are introduced to cooperate with them independently, and finally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Extensive experiments on three challenging indoor RGB-D semantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID, demonstrate the effectiveness of our ShapeConv when employing it over five popular architectures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and memory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become constants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identical to one with vanilla convolutional layers. Figure 1. Visual demonstration of why the shape of an RGB-D image matters. Regarding the images on the top, lines with the same color share a same shape, yet with different base. The corresponding patches are shown on the bottom.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the widespread use of depth sensors (such as Microsoft Kinect <ref type="bibr" target="#b31">[31]</ref>), the availability of RGB-D data has boosted the advancement of RGB-D semantic segmentation, which contributes to an indispensable task in the computer vision community. Thanks to the flourishing of Convolutional Neural Networks (CNNs), recent studies mostly resort to CNNs for tackling this problem. Convolutional layers, deemed as the core building blocks of CNNs, are accordingly the key elements in RGB-D semantic segmentation models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>However, RGB and depth information are inherently different from each other. In particular, RGB values capture the photometric appearance properties in the projected image space, while the depth feature encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. As a result, the convolution operator that is widely adopted for consuming RGB data might not be the optimal for processing the depth data. Taking <ref type="figure">Figure 1</ref> as an example, we would expect the corresponding patches of the same chairs to have the same features, as they share the same shape. The shape is a more inherent property of the underlying object and has stronger connection to the semantics. We would expect to achieve shape invariance in the learning process. When a vanilla convolution operator is applied on these corresponding patches, the resulting features are different due to the differences in their base component, hindering the learning from achieving shape invariance. On the other hand, the base components cannot be simply discarded for pursuing the shape invariance in the current layer, as they form the shape in a followup layer with a larger context.</p><p>To address these problems, we propose a Shape-aware Convlutional layer (ShapeConv), to learn the adaptive balance between the importance of shape and base information, giving the network the chance to focus more on the shape information whenever necessary for benefiting the RGB-D semantic segmentation task. We firstly decompose a patch 1 into two separate components, i.e., a basecomponent and a shape-component. The mean of patch values depicts the whereabout of the patch in a larger context, thus constitutes the base component, while the residual is the relative changes in the patch, which depicts the shape of the underlying geometry, thus constitutes to the shape component. Specifically, for an input patch (such as P 1 in <ref type="figure">Figure 1</ref>), the base describes where the patch is, i.e., the distance from the observation point; while the shape expresses what the patch is, e.g., a chair corner. We then employ two operations, namely, base-product and shape-product, to respectively process these two components with two learnable weights, i.e., base-kernel and shape-kernel. The output from these two is then combined in an addition manner to form a shape-aware patch, which is further convolved with a normal convolutional kernel. In contrast to the original patch, the shape-aware one is capable of adaptively learning the shape characteristic with the shape-kernel, and the base-kernel serves to balance the contributions of the shape and the base for the final prediction.</p><p>In addition, since the base-kernel and shape-kernel become constants in the inference phase, we can fuse them into the following convolution kernel, resulting in a network that is identical to the one with vanilla convolutional layers. The proposed ShapeConv can be easily plugged into most CNNs as a replacement of the vanilla convolution in semantic segmentation without introducing any computation and memory increase in the inference phase. This simple replacement transforms CNNs designed for RGB data into ones better suited for consuming RGB-D data.</p><p>To validate the effectiveness of the proposed method, we conduct extensive experiments on three challenging RGB-D indoor semantic segmentation benchmarks: NYU-Dv2 <ref type="bibr" target="#b25">[25]</ref>(-13,-40), SUN RGBD <ref type="bibr" target="#b26">[26]</ref>, and SID <ref type="bibr" target="#b0">[1]</ref>. We apply our ShapeConv to five popular semantic segmentation architectures and can observe promising performance improvements compared with baseline models. We found that ShapeConv can significantly improve the segmentation accuracy around the object boundaries (see <ref type="figure">Figure 5</ref>), which demonstrates the effective leveraging of the depth information 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs have been widely used for semantic segmentation on RGB images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33]</ref>. In general, existing segmentation architectures usually involve two stages: the backbone and the segmentation stage. The former stage is leveraged to extract features from RGB images, wherein popular models are ResNet <ref type="bibr" target="#b11">[12]</ref>, ResNeXt <ref type="bibr" target="#b29">[29]</ref> which are pre-trained on the ImageNet dataset <ref type="bibr" target="#b24">[24]</ref>. The latter stage aims to generate predictions based on the extracted features. Methods in this stage include Upsample <ref type="bibr" target="#b18">[19]</ref>, PPM <ref type="bibr" target="#b33">[33]</ref> and ASPP <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, etc. It is worth noting that both stages adopt the convolutional layers as the core building blocks.</p><p>As RGB semantic segmentation has been extensively studied in literature, a straightforward solution for RGB-D semantic segmentation is to adapt the well-developed architectures from the ones designed for RGB data. However, implementing such a idea is non-trivial due to the asymmetric modality problem between the RGB and the depth information. To tackle this, researchers have devoted efforts into two directions: designing dedicated architectures for RGB-D data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref>, and presenting novel layers to enhance or replace the convolutional layers in RGB semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">30]</ref>. Our method falls into the second category.</p><p>Methods in the first category propose to feed RGB and depth channels to two parallel CNNs streams, where the output features are fused with specific strategies. For example, <ref type="bibr" target="#b5">[6]</ref> presents a gate-fusion method, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">21]</ref> fuse the features in multi-levels of the backbone stages. Nevertheless, these methods mostly leverage separate networks to consume RGB and depth features, they are yet faced with two limitations: 1) it is hard to decide when is the best stage for the fusion to happen; and 2) the two-stream or multilevel way often results in large increase of computation.</p><p>In contrast, methods along the second direction target at designing novel layers based on the geometric characteristics of RGB-D data, which are more flexible and timeefficient. For instance, Wang et al. <ref type="bibr" target="#b27">[27]</ref> proposed the depthaware convolution to weight pixels based on a hand-crafted Gaussian function by leveraging the depth similarity between pixels. <ref type="bibr" target="#b30">[30]</ref> presents a novel operator called mal-leable 2.5D convolution, to learn the receptive field along the depth-axis. <ref type="bibr" target="#b4">[5]</ref> devises a S-Conv to infer the sampling offset of the convolution kernel guided by the 3D spatial information, enabling the convolutional layer to adjust the receptive field and geometric transformations. ShapeConv proposed a novel view of the content in each patch and a mechanism to leverage them adaptively with learnt weights. Moreover, ShapeConv can be converted into vanilla convolution in the inference phase, resulting in ZERO increase of memory and computation compared with the models with vanilla convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first provide the basic formulation of the Shape-aware convolutional layer (ShapeConv) for RGB-D data, followed by its application in the training and inference phase. We end this section with the method architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ShapeConv for RGB-D Data</head><p>Method Intuition. Given an input patch P ? R K h ?Kw?Cin , K h and K w are the spatial dimensions of the kernel; C in represents the channel numbers in the input feature map, the output features from the vanilla convolution layer are obtained by,</p><formula xml:id="formula_0">F = Conv(K, P),<label>(1)</label></formula><p>where K ? R K h ?Kw?Cin?Cout denotes the learnable weights of kernels in a convolutional layer (The bias terms are not included for simplicity.); C out represents the channel numbers in the output feature map. Each element of F ? R Cout is calculated as,</p><formula xml:id="formula_1">F cout = K h ?Kw?Cin i (K i,cout ? P i ).</formula><p>It can be easily recognized that F usually changes with respect to different values of P. Take the two patches in the <ref type="figure">Figure 1</ref>, P 1 and P 2 , as an example. The corresponding output features, F 1 and F 2 from the vanilla convolution layer are learned by:</p><formula xml:id="formula_2">F 1 = Conv(K, P 1 ), F 2 = Conv(K, P 2 ).</formula><p>Since P 1 and P 2 are not identical (different distances from the observation points), accordingly, their features are usually different, and this may lead to distinct prediction results.</p><p>Nevertheless, P 1 and P 2 , corresponding to the red regions in <ref type="figure">Figure 1</ref>, actually belong to the same class -chair. And vanilla convolutional layers cannot well handle such situations. In fact, there exists some invariants of these two patches, namely, the shape. It refers to the relative depth correlation under local features, which is however, unexpectedly ignored by the existing methods. In view of this, we propose to fill this gap via effectively modeling the shape for RGB-D semantic segmentation.</p><p>ShapeConv Formulation. Based on the aforementioned analysis, in this paper, we offer to decompose an input patch into two components: a base-component P B describing where the patch is, and a shape-component P S expressing what the patch is. Therefore, we refer the mean <ref type="bibr" target="#b2">3</ref> of patch values to be P B , and its relative values to be as P S :</p><formula xml:id="formula_3">P B = m(P), P S = P ? m(P),</formula><p>where m(P) is the mean function on P (over the K h ? K w dimensions), and P B ? R 1?1?Cin , and P S ? R K h ?Kw?Cin .</p><p>Note that directly convolved P S with K in Equation 1 is sub-optimal, as the values from P B contributes the class discrimination across patches. Thus, our ShapeConv instead leverages two learnable weights, W B ? R 1 and W S ? R K h ?Kw?K h ?Kw?Cin , to separately consume the above two components. The outputted features are then combined in an element-wise addition manner, which forms a new shape-aware patch with the same size as the original one P. The formulation of ShapeConv is given as,</p><formula xml:id="formula_4">F = ShapeConv(K, W B , W S , P) = Conv(K, W B P B + W S * P S ) = Conv(K, P B + P S ) = Conv((K, P BS ),<label>(2)</label></formula><p>where and * denote the base-product and shape-product operator, respectively, which are defined as,</p><formula xml:id="formula_5">P B = W B P B P B1,1,c in = W B ? P B1,1,c in ,<label>(3)</label></formula><formula xml:id="formula_6">P S = W S * P S P S k h ,kw ,c in = K h ?Kw i (W S i,k h ,kw ,c in ? P Si,c in ), (4) where c in , k h , k w are the indices of the elements in C in , K h , K w dimensions, respectively.</formula><p>We reconstruct the shape-aware patch P BS from the addition of P B and P S , and P BS ? R K h ?Kw?Cin , which enables it to be smoothly convolved by the kernel K of vanilla convolutional layer. Nevertheless, the P BS is equipped with the important shape information which is learned by the two additional weights, making the convolutional layer to focus on the situations when merely using depth values fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ShapeConv in Training and Inference</head><p>Training phase. The proposed ShapeConv in Section 3.1 can effective leverage the shape information of patches. However, replacing vanilla convolutional layer with ShapeConv in CNNs introduces more computational cost due to the two product operation in Equation 3 and 4. To tackle this problem, we propose to shift these two operations from patches to kernels,</p><formula xml:id="formula_7">! # = 2, % $% = 3, % &amp;'( = 2 ! = #?%&amp;'()*+(,, . ! , . " , ?) =()*+(0 #$ , ?) 7 +, ? ( % ! = ()*+(,, ?) ( ? - - + 1 , + . " * , ? 1 , 5 0 #$ = . ! 7 +, 6 ) 6 * 6 * * -? 8 - 6 ) 8 - 8 - -? 8 -</formula><formula xml:id="formula_8">K B = W B K B K B1,1,c in ,c out = W B ? K B1,1,c in ,c out , K S = W S * K S K S k h ,kw ,c in ,c out = K h ?Kw i (W S i,k h ,kw ,c in ? K Si,c in ,c out ),</formula><p>where K B ? R 1?1?Cin?Cout and K S ? R K h ?Kw?Cin?Cout denote the base-component of kernels and shape-component, respectively, and K = K B + K S .</p><p>We therefore re-formalize ShapeConv the Equation 2 to following:</p><formula xml:id="formula_9">F = ShapeConv(K, W B , W S , P) = Conv(W B m(K) + W S * (K ? m(K)), P) = Conv(W B K B + W S * K S , P) = Conv(K B + K S , P) = Conv(K BS , P),<label>(5)</label></formula><p>where m(K) is the mean function on K (over the K h ? K w dimensions). And we require</p><formula xml:id="formula_10">K BS = K B + K S , K BS ? R K h ?Kw?Cin?Cout .</formula><p>In fact, the two formulations of ShpeConv, i.e., Equation 2 and Equation 5 are mathematically equivalent, i.e.,</p><formula xml:id="formula_11">F = ShapeConv(K, W B , W S , P) = Conv(K, P BS ) = Conv(K BS , P),<label>(6)</label></formula><formula xml:id="formula_12">F cout = K h ?Kw?Cin i (K i,cout ? P BSi ) = K h ?Kw?Cin i (K BSi,c out ? P i ),<label>(7)</label></formula><p>please refer to the Supp. for detailed proof. In this way, we utilize the ShapeConv in Equation 5 in our implementation as illustrated in <ref type="figure">Figure 2</ref>(b) and (c). Inference phase. During inference, since the two additional weights i.e. W B and W S , become constants, we can fuse them into K BS as shown in <ref type="figure">Figure 2</ref>(c) with K BS = W B K B + W S * K S . And K BS shares the same tensor size with K in Equation 1, thus, our ShapeConv is actually the same as the vanilla convolutional layer in <ref type="figure">Figure 2</ref>(a). In other words, when replacing vanilla convolution with ShapeConv, there would introduce zero additional inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ShapeConv-enhanced Network Architecture</head><p>Different from devising specially dedicated architectures for RGB-D segmentation <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b16">17]</ref>, the proposed ShapeConv is a more generalized approach that can be easily plugged into most CNNs as a replacement for the vanilla convolution in semantic segmentation, which is then transformed for adapting the RGB-D data. <ref type="figure" target="#fig_1">Figure 3</ref> depicts an example of the overall method architecture. In order to leverage the advanced backbones in semantic segmentation, we firstly require to convert the input features from RGB images to RGB-D data via the concatenation of the RGB and D information. In practice, D can be depth values <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref> or HHA 4 images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>. We then replace the vanilla convolution layer with the ShapeConv in both the backbone and segmentation stages. It is worth noting that, W B is initialized to one, W S can be viewed as C in square (K h ? K w ) ? (K h ? K w ) matrices, which are initialized to the identity matrix. In this way, ShapeConv is equivalent to the vanilla convolution at the beginning of training since K BS = K. This initialization approach offers two advantages: 1) It makes the ShapeConvenhanced networks do not interfere with the RGB data, i.e., the RGB features are processed in the same way as before.</p><p>2) It facilitates ShapeConv to reuse the parameters from pretrained models. Thus, with this approach, future advances in RGB semantic segmentation architectures can be easily transferred to consuming the RGB-D data, greatly reducing the effort that would otherwise be spent on designing dedicated networks for RGB-D semantic segmentation. We have shown the results of building RGB-D segmentation networks with this style using several popular architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33]</ref> in Sec 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets and metrics. Among the existing RGB-D segmentation problems, the indoor semantic segmentation is rather challenging, as the objects are often complex and with severe occlusions <ref type="bibr" target="#b4">[5]</ref>. Thus, in order to validate the effectiveness of the proposed method, we conducted experiments on three indoor RGB-D benchmarks: NYU-Depth-V2 (NYUDv2-13 and -40) <ref type="bibr" target="#b25">[25]</ref>, SUN-RGBD <ref type="bibr" target="#b26">[26]</ref> and Stanford Indoor Dataset (SID) <ref type="bibr" target="#b0">[1]</ref>. NYUDv2 contains 1,449 RGB-D scene images, where 795 images are split for training and 654 images for testing. We adopted two popular settings for this dataset, i.e., 13-class <ref type="bibr" target="#b25">[25]</ref> and 40-class <ref type="bibr" target="#b8">[9]</ref>, where all pixels are labeled with 13 and 40 classes, respectively. SUN-RGBD is composed of 10,355 RGB-D indoor images with 37 categories for each pixel label. We followed the widely used setting in <ref type="bibr" target="#b26">[26]</ref> to split the dataset into a training set of 5285 images and a testing set of 5050 images. SID contains 70, 496 RGB-D images with 13 object categories. In particular, areas 1, 2, 3, 4, and 6 used for the training and Area 5 is for testing following <ref type="bibr" target="#b27">[27]</ref>.</p><p>We reported the results using the same evaluation protocol and metrics as FCN <ref type="bibr" target="#b18">[19]</ref>, i.e., Pixel Accuracy (Pixel Acc.), Mean Accuracy (Mean Acc.), Mean Region Intersection Over Union (Mean IoU), and Frequency Weighted Intersection Over Union (f.w. IoU). Comparison protocol. We adopted several popular architectures with different backbones as our baseline methods to demonstrate the effectiveness and generalization capability of ShapeConv. For all the baseline methods, we only replaced the vanilla convolutional layers with our ShapeConv, without any change to other settings. This guarantees that the obtained performance improvements is due to the application of ShapeConv, but not other factors. Implementation Details. We used the ResNet <ref type="bibr" target="#b11">[12]</ref> and ResNeXt <ref type="bibr" target="#b29">[29]</ref> initialized with the pre-trained model on Im-ageNet <ref type="bibr" target="#b24">[24]</ref> in the backbone stage. If not otherwise noted, the inputs of both the baseline and ours are the concatenation of RGB and HHA images. We adopted both singlescale and multi-scale testing strategies during inference. For the latter one, left-right flipped images and five scales are exploited: [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]. in tables of this section denotes the multi-scale strategy. Note that, no post-processing tricks like CRF <ref type="bibr" target="#b1">[2]</ref> is used in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on Different Datasets</head><p>NYUDv2 Dataset. We adopted two popular settings for this dataset, i.e., 13-class <ref type="bibr" target="#b25">[25]</ref> and 40-class <ref type="bibr" target="#b8">[9]</ref>, and show the results of baseline and our method with different backbones on NYUDv2-13 and NYUDv2-40 in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>, respectively. It can be seen that architectures with ShapeConv outperform the baselines with a large margin under all settings. We also compare the performance of our ShapeConv with several recently developed methods in <ref type="table">Table 3</ref> and Table 4. As illustrated in <ref type="table">Table 3</ref>, ShapeConv achieves the best over all the four metrics on NYUDv2-13. Compared to the recently proposed method <ref type="bibr" target="#b32">[32]</ref>, our approach yields around 6.3% improvements on Mean IOU which is the most commonly used metric for semantic segmentation. In addition, our method also achieves a competitive performance on NYUDv2-40 in <ref type="table" target="#tab_3">Table 4</ref>. <ref type="table">Table 3</ref>.</p><p>Performance comparison with other methods on NYUDv2-13 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pixel   <ref type="table">Table 5</ref>. It can be observed that our ShapeConv also produces a positive effect under all settings. We also compared the performance of ours with several recently developed methods in <ref type="table" target="#tab_4">Table 6</ref>. It is worth noting that the performance of the ShapeConv-enhanced Network with backbone of ResNet-50 in <ref type="table">Table 5</ref> has already achieved better results than several methods in <ref type="table" target="#tab_4">Table 6</ref>, such as 3DGNN-101 <ref type="bibr" target="#b22">[22]</ref> and RDF-152 <ref type="bibr" target="#b21">[21]</ref> which take the ResNet-101 and -152 as backbone, respectively. SID Dataset. Note that SID dataset is much larger than the other two datasets, contributing to a better testbed for  evaluating RGB-D semantic segmentation model capabilities. The results on SID dataset between the baseline with ours and the state-of-the-art methods are reported in <ref type="table" target="#tab_5">Table 7</ref>.</p><p>We can observe that our ShapeConv surpasses these methods with a large margin. Note that even though we utilized a strong baseline (ResNet-101 backbone) which surpasses MMAF-Net-152 (ResNet-152 backbone) with 1.7% Mean IoU, our ShapeConv can still achieves a 6% Mean IoU improvement. This highlights the effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Different Architectures</head><p>Our proposed ShapeConv is a general layer for RGB-D semantic segmentation which can be easily plugged into most CNNs as a replacement for the vanilla convolution in semantic segmentation. To verify its generalization properties, we also evaluated the effectiveness of our method in several representative semantic segmentation architectures: Deeplabv3+ <ref type="bibr" target="#b3">[4]</ref>, Deeplabv3 <ref type="bibr" target="#b2">[3]</ref>, UNet <ref type="bibr" target="#b23">[23]</ref>, PSPNet <ref type="bibr" target="#b33">[33]</ref> and FPN <ref type="bibr" target="#b17">[18]</ref> with different backbones (ResNet-50 <ref type="bibr" target="#b11">[12]</ref>, ResNet-101 <ref type="bibr" target="#b11">[12]</ref>) on NYUDv2-40 dataset, and reported the performance in <ref type="table" target="#tab_6">Table 8</ref>. We can see that ShapeConv brings significant performance improvements under all settings, demonstrating the generalization capability of our method.  <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the qualitative results on NYUDv2-13 and -40, more results can be found in the Supp. As shown in this figure, the depth information, especially the detailed one, can be well utilized by ShapeConv to extract the object features. For instance, the chair and table regions in the top example of <ref type="figure" target="#fig_3">Figure 4</ref>(a) are with gradually changed colors, making it hard to predict accurate segmentation boundaries of the baseline method. The shape fea-  <ref type="figure">Figure 5</ref>. Segmentation accuracy around object boundaries. In this figure, the left is the visualization of the "trimap" measure; The right is the percent of misclassified pixels within trimaps of different widths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualization</head><p>tures learned by ShapeConv makes the accurate cut following the geometric hints compare with the conventional convolutional layer. For other two cases, i.e., the chair in the bottom example of <ref type="figure" target="#fig_3">Figure 4</ref>(a) and the desk in the top example of <ref type="figure" target="#fig_3">Figure 4(b)</ref>, the ShapeConv can also significantly improve the segmentation results in edge areas compared with the baseline. It is worth noting that for the multiple bookshelves in the bottom example of <ref type="figure" target="#fig_3">Figure 4</ref>(b), ShapeConv achieves more consistent predictions. This is because our ShapeConv yields a positive tendency for smoothing neighborhood regions within same classes.</p><p>To validate the effectiveness of our method on modeling the depth information, we adopted the comparison strategy proposed by Kohli et al. <ref type="bibr" target="#b13">[14]</ref>. Specifically, we counted the relative number of misclassified pixels within a narrow band ("trimap") surrounding ground-truth object boundaries. As shown in <ref type="figure">Figure 5</ref>, our method outperforms the baseline across all trimap widths. This further demonstrates the segmentation effectiveness of our method on edge areas, where the shape information matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conducted ablation experiments to validate the indispensability of the two introduced weights in Equation 5. As can be observed in <ref type="table" target="#tab_8">Table 9</ref>, the model performance degrades when removing either W B or W S , or both. This proves that both the base-kernel and shape-kernel are essential for the final performance improvement, and combing these two achieves the best results. To provide a more in-depth analysis of ShapeConv, we conducted detailed ablation studies on the NYUDv2-40 dataset with deeplabv3+ and ResNet-101 as baseline and backbone, respectively. Results on more datasets can be found at the Supp. <ref type="table" target="#tab_0">Table 10</ref> illustrates the results and the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a ShapeConv layer to effectively leverage the depth information for RGB-D semantic segmentation. In particular, an input patch is firstly decomposed into two components, i.e., shape and base, which are then decorated with two corresponding learnable weights before the convolution is applied. We have conducted extensive experiments on several challenging indoor RGB-D semantic segmentation benchmarks and promising experimental results can be observed. Moreover, it is worth noting that our ShapeConv introducing no additional computation or memory in comparison with the vanilla convolution during inference, yet with superior performance.</p><p>In fact, the shape-component is inherent in the local geometry and highly relevant to the semantics in images. In the future, we plan to expand the application scope to other geometry entities, such as point clouds, where the shapebase decomposition is more challenging due to the additional degree of freedom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6 Figure 2 .</head><label>62</label><figDesc>Comparison of vanilla convolution and ShapeConv within a patch P. In this figure, K h = Kw = 2, Cin = 3, and Cout = 2, "+" denotes element-wise addition. (a) Vanilla convolution with kernel K; (b) ShapeConv with folding the WB and WS into K BS ; (c) The computation of K BS from K, WB and WS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>OursFigure 3 .</head><label>3</label><figDesc>The overall semantic segmentation network architecture. In this figure, yellow and orange cube denote the RGB and D inputs; "C" denotes channel-wise concatenation; Green and blue boxes denote architectures consisting of vanilla convolutional layers and ShapeConv layers, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization results from NYUDv2 dataset. Input column denotes RGB, Depth, HHA images from top to bottom; the black regions in the GT, Baseline and Ours indicate the ignored category. The upper and lower cases are from NYUDv2-40 and NYUDv2-13, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with baselines on NYUDv2-13 dataset. Deeplabv3+ is the adopted architecture.</figDesc><table><row><cell>Back bone</cell><cell>Setting</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell></cell><cell>Baseline</cell><cell>80.0</cell><cell>72.5</cell><cell>60.8</cell><cell>67.6</cell></row><row><cell></cell><cell>Baseline</cell><cell>80.6</cell><cell>72.7</cell><cell>61.6</cell><cell>68.5</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>80.4</cell><cell>73.0</cell><cell>61.8</cell><cell>68.1</cell></row><row><cell>50 [12]</cell><cell>Ours</cell><cell>81.1</cell><cell>73.4</cell><cell>62.7</cell><cell>69.1</cell></row><row><cell></cell><cell>+</cell><cell>0.4</cell><cell>0.5</cell><cell>1.0</cell><cell>0.5</cell></row><row><cell></cell><cell>+</cell><cell>0.5</cell><cell>0.7</cell><cell>1.1</cell><cell>0.6</cell></row><row><cell></cell><cell>Baseline</cell><cell>80.0</cell><cell>73.4</cell><cell>61.3</cell><cell>67.6</cell></row><row><cell></cell><cell>Baseline</cell><cell>81.0</cell><cell>74.3</cell><cell>63.1</cell><cell>68.9</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>81.2</cell><cell>74.9</cell><cell>62.9</cell><cell>69.1</cell></row><row><cell>101 [12]</cell><cell>Ours</cell><cell>81.9</cell><cell>75.7</cell><cell>64.0</cell><cell>70.1</cell></row><row><cell></cell><cell>+</cell><cell>1.2</cell><cell>1.5</cell><cell>1.6</cell><cell>1.5</cell></row><row><cell></cell><cell>+</cell><cell>0.9</cell><cell>1.4</cell><cell>0.9</cell><cell>1.2</cell></row><row><cell></cell><cell>Baseline</cell><cell>81.8</cell><cell>73.9</cell><cell>63.2</cell><cell>70.1</cell></row><row><cell></cell><cell>Baseline</cell><cell>82.2</cell><cell>74.4</cell><cell>63.7</cell><cell>70.6</cell></row><row><cell>ResNext</cell><cell>Ours</cell><cell>82.6</cell><cell>75.7</cell><cell>65.1</cell><cell>71.2</cell></row><row><cell>101 32x8d</cell><cell>Ours</cell><cell>82.9</cell><cell>76.0</cell><cell>65.6</cell><cell>71.6</cell></row><row><cell>[29]</cell><cell>+</cell><cell>0.8</cell><cell>1.8</cell><cell>1.9</cell><cell>1.1</cell></row><row><cell></cell><cell>+</cell><cell>0.7</cell><cell>1.6</cell><cell>1.9</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison with baselines on NYUDv2-40 dataset. Deeplabv3+ is the adopted architecture.</figDesc><table><row><cell>Back bone</cell><cell>Setting</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell></cell><cell>Baseline</cell><cell>73.1</cell><cell>57.7</cell><cell>45.6</cell><cell>59.2</cell></row><row><cell></cell><cell>Baseline</cell><cell>74.2</cell><cell>59.0</cell><cell>47.1</cell><cell>60.2</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>74.1</cell><cell>59.1</cell><cell>47.3</cell><cell>60.5</cell></row><row><cell>50 [12]</cell><cell>Ours</cell><cell>75.0</cell><cell>60.4</cell><cell>48.8</cell><cell>61.4</cell></row><row><cell></cell><cell>+</cell><cell>1.0</cell><cell>1.4</cell><cell>1.7</cell><cell>1.3</cell></row><row><cell></cell><cell>+</cell><cell>0.8</cell><cell>1.4</cell><cell>1.7</cell><cell>1.2</cell></row><row><cell></cell><cell>Baseline</cell><cell>73.4</cell><cell>58.9</cell><cell>45.9</cell><cell>59.7</cell></row><row><cell></cell><cell>Baseline</cell><cell>74.4</cell><cell>60.2</cell><cell>47.6</cell><cell>60.7</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>74.5</cell><cell>59.5</cell><cell>47.4</cell><cell>60.8</cell></row><row><cell>101 [12]</cell><cell>Ours</cell><cell>75.5</cell><cell>60.7</cell><cell>49.0</cell><cell>61.7</cell></row><row><cell></cell><cell>+</cell><cell>1.1</cell><cell>0.6</cell><cell>1.59</cell><cell>1.1</cell></row><row><cell></cell><cell>+</cell><cell>1.1</cell><cell>0.5</cell><cell>1.4</cell><cell>1.0</cell></row><row><cell></cell><cell>Baseline</cell><cell>74.7</cell><cell>61.5</cell><cell>48.9</cell><cell>61.5</cell></row><row><cell></cell><cell>Baseline</cell><cell>75.4</cell><cell>62.6</cell><cell>50.3</cell><cell>62.2</cell></row><row><cell>ResNext</cell><cell>Ours</cell><cell>75.8</cell><cell>62.8</cell><cell>50.2</cell><cell>62.6</cell></row><row><cell>101 32x8d</cell><cell>Ours</cell><cell>76.4</cell><cell>63.5</cell><cell>51.3</cell><cell>63.0</cell></row><row><cell>[29]</cell><cell>+</cell><cell>1.1</cell><cell>1.3</cell><cell>1.3</cell><cell>1.1</cell></row><row><cell></cell><cell>+</cell><cell>1.0</cell><cell>0.9</cell><cell>1.0</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison with other methods on NYUDv2-40 dataset.</figDesc><table><row><cell>Method</cell><cell cols="5">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell cols="2">FCN [19]</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell><cell>49.5</cell></row><row><cell cols="2">LSD-GF [6]</cell><cell>71.9</cell><cell>60.7</cell><cell>45.9</cell><cell>59.3</cell></row><row><cell cols="2">D-CNN [27]</cell><cell>-</cell><cell>61.1</cell><cell>48.4</cell><cell>-</cell></row><row><cell cols="2">MMAF-Net [8]</cell><cell>72.2</cell><cell>59.2</cell><cell>44.8</cell><cell>-</cell></row><row><cell cols="2">ACNet [13]</cell><cell>-</cell><cell>-</cell><cell>48.3</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell>75.8</cell><cell>62.8</cell><cell>50.2</cell><cell>62.6</cell></row><row><cell>CFN [17]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>47.7</cell><cell>-</cell></row><row><cell cols="2">3DGNN [22]</cell><cell>-</cell><cell>55.7</cell><cell>43.1</cell><cell>-</cell></row><row><cell>RDF [21]</cell><cell></cell><cell>76.0</cell><cell>62.8</cell><cell>50.1</cell><cell>-</cell></row><row><cell cols="2">M2.5D [30]</cell><cell>76.9</cell><cell>-</cell><cell>50.9</cell><cell>-</cell></row><row><cell cols="2">SGNet [5]</cell><cell>76.8</cell><cell>63.3</cell><cell>51.1</cell><cell>-</cell></row><row><cell>Ours</cell><cell></cell><cell>76.4</cell><cell>63.5</cell><cell>51.3</cell><cell>63.0</cell></row><row><cell cols="6">Table 5. Performance comparison with baselines on SUN-RGBD</cell></row><row><cell cols="6">dataset. The architectures adopted in this table is deeplabv3+ with</cell></row><row><cell cols="2">different backbones.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>Setting</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell cols="2">Baseline</cell><cell>81.1</cell><cell>56.5</cell><cell>45.5</cell><cell>69.7</cell></row><row><cell cols="2">Baseline</cell><cell>81.4</cell><cell>57.5</cell><cell>46.6</cell><cell>70.0</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>81.6</cell><cell>56.8</cell><cell>46.3</cell><cell>70.3</cell></row><row><cell>50 [12]</cell><cell>Ours</cell><cell>81.9</cell><cell>57.9</cell><cell>47.7</cell><cell>70.6</cell></row><row><cell></cell><cell>+</cell><cell>0.5</cell><cell>0.3</cell><cell>0.8</cell><cell>0.6</cell></row><row><cell></cell><cell>+</cell><cell>0.5</cell><cell>0.4</cell><cell>1.1</cell><cell>0.6</cell></row><row><cell cols="2">Baseline</cell><cell>81.6</cell><cell>57.8</cell><cell>46.9</cell><cell>70.4</cell></row><row><cell cols="2">Baseline</cell><cell>81.6</cell><cell>58.4</cell><cell>47.6</cell><cell>70.5</cell></row><row><cell>ResNet</cell><cell>Ours</cell><cell>82.0</cell><cell>58.5</cell><cell>47.6</cell><cell>71.2</cell></row><row><cell>101 [12]</cell><cell>Ours</cell><cell>82.2</cell><cell>59.2</cell><cell>48.6</cell><cell>71.3</cell></row><row><cell></cell><cell>+</cell><cell>0.4</cell><cell>0.7</cell><cell>0.7</cell><cell>0.8</cell></row><row><cell></cell><cell>+</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison on SUN-RGBD dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell>3DGNN-101 [22]</cell><cell>-</cell><cell>55.7</cell><cell>44.1</cell><cell>-</cell></row><row><cell>D-CNN-50 [27]</cell><cell>-</cell><cell>53.5</cell><cell>42.0</cell><cell>-</cell></row><row><cell>MMAF-Net-152 [8]</cell><cell>81.0</cell><cell>58.2</cell><cell>47.0</cell><cell>-</cell></row><row><cell>SGNet-101 [5]</cell><cell>81.0</cell><cell>59.8</cell><cell>47.5</cell><cell>-</cell></row><row><cell>Ours-101</cell><cell>82.0</cell><cell>58.5</cell><cell>47.6</cell><cell>71.2</cell></row><row><cell>CFN-101 [17]</cell><cell>-</cell><cell>-</cell><cell>48.1</cell><cell>-</cell></row><row><cell>3DGNN-101 [22]</cell><cell>-</cell><cell>57.0</cell><cell>45.9</cell><cell>-</cell></row><row><cell>RDF-152 [21]</cell><cell>81.5</cell><cell>60.1</cell><cell>47.7</cell><cell>-</cell></row><row><cell>SGNet-101 [5]</cell><cell>82.0</cell><cell>60.7</cell><cell>48.6</cell><cell>-</cell></row><row><cell>Ours-101</cell><cell>82.2</cell><cell>59.2</cell><cell>48.6</cell><cell>71.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison on SID dataset. The architectures of baseline and ours adopted in this table is deeplabv3+ with ResNet-101 backbone and the "+" denote the deltas relative to the baseline method.</figDesc><table><row><cell>Method</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell>D-CNN [27]</cell><cell>65.4</cell><cell>55.5</cell><cell>39.5</cell><cell>49.9</cell></row><row><cell>MMAF-Net-152 [8]</cell><cell>76.5</cell><cell>62.3</cell><cell>52.9</cell><cell>-</cell></row><row><cell>Baseline-101</cell><cell>78.7</cell><cell>63.2</cell><cell>54.6</cell><cell>65.6</cell></row><row><cell>Ours-101</cell><cell>82.7</cell><cell>70.0</cell><cell>60.6</cell><cell>71.2</cell></row><row><cell>+</cell><cell>4.0</cell><cell>6.8</cell><cell>6.0</cell><cell>5.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Performance comparison with different baseline methods on NYUDv2-40 dataset.</figDesc><table><row><cell>Architecture</cell><cell>Back bone</cell><cell>Setting</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell></cell><cell>Res</cell><cell>Baseline</cell><cell>73.4</cell><cell>58.9</cell><cell>45.9</cell><cell>59.7</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>74.5</cell><cell>59.5</cell><cell>47.4</cell><cell>60.8</cell></row><row><cell>Deeplabv3+</cell><cell>101</cell><cell>+</cell><cell>1.1</cell><cell>0.6</cell><cell>1.5</cell><cell>1.1</cell></row><row><cell>[4]</cell><cell>Res</cell><cell>Baseline</cell><cell>73.1</cell><cell>57.7</cell><cell>45.6</cell><cell>59.2</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>74.1</cell><cell>59.1</cell><cell>47.3</cell><cell>60.5</cell></row><row><cell></cell><cell>50</cell><cell>+</cell><cell>1.0</cell><cell>1.4</cell><cell>1.7</cell><cell>1.3</cell></row><row><cell></cell><cell>Res</cell><cell>Baseline</cell><cell>73.3</cell><cell>57.3</cell><cell>45.1</cell><cell>59.2</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>73.6</cell><cell>58.5</cell><cell>46.4</cell><cell>59.7</cell></row><row><cell>Deeplabv3</cell><cell>101</cell><cell>+</cell><cell>0.3</cell><cell>1.2</cell><cell>1.3</cell><cell>0.5</cell></row><row><cell>[3]</cell><cell>Res</cell><cell>Baseline</cell><cell>71.6</cell><cell>55.5</cell><cell>43.2</cell><cell>57.2</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>72.8</cell><cell>56.6</cell><cell>44.9</cell><cell>58.5</cell></row><row><cell></cell><cell>50</cell><cell>+</cell><cell>1.2</cell><cell>1.1</cell><cell>1.7</cell><cell>1.3</cell></row><row><cell></cell><cell>Res</cell><cell>Baseline</cell><cell>70.9</cell><cell>54.7</cell><cell>42.1</cell><cell>57.7</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>72.3</cell><cell>56.5</cell><cell>43.9</cell><cell>58.8</cell></row><row><cell>UNet</cell><cell>101</cell><cell>+</cell><cell>1.4</cell><cell>1.8</cell><cell>1.8</cell><cell>1.1</cell></row><row><cell>[23]</cell><cell>Res</cell><cell>Baseline</cell><cell>70.0</cell><cell>51.7</cell><cell>39.7</cell><cell>55.5</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>70.8</cell><cell>54.1</cell><cell>42.0</cell><cell>56.9</cell></row><row><cell></cell><cell>50</cell><cell>+</cell><cell>0.8</cell><cell>2.4</cell><cell>2.3</cell><cell>1.4</cell></row><row><cell></cell><cell>Res</cell><cell>Baseline</cell><cell>72.8</cell><cell>56.8</cell><cell>44.2</cell><cell>58.9</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>73.3</cell><cell>59.2</cell><cell>46.3</cell><cell>59.6</cell></row><row><cell>PSPNet</cell><cell>101</cell><cell>+</cell><cell>0.5</cell><cell>2.4</cell><cell>2.1</cell><cell>0.7</cell></row><row><cell>[33]</cell><cell>Res</cell><cell>Baseline</cell><cell>71.1</cell><cell>53.6</cell><cell>42.0</cell><cell>56.7</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>72.0</cell><cell>56.2</cell><cell>44.0</cell><cell>57.7</cell></row><row><cell></cell><cell>50</cell><cell>+</cell><cell>0.9</cell><cell>2.6</cell><cell>2.0</cell><cell>1.0</cell></row><row><cell></cell><cell>Res</cell><cell>Baseline</cell><cell>72.8</cell><cell>57.3</cell><cell>44.7</cell><cell>59.1</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>73.6</cell><cell>58.4</cell><cell>45.9</cell><cell>60.0</cell></row><row><cell>FPN</cell><cell>101</cell><cell>+</cell><cell>0.8</cell><cell>1.1</cell><cell>1.2</cell><cell>0.9</cell></row><row><cell>[18]</cell><cell>Res</cell><cell>Baseline</cell><cell>70.3</cell><cell>52.8</cell><cell>40.9</cell><cell>56.0</cell></row><row><cell></cell><cell>Net</cell><cell>Ours</cell><cell>71.5</cell><cell>54.9</cell><cell>42.8</cell><cell>57.5</cell></row><row><cell></cell><cell>50</cell><cell>+</cell><cell>1.2</cell><cell>2.1</cell><cell>1.9</cell><cell>1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Performance comparison with and without WB and WS in ShapeConv on NYUDv2-40. The architecture adopted in this table is deeplabv3+ with ResNet-101 as backbone.</figDesc><table><row><cell>W B W S</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell></cell><cell>73.4</cell><cell>58.9</cell><cell>45.9</cell><cell>59.7</cell></row><row><cell></cell><cell>73.9</cell><cell>59.4</cell><cell>47.0</cell><cell>60.1</cell></row><row><cell></cell><cell>74.1</cell><cell>59.2</cell><cell>46.3</cell><cell>60.1</cell></row><row><cell></cell><cell>74.5</cell><cell>59.5</cell><cell>47.4</cell><cell>60.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Ablation study of the proposed ShapeConv on the NYUDv2-40 dataset. RGB, Detph and HHA denote the inputs consisting of RGB images, depth images and HHA images. key observations from this table are as follows: 1) The input features with HHA outperform the Depth images for the baseline and ours; 2) Replacing the vanilla convolution with ShapeConv leads to considerable performance improvements on both Depth and HHA; 3) The multi-scale setting in testing phase brings more performance gains; 4) Cascading the ShapeConv with HHA and multi-scale testing can achieve the best result.</figDesc><table><row><cell>Setting</cell><cell cols="4">Pixel Acc.(%) Acc.(%) IoU.(%) IoU.(%) Mean Mean f.w.</cell></row><row><cell>a.RGB</cell><cell>71.8</cell><cell>56.9</cell><cell>43.9</cell><cell>57.3</cell></row><row><cell>b.RGB+Depth</cell><cell>72.8</cell><cell>58.9</cell><cell>44.9</cell><cell>57.7</cell></row><row><cell>c.RGB+Depth</cell><cell>73.9</cell><cell>59.1</cell><cell>46.8</cell><cell>60.0</cell></row><row><cell>d.RGB+HHA</cell><cell>73.4</cell><cell>58.9</cell><cell>45.9</cell><cell>59.7</cell></row><row><cell>e.RGB+HHA</cell><cell>74.4</cell><cell>60.2</cell><cell>47.6</cell><cell>60.7</cell></row><row><cell>f.RGB+Depth+ShapeConv</cell><cell>73.9</cell><cell>58.2</cell><cell>46.2</cell><cell>60.0</cell></row><row><cell>g.RGB+Depth+ShapeConv</cell><cell>74.8</cell><cell>59.2</cell><cell>47.5</cell><cell>60.8</cell></row><row><cell>h.RGB+HHA+ShapeConv</cell><cell>74.5</cell><cell>59.5</cell><cell>47.4</cell><cell>60.8</cell></row><row><cell>i.RGB+HHA+ShapeConv</cell><cell>75.5</cell><cell>60.7</cell><cell>49.0</cell><cell>61.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The operation unit of input features for the convolutional layer, whose spatial size is the same as the convolution kernel.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is released through https://github.com/hanchaoleng/ShapeConv.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As the depth values are obtained from a fixed observation point, we notice that the rotational transformations cannot be addressed due to the angle of view limitation. As a result, we focus more on the translational transformations in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Horizontal disparity, Height above ground and normal Angle to the vertical axis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial information guided convolution for real-time rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2313" to="2324" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3029" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal attention-based fusion model for semantic segmentation of rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based multi-modal fusion network for semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11402" to="11409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1311" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-view deep learning for consistent semantic mapping with rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Sang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4980" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Malleable 2.5 d convolution: Learning receptive fields along the depth-axis for rgb-d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09365</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense rgb-d semantic mapping with pixel-voxel neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulak</forename><surname>Purkait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duckett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rustam</forename><surname>Stolkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3099</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
