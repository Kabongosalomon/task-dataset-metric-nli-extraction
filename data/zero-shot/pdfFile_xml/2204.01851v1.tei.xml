<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Grassucci</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gioia</forename><surname>Mancini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Brignone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Comminiello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Information Engineering, Electronics and Telecommunications (DIET)</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dual Quaternions</term>
					<term>Quaternion Neural Networks</term>
					<term>Quaternion Ambisonics Signals</term>
					<term>Dual Quaternion Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial audio methods are gaining a growing interest due to the spread of immersive audio experiences and applications, such as virtual and augmented reality. For these purposes, 3D audio signals are often acquired through arrays of Ambisonics microphones, each comprising four capsules that decompose the sound field in spherical harmonics. In this paper, we propose a dual quaternion representation of the spatial sound field acquired through an array of two First Order Ambisonics (FOA) microphones. The audio signals are encapsulated in a dual quaternion that leverages quaternion algebra properties to exploit correlations among them. This augmented representation with 6 degrees of freedom (6DOF) involves a more accurate coverage of the sound field, resulting in a more precise sound localization and a more immersive audio experience. We evaluate our approach on a sound event localization and detection (SELD) benchmark. We show that our dual quaternion SELD model with temporal convolution blocks (DualQSELD-TCN) achieves better results with respect to real and quaternion-valued baselines thanks to our augmented representation of the sound field. Full code is available at: https://github.com/ispamm/DualQSELD-TCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, spatial audio is knowing an increasing attention also due to the widespread developing of applications requiring an immersive audio experience, such as virtual reality, scene characterization, speech enhancement or separation, and sound source localization <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Indeed, while virtual reality (VR) builds spaces different from reallife, augmented reality (AR) expands them through an enlarged user listening experience that is often based on the user immersion into a 3D sound field <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Frequently, this spatial sound field is acquired through First Order Ambisonics (FOA) microphones <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, which are arrays of four ideally coincident capsules that decompose the sound field into a combination of spherical harmonics. Due to the strong correlation among the four Ambisonics signals, quaternion neural networks (QNNs) have demonstrated their ability to suitable model these inputs and grasp information coming from each capsule. Indeed, QNNs handle the four signals as a single component and thanks to the quaternion algebra properties, in particular the Hamilton product, they preserve signals relations and correlations. These models have shown interestingly results in different tasks including sound event localization and detection (SELD), which is the task of jointly learning the temporal and spatial location of a sound and its class <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Despite QNNs success in spatial source localization, no tests have been made with multiple Ambisonics microphones since these networks do not properly handle non-4D inputs due to the four-dimensional nature of quaternion numbers. However, an array of two Ambisonics provides a more precise sound field coverage, thus an appropriate processing of these features may bring better localization predictions <ref type="bibr" target="#b17">[18]</ref>. Moreover, while the sound event detection (SED) sub-task is relatively easy, the sound direction of arrival (DOA) estimation sub-task strongly depends on the accuracy of the sound field reconstruction.</p><p>Recently, dual quaternion neural networks (Du-alQNNs) have been shown to be particularly suitable for modelling transformations in 3D space. Due to the 6 degrees of freedom (6DOF) of the the unit dual quaternion representation, DualQNNs can properly model rotations and translations of rigid bodies <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, or knowledge graph embeddings <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. DualQNNs lay their foundations in dual quaternion numbers, which are a composition of two quaternions, thus particularly suitable for encapsulating eight-dimensional inputs.</p><p>For these reasons, we propose to exploit Du-alQNNs properties, including the ability to model 6DOF transformations in the 3D space, to represent an array of two Ambisonics microphones, whose 6DOF <ref type="bibr" target="#b24">[25]</ref> perfectly fit with our augmented characterization. Thanks to the unit dual quaternion representation, our method is able to more precisely reconstruct and augment the spatial sound field, thus improving the localization capability of the model and the quality of user audio immersion in AR and VR applications. We evaluate the potential of our approach in the SELD task with a focus on the DOA sub-task in the recent benchmark Learning 3D Audio Sources (L3DAS21) dataset <ref type="bibr" target="#b10">[11]</ref>. We show how the proposed 6DOF dual quaternion characterization for an array of two Ambisonics microphones better models the spatial sound field, thus achieving a more precise localization. Specifically, our contributions are:</p><p>? We propose a dual quaternion representation for an array of Ambisonics microphones, which exploits quaternion algebra properties to preserve correlations among input signals and dual quaternion features to properly reconstruct the 3D spatial sound field.</p><p>? We show how the proposed dual quaternion sound event localization and detection network (DualQSELD-TCN) presents an increased ability for the localization sub-task due to the dual quaternion representation.</p><p>? We build the global SELD (G-SELD) metrics, a more robust evaluation metrics for the SELD task, which balances the assessments computed with both the location-sensitive detection metrics and the class-sensitive localization metrics.</p><p>The rest of the paper is organized as follows. In Section 2 we expound the background on quaternion algebra and dual quaternion operations. Section 3 introduces the dual quaternion Ambisonics representation, while Section 4 presents the proposed DualQSELD-TCN, which is evaluated in Section 5. Finally, we draw conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Quaternions and Dual Quaternions Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Quaternion numbers</head><p>A quaternion number is a direct noncommutative extension of a complex-valued number, involving three imaginary units and four real-valued coefficients. More generally, the set of quaternion numbers H lies in a four-dimensional associative normed division algebra, belonging to the class of Clifford algebras <ref type="bibr" target="#b25">[26]</ref>. A quaternion number is represented as</p><formula xml:id="formula_0">q = q W + q X? + q Y? + q Z? ,<label>(1)</label></formula><p>whereby q W is the real part and q = q X? +q Y? +q Z? the imaginary one, in which the units comply wit? ? 2 =? 2 =? 2 = ?1, yielding to the multiplication ?? = ???;?? = ???;?? = ???. If the real part q W is equal to 0, the resulting element is called a pure quaternion. The conjugate of a quaternion is q * = q W ? q X? ? q Y? ? q Z? , while a quaternion with |q| = 1, where | ? | euclidean norm in R 4 , is a unit quaternion. The addition of two quaternions q and p is performed element-wise as q + p = (q W +p W )+(q X +p X )?+(q Y +p Y )?+(q Z +p Z )?. The multiplication between two quaternions is instead defined by q?p = (q W p W ?q?p, q?p+q W p+p W q), in which ? is the dot product among vectors and ? is the cross product. The multiplication between quaternions, also known as Hamilton product, was defined to properly model interplays among imaginary units due to the non-commutativity of cross products in this domain. The Hamilton product can be also expressed in matrix form as follows:</p><formula xml:id="formula_1">q ? p = ? ? ? ? q W ?q X ?q Y ?q Z q X q W ?q Z q Y q Y q Z q W ?q X q Z ?q Y q X q W ? ? ? ? ? ? ? ? p W p X p Y p Z ? ? ? ? .<label>(2)</label></formula><p>Moreover, quaternions are particularly appropriate to represent rotation in R 3 . Indeed, a quaternion can be expressed through polar coordinates as</p><formula xml:id="formula_2">q ? = q 0 + q = cos ? + u sin ?,<label>(3)</label></formula><p>with ? ? (??, ?] and u unit vector that indicates the direction. The rotation matrix can be then defined following <ref type="bibr" target="#b26">[27]</ref>, and applied to the pure quaternion vector to be rotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dual Quaternion numbers</head><p>Dual numbers have a similar form to complex numbers and still being part of a hypercomplex number system discovered by Clifford <ref type="bibr" target="#b27">[28]</ref>. They are composed of two elements, a real part and a dual part multiplied by the dual unit. Formally, they can be introduced as</p><formula xml:id="formula_3">d = d 1 + d 2 ,<label>(4)</label></formula><p>with d 1 , d 2 ? R and dual unit complying with the properties = 0, 2 = 0. As for quaternions, also this domain involves the conjugate operation, which is described as d * = d 1 ? d 2 . Addition and subtraction of dual numbers are element-wise operations:</p><formula xml:id="formula_4">d+c = (d 1 +c 1 )+ (d 2 +c 2 ), while the multiplication is dc = d 1 c 1 + (d 1 c 2 + c 1 d 2 ) + 2 (c 2 d 2 )</formula><p>, however, since 2 = 0, the last term vanishes. As for the Hamilton product, also dual numbers product has a matrix form which follows</p><formula xml:id="formula_5">dc = d 1 c 1 0 c 1 d 2 d 1 c 2 .<label>(5)</label></formula><p>Dual quaternions are dual numbers involving quaternions instead of real coefficients with the dual unit that commutes with each element of the algebra that is ? =? ; ? =? ; ? =? . Differently from quaternions, however, they do not form a division algebra. Given two quaternions q and q , a dual quaternion number is expressed as</p><formula xml:id="formula_6">q d = q + q .<label>(6)</label></formula><p>Interestingly, due to their complexity, dual quaternions have two conjugation operations. The first one is computed by conjugating both the quaternions q and q resulting in q * 1</p><formula xml:id="formula_7">d = q W ? q X? ? q Y? ? q Z? + (q ,W ? q ,X? ? q ,Y? ? q ,Z? ).</formula><p>The second conjugation is computed by conjugating the dual unit too, thus the formula becomes q * 2</p><formula xml:id="formula_8">d = q W ?q X? ?q Y? ?q Z? + (?q ,W +q ,X? +q ,Y? +q ,Z? ).</formula><p>As for previous number systems, also dual quaternions involve an element-wise addition: q d + p d = (q + p) + (q + p ). The multiplication operation can be instead introduced involving the Hamilton product in (2) as</p><formula xml:id="formula_9">q d p d = q ? p + (q ? p + q ? p).</formula><p>Therefore, its matrix form can be written as:</p><formula xml:id="formula_10">q d p d = (q ? p ) 0 q ? p (q ? p) ,<label>(7)</label></formula><p>whereby ? is the Hamilton product in (2) and the upper-right term is 0 due to the dual unit property 2 = 0. Interestingly, dual quaternions are suitable for jointly applying rotation and translation. Given a rotation matrix R and a translation vector t = (t 1 , t 2 , t 3 ), a point v can be rotated and then translated by Rv + t. Then, employing the quaternion polar form in <ref type="formula" target="#formula_2">(3)</ref>, the rotation-translation transformation can be encapsulated in a dual quaternion as</p><formula xml:id="formula_11">? = q ? + 2 tq ? .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dual Quaternion Ambisonics Signals</head><p>First-order Ambisonics (FOA) microphones are composed of 4 capsules in which the first one corresponds to the spherical harmonic of order 0 and it is usually a pressure microphone that is an omnidirectional microphone. The latter three capsules capture instead the acoustic velocity, corresponding to the harmonic functions of order 1. Respectively, these capsules are named W, X, Y, and Z. Therefore, a discrete-time signal s[n] with angles ?, ? can be represented in the so-called B-format Ambisonics representation:</p><formula xml:id="formula_12">? ? ? ? ? ? ? ? ? x W [n] = s[n]/ ? 3 x X [n] = s[n] cos ? cos ? x Y [n] = s[n] sin ? cos ? x Z [n] = s[n] sin ?.<label>(9)</label></formula><p>This representation involves highly correlated components that can be straightforwardly enclosed in a quaternion by treating the four signals as the real-valued coefficients of the quaternion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. However, when dealing with an array of two microphones, we need to encapsulate the resulting eight channels in two different quaternions, where each microphone signal will have the form in <ref type="bibr" target="#b8">(9)</ref>. Nevertheless, these representations are processed as different entities since QNNs deal with fourdimensional inputs only. This separation may cause a loss of correlated information coming from the eight capsules signal. An array of two Ambisonics microphones is usually employed to have a more wide and precise coverage of the spatial sound field. Nevertheless, using two different quaternions to treat them just build two different spatial representations, while the proper way should be building an augmented representation of it.</p><p>To address this issue, we propose to exploit the augmented representation given by the dual quaternion form and enclose the B-format Ambisonics signals of microphones A and B in</p><formula xml:id="formula_13">x = x A W [n] + x A X [n]? + x A Y [n]? + x A Z [n]? + (x B W [n] + x B X [n]? + x B Y [n]? + x B Z [n]?).<label>(10)</label></formula><p>Thanks to the dual quaternion characterization, we build a more compact representation of the two microphones signals. However, (10) has eight degrees of freedom (8DOF) while the dual ambisonics spatial field has just six degrees. Therefore, to parameterize the spatial sound field, two constraints have to be introduced for a dual quaternion. A common approach in kinematics, where known methods want to parameterize rotations (3DOF) and translations (3DOF) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>, is normalizing (10) to a unit dual quaternion. Given the signal components of the two quaternions x A and x B in the dual representation (10), the constraints are:</p><formula xml:id="formula_14">x A 2 W + x A 2 X + x A 2 Y + x A 2 Z = 1 (11) x A W x B W + x A X x B X + x A Y x B Y + x A Z x B Z = 0.<label>(12)</label></formula><p>Following <ref type="bibr" target="#b22">[23]</ref>, the first constraint (11) is forced by</p><formula xml:id="formula_15">x A = x A |x A | = x A W + x A X + x A Y + x A Z x A 2 W + x A 2 X + x A 2 Y + x A 2 Z .<label>(13)</label></formula><p>The second constraint, instead, can be imposed applying the followin?</p><formula xml:id="formula_16">x B = x B ? x B x A x A 2 x A .<label>(14)</label></formula><p>By applying these transformations to the dual quaternion input we reduce it to have 6DOF. This representation allows an accurate reconstruction of the spatial sound field leading to a more precise localization in the 3D space and a more immersive audio experience for AR and VR applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dual Quaternion Network for Sound</head><p>Event Localization and Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quaternion and Dual Quaternion Networks</head><p>In this section, we expound the main concepts underlying quaternion and dual quaternion neural networks. We give formal definitions for fullyconnected (FC) layers in real, quaternion and dual quaternion domain, however, the same definitions hold for convolutional layers too <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Given an input x ? R m?1 and a set of parameters W ? R n?m of weights and b ? R n?1 biases, a realvalued FC layer takes the form</p><formula xml:id="formula_17">y = Wx + b,<label>(15)</label></formula><p>whereby y ? R n?1 is the output and W contains n ? m parameters. Quaternion Neural Networks (QNNs) define each input, weight, bias and output as a quaternion in <ref type="bibr" target="#b0">(1)</ref>. Therefore, the multiplication in (15) becomes a multiplication between two quaternions and has to be performed following the Hamilton product in <ref type="bibr" target="#b1">(2)</ref>. The quaternion FC (Q-FC) layer with a weight matrix W = W W + W X? + W Y? + W Z? is then:</p><formula xml:id="formula_18">y = W ? x + b.<label>(16)</label></formula><p>The weight matrix W ? H n?m comprises realvalued submatrices, which are composed according to <ref type="bibr" target="#b1">(2)</ref>. Due to the reusing of submatrices, a quaternion weight matrix has n ? m/4 parameters, thus, quaternion layers save 75% of parameters <ref type="bibr" target="#b30">[31]</ref>. Moreover, since submatrices are also shared among input components (e.g., W W is multiplied for each one of the components of x, and so on), QNNs are capable of learning correlations among input dimensions such as pixels of RGB images or signals of multichannel audio <ref type="bibr" target="#b13">[14]</ref>.</p><p>As for quaternion models, also Dual Quaternion neural networks (DualQNNs) operate with dual quaternion inputs, weights and outputs. Therefore, the weight matrix W takes the form of (7) involving two quaternion weight matrices Q = Q W + Q X? + Q Y? +Q Z? and Q = Q ,W +Q ,X? +Q ,Y? +Q ,Z? . Interestingly, the resulting matrix is more sparse with respect to real and quaternion-valued matrices due to the zero-block component in <ref type="bibr" target="#b6">(7)</ref> [20].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DualQSELD-TCN</head><p>To test the dual quaternion representation abilities in modelling a real-world 3D spatial sound field, we perform the task of sound event localization and detection (SELD). This task consists in jointly performing two sub-tasks: the first one is the sound event detection (SED) that aims at learning the class of the sound and the segment of the audio frame at which this sound appears, while the second one is the direction of arrival (DOA) estimation, thus learning the spatial coordinates of the sound source. The latter is the most arduous challenge and the one on which we focus our attention. Indeed, while the detection task is not heavily influenced by the kind of input representation we employ, a dual quaternion model should instead better capture the DOA thanks to its augmented representation of the 3D space.</p><p>To this end, we propose a dual quaternion sound event localization and detection network with temporal convolutional blocks (DualQSELD-TCN) inspired by <ref type="bibr" target="#b14">[15]</ref>. In this model, we exploit operations in the dual quaternion domain, including the product in <ref type="bibr" target="#b6">(7)</ref> for neural layers and we enclose the two microphones signals as in <ref type="bibr" target="#b9">(10)</ref>. We extract the features through a short-time Fourier transform (STFT) with an Hamming window of length 512. Magnitudes from the two microphones are then stacked resulting in an input of T ? 256 ? 8 with T frames and where the last dimension becomes 16 whether we involve also the phase information. The model, whose overall architecture is displayed in <ref type="figure" target="#fig_0">Fig. 1b</ref>, then handles the eightchannel input through a dual quaternion convolution (DualQ-Conv) block and a DualQ temporal convolution (DualQ-TC) block, which together compose a DualQ-Conv-TC block <ref type="figure" target="#fig_0">(Fig. 1a</ref>). Instead, the processing of the sixteen-channel input is performed by employing two parallel DualQ-Conv-TC blocks <ref type="figure" target="#fig_0">(Fig. 1c</ref>), allowing to encapsulate the magnitude and the phase of each microphone in a dual quaternion and process them appropriately. Finally, in both cases, two separate branches take care of predicting the SED and the DOA. More in detail, the first architecture block comprises three DualQ-Conv2D layers, each followed by batch normalization layer, ReLU activation function, max pooling applied to the frequency axis and dropout. The input should then be properly organized to be fed into the DualQ-TC block, therefore we stack frequency and channel axis together <ref type="bibr" target="#b14">[15]</ref>. The core part of our model is the TC block, which has been introduced to remove the computational-heavy sequential processing of recurrent networks <ref type="bibr" target="#b31">[32]</ref>. It involves 10 residual blocks containing DualQ noncausal dilated convolutions (DualQ-DConv) that enlarge the receptive field considering also future samples rather than only present and past ones <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. These layers have a dilation rate computed by employing the first ten numbers of the Fibonacci sequence. A batch normalization layer is applied to the output of each DualQ-DConv and, consequently, we employ a gated tanh unit (GTU) with the form</p><formula xml:id="formula_19">y = tanh(W f * x) ?(W g * x),<label>(17)</label></formula><p>in which W f and W g are the convolutional filters of the left and right side of the gate, with tanh and sigmoid activation functions, respectively. The input goes then into a spatial dropout and in two parallel 1D DualQ-Conv layers, where the first one is used for the skip connection and the second one to preserve the dimensionality of the residual connection that flows into the next residual block together with the original input. The sum of all the skip connections is then passed through a ReLU activation function and then to the last DualQ-Conv layers with ReLU and tanh, respectively, with max pooling applied after each activation in order to match the correct output dimension. Finally, in the eight-channel input case, the classifier branches are composed of DualQ fully connected layers (DualQ-FC) while, in the sixteen-channel input case, the classifier branches are completely real-valued. This is because the two parallel DualQ-TC blocks learn different representations and a dual quaternion representation of them would not be appropriate. In both cases, dropout is applied and an ending realvalued FC is employed to specify the number of classes for SED and the cartesian coordinates for DOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Discussion</head><p>In this section, we present the experimental evaluation and the results discussion. To be consistent with previous literature where novel hypercomplex models are usually compared with their realvalued and quaternion-valued counterparts <ref type="bibr" target="#b34">[35]</ref>, we evaluate the performance of the proposed approach against the real-valued SELD-TCN baseline <ref type="bibr" target="#b31">[32]</ref> and its quaternion counterpart, QSELD-TCN <ref type="bibr" target="#b14">[15]</ref>. We slightly modify the latter to properly process  signals from two Ambisonics, thus we create two parallel Q-Conv-TC blocks in order to involve a quaternion for each microphone, resulting in an architecture similar to the DualQSELD-TCN parallel in <ref type="figure" target="#fig_0">Fig. 1c</ref> but using quaternion-valued layers instead of dual quaternion-valued layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">L3DAS21 Dataset</head><p>The Learning 3D Audio Sources 2021 (L3DAS21) dataset for the SELD task contains approximately 15 hours of simulated office environment sounds divided in 900 1-minute-long samples, obtained through two MSMP B-format Ambisonics <ref type="bibr" target="#b10">[11]</ref>. In each frame, there may be up to three simultaneously active sounds belonging to the 14 sound classes selected from the FSD50K dataset <ref type="bibr" target="#b35">[36]</ref>. Samples have a frequency of 32 kHz, 16 bit and the difference of amplitude among sounds ranges from 0 to 20 dBFS. The target consists of a matrix with dimension [n f rame, n class ? n overlap] for the SED sub-task, while for DOA sub-task we have a larger matrix of [n f rame, n class ? n overlap ? 3], where 3 is for the three x, y, z coordinates in the 3D space, n overlap = 3 and n class = 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Metrics</head><p>To evaluate the performance of our model in a robust way, we employ several objective metrics. We compute the F score as suggested by the L3DAS21 Challenge and the error rate (ER) through the location-sensitive detection metrics <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. We build a score from these metrics, which we name location-sensitive detection (LSD score ), defined as</p><formula xml:id="formula_20">LSD score = ER + (1 ? F) 2 .<label>(18)</label></formula><p>Then, we consider other two metrics suggested by the DCASE21 Challenge <ref type="bibr" target="#b36">[37]</ref>, the localization error (LE) and the localization recall (LR), which are computed through the class-sensitive localization metrics <ref type="bibr" target="#b37">[38]</ref>. We give rise to another score from the latter ones, named class-sensitive localization (CSL score ), of the form</p><formula xml:id="formula_21">CSL score = (LE/180) + (1 ? LR) 2 .<label>(19)</label></formula><p>Finally, we also involve a novel global metrics that combines LSD score and CSL score , thus taking into account both the location-sensitive detection and the class-sensitive localization and then being more robust to unbalanced results. Indeed, the SELD task comprises two sub-tasks SED and DOA and an unsuitable or insufficient assessment of the two may compromise the global evaluation of the model. We name the average of LSD score and CSL score as global SELD (G-SELD score ). The closer to 0 the value of the G-SELD score , the better the predictions are, while values equal to 1 indicates bad performance. We believe that the G-SELD score is a more robust evaluation metrics, being based on four different metrics that are computed with two diverse evaluation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Architecture and training</head><p>In this subsection, we provide training and architectures details to reproduce our experiments.</p><p>We test two versions of the proposed DualQSELD-TCN network, named DualQSELD-TCN ( <ref type="figure" target="#fig_0">Fig. 1b)</ref> and DualQSELD-TCN parallel (DualQSELD-TCN / / ) in <ref type="figure" target="#fig_0">Fig. 1c</ref>, the latter with two parallel DualQ-Conv-TC blocks in order to encapsulate tha magnitudes and phases features of each microphone in two different dual quaternions. The proposed model comprises the initial convolutional blocks with P = 192 filters of dimension 3 ? 3 for each layer and pooling of mp = <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>, respectively, while the dropout probability is 0.3. In the DualQ-TC block, where we stack D = 10 resblocks resulting in a receptive field of 287, the DualQ dilated convolutions involves G = 384 filters with size 3, while both the parallel 1D DualQ-Conv have a kernel size equal to 1 and U = L = 384, while the spatial dropout probability is set to 0.5. The last two DualQ-Conv of the DualQ-TC block still have V = 384 filters of size 3. Finally, the classifier branches are then composed of DualQ-FC layers of R = 384 nodes each (FC layers of R = 128 nodes each in the case of DualQSELD-TCN / / ), with the final real-valued layer with a number of nodes equal to the required target. We train each model for a minimum of 1000 epochs and an early stopping with patience equal to 300, through the Adam optimizer having an initial learning rate of 0.0001. We jointly optimize the SED binary cross-entropy loss and the DOA mean squared error by weighting the latter 5 times with respect to the first one <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results discussion</head><p>We perform the tests in two scenarios. First, we involve as input to the model the magnitudes extracted from the two Ambisonics microphones, thus the eight channels are encapsulated in the dual quaternion as in <ref type="bibr" target="#b9">(10)</ref>. Then, we want to further push our approach leveraging also the phase information, thus passing to the model a sixteen-channel input.</p><p>In the first section of <ref type="table" target="#tab_1">Table 1</ref>, we report the computed scores for the comparison with the baselines SELD-TCN and QSELD-TCN in the real and quaternion domain, respectively. The quaternion network performs better than the real-valued one, indicating the advantages of processing Ambisonics signals through quaternion algebra, as stated in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. Our approach, exceeds both the baselines, especially in the CSL score , thus having better LE and LR scores. This means that our DualQSELD-TCN model improves the sound localization with respect to the baselines, thanks to the augmented representation of the dual quaternion. Once we ensure that the proposed model can build a more accurate representation of the sound field, we further push up its capabilities considering also the phase information as input features. This results in more accurate scores for both the tasks with a G-SELD score improved from 0.439 to 0.356. Finally, we achieve the best scores with the parallel version DualQSELD-TCN / / , which encapsulates magnitudes and phases of each microphone in two diverse dual quaternion representations. This enhanced model gains the best G-SELD score of 0.324.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce a novel augmented representation for the spatial sound field acquired through an array of Ambisonics microphones. Our dual quaternion representation exploits the quaternion algebra to preserve correlations among signals, while building an augmented characterization of the sound field with six degrees of freedom.</p><p>We show the improved abilities of our approach in the sound event and localization (SELD) task, where the proposed dual quaternion SELD-TCN (DualQSELD-TCN) network outperforms both real and quaternion-valued baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed DualQSELD-TCN model. The DualQ-Conv-TC block (a) involves an initial DualQ convolution blocks and 10 residual blocks with DualQ temporal convolutions and gated tanh unit. The overall architecture (b) takes the Ambisonics signals in input, extracts the features and pass them to the DualQ-Conv-TC block. Finally, two parallel branches give the predictions for the SED and DOA sub-task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Metrics results on the L3DAS21 dataset. The first section reports the comparisons with real-valued and quaternionvalued baselines, while the last section involves further experiments with the proposed approach to improve the spatial sound field representation.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell>Features</cell><cell cols="3">LSD score ? CSL score ? G-SELD score ?</cell></row><row><cell>SELD-TCN</cell><cell>1.6M</cell><cell>Mag</cell><cell>0.533</cell><cell>0.413</cell><cell>0.473</cell></row><row><cell>QSELD-TCN</cell><cell>0.8M</cell><cell>Mag</cell><cell>0.506</cell><cell>0.404</cell><cell>0.455</cell></row><row><cell>DualQSELD-TCN</cell><cell>1.8M</cell><cell>Mag</cell><cell>0.512</cell><cell>0.365</cell><cell>0.439</cell></row><row><cell>DualQSELD-TCN</cell><cell>1.8M</cell><cell>Mag+Phase</cell><cell>0.410</cell><cell>0.303</cell><cell>0.356</cell></row><row><cell>DualQSELD-TCN / /</cell><cell>3.6M</cell><cell>Mag+Phase</cell><cell>0.369</cell><cell>0.279</cell><cell>0.324</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An overview of deep-learningbased audio-visual speech enhancement and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michelsanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Audio, Speech, and Language Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1368" to="1396" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Drone audition: Sound source localization using on-board microphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Manamperi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Abhayapala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Samarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Process</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Source localization using distributed microphones in reverberant environments based on deep learning and ray space transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Comanducci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Borra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bestagini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Antonacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Process</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2238" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual reality and choreographic practice: The potential for new creative methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cisneros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whatley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Body, Space &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Design for immersive experience: Role of spatial audio in extended reality applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kailas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design for Tomorrow</title>
		<editor>A. Chakrabarti, R. Poovaiah, P. Bokil, V. Kant</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="853" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Move2hear: Active audio-visual source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="275" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zahorik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D audio augmented reality: implementation and experiments</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="296" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Saladnet: Self-attentive multisource localization in the ambisonics domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Grumiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu?rin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Process. to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="336" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Production of six-degrees-of-freedom (6DoF) navigable audio using 30 ambisonic microphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mr?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabaci?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ciotucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>?ernicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Immersive and 3D Audio: from Architecture to Automotive (I3DA)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A dataset of higherorder ambisonic room impulse responses and 3D models measured in a room with varying furniture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>G?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pulkki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Immersive and 3D Audio: from Architecture to Automotive (I3DA)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine learning for 3D audio signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Gramaccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jamili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marinoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Massaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Medaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nachira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nucciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paglialunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Challenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Workshop on Machine Learning for Signal Process</title>
		<imprint>
			<publisher>MLSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marinoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pennese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Masiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10372</idno>
		<title level="m">L3DAS22 challenge: Learning 3D audio sources in a real office environment</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sound event localization and detection of overlapping sources using convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Process. PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Frequency-domain adaptive filtering: From real to hypercomplex signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scarpiniti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7745" to="7749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient sound event localization and detection in the quaternion domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brignone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems II</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
			<publisher>Express Briefs</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quaternion neural networks for 3D sound source localization in reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Celsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop on Machine Learning for Signal Process</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-microphone end-to-end speaker joint identification and localization via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salvati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Direction of arrival estimation of noisy speech using convolutional recurrent neural networks with higher-order ambisonics signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Poschadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hupke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peissig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference</title>
		<imprint>
			<biblScope unit="page" from="211" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predicting rigid body dynamics using dual quaternion recurrent neural networks with quaternion attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poppelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08734v1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rigid body movement prediction using dual quaternion recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>P?ppelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Nutakki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Industrial Technology (ICIT)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="756" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical dual quaternion-based recurrent neural network as a flexible internal body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. on Neural Netw. (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual quaternions as a tool for modeling, control, and estimation for spacecraft robotic servicing missions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsiotras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valverde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Astronaut. Sci</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="595" to="629" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conference on Artificial Intelligence</title>
		<meeting>of the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6894" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Node co-occurrence based graph neural networks for knowledge graph link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Van Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. on Web Search and Data Mining</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Six-degrees-offreedom binaural audio reproduction of first-order ambisonics with distance information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plinge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Thiergart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>Rummukainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AES Int. Conf. on Audio for Virtual and Augmented Reality (AVAR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quaternions and Caley Numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of Mathematics and Its Applications</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Quaternions and rotation sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kuipers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1921" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Preliminary sketch of biquaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the London Mathematical Society s1-4</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A quaternionvalued variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Conf. on Neural Netw. (IJCNN)</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of quaternion neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linar?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sound event localization &amp; detection via temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guirguis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guntoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdulatif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seld-Tcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Signal Process. Conf. (EUSIPCO)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grassucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04176</idno>
		<title level="m">PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fsd50k: An open dataset of human-labeled sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Process</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="829" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Overview and evaluation of sound event localization and detection in dcase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Audio, Speech, and Language Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint measurement of localization and detection of sound events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Process. to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="333" to="337" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
