<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Compact Support Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Statistics Department</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Mou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Statistics Department</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Compact Support Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Article</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks have proven to be extremely useful in many applications, including object detection, speech and handwriting recognition, medical imaging, etc. They have become the state of the art in these applications, and in some cases they even surpass human performance. However, neural networks have been observed to have a major disadvantage: they don't know when they don't know, i.e. don't know when the input is out-of-distribution (OOD), i.e. far away from the data they have been trained on. Instead of saying "I don't know", they give some output with high confidence <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. An explanation of why this is happening for the rectified linear unit (ReLU) based networks has been given in <ref type="bibr" target="#b2">[3]</ref>. This issue is very important for safety-critical applications such as space exploration, autonomous driving, medical diagnosis, etc. In these cases it is important that the system know when the input data is outside its nominal range, to alert the human (e.g. driver for autonomous driving or radiologist for medical diagnostic) to take charge in such cases.</p><p>A common way to address the problem of high confidence predictions for OOD examples is through ensembles <ref type="bibr" target="#b4">[5]</ref>, where multiple neural networks are trained with different random initializations and their outputs are averaged in some way. The reason why ensemble methods have low confidence on OOD samples is that each NN's high-confidence domain is random outside the training data, and the common high-confidence domain is therefore shrunk through averaging. This works well when the representation space (the space of the NN before the output layer) is high dimensional, but fails when this space is low dimensional <ref type="bibr" target="#b5">[6]</ref>.</p><p>Another popular approach is adversarial training <ref type="bibr" target="#b6">[7]</ref>, where the training is augmented with adversarial examples generated by maximizing the loss starting from perturbed examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2104.00269v3 [cs.LG] 7 Dec 2021</head><p>This method is modified in adversarial confidence enhanced training (ACET) <ref type="bibr" target="#b2">[3]</ref> where the adversarial samples are added through a hybrid loss function. However, the instance space is extremely vast when it is high dimensional, and a finite number of training examples can only cover an insignificant part, and no matter how many OOD examples are used, there will always be parts of the instance space that have not been explored. Other methods include the estimation of the uncertainty using dropout <ref type="bibr" target="#b7">[8]</ref>, softmax calibration <ref type="bibr" target="#b8">[9]</ref>, and the detection of OOD inputs <ref type="bibr" target="#b9">[10]</ref>. CutMix <ref type="bibr" target="#b10">[11]</ref> is a method to generate training samples with larger variability, which help improve generalization and OOD detection. All these methods are complementary to the proposed approach and could be used together with the classifiers introduced in this paper to improve accuracy and OOD detection.</p><p>In <ref type="bibr" target="#b11">[12]</ref> are trained two auto-regressive models, one for the foreground in-distribution data and one for the background, and the likelihood ratio is used to decide for each observation whether it is OOD or not. This is a generative model, while our model is discriminative.</p><p>A number of works assume that the distance in the representation space is meaningful. A trust score was proposed in <ref type="bibr" target="#b12">[13]</ref> to measure the agreement between a given classifier and a modified version of a k-nearest neighbor (k-NN) classifier. While this approach does consider the distance of the test samples to the training set, it only does so to a certain extent since the k-NN does not have a concept of "too far", and is also computationally expensive. A simple method based on the Mahalanobis distance is presented in <ref type="bibr" target="#b13">[14]</ref>. It assumes that the observations are normally distributed in the representation space, with a shared covariance matrix for all classes. Our distribution assumption is much weaker, assuming that the observations are clustered into a number of clusters, not necessarily Gaussian. In our representation, each class is usually covered by one or more compact support neurons, and each neuron could be involved in multiple classes. Furthermore, <ref type="bibr" target="#b13">[14]</ref> simply replaces the last layer of the NN with their Mahalanobis measure and makes no attempt to further train the new model, while the CSN layers can be trained together with the whole network.</p><p>The Generalized ODIN <ref type="bibr" target="#b14">[15]</ref> decomposes the output into a ratio of a class-specific function h i (x) and a common denominator g(x), both defined over instances x of the representation space. Good results are obtained using h i based on the Euclidean distance or cosine similarity. Again, this approach assumes that the observations are grouped in a single cluster for each class, which explains why it uses very deep models (with 34-100 layers) that are more capable to obtain representations that satisfy this assumption. Our method does not make the single cluster per class assumption, and can use deep or shallow models. The Deterministic Uncertainty Quantification (DUQ) <ref type="bibr" target="#b5">[6]</ref> method uses an RBF network and a special gradient penalty to decrease the prediction confidence away from the training examples. The authors also propose a centroid updating scheme to handle the difficulties in training an RBF network. They claim that regularization of the gradient is needed in deep networks to enforce a local Lipschitz condition on the prediction function that will limit how fast the output will change away from the training examples. While their smoothness and Lipschitz conditions might be necessary conditions, they are not sufficient conditions since a smoothly changing function could still have arbitrarily high confidence far away from the training examples. In contrast, our proposed Compact Support Neural Network (CSNN) is guaranteed to have zero outputs away from the training examples, which reflects in lowest possible confidence. Furthermore, the maximum gradient of the CSN layer can be computed explicitly and the Lipschitz condition can be directly enforced by decreasing the neuron support and weight decay. The authors of DUQ also encourage their gradient to be bounded away from zero everywhere, which they recognize is based on a speculative argument. In contrast, the CSNN gradient is zero away from the training examples, while still obtaining better OOD detection and smaller test errors than DUQ.</p><p>The contributions of this paper are the following:</p><p>? It introduces a novel neuron formulation that generalizes the standard neuron and the radial basis function (RBF) neuron as two extreme cases of a shape parameter. Moreover one can smoothly transition from a regular neuron to a RBF neuron by gradually changing this parameter. The RBF correspondent to a ReLU neuron is also introduced and observed to have compact support, i.e. its output is zero outside a bounded domain. ? It introduces a novel way to train a compact support neural network (CSNN) or an RBF network, starting from a pre-trained regular neural network. For that purpose, the construction mentioned above is used to smoothly bend the decision boundary of the standard neurons, obtaining the compact support or RBF neurons. ? It proves the universal approximation theorem for the proposed neural network, which guarantees that the network will approximate any function from L p (R k ) with an arbitrary degree of accuracy. ? It shows through experiments on standard datasets that the proposed network usually outperforms existing out-of-distribution detection methods from the literature, both in terms of smaller test errors on the in-distribution data and larger Areas under the ROC curve (AUROC) for detecting OOD samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>This paper investigates the neuron design as the root cause of high confidence predictions on OOD data, and proposes a different type of neuron to address its limitations. The standard neuron is</p><formula xml:id="formula_0">f (x) = ?(w T x + b), a projection (dot product) x ? w T x + b onto a direction w,</formula><p>followed by a nonlinearity ?(?). In this design, the neuron has a large response for vectors x ? R p that are in a half-space. This can be an advantage when training the neural network (NN) since it creates high connectivity in the weight space and makes the neurons sensitive to far-away signals. However, it can be a disadvantage when using the trained NN, since it leads to neurons unpredictably firing with high responses to far-away signals, which can result (with some probability) in high confidence responses of the whole network for examples that are far away from the training data.</p><p>To address these problems, a type of radial basis function (RBF) neuron <ref type="bibr" target="#b3">[4]</ref>, f (x) = g( x ? ? 2 ), is used and modified to have zero response at some distance R from ?. Therefore the neuron has compact support, and the same applies to a layer formed entirely of such neurons. Using one such compact support layer before the output layer one can guarantee that the space where the NN has a non-zero response is bounded, obtaining a more reliable neural network.</p><p>The loss function of such a compact support NN has many flat areas and it can be difficult to train directly by backpropagation. However, the paper introduces a different way to train it, by starting with a trained regular NN and gradually bending the neuron decision boundaries to make them have smaller and smaller support.</p><p>The compact support neural network consists of a number of layers, where the layer before last contains only compact support neurons, which will be described next. The last layer is a regular linear layer without bias, so it can output an all-zero vector when appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Compact Support Neuron</head><p>The radial basis function (RBF) neuron <ref type="bibr" target="#b3">[4]</ref>, f (x, w) = g( x ? w 2 ), for x, w ? R d , has a g(u) = exp(??u) activation function. However, in this paper g(u) = max(R ? u, 0) will be used because it is related to the ReLU. A flexible representation. Introducing an extra parameter ? = 1, the RBF neuron can be written as: Using the parameter ?, one can smoothly interpolate between an RBF neuron when ? = 1 and a standard projection neuron when ? = 0. However, starting with an RBF neuron with g(u) = exp(??u), the projection neuron is obtained for ? = 0 as f ? (x, w) = exp(2w T x), but which has an undesirable exponential activation function. The compact support neuron. In order to obtain a standard ReLU based neuron</p><formula xml:id="formula_1">f ? (x, w) = g(?( x 2 + w 2 ) ? 2w T x).<label>(1)</label></formula><formula xml:id="formula_2">f ? (x, w) = ?(w T x) with ?(u) = max(u, 0) for ? = 0, the activation g(u) = ?(R ? u)</formula><p>will be used, and the above construction is modified to obtain the compact support neuron:</p><formula xml:id="formula_3">f ? (x, w, b, R) = ?[?(R ? x 2 ? w 2 ? b) + 2w T x + b],<label>(2)</label></formula><p>where a bias term b is also introduced for the standard neuron. Usually b = 0 is used for simplicity. The parameter R determines the radius of the support of the neuron when ? &gt; 0. In fact, one can easily check that the support of f ? (x, w, b, R) from eq. (2) (i.e. the domain where it takes nonzero values) is a sphere of radius</p><formula xml:id="formula_4">R 2 ? = R + b(1/? ? 1) + w 2 (1/? 2 ? 1)<label>(3)</label></formula><p>centered at w ? = w/?. Therefore the neuron from (2) has compact support for any ? &gt; 0 and the support gets tighter as ? increases. <ref type="figure" target="#fig_0">Figure 1</ref>, left shows the response on a 1D input x for the RBF neuron y = exp(?|x ? 2| 2 ) and the compact support neuron y = f ? (x, 2, 0, 1) from Eq (2) for ? ? {0, 0.8, 1}. Observe that the standard neuron with ReLU activation y = max(x, 0) is obtained as y = f ? (x, 2, 0, 1) for ? = 0. <ref type="figure" target="#fig_0">Figure 1</ref>, right shows on a 2D example the support</p><formula xml:id="formula_5">of f ? (x, w, b, R) from (2) for several values of ? ? [0, 1], where x = (x 1 , x 2 ), w = (0, 2), b = 0 and R = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Compact Support Neural Network</head><p>For a layer containing only compact support neurons (CSN), the weights can be combined into a matrix W T = (w 1 , ..., w K ), the biases into a vector b = (b 1 , ..., b K ) and the radii into a vector r = (R 1 , ..., R K ) and the CSN layer can be written as:</p><formula xml:id="formula_6">f ? (x, W, b, r) = ?(?[r ? b ? x T x ? Tr(WW T )] + 2Wx + b),<label>(4)</label></formula><p>where f ? (x, W, b, r) = ( f 1 (x), ..., f K (x)) T is the vector of neuron outputs of that layer. This formulation enables the use of standard neural network machinery (e.g. PyTorch) to train a CSN. In practice usually no bias term is used (i.e. b = 0), except in low dimension experiments. The radius parameter r is trainable. The simplest compact support neural network (CSNN) has two layers: a hidden layer containing compact support neurons <ref type="bibr" target="#b1">(2)</ref>, and an output layer which is a standard fully connected layer without bias, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, left. A Batch Normalization layer without trainable parameters can be added to normalize the data as described below. A LeNet network with a CSN layer before the last layer is shown in <ref type="figure" target="#fig_1">Figure 2</ref> Our experiments on three real datasets indicate that indeed x ? 1 when the inputs x are normalized this way. To achieve this goal, a Batch Normalization layer without trainable parameters is added before the CSN layer. Training. Like the RBF network, training a neural network with such neurons with ? = 1 is difficult because the loss function has many local optima. To make matters even worse, the compact support neurons have small support when ? is close to 1, and consequently the loss function has flat regions between the local minima. This is why another training approach is taken. Using equations (4) a CSNN can be trained by first training a regular NN (? = 0) and then gradually increasing the shape parameter ? from 0 towards 1 while continuing to update the NN parameters. Observe that whenever ? &gt; 0 the NN has compact support, but the support gets smaller as ? gets closer to 1. The training procedure is described in detail in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Compact Support Neural Network (CSNN) Training</head><p>Input:</p><formula xml:id="formula_7">Training set T = {(x i , y i ) ? R p ? R} n i=1 , Output: Trained CSNN. 1: Train a regular NN f(x) = L?(2Wg(x) + b)</formula><p>where W, L are the last two layer weight matrices and g(x) is the rest of the NN. 2: Freeze g(x), compute u i = g(x i ), i = 1, ..., n, their mean ? and standard deviation ?. <ref type="bibr" target="#b2">3</ref></p><formula xml:id="formula_8">: Obtain normalized versions v i of u i as v i = (u i ? ?)/ ? d?, i = 1, .</formula><p>.., n. 4: for e= 1 to N epochs do <ref type="bibr">5:</ref> Set ? = e/N epochs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Use the examples (v i , y i ) to update (W, L, b, r) based on one epoch of</p><formula xml:id="formula_9">f(v) = L?(?[r ? v T v ? Tr(WW T ) ? b] + 2Wv + b) 7: end for</formula><p>In practice the training is stopped at an ? ? 1 where the training and validation errors still take acceptable values, e.g. a validation error less than the validation error for ? = 0. However, the larger the value of ?, the tighter the support is around the training data and the better the generalization. The whole network can be then fine-tuned using a few more epochs of backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Datasets used</head><p>Real data experiments are conducted on classifiers trained on three datasets: MNIST <ref type="bibr" target="#b15">[16]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b16">[17]</ref>. The OOD (out of distribution) detection is evaluated using the respective test sets as in-sample data and other datasets as OOD data, such as the test sets of EMNIST <ref type="bibr" target="#b17">[18]</ref>, FashionMNIST <ref type="bibr" target="#b18">[19]</ref> and SVHN <ref type="bibr" target="#b19">[20]</ref>, and the validation set of ImageNet <ref type="bibr" target="#b20">[21]</ref>. For MNIST is also used as OOD data a grayscale version of CIFAR-10, obtained by converting the 10,000 test images to gray-scale and resizing them to 28 ? 28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Neural Network Architecture</head><p>For MNIST a 4-layer LeNet convolutional neural network (CNN) backbone is used, with two 5 ? 5 convolution layers with 32 and 64 filters respectively, followed by ReLU and 2 ? 2 max pooling, and two fully connected layers with 256 and 10 neurons. For the other two datasets, a ResNet-18 <ref type="bibr" target="#b21">[22]</ref> backbone is used, with 4 residual blocks with 64, 128, 256 and 512 filters respectively.</p><p>For the CSNN two architectures, illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, will be investigated. The first is a small one (called CSNN), illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, left, which takes the output of the last convolutional layer of the backbone as input, normalized as described in Section 2.2 using a batch normalization layer without any learnable affine parameters. The second one is a full network (called CSNN-F), illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, right, where the backbone (LeNet or ResNet) is part of the backpropagation and a batch normalization layer (BN) without any learnable parameters is used between the backbone and the CSN layer.</p><p>All experiments were conducted on a MSI GS-60 Core I7 laptop with 16GB RAM and Nvidia GTX 970M GPU, running the Windows 10 operating system. The CSNN and CSNN-F networks were implemented in PyTorch 1.90.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Training details</head><p>For all datasets, data augmentation with padding (3 pixels for MNIST, 4 pixels for the rest) and random cropping is used to train the backbones. For CIFAR-100, random rotation up to 15 degrees is also used.No data augmentation is used when training the CSNN and CSNN-F.</p><p>The CSNN was trained for 510 epochs with R = 0.01, of which 10 epochs at ? = 0. The Adam optimizer is used with learning rate 0.001 and weight decay 0.0001. SGD obtained similar results. The CSNN-F was trained with SGD with a learning rate of 0.001 and weight decay 0.0005. Its layers were initialized with the trained backbone and the trained CSNN. Then ? was kept fixed for two epochs and increased by 0.005 every epoch for 4 more epochs.</p><p>Training the CSNN from ? = 0 to ? = 1 for 510 epochs takes less than an hour. Each epoch of the CSNN-F takes less than a minute with the LeNet backbone and about 3 minutes with the ResNet-18 backbone. The CSNN-F was obtained by merging the corresponding CSNN head with the ResNet or LeNet backbone and training them together for 6 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">OOD detection</head><p>The out of distribution (OOD) detection is performed similarly to the way it is done in a standard CNN. For any observation, the maximum value of the network's raw outputs is used as the OOD score for predicting whether the observation is OOD or not. If the observation is in-distribution, its score will usually be large, and if it is OOD, it will be usually close to zero or even zero. The ROC (Receiver Operating Characteristic) curve based on these scores for the test set of the in-distribution data (as class 0) and one OOD dataset (as class 1) will give us the AUROC (Area under the ROC). If the two distributions are not separable (have concept overlap), some of the OOD scores will be large, but for the OOD observations that are away from the area of overlap they will be small or even zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Methods compared</head><p>The OOD (out of distribution) detection results of the CSNN and CSNN-F are compared with the Adversarial Confidence Enhanced Training (ACET) <ref type="bibr" target="#b2">[3]</ref>, Deterministic Uncertainty Quantification (DUQ) <ref type="bibr" target="#b5">[6]</ref>, a standard CNN and a standard RBF network. The RBF network has the same architecture as the CSNN-F, but with an RBF layer instead of the CSN layer, and has a learnable ? for each neuron. Also shown are results for an ensemble of five or 10 CNNs trained with different random initializations, however these methods are more computationally expensive and are not included in our comparison. The ACET results are taken directly from <ref type="bibr" target="#b2">[3]</ref>, and the DUQ, CNN and ensemble results were obtained using the DUQ authors' PyTorch code. The parameters for the CNN, RBF and ensemble were tuned to obtain the smallest average test error. For DUQ multiple models were trained with various combinations of the length scales ? ? {0.05, 0.1, 0.2, 0.3, 0.5, 1.0} and gradient penalty ? ? {0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0} and selected the combination with the best test error-AUROC trade-off. For the CSNN methods, for each dataset the classifier was selected to correspond to the largest ? where the test error takes a value comparable to the other methods compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>This section presents theoretical results on the gradient of the proposed CSNN and its universal approximation properties, plus experimental results on a number of public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gradient Bound</head><p>The authors of the Deterministic Uncertainty Quantification (DUQ) <ref type="bibr" target="#b5">[6]</ref> paper claim that gradient regularization is needed in deep networks to enforce a local Lipschitz condition on the prediction function that limits how fast the output will change away from the training examples. Thus it is of interest to see whether this Lipschitz condition is satisfied for the proposed CSNN.</p><p>The following result proves that indeed the Lipschitz condition <ref type="bibr" target="#b5">[6]</ref> is satisfied for the CSN layer: Theorem 1. The gradient of a CSN neuron is bounded by:</p><formula xml:id="formula_10">? x f ? (x, w, b, R) 2 ? ? 2 R + b?(1 ? ?) + w 2 (1 ? ? 2 )<label>(5)</label></formula><p>Proof. The maximum gradient of a CSN neuron can be explicitly computed as follows. The gradient of</p><formula xml:id="formula_11">f ? (x, w, b, R) = ?[?(R ? x T x ? w T w ? b) + 2w T x + b],</formula><p>where ?(u) = max(u, 0), is:</p><formula xml:id="formula_12">? x f ? (x, w, b, R) = ??x + w if ?(R ? x T x ? w T w ? b) + 2w T x + b ? 0, otherwise 0.</formula><p>The above inequality can be rearranged after dividing by ? and regrouping terms as:</p><formula xml:id="formula_13">x T x ? 2w T x/? + w T w/? 2 ? R ? b + b/? ? w T w + w T w/? 2 = R 2 ? ,</formula><p>where R 2 ? was defined in Eq. (3). Thus the gradient is</p><formula xml:id="formula_14">? x f ? (x, w, b, R) = ??x + w if x ? w/? 2 ? R 2 ? , otherwise 0,<label>(6)</label></formula><p>and therefore the maximum gradient norm is:</p><formula xml:id="formula_15">max x ? x f ? (x, w, b, R) 2 =max x ? 2 x?w/? 2 =? 2 R 2 ? =? 2 R + b?(1??) + w 2 (1?? 2 ),</formula><p>where the last equality was again obtained using Eq. (3). Thus a small gradient can be enforced everywhere by penalizing the R and w 2 to be small using weight decay, or by making ? close to 1, or both (all assuming b = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Universal Approximation for the Compact Support Neural Network</head><p>It was proved in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> that standard two-layered neural networks have the universal approximation property, in the sense that they can approximate continuous functions or integrable functions with arbitrary accuracy, under certain assumptions. Similar results have been proved for RBF networks in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, thus it is of interest whether such results can be proved for the proposed compact support neural network. In this section, the universal approximation property of the compact support neural networks will be proved.</p><p>Let L ? (R d ) and L p (R d ) and be space of functions f : R d ? R such that they are essentially bounded and respectively their p-th power f p are integrable. Denote the L p and L ? norms by || ? || p and || ? || ? respectively.</p><p>The family of networks considered in this study consists of two-layer neural networks, which can be written as:</p><formula xml:id="formula_16">q(x) = H ? i=1 ? i f (x, w i , b i )<label>(7)</label></formula><p>where H is the number of hidden nodes, ? i ? R are the weights from the i-th hidden node to the output nodes, f (x, w i , b i ) is the representation of the hidden neuron with weights w i ? R n and biases b i ? R. The neurons used in the hidden layer can be either regular neurons or radial-basis function (RBF) neurons. The regular neuron can be written as :</p><formula xml:id="formula_17">f (x, w, b) = g(w T x + b)<label>(8)</label></formula><p>where g is an activation function such as the sigmoid or ReLU. An RBF neuron has the following representation:</p><formula xml:id="formula_18">f (x, w, b) = g( ||x ? w|| b )<label>(9)</label></formula><p>where the exponential function g(x) = exp(?x) is used for activation in RBF networks. The studies on regular neurons <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> showed that if the activation function g used in the hidden layer is continuous almost everywhere, locally essentially bounded, and not a polynomial, then a two-layered neural network can approximate any continuous function with respect to the || ? || ? norm.</p><p>Universal approximation results for RBF networks is quite limited are due to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, which proved that if the RBF neuron used in the hidden layer is continuous almost everywhere, bounded and integrable on R n , the RBF network can approximate any function in L p (R n ) with respect to the L p norm with 1 ? p ? ?. More exactly, in <ref type="bibr" target="#b24">[25]</ref> is proved the following statement: </p><formula xml:id="formula_19">= ? H i=1 ? i K( x?z i ? ) is dense in L p (R d ) for every p ? [1, ?).</formula><p>The above result will be used to prove the following statement for the CSNN: Theorem 3. Let ? ? (0, 1], R 0 ? 0, and let g : R ? R be any non-negative, continuous increasing activation function. Then the family of two-layer compact support neural networks:</p><formula xml:id="formula_20">q(x) = ? H i=1 ? i f ? (x, w i , R w i ), with f ? (x, w, R) = g(?(R ? x 2 ? w 2 ) + 2w T x), and R w = 1 ? R 0 ? w 2 ( 1 ? 2 ? 1) is dense in L p (R d ) for every p ? [1, ?).</formula><p>Proof. Since 0 &lt; ? ? 1, it implies that:</p><formula xml:id="formula_21">f ? (x, w, R) = g(?(R? x 2 ? w 2 ) + 2w T x) = g(?(R? w 2 + w 2 ? 2 ) ? ?(x T x ? 2 w T ? x + w T w ? 2 )) = g[?(R+ w 2 ( 1 ? 2 ?1))?? x? w ? 2 ].<label>(10)</label></formula><p>Therefore</p><formula xml:id="formula_22">f ? (x, w, R w ) = K( x ? w/? ? ) where ? = 1/ ? ?, K(x) = g(R 0 ? x 2 ) and R w = 1 ? R 0 ? w 2 ( 1 ? 2 ? 1).</formula><p>Observe that K(x) is bounded because g ? 0 is increasing and thus</p><formula xml:id="formula_23">0 ? K(x) = g(R 0 ? x 2 ) ? g(R 0 ).</formula><p>Since K(x) is also integrable and continuous and R d K(x)dx = 0, Theorem 2 applies with z i = w i /? and ? = 1/ ? ?, obtaining the desired result.</p><p>Observe that universal approximation results for ? = 0 have already been proved in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and that the standard RBF kernel can be obtained in Theorem 3 by taking g(x) = exp(x), ? = 1 and R 0 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">2D Example</head><p>This experiment is on the moons 2D dataset, where the data is organized on two intertwining half-circle like shapes, one containing the positives and one the negatives. The data is scaled so that all observations are in the interval [0, 1] 2 (shown as a white rectangle in <ref type="figure" target="#fig_5">Figure 3</ref>. The out of distribution (OOD) data is generated starting with 100 ? 100 = 10000 samples on a grid spanning [?0.5, 1.5] 2 and removing all samples at distance at most 0.1 from the moons data, obtaining 8763 samples.</p><p>A two layer CSNN is used, with 128 CSN neurons in the hidden layer, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> The confidence map for the trained classifier is shown in <ref type="figure" target="#fig_5">Figure 3</ref>, right. One can see that the confidence is 0.5 (white) almost everywhere except near the training data, where it is close to 1 (black). This assures us that the method works as expected, shrinking the support of the neurons to a small domain around the training data. One can also see that the support is already reasonably small for ? = 0.6 and it gets tighter and tighter as ? gets closer to 1. The training/test errors vs. ? are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. Also shown is the AUROC (Area under the ROC curve) for detecting the OOD data described above against the test set. Observe that the training and test errors for ? = 0 are quite large, because the standard 2-layer NN with 128 neurons cannot fit this data well enough, and decrease as the neuron support decreases and the model is better capable to fit the data.</p><formula xml:id="formula_24">? = 0 ? = 0.6 ? = 0.95 ? = 1</formula><p>activation patterns ? = 0 activation patterns ? = 0.825 confidence map ? = 0.825 It is known <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> that the output of a ReLU-based neural network is piecewise linear and the domains of linearity are given by the activation pattern of the neurons. The activation pattern of the neurons contains the domains where the set of active neurons (with nonzero output) does not change. These activation pattern domains are polytopes, as shown in in <ref type="figure" target="#fig_7">Figure 5</ref>, left, for a two-layer NN with 32 neurons. The activation domains for a CSNN are intersections of circles, as illustrated in <ref type="figure" target="#fig_7">Figure 5</ref>, middle, with the domain where all neurons are inactive shown in white. The corresponding confidence map is shown in <ref type="figure" target="#fig_7">Figure 5</ref>, right.</p><p>In real data applications one does not need to go all the way to ? = 1 since even for smaller ? the support is still bounded and if the instance space is high dimensional (e.g. 512 to 1024 in the real data experiments below), the volume of the support of the CNN will be MNIST CIFAR-10 CIFAR-100 extremely small compared to the instance space, making it unlikely to have high confidence on out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Real Data Experiments</head><p>In <ref type="figure" target="#fig_8">Figure 6</ref> are shown the train/test errors vs ? for the CSNN on the three datasets. Also shown are the Area under the ROC curve (AUROC) for OOD detection on CIFAR-10 or CIFAR-100. Observe that all curves on the real data are very smooth, even though they are obtained from one run, not averaged. One could see that the training and test errors stay flat for a while then they start increasing from a certain ? that depends on the dataset. At the same time, the AUROC stays flat and slightly increases, and there is a range of values of ? where the test error is low and the AUROC is large. Different values of ? make different trade-offs between test error and AUROC. In practice, ? should be chosen as large as possible where an acceptable validation error is still obtained, to have the smallest support possible. For example one could choose the largest ? such that the validation error at ? is less then or equal to the validation error at ? = 0.</p><p>The results are shown in <ref type="table">Table 1</ref>. All results except the ACET results are averaged over 10 runs and the standard deviation is shown in parentheses. The results of the best non-ensemble method are shown in bold and the second best in red. <ref type="table">Table 1</ref>. OOD detection comparison in terms of Area under the ROC curve (AUROC) for models trained and tested on several datasets. For each model the test error in % is shown in the "Train on" row. The ACET results are taken from <ref type="bibr" target="#b2">[3]</ref>. All other results are averaged over 10 runs and the standard deviation is shown in parentheses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>From <ref type="table">Table 1</ref> one could see that the trained RBF has difficulties in fitting the in distribution data well, with much larger test errors on CIFAR-10 and CIFAR-100 than the standard CNN.</p><p>The most probable cause is that the training is stuck in a local optimum, because the training errors were we also observed to be large (not shown in <ref type="table">the table)</ref>.</p><p>Further comparing the test errors, <ref type="table">Table 1</ref> reveals that the proposed CSNN-F method obtain the smallest test errors on in-distribution data among the non-ensemble methods compared. These findings strengthen the claim that the CSNN can fit the in distribution data as well as a standard CNN, claim also supported by the universal approximation statement from Theorem 3.</p><p>Comparing the detection AUROC on different OOD datasets, the proposed CSNN and CSNN-F methods obtain the best results on MNIST and CIFAR-100 and ACET and SNGP obtain the best results on CIFAR-10. However, the test errors of ACET are the highest among all methods by a large margin.</p><p>One should be aware that there usually is a trade-off to be made between small test errors and good OOD detection. ACET makes this trade-off in the favor of a high AUROC by training with adversarial samples, while the other methods are trying to obtain small test errors, comparable with a standard CNN. Observe that the test errors of the CSNN-F approach are smaller than the CSNN, and the AUROCs are comparable.</p><p>Compared to ACET both CSNN and CSNN-F obtain smaller test errors on all three datasets and better average AUROC on two out of three datasets. Compared to DUQ, the CSNN and CSNN-F obtain comparable test errors and better average AUROC on all three datasets. The RBF network cannot obtain a small training or test error in most cases and the test errors and OOD detection results are poor and have a large variance.</p><p>Comparing the training time, the CSNN methods are about 4 times faster than training a 5-ensemble, 8 times faster than a 10-ensemble and about 3 times faster than DUQ.</p><p>Overall, compared to the other non-ensemble OOD detection methods evaluated, the proposed methods obtain smaller test errors and better OOD detection performance (except ACET, which has large test errors). Training the whole deep network together with the CSNN results in smaller test errors and improved OOD detection performance.</p><p>In the synthetic experiment in <ref type="figure" target="#fig_5">Figure 3</ref> the train and test errors were close to 0 for ? = 1 because there are neurons close to all observations. However, in the real data applications where the representation space is high dimensional, the training, test and validation errors might first decrease a little bit but ultimately increase as ? approaches 1. For example one could see the test errors vs ? for the synthetic dataset in <ref type="figure" target="#fig_6">Figure 4</ref> and for the real datasets in <ref type="figure" target="#fig_8">Figure 6</ref>. This is due to the curse of dimensionality, which makes all distances between observations relatively large and the CSNN centers will also be far from the observations, thus the neurons will have a larger radius to cover the training data.</p><p>It is worth noting that in contrast to the weights of a standard neuron, the weights of the compact support neuron exist in the same space as the neuron inputs and they can be regarded as templates. Thus they have more meaning, and one could easily visualize the type of responses that make them maximal, using standard neuron visualization techniques such as <ref type="bibr" target="#b27">[28]</ref>. Furthermore, one can also obtain samples from the compact support neurons, e.g. for generative or GAN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper brought four contributions. First, it introduced a novel neuron formulation that is a generalization of the the standard projection based neuron and the RBF neuron as two extreme cases of a shape parameter ? ? [0, 1]. It obtains a novel type of neuron with compact support by using ReLU as the activation function. Second, it introduced a novel way to train the compact support neural network of a RBF network by starting with a pretrained standard neural network and gradually increasing the shape parameter ?. This training approach avoids the difficulties in training the compact support NN and the RBF networks, which have many local optima in their loss functions. Third, it proves the universal approximation property of the proposed neural network, in that it can approximate any function from L p (R d ) with arbitrary accuracy, for any p ? 1. Finally, through experiments it shows that the proposed compact support neural network outperforms the standard NN and the RBF network and even usually outperforms existing state-of-the-art OOD detection methods, both in terms of smaller test errors on in-distribution data and larger AUROC for detecting OOD samples.</p><p>The OOD detection feature is important in safety critical applications such as autonomous driving, space exploration and medical imaging.</p><p>The results have been obtained without any adversarial training or ensembling, and adversarial training or ensembling could be used in the proposed framework to obtain further improvements.</p><p>In the real data applications, the compact support layer was used as the last layer before the output layer. This ensures that the compact support is involved in the most relevant representation space of the CNN. However, because the CNN still has many projectionbased layers to obtain this representation, it means that the corresponding representation in the original image space does not have compact support and high confidence erroneous predictions are still possible. Architectures with multiple compact support layers that have even smaller support in the image space are subject to future study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: 1D example. Comparison between RBF neuron y = exp(?|x ? 2| 2 ) and compact support neurons y = f ? (x, 2, 0, 1) from (2) for ? ? {0, 0.8, 1}. Right: 2D example. The construction (2) smoothly interpolates between a standard neuron (? = 0) and an RBF-type of neuron (? = 1). Shown are the decision boundaries for f ? (x, w, 0, 1) with x = (x 1 , x 2 ), w = (0, 2) for ? ? {0, 0.1, 0.5, 0.8, 1} and the corresponding centers w/? as "*".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Left: a simple compact support neural network (CSNN), with the CSN layer described in<ref type="bibr" target="#b3">(4)</ref>. Right: a CSNN-F with LeNet backbone, where all layers are trainable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, right. Normalization. It is desirable that x be approximately 1 on the training examples so that the value of the radius R does not depend on the dimension d of x. These goals can be achieved by standardizing the variables to have zero mean and standard deviation 1/ ? d on the training examples. This way x 2 ? 1 when the dimension d is large (under assumptions of normality and independence of the variables of x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b24">(Park 1991</ref>) Let K : R d ? R be an integrable bounded function such that K is continuous almost everywhere and R d K(x)dx = 0. Then the family of functions q(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, left. The CSNN is trained on 200 training examples for 2000 epochs with R = 0.02 and ? increasing from 0 to 1 as ? i = min(1, max(0, (i ? 100)/1500)), i = 1, ..., 2000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>The confidence map (0.5 for white and 1 for black) of the trained CSNN on the moons dataset for different values of ? ? [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>CSNN train and test errors, and AUROC for OOD detection vs. ? for the moons data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Example of activation pattern domains for a regular NN and a CSNN (? = 0.825), and the resulting confidence map (0.5 for white and 1 for black) for ? = 0.825 for a 32 neuron 2-layer CSNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Train and test errors, and Area under ROC Curve (AUROC) for OOD detection vs ? for CSNN classifiers trained on three real datasets. These plots are obtained from one training run.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability Statement:</head><p>The following publicly available datasets have been used in experiments: MNIST <ref type="bibr" target="#b15">[16]</ref>, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b16">[17]</ref>, EMNIST <ref type="bibr" target="#b17">[18]</ref>, FashionMNIST <ref type="bibr" target="#b18">[19]</ref>, SVHN <ref type="bibr" target="#b19">[20]</ref>, and the validation set of ImageNet <ref type="bibr" target="#b20">[21]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>The following abbreviations are used in this manuscript: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bitterwolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Radial basis functions, multi-variable functional interpolation and adaptive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Signals and Radar Establishment Malvern</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Uncertainty Estimation Using a Single Deep Deterministic Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML. JMLR. org</title>
		<imprint>
			<biblScope unit="page" from="1321" to="1330" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutmix</surname></persName>
		</author>
		<title level="m">Regularization strategy to train strong classifiers with localizable features. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="14707" to="14718" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">To trust or not to trust a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5541" to="5552" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="7167" to="7177" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">EMNIST: an extension of MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05373</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
	<note>Mathematics of control, signals and systems</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximation and radial-basis-function networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A randomized gradient-free attack on relu networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="215" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
