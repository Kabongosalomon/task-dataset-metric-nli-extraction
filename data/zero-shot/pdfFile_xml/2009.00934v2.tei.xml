<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAIL: Self-Augmented Graph Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-15">15 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Pei</surname></persName>
							<email>pei@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
							<email>lizhong.ding@inceptioniai.org</email>
							<affiliation key="aff4">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longfei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<email>chuxuzhang@brandeis.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xzhang33@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAIL: Self-Augmented Graph Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-15">15 Dec 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies learning node representations with graph neural networks (GNNs) for unsupervised scenario. Specifically, we derive a theoretical analysis and provide an empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the locality of graph structure. To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed SAIL -a novel Self-Augmented graph contrastive Learning framework, with two complementary self-distilling regularization modules, i.e., intra-and inter-graph knowledge distillation. We demonstrate the competitive performance of SAIL on a variety of graph applications. Even with a single GNN layer, SAIL has consistently competitive or even better performance on various benchmark datasets, comparing with state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graph neural networks (GNNs) have been a leading effective framework of learning graph representations. The key of GNNs roots at the repeated aggregation over local neighbors to obtain smoothing node representations by filtering out noise existing in the raw node features. With the enormous architectures proposed <ref type="bibr" target="#b11">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b5">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b30">Veli?kovi? et al. 2018)</ref>, learning GNN models to maintain local smoothness usually depends on supervised signals (e.g., node labels or graph labels). However, labeled information is not always available in many scenarios. Along with the raising attention on self-supervised learning <ref type="bibr" target="#b31">(Veli?kovi? et al. 2019;</ref><ref type="bibr" target="#b6">Hassani and Khasahmadi 2020)</ref>, pre-training GNNs without labels has become an alternative way to learn GNN models.</p><p>There are a group of unsupervised node representation learning models in the spirit of self-supervised learning. As one of the most representatives, predicting contextual neighbors (e.g., DeepWalk <ref type="bibr" target="#b23">(Perozzi, Al-Rfou, and Skiena 2014)</ref> or node2vec <ref type="bibr" target="#b4">(Grover and Leskovec 2016)</ref>) enforces the locally connected nodes to have similar representations. Self-supervising signals of this method are designed to extract local structure dependency but discard the contribution of node feature smoothness which has been utilized to improve expressive capability of GNNs <ref type="bibr" target="#b11">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b35">Wu et al. 2019)</ref>. Another line pays attention to maximizing the mutual information (MI) criterion to make agreement on multi-view graph representations <ref type="bibr" target="#b6">(Hassani and Khasahmadi 2020;</ref><ref type="bibr" target="#b38">You et al. 2020a)</ref>, in which each view of augmented graph is generated by operations on nodes, attributes, etc. However, most of them aim at making agreement on the graph-level representations <ref type="bibr" target="#b38">(You et al. 2020a;</ref><ref type="bibr" target="#b26">Sun et al. 2019)</ref>, which might be not suitable for node-level tasks.</p><p>Instead of creating multiple views through graph augmentation <ref type="bibr" target="#b6">(Hassani and Khasahmadi 2020)</ref> methods, there are recent works building upon self-augmented views created by the intermedian hidden layers of GNN. As a pioneering work, deep graph infomax (DGI) <ref type="bibr" target="#b31">(Veli?kovi? et al. 2019)</ref> proposes to maximize MI between the summarized graph and node embeddings. However, the summarized graph embedding contains the global context that might not be shared by all nodes. Inspired by DGI, graphical mutual information (GMI) <ref type="bibr" target="#b22">(Peng et al. 2020)</ref> turns to maximize the edge MI between the created views of two adjacent nodes. As GMI focuses on the edge MI maximization task, it lacks a bird's eye on the learned node representations. The learned GNN might bias towards performing well on edge prediction task, but downgrades on the other tasks like node clustering or classification. Recently some works <ref type="bibr" target="#b19">(Mandal et al. 2021)</ref> attempt to bring the idea of meta-learning to train GNNs with meta knowledge which can help to avoid the bias caused by single pretext task. However, the meta-GNN might contain knowledge that cause task discrepancy issue .</p><p>With the knowledge of the previous work, we just wonder can we advance the expressivity of GNNs with the knowledge extracted by themselves in an unsupervised way? In order to answer this question, we theoretically dissect the graph convolution operations (shown in Theorem 1), and find that the smoothness of node representations generated by GNNs is dominated by smoothness of neighborhood embeddings from previous layers and the structural similarity. It suggests that improving the graph representations of shallow layer can indirectly help get better node embed-dings of deep layer or the final layer. Based on this observation, we propose SAIL, a Self-Augmented graph contrastive Learning framework, in which we mainly use two different views (i.e., non-linear mappings of input node feature and the final layer of GNN) of transformed node representations.</p><p>More specifically, we propose to iteratively use the smoothed node representations from the output layer of GNNs to improve the node representations of shallow layer or the input layer (e.g., non-linear mapping of input node features). The most recent work ) also shares a similar idea for supervised task, while the different thing is that it forces the knowledge flow from lowlevel to high-level neural representations. Besides attempting to make an agreement on the selected views, we introduced a self-distilling module to raise consistency regularization over node representations from both local and global perspectives. The design of self-distilling module is inspired by a recent work (Wang and Isola 2020) on the importance of alignment and uniformity for a successful contrastive learning method. With a given distribution of positive pair, the alignment calculates the expected similarity of connected nodes (i.e. locally closeness), and the uniformity measures how well the encoded node embeddings are globally distributed. The intra-distilling module aims at forcing the learnt representations and node features to have consistent uniformity distribution. Inspired by another piece of work <ref type="bibr" target="#b10">(Ishida et al. 2020)</ref> indicating out the alleviating the learning bias by injecting noise into objective, we design an inter-distilling framework to align the node representations from a copied teacher model to noisy student model. Through multiple runs of the inter-distilling module, we implicitly mimic the deep smoothing operation with a shallow GNN (e.g., only a single GNN layer), while avoiding noisy information from high-order neighbors to cause the known oversmoothing issue <ref type="bibr" target="#b15">Li, Han, and Wu 2018)</ref> since shallow GNN only depends on the local neighbors. The proposed SAIL can learn shallow but powerful GNN. Even with a single GNN layer, it has consistently competitive or even better performance on various benchmark datasets, comparing to state-of-the-art baselines. We summarize the contributions of this work as follow:</p><p>-We present SAIL, to the best of our knowledge, the first generic self-supervised framework designed for advancing the expressivity of GNNs through distilling knowledge of self-augmented views but not depending on any external teacher model. -We introduce a universal self-distilling module for unsupervised learning graph neural networks. The presented self-distilling method can bring several advantages including but not limited to: 1) distilling knowledge from a self-created teacher model following the graph topology; 2) we can mimic a deep smoothing operation with a shallow GNN by iteratively distilling knowledge from teacher models to guide a noisy student model with future knowledge. -We demonstrate the effectiveness of proposed method with thorough experiments on multiple benchmark datasets and various tasks, yielding consistent improve-ment comparing with state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Graph Neural Networks. In recent years, we have witnessed a fast progress of graph neural network in both methodology study and its applications. As one of the pioneering research, spectral graph convolution methods <ref type="bibr" target="#b2">(Defferrard, Bresson, and Vandergheynst 2016)</ref> generalized the convolution operation to non-Euclidean graph data. <ref type="bibr" target="#b11">Kipf et al. (Kipf and Welling 2017)</ref> reduced the computation complexity to 1-order Chebyshev approximation with an affined assumption. <ref type="bibr" target="#b21">NT et al. (NT and Maehara 2019)</ref> justified that classical graph convolution network (GCN) and its variants are just low-pass filter. At the same time, both of studies <ref type="bibr" target="#b12">Klicpera, Wei?enberger, and G?nnemann 2019)</ref> proposed to replace standard GCN layer with a normalized high-order low-pass filters (e.g. personalized PageRank, heats kernel). This conclusion can help to answer why simplified GCN proposed by <ref type="bibr" target="#b35">Wu et al. (Wu et al. 2019</ref>) has competitive performance with complicated multi-layer GNNs. Besides GCNs, many novel GNNs have been proposed, such as multi-head attention models <ref type="bibr" target="#b30">(Veli?kovi? et al. 2018;</ref><ref type="bibr" target="#b18">Ma et al. 2019)</ref>, recurrent graph neural network , RevGNN <ref type="bibr" target="#b14">(Li et al. 2021)</ref>, heterogeneous graph neural network . Self-supervised Learning for GNNs. In addition to the line of work following Deepwalk <ref type="bibr" target="#b23">(Perozzi, Al-Rfou, and Skiena 2014)</ref> for constructing self-supervising signals, mutual information (MI) maximization <ref type="bibr" target="#b31">(Veli?kovi? et al. 2019</ref>) over the input and output representations shows up as an alternative solution. In analogy to discriminating that output image representation is generated from the input image patch or noisy image, Veli?kovi? et al. <ref type="bibr" target="#b31">(Veli?kovi? et al. 2019)</ref> propose deep graph infomax (DGI) criterion to maximize the mutual information between a high-level "global" graph summary vector and a "local" patch representation. Inspired by DGI, more and more recent methods like InfoGraph <ref type="bibr" target="#b26">(Sun et al. 2019)</ref>, GraphCL <ref type="bibr" target="#b38">(You et al. 2020a</ref>), GCC ) and pre-training graph neural networks <ref type="bibr" target="#b9">(Hu et al. 2019</ref>) are designed for learning graph representations. Most of self-supervised learning methods can be distinguished by the way to do data augmentation and the predefined pretext tasks <ref type="bibr" target="#b36">(Xie et al. 2021;</ref><ref type="bibr" target="#b27">Sun, Lin, and Zhu 2020;</ref><ref type="bibr" target="#b39">You et al. 2020b;</ref><ref type="bibr" target="#b41">Zhao et al. 2021;</ref><ref type="bibr" target="#b37">Xu et al. 2021)</ref>. For example, graph contrastive coding (GCC) borrows the idea from the momentum contrastive learning , and aims at learning transferrable graph neural networks taking the node structural feature as the input. Both of GraphCL <ref type="bibr" target="#b38">(You et al. 2020a</ref>) and InfoGraph <ref type="bibr" target="#b26">(Sun et al. 2019)</ref> are designed for learning agreement of graph-level representations of augmented graph patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Let G = {V, E, X} denote an attribute graph where V is the node set {v i ? V}, E is the edge set, and X ? R N ?F is the node feature matrix where each row x i stands for the node feature of v i . We use A represents the node relation  <ref type="figure">Figure 1</ref>: Overall architecture of the proposed self-supervised GNN with intra-and inter-distilling modules. The intra-distilling module aims at forcing the learnt representations and node features to have consistent uniformity distribution. The interdistilling module consists of three operations including: 1) creating a teacher model by copying the target model (i.e., ? t ? ? s ), 2) fading the target model to a student model by injecting noise into the model parameters (l(? s , ?) = w? s + (1 ? w)?), 3) supervising the faded target model (i.e., student model) with future knowledge (i.e., H t ).</p><formula xml:id="formula_0">H s H s X sXs X t H t Inter-distill ? t ? ? s Distill H t toX s H s ? s = l(? s , ?)</formula><p>matrix where a ij = 1 if there existing a link between node v i and v j , i.e., e ij ? E, otherwise a ij = 0. We define the degree matrix D = diag(d 1 , d 2 , ? ? ? , d N ) where each element equals to the row-sum of adjacency matrix d i = j a ij .</p><p>Our goal is to learn a graph neural encoder ?(X, A|?) = H, where H ? {h 1 , h 2 , ? ? ? , h N } is the representation learned for nodes in V. Deep graph encoder ? usually has oversmoothing problem. In this work, we instantiate a GNN with single layer to validate the effectiveness of proposed method to learn qualified node representations with shallow neighbors. But the analysis results in the following section can be easily extended to deeper GNNs. The graph neural encoder <ref type="bibr" target="#b35">Wu et al. 2019)</ref> used in this paper is defined as:</p><formula xml:id="formula_1">? = D ? 1 2 (A + I)D ? 1 2 ?(X,?) = ?(? 2 XW)<label>(1)</label></formula><p>where W ? R F ?F ? is learnable parameter and ?(?) denotes the activation function. The vector h i ? R F ? actually summarizes a subgraph containing the second-order neighbors centered around node v i . We refer to H and X = XW as the self-agumented node representations after transformed raw node featatures X. X denotes the low-level node feature, which might contain lots of noisy information. Definition 1 (Second-order Graph Regularization). The objective of second-order graph regularization is to minimize the following equation</p><formula xml:id="formula_2">eij ?E s ij ||h i ? h j || 2 2 (2)</formula><p>where s ij is the second-order similarity which can be defined as cosine similarity s ij = c?N (i)?N (j) ?ic??jc ||?i?||2||?j?||2 , and h i denotes the node representation. Theorem 1. Suppose that a GNN aggregates node representations as h l i = ?( j?N (i)?vi ? ij h l?1 j ), where ? ij stands for the element of a normalized relation matrix. If the firstorder gradient of the selected activation function ?(x) satisfies |? ? (x)| ? 1, then the graph neural operator approximately equals to a second-order proximity graph regularization over the node representations.</p><p>Proof. Here we mainly focus on analyzing GNNs whose aggregation operator mainly roots on weighted sum over the neighbors, i.e. h l = ?( j?N (i)?vi ? ij h l?1 j ). Typical examples include but not limited to GCN <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref> where ? ij can be the element of normalized adjacent ma-</p><formula xml:id="formula_3">trix? = D ? 1 2 (A + I)D ? 1 2 or? 2 . The node representation h l i can be divided into three parts: the node representations ? ii h l?1 i , the sum of common neighbor representations S i = c?N (i)?N (j) ? ic h l?1 c , the sum of non-common neighbor representations D i = q?N (i)?N (i)?N (j) ? iq h l?1 q . Let y = ?(x)</formula><p>, and suppose that the selected activation function</p><formula xml:id="formula_4">holds |? ? (x)| ? 1. We can have (y1?y2) 2 (x1?x2) 2 = |y1?y2| 2 |x1?x2| 2 ? 1. Let's reformulate the definition of h l as h l = ?(? l ) and h l = j?N (i)?vi ? ij h l?1 j . Then we can have ||h l i ? h l j || 2 ? ||? l i ?? l j || 2 .</formula><p>The distance between the representations h l i and h l j satisfies:</p><formula xml:id="formula_5">||h l i ? h l j || 2 ? ||? l i ?? l j || 2 = ||(? ii h l?1 i ? ? jj h l?1 j ) + (S i ? S j ) + (D i ? D j )|| 2 ? ||(? ii h l?1 i ? ? jj h l?1 j )|| 2 + ||(S i ? S j )|| 2 + ||(D i ? D j )|| 2 ? ||(? ii h l?1 i ? ? jj h l?1 j )|| 2 local f eature smoothness + ||D i || 2 + ||D j || 2 non?common neighbor + || c?N (i)?N (j) (? ic ? ? jc )h l?1 c || 2 structure proximity<label>(3)</label></formula><p>From Equation 3, we can see that the upper bound of similarity of a pair of nodes is mainly influenced by local feature smoothness and structure proximity. According to the proof shown above, if a pair of node (v i , v j ) has smoothed local features and similar structure proximity with many common similar neighbors (i.e. ? ic ? ? jc ), the obtained node representation of a GNN will also enforce their node representations to be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning from Self-augmented View</head><p>From the conclusion given in Theorem 1, we can see that the quality of each GNN layer has close relation to previous layer. As the initial layer, the quality of input layer fea-tureX will propagate from the bottom to the top layer of a given GNN model. As a graph neural layer can work as a low-pass filter (NT and Maehara 2019), its output H are actually smoothed node representations after filtering out the noisy information existing in the low-level features. Usually single GNN layer might not perfectly get a clean node representations. By stacking multiple layers, a deep GNN model can repeatedly improved representations from previous layer. However, deep GNN models tend to oversmooth the node representations with unlimited neighborhood mixing. In this work, we attempt to improve the GNN model with shallow neighborhood by shaping the low-level node features with relatively smoothed node representations.</p><p>To overcome the above-discussed challenges, we propose to transfer the learnt knowledge in the last GNN layer H to shape X in both local and global view. Concretely, instead of constructing contrastive learning loss over the node representations h at the same GNN layer, we turn to maximize the neighborhood predicting probability between a node representation h and its input node features x in its neighbors. Formally, for a sample set</p><formula xml:id="formula_6">{v i , v j , v k } where e ij ? E but e ik / ? E, the loss ? i jk is defined on the pairwise compari- son of (h i , x j ) and (h i , x k ))</formula><p>. Therefore, our self-supervised learning for GNN has the loss function defined below,</p><formula xml:id="formula_7">L ssl = eij ?E e ik / ?E ??(?(h i , x j ), ?(h i , x k ))+?R(G),<label>(4)</label></formula><p>where ?(?) can be an arbitrary contrastive loss function, ? is a scoring function, and R is the regularization function with weight ? for implementing graph structural constraints (to be introduced in next section). There are lots of candidates for contrastive loss ?(). In this work we use logistic pairwise</p><formula xml:id="formula_8">loss ln ?(?(h i , x j ) ? ?(h i , x k )), wehre ?(x) = 1 1+exp(?x) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-distilling Graph Knowledge Regularization</head><p>The objective function defined in Eq. (4) models the interactions between output node representations h and input node features x, which can be regarded as an intra-model knowledge distillation process from smoothed node embeddings to denoise low-level node features. However, the raised contrastive samples over edge connectivity might fail to represent a whole picture of node representation distribution, and cause a bias to learning node representations favoring to predict edges. We present a self-distilling method shown in <ref type="figure">Figure 1</ref> consists of intra-and inter-distilling modules. Intra-distilling module: To supplement the loss defined on the individual pairwise samples, we introduce a regularization term to ensure the distribution consistency on the relations between the learned node representations H and the node features X over a set of randomly sampled nodes. Let LS = {LS 1 , LS 2 , ? ? ? , LS N } denote the randomly sampled pseudo relation graph, where LS i ? V and |LS i | = d is the number of sampled pseudo local neighbors for center node </p><formula xml:id="formula_9">? t ? ? s ; 7 ? s ? w? s + (1 ? w)?; 8 Optimize L ssl (? t , ? s , X, A); 9 return ? s ; v i . The estimated proximity for each node in i-th local struc- ture LS i is computed by S t ij = exp(?(h i , x j )) vj ?LSi exp(?(h i , x j )) S s ij = exp(?( x i , x j )) vj ?LSi exp(?( x i , x j ))<label>(5)</label></formula><p>where S t ij and S s ij denote the similarity estimated from different node representations between node v i and v j . The S t ij will act as the teacher signal to guide the node features X = { x 1 , x 2 , ? ? ? , x N } to agree on the relation distribution over a random sampled graph. For the node v i , the relation distribution similarity can be measured as <ref type="bibr">]</ref> ). Then we can compute the relation similarity distribution over all the nodes as</p><formula xml:id="formula_10">S i = CrossEntropy(S t [i,?] , S s [i,?</formula><formula xml:id="formula_11">R intra = N i=1 S i ,<label>(6)</label></formula><p>where R intra acts as a regularization term generated from the intra-model knowledge, and to push the learned node representations H and node features X being consistent at a subgraph-level.</p><p>Inter-distilling Module: The second regularization is to introduce the inter-distilling module for addressing the over-smoothing issue. The inter-distilling module can guide the target GNN model by transferring the learned selfsupervised knowledge. Through multiple implementations of the inter-distilling module, we implicitly mimic the deep smoothing operation with a shallow GNN (e.g. a single GNN layer), while avoiding to bring noisy information from highorder neighbors. The overall inter-distilling framework is shown in <ref type="figure">Figure 1</ref>. We create a teacher model ? t by copying the target GNN model, then inject noise into the target model that will degrade into a student model ? s after a fix number of iterations. Working with a self-created teacher and student model {? t , ? s } with the same architectures shown in Eq. (1), student model ? s (X, A) = {H s , X s } distills knowledge from the teacher model ? t . Since no label is available, we propose to implement representation distillation <ref type="bibr" target="#b29">(Tian, Krishnan, and Isola 2020)</ref> with the constraint of graph structure. The knowledge distillation module con-sists of two parts, defined as</p><formula xml:id="formula_12">R inter = KD(H t , X s |G) + KD(H t , H s |G)<label>(7)</label></formula><p>where H t is the node representations from teacher model ? t , and X s = XW. To define module KD(?), we should meet several requirements: 1) this function should be easy to compute and friendly to back-propagation strategy; and 2) it should stick to the graph structure constraint. We resort to the conditional random field (CRF) <ref type="bibr" target="#b13">(Lafferty, McCallum, and Pereira 2001)</ref> to capture the pairwise relationship between different nodes. For a general knowledge distillation module KD(Y, Z|G), the dependency of Z on Y can be given following the CRF model:</p><formula xml:id="formula_13">P (Z|Y ) = 1 C(Y ) exp(?E(Z|Y )),<label>(8)</label></formula><p>where C(?) is the normalization factor and E(?) stands for the energy function, defined as follows:</p><formula xml:id="formula_14">E(Z i |Y i ) = ? u (Z i , Y i ) + ? p (Z i , Z j , Y i , Y j ) = (1 ? ?)||Z i ? Y i || 2 2 + ? j?N (i) ? ij ||Z i ? Z j || 2 2 ,<label>(9)</label></formula><p>where ? u and ? p are the unary and pairwise energy function, respectively. The parameter ? ? [0, 1] is to control the importance of two energy functions. When Z is the node feature of student model and Y is the node representation from teacher model ? m?1 , the energy function defined in Equation <ref type="formula" target="#formula_14">(9)</ref> enforces the node v i representation from student model to be close to that in the teacher model and its neighbor nodes. After obtaining the energy function, we can resolve the CRF objective with the mean-field approximation method by employing a simple distribution Q(Z) to approximate the distribution P (Z|Y ). Specifically distribution Q(Z) can be initialized as the product marginal distributions as Q(Z) = ? N i=1 Q i (Z i ). Through minimizing the KL divergence between these two distributions as follows:</p><p>arg min KL(Q(Z)||P (Z|Y )).</p><p>(10)</p><p>Then we can get the optimal Q * i (Z i ) as follows:</p><formula xml:id="formula_15">ln Q * i (Z i ) = E j =i [ln P (Z j |Y j )] + const.<label>(11)</label></formula><p>According to Equation <ref type="formula" target="#formula_13">(8)</ref> and <ref type="formula" target="#formula_14">(9)</ref>, we can get</p><formula xml:id="formula_16">Q * i (Z i ) ? exp((1??)||Z i ?Y i || 2 2 +? j?N (i) ? ij ||Z i ?Z j || 2 2 ),<label>(12)</label></formula><p>which shows that Q * i (Z i ) is a Gaussian function. By computing its expectation, we have the optimal solution for Z i as follows:</p><formula xml:id="formula_17">Z * i = (1 ? ?)Y i + ? j?N (i) ? ij Z j (1 ? ?) + ? j?N (i) ? ij<label>(13)</label></formula><p>Then we can get the cross-model knowledge distillation rule by enforcing the node representations from student model to have minimized metric distance to Z * i . After replacing the random variables Y i as the node representation h t i of teacher model ? t , then we can get the final distillation regularization as follows:</p><formula xml:id="formula_18">KD(H t , X s |G) = || x s i ? (1 ? ?)h t i + ? j?N (i) ? ij x s j (1 ? ?) + ? j?N (i) ? ij || 2 2 (14) KD(H t , H s |G) = ||h s i ? (1 ? ?)h t i + ? j?N (i) ? ij h s j (1 ? ?) + ? j?N (i) ? ij || 2 2</formula><p>(15) where x s i denotes the feature of node v i from the student model ? s , and h s i denotes the output node representation for node v i . In terms of ? ij in Eq. (13), we have many choices such as attentive weight, or mean pooling etc. In this work, we simply initialize it with mean-pooling operation over the node representations.</p><p>The overall self-supervised learning objective in this work can be extended as follows by taking the two regularization terms:</p><formula xml:id="formula_19">L ssl = eij ?E e ik / ?E ? ?(?(h s i , x s j ), ?(h s i , x s k )) + ?(R intra + R inter ).<label>(16)</label></formula><p>where the initial teacher model ? t can be initialized through optimizing the proposed self-supervised learning without the cross-model distillation regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Evaluation</head><p>We compare the proposed method with various state-ofthe-art methods on six datasets including three citation networks (Cora, Citeseer, Pubmed) (Kipf and Welling 2017), product co-purchase networks (Computers, Photo), and one co-author network subjected to a subfield Computer Science (CS). The product and co-author graphs are benchmark datasets from pytorch geometric <ref type="bibr" target="#b3">(Fey and Lenssen 2019)</ref>. Due to the limited space, more details about the experimental setting can be found in the appendix.</p><p>Overall Performance Node Classification. The GNN methods compared here include convolution or attentive neural networks.   summarizes the overall performance. The performance of simple GCN learned by the proposed method SAIL consistently outperforms the other baselines learned by supervised and unsupervised objectives. It's noted that DisenGCN iteratively applies attentive routing operation to dynamically reshape node relationships in each layer. By default, it has 5-layers and iteratively applies 6 times, which typically is a deep GNN. From the empirical results, we can see that SAIL can empower the single-layer GNN through iterative inter-model knowledge distillation.</p><p>Node Clustering. In node clustering task, we aim at evaluating the quality of the node representations learned by unsupervised methods. The performance is validated by measuring the normalized mutual information (NMI). The node representations are learned with the same experimental setting for the node classification task. From the results shown in <ref type="table" target="#tab_3">Table 2</ref>, we can see that the proposed method are superior to the baselines in most of cases.</p><p>Link Prediction. In addition, we attempt answer the question about whether the learned node representations can keep the node proximity. The performance of each method is measured with AUC value of link prediction. All of the methods in this experiment have the same model configuration as the node classification task. From the results shown in <ref type="table" target="#tab_4">Table 3</ref>, we can see that SAIL still outperforms most of the baselines learned with supervised and unsupervised objectives. From the results in <ref type="table">Table 1</ref>, we see that classification of nodes in graph CS is indeed an easy task, and most of the GNNs models have similar good performance. However, for link prediction shown in <ref type="table" target="#tab_4">Table 3</ref>, unsupervised models (CAN and our model) learned better h than those with supervision information, obviously because the supervision information is for node classification, not for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring Locality and Feature Smoothness</head><p>Based on our theoretical understanding in Theorem 1, the representation smoothness is mainly influenced by the local structural proximity and feature closeness. Follow the ideas of recent works <ref type="bibr" target="#b8">(Hou et al. 2020;</ref><ref type="bibr" target="#b0">Chen et al. 2020a</ref>), we conduct empirical study on the node representation smoothness before and after being encoded by the GNNs.</p><p>Before encoding. With a given graph and node features, we calculate the inverse of average pairwise distance among the raw feature <ref type="bibr" target="#b8">(Hou et al. 2020)</ref>, and clustering coefficient <ref type="bibr" target="#b34">(Watts and Strogatz 1998)</ref> to study the feature smoothness and structural locality, respectively. Combining with the node classification results, we empirically found that most of the neural encoders (e.g., GNNs, even simple MLPs) performed well on node classification in graphs like CS, which has a strong locality and large feature smoothness shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Interestingly, for graphs with a strong locality but a low node feature smoothness (e.g., "Computers", "Photo"), unsupervised methods can leverage the graph structure to achieve better performance than supervised methods.</p><p>After encoding. Chen et a.  propose to use mean average distance (MAD) between the target node and its neighbors for measuring the smoothness of node representations, and the MAD gap between the neighbor and remote nodes to measure the over-smoothness. Let's denote  M AD gap = M AD rmt ? M AD nei , where M AD nei represents the MAD between the target node and its neighbors, and M AD rmt is defined to measure the MAD of remote nodes. If we get a small MAD but relatively large MAD gap, then we can say that the learnt node representations are not over-smoothed. We define a variant metric M AD ratio = MADgap MADnei to measure the information-to-noise ratio brought by the relative changes of MAD of remote nodes over neighbors. We use the node representations with the same setting as node classification task. The results shown in Table 5 demonstrate that the proposed method can achieve the best smoothing performance (i.e. smallest M AD nei ). In terms of over-smoothing issue, SAIL has the relative large M AD gap comparing with the scale of the achieved MAD of each method, which can be measured by M AD ratio . The empirical results reflect that the proposed method can help to tell the difference from local neighbors and remote nodes.</p><p>Robustness Against Incomplete Graphs. With the same experimental configuration as link prediction task, we also validate the performance of learned node embeddings from incomplete graph for node classification task. According to the results in <ref type="table" target="#tab_5">Table 4</ref>, SAIL still outperforms baseline methods in most cases, demonstrating the robustness of SAIL performance. It has a larger average downgrade in terms of node classification accuracy than CAN, but still has better classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct node classification experiments to validate the contribution of each component of the proposed SAIL, where EMI denotes the edge MI loss l i jk , Intra stands for intra-distill module R intra and Inter represents th R inter in Eq. 16. From the results shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we can see that both intra-and inter-distilling module can jointly improve the qualification of learnt node representations. Combining with the edge MI maximization task, we can see a significant improvement on the node classification accuracy. Due to the limited space, we present the ablation studies on link prediction and node clustering tasks in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Citeseer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this work, we propose a self-supervised learning method (SAIL) regularized by graph structure to learn unsupervised node representations for various downstream tasks. We conduct thorough experiments on node classification, node clustering, and link prediction tasks to evaluate the learned node representations. Experimental results demonstrate that SAIL helps to learn competitive shallow GNN which outperforms the state-of-the-art GNNs learned with supervised or unsupervised objectives. This initial study might shed light upon a promising way to implement self-distillation for graph neural networks. In the future, we plan to study how to improve the robustness of the proposed method against adversarial attacks and learn transferable graph neural network for downstream tasks like few-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setting Baselines</head><p>The baselines in this work include both state-of-theart supervised and unsupervised learning GNNs. The supervised methods are Chebyshev filters (Cheb-Net) <ref type="bibr" target="#b2">(Defferrard, Bresson, and Vandergheynst 2016)</ref>, MLP <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>, Graph Convolution Network (GCN) <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>, Simplified GCN (SGC) , Disentangled Graph Convolution Network (DisenGCN) <ref type="bibr" target="#b18">(Ma et al. 2019)</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b30">(Veli?kovi? et al. 2018)</ref>, Graph Markov Neural Network (GMNN) <ref type="bibr" target="#b25">(Qu, Bengio, and Tang 2019)</ref> and GraphSAGE <ref type="bibr" target="#b5">(Hamilton, Ying, and Leskovec 2017)</ref>. The unsupervised ones include Deep Graph Infomax (DGI) <ref type="bibr" target="#b31">(Veli?kovi? et al. 2019)</ref>, CAN <ref type="bibr" target="#b20">(Meng et al. 2019)</ref>, GMI <ref type="bibr" target="#b22">(Peng et al. 2020)</ref>, and Multi-view Graph Representation Learning (MVGRL) <ref type="bibr" target="#b6">(Hassani and Khasahmadi 2020)</ref>. It's noted that the performance of MVGRL shown in this work is different the published results. In terms of the inconsistent results, we find that many researchers also have the same problem. Please refer to issues at https://github.com/kavehhassani/mvgrl/issues/. From a closed issue about the source of Cora, we can see that the split of Cora used in MVGRL is from DGL, which is different from the Cora used in official implementation of many classical GNN baselines like GCN, GAT. For a fair comparison, we use the official code to implement MVGRL with the benchmark data from https://github.com/tkipf/gcn (https://github.com/tkipf/gcn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The data statistics can be found in <ref type="table" target="#tab_7">Table 6</ref>. We implement the baselines with the official code provided by the authors. If the results are not previously reported, we implement them with suggested hyperparameters according to the original paper. The results shown in this work are average value over 10 runs for each task. For the node classification, the experimental configuration for the three citation data is the same as <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>. To make a fair comparison on the other co-purchase and co-author networks, we randomly select 20% nodes for supervised training GNNs, 10% node as validation set, and the rest of nodes are left for testing. For the unsupervised methods, they can access to the complete graph without using node labels. For the link prediction task, we randomly select 20% edges for each node as testing, and the rest as the training set. All baselines involved in this experiment only work on the graph built from the training set. For the supervised learning models like GCN, GAT, DisenGCN, GMNN, we still use the same configure as the node classification task to select labelled nodes to train the model, which means the nodes in the selected testing edges can still be labelled nodes for the supervised training strategy. While the unsupervised methods can only access to the edges in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Configuration</head><p>The proposed method has hyper-paramters including regularization weight ?, ? for balancing the contribution of teacher model and local neighbors. The hyper-parameter ? controls the number of steps to update the teacher model, while w controls the degradation degree of target model. In this work, we set ? to 30. Since we use the mean pooling operation, then the ? equals to 1 N (i) by default. For each hyperparameter, we use grid search method to valid the best configuration from pre-defined search space, specifically, ? ? {0, 0.1, 0.5, 1.0}, ? ? {0.0, 0.1, 0.5, 1.0}, w ? [0.0, 1.0], and embedding dimension F ? = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Smoothness</head><p>In this work, we use two kinds of methods to measure the feature smoothness before and after being encoded by GNNs. We use raw feature X = {x i |v i ? V} to conduct an empirical analysis to show the potential impact of raw feature smoothness on the difficulty to learn qualified node representations. The specific definition can found as follows. Definition 2 (Feature Smoothness <ref type="bibr" target="#b8">(Hou et al. 2020)</ref>). We define the feature smoothness ? f over the normalized space X = [0, 1] F as follows:</p><formula xml:id="formula_20">? f = || N i=1,vi?V j?N (i) (x i ? x j ) 2 || 1 |E| ? F ,</formula><p>where || ? || 1 denotes the Manhattan norm. Noted that we use normalized 1 ? f for better showing the influence of feature smoothness and locality on the model performance. This metric is used in the "before encoding" section, which is at the page 6 of submitted manuscript.</p><p>The other method is proposed by Chen et al. ). They study the over-smoothing issue of encoded node embeddings through measuring the mean average cosine distance (MAD) of node pairs. When attempting to follow the same setting to calculate theMADs, we observe that there exists graphs with very few number of nodes having remote nodes. Therefore, we relax the condition to separate the neighbor and remote nodes. In this work, we modify the nodes that can be reached in larger than 3 steps as  the remote nodes. Besides the way to define nearby and remote nodes, we also note that Chen et al. use a different way to split the experimental data is very different from classical GCN <ref type="bibr" target="#b11">(Kipf and Welling 2017)</ref>. In this work, the obtained MAD values are relative smaller than the results shown in ). We guess the reason lies at the different experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>We conduct ablation studies on the node clustering and link prediction. The results shown in <ref type="table" target="#tab_8">Table 7</ref> and 8 have the similar trend as the results for node classification. Both of interand intra-distillation show positive impact on advancing the performance of the base model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Locality and node feature smoothness in the six graphs used in experimental evaluation. The larger feature smoothness value is, the more smoothing feature we have.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study of the distillation component influence in node classification accuracy (in %).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: SAIL 1 Input: graph G = {V, E, X}, hyperparameters= {?, ?} 2 Output: learned GNN ? 3 initialize ? 0 by optimizing Eq. (16) without R cross 4 for m ? 1 to n do 5 if m%? == 0 then 6</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 Table 1 :</head><label>11</label><figDesc>Accuracy (with standard deviation) of node classification (in %). The best results are highlighted in bold. Results without standard deviation are copied from original works.</figDesc><table><row><cell>Input</cell><cell>Methods</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell><cell>Computers</cell><cell>Photo</cell><cell>CS</cell></row><row><cell></cell><cell>ChebNet (Defferrard, Bresson, and Vandergheynst 2016)</cell><cell>81.2</cell><cell>69.8</cell><cell>74.4</cell><cell>70.5?0.5</cell><cell>76.9?0.3</cell><cell>92.3?0.1</cell></row><row><cell></cell><cell>MLP (Kipf and Welling 2017)</cell><cell>55.1</cell><cell>46.5</cell><cell>71.4</cell><cell>55.3?0.2</cell><cell>71.1?0.1</cell><cell>85.5?0.1</cell></row><row><cell></cell><cell>GCN (Kipf and Welling 2017)</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell><cell>76.11?0.1</cell><cell>89.0?0.3</cell><cell>92.4?0.1</cell></row><row><cell>X,A,Y</cell><cell>SGC (Wu et al. 2019) GAT (Veli?kovi? et al. 2018)</cell><cell cols="3">81.0 ? 0.0 71.9?0.1 78.9?0.0 83.0?0.7 72.5?0.7 79.0?0.3</cell><cell>55.7?0.3 71.4?0.6</cell><cell>69.7?0.5 89.4?0.5</cell><cell>92.3?0.1 92.2?0.2</cell></row><row><cell></cell><cell>DisenGCN (Ma et al. 2019)</cell><cell>83.7*</cell><cell>73.4*</cell><cell>80.5</cell><cell>52.7?0.3</cell><cell>87.7?0.6</cell><cell>93.3?0.1*</cell></row><row><cell></cell><cell>GMNN (Qu, Bengio, and Tang 2019)</cell><cell>83.7*</cell><cell>73.1</cell><cell>81.8*</cell><cell>74.1?0.4</cell><cell>89.5?0.6</cell><cell>92.7?0.2</cell></row><row><cell></cell><cell>GraphSAGE (Hamilton, Ying, and Leskovec 2017)</cell><cell>77.2?0.3</cell><cell cols="2">67.8?0.3 77.5 ?0.5</cell><cell>78.9?0.4</cell><cell>91.3?0.5</cell><cell>93.2 ?0.3</cell></row><row><cell></cell><cell>CAN (Meng et al. 2019)</cell><cell>75.2?0.5</cell><cell cols="2">64.5?0.2 64.8?0.3</cell><cell>78.2?0.5</cell><cell>88.3?0.6</cell><cell>91.2?0.2</cell></row><row><cell></cell><cell>DGI (Veli?kovi? et al. 2019)</cell><cell>82.3?0.6</cell><cell cols="2">71.8?0.7 76.8?0.6</cell><cell>68.2?0.6</cell><cell>78.2?0.3</cell><cell>92.4?0.3</cell></row><row><cell>X,A</cell><cell>GMI (Peng et al. 2020)</cell><cell>82.8?0.7</cell><cell cols="2">73.0?0.3 80.1?0.2</cell><cell>62.7?0.5</cell><cell>86.2?0.3</cell><cell>92.3?0.1</cell></row><row><cell></cell><cell>MVGRL (Hassani and Khasahmadi 2020)</cell><cell>83.5?0.4</cell><cell cols="2">72.1?0.6 79.8?0.7</cell><cell>88.4?0.3*</cell><cell>92.8?0.2</cell><cell>93.1?0.3</cell></row><row><cell></cell><cell>SAIL</cell><cell>84.6?0.3</cell><cell cols="2">74.2?0.4 83.8?0.1</cell><cell>89.4?0.1</cell><cell cols="2">92.5?0.1* 93.3?0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Node clustering performance measured by normalized mutual information (NMI) in %.</figDesc><table><row><cell cols="6">Methods Cora Citeseer PubMed Computers Photo</cell><cell>CS</cell></row><row><cell>CAN</cell><cell>51.7</cell><cell>35.4</cell><cell>26.7</cell><cell>42.6</cell><cell>53.4*</cell><cell>71.3</cell></row><row><cell>GMI</cell><cell>55.9</cell><cell>34.7</cell><cell>23.3</cell><cell>34.5</cell><cell>47.2</cell><cell>73.9</cell></row><row><cell>DGI</cell><cell>50.5</cell><cell>40.1*</cell><cell>28.8</cell><cell>36.8</cell><cell>42.1</cell><cell>74.7*</cell></row><row><cell cols="2">MVGRL 56.1*</cell><cell>37.6</cell><cell>34.7</cell><cell>46.2*</cell><cell>12.15</cell><cell>66.5</cell></row><row><cell>SAIL</cell><cell>58.1</cell><cell>44.6</cell><cell>33.3*</cell><cell>49.1</cell><cell>66.5</cell><cell>76.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>AUC (in %) of link prediction.</figDesc><table><row><cell>Methods</cell><cell cols="5">Cora Citeseer PubMed Computers Photo</cell><cell>CS</cell></row><row><cell>GMNN</cell><cell>87.5</cell><cell>86.9</cell><cell>88.8</cell><cell>82.1</cell><cell>86.7</cell><cell>91.7</cell></row><row><cell>GAT</cell><cell>90.5</cell><cell>89.1</cell><cell>80.5</cell><cell>84.5</cell><cell>88.4</cell><cell>92.2</cell></row><row><cell>GCN</cell><cell>82.6</cell><cell>83.2</cell><cell>88.5</cell><cell>82.1</cell><cell>86.7</cell><cell>89.7</cell></row><row><cell>DisenGCN</cell><cell>93.3</cell><cell>92.0</cell><cell>91.1</cell><cell>78.9</cell><cell>77.6</cell><cell>94.5</cell></row><row><cell>DGI</cell><cell>69.2</cell><cell>69.0</cell><cell>85.2</cell><cell>75.1</cell><cell>74.2</cell><cell>79.7</cell></row><row><cell>MVGRL</cell><cell>89.5</cell><cell>94.4</cell><cell>96.1*</cell><cell>74.6</cell><cell>73.1</cell><cell>83.1</cell></row><row><cell>CAN</cell><cell>94.8</cell><cell>94.8</cell><cell>91.9</cell><cell>94.9*</cell><cell>95.0</cell><cell>97.1*</cell></row><row><cell>GMI</cell><cell>95.1*</cell><cell>96.0*</cell><cell>96.0</cell><cell>85.5</cell><cell>91.9</cell><cell>95.5</cell></row><row><cell>SAIL</cell><cell>97.3</cell><cell>98.4</cell><cell>98.5</cell><cell>94.9</cell><cell>94.6*</cell><cell>97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (in %) of node classification after randomly 20% neighbors removal. The last column shows the average classification accuracy downgrades comparing with the results inTable 1.</figDesc><table><row><cell>Input</cell><cell>Methods</cell><cell cols="5">Cora Citeseer PubMed Computers Photo</cell><cell>CS</cell><cell>Avg ?</cell></row><row><cell></cell><cell>GMNN</cell><cell>77.2</cell><cell>68.8</cell><cell>79.8</cell><cell>70.8</cell><cell>87.5</cell><cell>91.6</cell><cell>4.1%</cell></row><row><cell>X,A,Y</cell><cell>GAT GCN</cell><cell>77.7 76.0</cell><cell>65.6 67.2</cell><cell>76.7 77.7</cell><cell>69.3 67.5</cell><cell>89.4 88.7</cell><cell>90.4 89.9</cell><cell>4.0% 4.5%</cell></row><row><cell></cell><cell>DisenGCN</cell><cell>77.6</cell><cell>68.2</cell><cell>78.3</cell><cell>37.5</cell><cell>48.8</cell><cell cols="2">92.4 15.1%</cell></row><row><cell></cell><cell>CAN</cell><cell>73.2</cell><cell>64.0</cell><cell>63.5</cell><cell>77.5</cell><cell>88.1</cell><cell>91.1</cell><cell>1.1%</cell></row><row><cell></cell><cell>DGI</cell><cell>72.3</cell><cell>70.1</cell><cell>71.5</cell><cell>67.6</cell><cell>77.4</cell><cell>91.7</cell><cell>4.0%</cell></row><row><cell>X,A</cell><cell>GMI</cell><cell>77.4</cell><cell>68.3</cell><cell>76.9</cell><cell>54.9</cell><cell>82.5</cell><cell>89.2</cell><cell>6.1%</cell></row><row><cell></cell><cell>MVGRL</cell><cell>69.5</cell><cell>62.5</cell><cell>76.5</cell><cell>87.2</cell><cell>92.5</cell><cell>91.8</cell><cell>6.2%</cell></row><row><cell></cell><cell>SAIL</cell><cell>81.0</cell><cell>71.3</cell><cell>81.2</cell><cell>88.5</cell><cell>92.4</cell><cell>92.7</cell><cell>2.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Empirical analysis to shown the quality of learned node representations measured by mean average distance (MAD).</figDesc><table><row><cell>Data</cell><cell>Metrics</cell><cell cols="5">GCN GAT DisenGCN GMNN CAN</cell><cell>DGI</cell><cell cols="3">GMI MVGRL SAIL</cell></row><row><cell></cell><cell>M ADnei</cell><cell cols="2">0.075 0.029</cell><cell>0.215</cell><cell>0.088</cell><cell cols="3">0.059 0.312 0.069</cell><cell>0.240</cell><cell>0.013</cell></row><row><cell>Cora</cell><cell>M ADgap</cell><cell cols="2">0.308 0.083</cell><cell>0.471</cell><cell>0.390</cell><cell cols="3">0.900 0.557 0.966</cell><cell>0.661</cell><cell>0.322</cell></row><row><cell></cell><cell>M ADratio</cell><cell>4.13</cell><cell>2.86</cell><cell>2.19</cell><cell>4.43</cell><cell>15.2</cell><cell>1.77</cell><cell>13.83</cell><cell>2.75</cell><cell>25.3</cell></row><row><cell></cell><cell>M ADnei</cell><cell cols="2">0.049 0.014</cell><cell>0.194</cell><cell>0.059</cell><cell cols="3">0.057 0.289 0.122</cell><cell>0.174</cell><cell>0.007</cell></row><row><cell>Citeseer</cell><cell>M ADgap</cell><cell cols="2">0.308 0.083</cell><cell>0.471</cell><cell>0.390</cell><cell cols="3">0.921 0.497 0.879</cell><cell>0.491</cell><cell>0.429</cell></row><row><cell></cell><cell>M ADratio</cell><cell>4.13</cell><cell>2.86</cell><cell>2.19</cell><cell>4.43</cell><cell>15.1</cell><cell>1.72</cell><cell>6.2</cell><cell>2.82</cell><cell>61.2</cell></row><row><cell></cell><cell>M ADnei</cell><cell cols="2">0.043 0.024</cell><cell>0.054</cell><cell>0.068</cell><cell cols="3">0.037 0.112 0.086</cell><cell>0.180</cell><cell>0.024</cell></row><row><cell>Pubmed</cell><cell>M ADgap</cell><cell cols="2">0.155 0.083</cell><cell>0.224</cell><cell>0.438</cell><cell cols="3">0.757 0.145 0.833</cell><cell>0.541</cell><cell>0.294</cell></row><row><cell></cell><cell>M ADratio</cell><cell>6.02</cell><cell>6.43</cell><cell>4.17</cell><cell>6.39</cell><cell>20.6</cell><cell>1.29</cell><cell>8.63</cell><cell>3.00</cell><cell>12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Summary of data statistics.</figDesc><table><row><cell>Data</cell><cell cols="4">#Nodes #Edges #Features Labels</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Computers</cell><cell>13,752</cell><cell>287,209</cell><cell>767</cell><cell>10</cell></row><row><cell>Photo</cell><cell>7,650</cell><cell>143,663</cell><cell>745</cell><cell>8</cell></row><row><cell>CS</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the distillation component on link prediction task, measured by AUC in %.</figDesc><table><row><cell>Distillation</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>EMI</cell><cell>95.6</cell><cell>95.8</cell><cell>96.1</cell></row><row><cell>EMI+Intra</cell><cell>95.9</cell><cell>96.1</cell><cell>96.8</cell></row><row><cell>EMI+inter</cell><cell>96.5</cell><cell>96.8</cell><cell>97.1</cell></row><row><cell>EMI+Inter+Intra</cell><cell>97.3</cell><cell>98.3</cell><cell>98.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of the distillation component on node clustering task, measured by Normalized Mutual Information (NMI) in %.</figDesc><table><row><cell>Distillation</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>EMI</cell><cell>51.2</cell><cell>42.6</cell><cell>31.7</cell></row><row><cell>EMI+Intra</cell><cell>53.4</cell><cell>42.8</cell><cell>32.1</cell></row><row><cell>EMI+inter</cell><cell>55.6</cell><cell>43.5</cell><cell>32.8</cell></row><row><cell>EMI+Inter+Intra</cell><cell>58.1</cell><cell>44.6</cell><cell>33.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On Self-Distilling Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring and Improving the Use of Graph Information in Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Do we need zero training loss after achieving zero training error?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training Graph Neural Networks with 1000 Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4424" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Medya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00137</idno>
		<title level="m">Meta-Learning with Graph Neural Networks: Methods and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coembedding attributed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GMNN: Graph Markov Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In-foGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-stage selfsupervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5892" to="5899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Consistent MetaReg: Alleviating Intra-task Discrepancy for Better Meta-knowledge. In IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3094" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Selfsupervised learning of graph neural networks: A unified review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">InfoGCL: Information-Aware Graph Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<biblScope unit="page" from="793" to="803" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view Self-supervised Heterogeneous Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="319" to="334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
