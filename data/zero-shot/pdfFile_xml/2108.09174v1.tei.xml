<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance Trans4Trans (a) Hardware components (b) Test scenario (c) Dual-head Trans4Trans Correction RGB-D Camera Bone-conducting Earphone Trans4Trans Fog Night Snow Rain Normal (d) Single-head Trans4Trans Walking view Driving view</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
							<email>ji-aming.zhang@kit.edu</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Authors are with Computer Vision for Human-Computer Interaction Lab,</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<email>kailun.yang@kit.edu</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Authors are with Computer Vision for Human-Computer Interaction Lab,</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Constantinescu</surname></persName>
							<email>angela.constantinescu@kit.edu</email>
							<affiliation key="aff3">
								<orgName type="department">authors are with Center for Digital Accessibility and Assis-tive Technology</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
							<email>kunyu.peng@kit.edu</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Authors are with Computer Vision for Human-Computer Interaction Lab,</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>M?ller</surname></persName>
							<email>karin.mueller2@kit.edu</email>
							<affiliation key="aff3">
								<orgName type="department">authors are with Center for Digital Accessibility and Assis-tive Technology</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<email>rainer.stiefelhagen@kit.edu</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Authors are with Computer Vision for Human-Computer Interaction Lab,</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">authors are with Center for Digital Accessibility and Assis-tive Technology</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agx</forename><surname>Xavier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">&quot;Obstacle&quot;</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hangzhou SurImage Company Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance Trans4Trans (a) Hardware components (b) Test scenario (c) Dual-head Trans4Trans Correction RGB-D Camera Bone-conducting Earphone Trans4Trans Fog Night Snow Rain Normal (d) Single-head Trans4Trans Walking view Driving view</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Transparent object Walkable path &quot;Forward&quot; &quot;Glass door&quot; (Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accidental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fog Night</head><p>Snow Rain Normal Accidental <ref type="figure">Fig. 1:</ref> (a) the assistive system equipped with smart vision glasses and a portable GPU processor is tested (b) in front of a glass door.</p><p>The input image is segmented as walkable path and glass door by (c) our Transformer for Transparency (Trans4Trans) model, which are safety-critical for navigation. The user interface consists of vibration and voice feedback. After training on normal and adverse data, our (d) Trans4Trans model reaches high robustness when applied to various real-world driving scenarios.</p><p>Abstract-Transparent objects, such as glass walls and doors, constitute architectural obstacles hindering the mobility of people with low vision or blindness. For instance, the open space behind glass doors is inaccessible, unless it is correctly perceived and interacted with. However, traditional assistive technologies rarely cover the segmentation of these safety-critical transparent objects. In this paper, we build a wearable system with a novel dualhead Transformer for Transparency (Trans4Trans) perception model, which can segment general-and transparent objects. The two dense segmentation results are further combined with depth information in the system to help users navigate safely and assist them to negotiate transparent obstacles. We propose a lightweight Transformer Parsing Module (TPM) to perform multi-scale feature interpretation in the transformer-based decoder. Benefiting from TPM, the double decoders can perform joint learning from corresponding datasets to pursue robustness, meanwhile maintain efficiency on a portable GPU, with negligible calculation increase. The entire Trans4Trans model is constructed in a symmetrical encoder-decoder architecture, which outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining mIoU of 45.13% and 75.14%, respectively. Through a user study and various pre-tests conducted in indoor and outdoor scenes, the usability and reliability of our assistive</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: (a) the assistive system equipped with smart vision glasses and a portable GPU processor is tested (b) in front of a glass door.</p><p>The input image is segmented as walkable path and glass door by (c) our Transformer for Transparency (Trans4Trans) model, which are safety-critical for navigation. The user interface consists of vibration and voice feedback. After training on normal and adverse data, our (d) Trans4Trans model reaches high robustness when applied to various real-world driving scenarios.</p><p>Abstract-Transparent objects, such as glass walls and doors, constitute architectural obstacles hindering the mobility of people with low vision or blindness. For instance, the open space behind glass doors is inaccessible, unless it is correctly perceived and interacted with. However, traditional assistive technologies rarely cover the segmentation of these safety-critical transparent objects. In this paper, we build a wearable system with a novel dualhead Transformer for Transparency (Trans4Trans) perception model, which can segment general-and transparent objects. The two dense segmentation results are further combined with depth information in the system to help users navigate safely and assist them to negotiate transparent obstacles. We propose a lightweight Transformer Parsing Module (TPM) to perform multi-scale feature interpretation in the transformer-based decoder. Benefiting from TPM, the double decoders can perform joint learning from corresponding datasets to pursue robustness, meanwhile maintain efficiency on a portable GPU, with negligible calculation increase. The entire Trans4Trans model is constructed in a symmetrical encoder-decoder architecture, which outperforms state-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2 datasets, obtaining mIoU of 45.13% and 75.14%, respectively. Through a user study and various pre-tests conducted in indoor and outdoor scenes, the usability and reliability of our assistive This work was supported in part through the AccessibleMaps project by the Federal Ministry of Labor and Social Affairs (BMAS) under the Grant No. 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, and in part by Hangzhou SurImage Company Ltd. (Corresponding author: Kailun Yang.) <ref type="bibr" target="#b0">1</ref> Authors are with Computer Vision for Human-Computer Interaction Lab, and 2 authors are with Center for Digital Accessibility and Assistive Technology, Karlsruhe Institute of Technology, Germany (e-mail: jiaming.zhang@kit.edu, kailun.yang@kit.edu, angela.constantinescu@kit.edu, kunyu.peng@kit.edu, karin.mueller2@kit.edu, rainer.stiefelhagen@kit.edu).</p><p>Code will be made publicly available at: https://github.com/jamycheung/ Trans4Trans. system have been extensively verified. Meanwhile, the Tran4Trans model has outstanding performances on driving scene datasets. On Cityscapes, ACDC, and DADA-seg datasets corresponding to common environments, adverse weather, and traffic accident scenarios, mIoU scores of 81.5%, 76.3%, and 39.2% are obtained, demonstrating its high efficiency and robustness for real-world transportation applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A SSISTED navigation of pedestrians and automated driving of intelligent vehicles are inextricably intertwined in the Intelligent Transportation Systems (ITS) field <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, both with the aim to improve traffic flow towards the utopia of all road participants. To this end, it is necessary to expand the coverage of assistance systems from drivers to pedestrians, especially those with visual impairments, who are one of the most vulnerable road users <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>To assist the navigation of visually impaired people, it is essential to attain efficient and robust walking scene understanding, which shares similar challenges with the ITS research line on driving surrounding segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In real-world scenes, modern fully glazed facades and transparent objects are very common, but they are rarely addressed in existing semantic perception systems either for automated transportation or assisted navigation. Knowledge of glass architecture <ref type="bibr" target="#b11">[12]</ref> and glass doors <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> are particular important for visually impaired people, because transparent objects often present architectural barriers which hinder the mobility of people with low vision or blindness. For example, if an inaccessible area blocked by a transparent door ( <ref type="figure">Fig. 1(b)</ref>) is detected by an assistive system as accessible, it could lead to a wrong interaction and cause harm to the user.</p><p>However, understanding transparent objects is a puzzle for most vision-based navigation assistance systems <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, as notoriously, satisfactory dense prediction of transparent objects is difficult to obtain. 3D vision-based methods hardly recover the depth information of texture-less areas or transparent surfaces <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, whereas traditional segmentation-based solutions do not cover the transparent object in 2D images <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Additionally, guide dogs often get confused leading people with blindness to full-pane windows. The appearance difference between glass doors and large glass windows is insignificant, thus it troubles people with residual sight <ref type="bibr" target="#b19">[20]</ref>. Moreover, a system to identify landmarks such as doors is particularly appreciated by people with visual impairments, since finding a door or a building entrance is difficult due to the inaccuracy of GPS <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>To tackle these challenges, we put forward a wearable system for walking scene perception, covering object-and walkable area segmentation, with the ultimate goal to enable visually impaired people navigate in real-world scenes safely and independently. We propose an efficient transformer-based semantic segmentation architecture dubbed Trans4Trans, precisely Transformer for Transparency, as shown in <ref type="figure">Fig. 1(c)</ref>. Since transparent objects are often texture-less or share identical content as their surroundings, it is essential to associate longrange visual cues to robustly infer transparent regions. For this reason, Trans4Trans is established with both transformer-based encoder and decoder to fully exploit the long-range context modeling capacity of self-attention layers in transformers <ref type="bibr" target="#b21">[22]</ref>. We design a Transformer Parsing Module (TPM) to fuse multi-scale feature maps generated from embeddings of dense partitions. The symmetric transformer-based decoder with a dual-head structure, thereby, is able to consistently parse feature maps from the transformer-based encoder (see <ref type="figure">Fig. 1</ref>(c)). Along with semantically predicting general thing-and stuff categories like walkable areas, the dual-head design allows to segment transparent objects completely and accurately, which unifies various detection tasks and enhances the perception reliability relevant for safety-critical navigation assistance.</p><p>Trans4Trans has been integrated in our wearable assistive system (see <ref type="figure">Fig. 1(a)</ref>) comprising a pair of smart vision glasses and a mobile GPU processor, which can deliver a holistic scene understanding swiftly and precisely, owing to the high efficiency of our model. Based on the entire semantic information, the user interface is designed with a customized set of acoustic feedback, including the sonification of the detected objects, the identified walkable directions, and warnings of close-range obstacles. The voice user interface yields intuitive navigation suggestions while requiring no prior knowledge.</p><p>A comprehensive variety of experiments has been conducted on multiple semantic segmentation datasets <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> to verify the effectiveness of the assistive system. Particularly, the proposed model achieves state-of-the-art accuracy on the test sets of Stanford2D3D <ref type="bibr" target="#b22">[23]</ref> and Trans10K-v2 <ref type="bibr" target="#b23">[24]</ref>. Considering the synergy towards traffic safety and the shared challenges between walking-and driving scene understanding, Trans4Trans is further verified on driving scene segmentation benchmarks including Cityscapes <ref type="bibr" target="#b24">[25]</ref>, ACDC <ref type="bibr" target="#b25">[26]</ref>, and DADA-seg <ref type="bibr" target="#b26">[27]</ref>, demonstrating its efficiency and robustness for various ITS application scenarios (see <ref type="figure">Fig. 1(d)</ref>). Finally, a user study with visually impaired people and a series of field tests validate the usability and reliability of our portable system for navigational perception and assistance in the real world. To the best of our knowledge, this is the first work to use vision transformers for assisting people with visual impairments.</p><p>This work is the extension of our conference paper <ref type="bibr" target="#b27">[28]</ref>, extended with a detailed description of the proposed Trans4Trans architecture and an extended set of experiments and analyses. In summary, we deliver the following contributions:</p><p>? We build a wearable assistive system with a pair of smart vision glasses and a portable GPU processor for aiding people with visual impairments during navigation. ? We put forward an efficient segmentation architecture with transformer-based encoder and decoder, namely Transformer for Transparency (Trans4Trans). Its dualhead design can unify general-and transparent object segmentation. We propose a Transformer Parsing Module (TPM) to harvest multi-scale feature representations generated from embeddings of dense partitions. ? Trans4Trans has surpassed state-of-the-art Convolutional Neural Networks (CNNs) and transformer-based methods on Stanford2D3D and Trans10K-v2 datasets, while maintaining high efficiency on the assistive system. ? Trans4Trans is verified on driving scene segmentation benchmarks including Cityscapes, ACDC, and DADAseg, demonstrating the robustness in different conditions and effectiveness towards real-world applications. ? We design an assistive algorithm based on multiple segmentation results and the depth information. We create a user interface via a customized set of acoustic feedback and conduct a user study and various field tests, evidencing the usability and reliability of the assistive system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation for Visual Assistance</head><p>Whereas traditional assistance systems rely on multiple monocular detectors and depth sensors <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref> for different perception tasks, semantic segmentation allows to solve many navigational detection problems in a unified way and has been employed in visual assistance. As pioneer approaches, semantic paintbrush <ref type="bibr" target="#b29">[30]</ref> was proposed as an augmented reality system for people with low vision, while semantic labeling was applied to the problem of navigation using prosthetic vision <ref type="bibr" target="#b30">[31]</ref>.</p><p>Yang et al. <ref type="bibr" target="#b4">[5]</ref> put forward to seize pixel-wise semantic segmentation to unify terrain detection tasks covering traversability perception, stairs navigation, and water-hazards negotiation. Mao et al. <ref type="bibr" target="#b31">[32]</ref> argued for efficient panoptic segmentation towards a comprehensive labeling and verified the performance in adverse conditions like snowy scenes. In <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, instance-specific segmentation models like Mask R-CNN <ref type="bibr" target="#b34">[35]</ref> were directly used for content-aware sensing. Liu et al. <ref type="bibr" target="#b17">[18]</ref> proposed HIDA to enable holistic indoor understanding for detection and avoidance of obstacles based on 3D point cloud semantic instance segmentation. Dense semantic segmentation has also been leveraged to address intersection perception like the detection of crosswalks, sidewalks, and blind roads <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Moreover, we observe the trend of using semantic segmentation in various navigation assistance platforms <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Yet, both of these systems cannot resolve the problems emerged with transparent objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transparent Object Sensing</head><p>Classical visual assistance systems <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> resorted to multi-sensor fusion to overcome the difficulties in handling transparent obstacles like glass objects, French windows and French doors by using ultrasonic sensors together with RGB-D cameras. Multimodal and multispectral information were also frequently used. Chen et al. <ref type="bibr" target="#b40">[41]</ref> enhanced the depth estimation via consumer RGB-D cameras by fusing infrared stereo vision and color stereo vision. Okazawa et al. <ref type="bibr" target="#b41">[42]</ref> performed simultaneous recognition of ordinary non-transparent objects and transparent objects by utilizing the difference in transmission characteristics under multispectral scenes. Moreover, polarization cues <ref type="bibr" target="#b42">[43]</ref> and reflection priors <ref type="bibr" target="#b43">[44]</ref> were often explored for transparency perception. For instance, Xiang et al. <ref type="bibr" target="#b44">[45]</ref> built a polarization-driven semantic segmentation architecture by bridging RGB and polarization dimensions dynamically using efficient attention connections, which considers the optical features of polarimetric information for robust representation of diverse materials and lifts the performance of classes with polarization properties like glass.</p><p>Recently, large-scale transparent object segmentation datasets emerge <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Mei et al. <ref type="bibr" target="#b13">[14]</ref> constructed the glass detection dataset in daily-life scenes. Xie et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref> built the Trans10K dataset and validated that while pure RGB-based transparent object segmentation is rather a largely unsolved task, it is potential for realworld usages with the increased data amount. This spurs the community to go beyond traditional perception paradigms relying on sensor fusion and develop novel methods addressing transparent object segmentation. AdaptiveASPP <ref type="bibr" target="#b46">[47]</ref> was designed as an enhanced version of ASPP <ref type="bibr" target="#b47">[48]</ref> to appropriately harvest rich features at multi-stage levels in joint segmentation and boundary predictions. EBLNet <ref type="bibr" target="#b48">[49]</ref> integrated a point-based graph convolution module to model global shape representations. Whereas these methods have reached high accuracy on glass-like object detection, we aim for a both efficient and robust semantic segmentation desirable for realworld navigation assistance. We establish a transformer-based system to assist transparency perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention-and Transformer-based Semantic Segmentation</head><p>Since Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b49">[50]</ref> achieved semantic segmentation end-to-end by viewing it as a densepixel classification task, modern methods develop upon this paradigm by augmenting FCNs with context aggregation modules. PPM <ref type="bibr" target="#b50">[51]</ref> uses multiple scales of pooling operators in a pyramid manner, while ASPP <ref type="bibr" target="#b47">[48]</ref> leverages atrous convolutions of different dilation rates to enlarge receptive fields. DANet <ref type="bibr" target="#b51">[52]</ref>, OCNet <ref type="bibr" target="#b52">[53]</ref>, and CCNet <ref type="bibr" target="#b53">[54]</ref> devise variants of non-local attention blocks to exploit long-range pixel relations. Disentangled and asymmetric versions <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> of nonlocal modules have also been designed to reduce computation complexity of dense pixel-pair associations.</p><p>Inspired by Vision Transformer <ref type="bibr" target="#b57">[58]</ref> that utilizes transformer layers to sequences of image patches for visual recognition, SETR <ref type="bibr" target="#b58">[59]</ref> and Segmenter <ref type="bibr" target="#b59">[60]</ref> directly append upsampling and segmentation heads atop ViT, encoding long-range context information from the very first layer. Leveraging the advance of DETR <ref type="bibr" target="#b60">[61]</ref>, MaX-DeepLab <ref type="bibr" target="#b61">[62]</ref> and MaskFormer <ref type="bibr" target="#b62">[63]</ref> look at image segmentation from the lens of mask prediction and classification. With these successes, various transformer architectures for dense image segmentation appear <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. PVT <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> and SegFormer <ref type="bibr" target="#b64">[65]</ref> propose pyramid structures of vision transformers for collecting hierarchical feature representations. ECANet <ref type="bibr" target="#b56">[57]</ref> and CSWin transformer <ref type="bibr" target="#b68">[69]</ref> advocate performing self-attention in horizontal or vertical stripes to achieve powerful modeling capacity while lowering computation overheads.</p><p>In this research, we propose an efficient Trans4Trans framework with focus set on assisting navigation of visually impaired people in the wild. Differing from existing works that either stack attention layers <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref> and encoder-decoder transformers on CNN backbones <ref type="bibr" target="#b23">[24]</ref>, or employing CNNbased decoders on top of transformer encoders <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b66">[67]</ref>, in Trans4Trans both encoder and decoder are based on transformer, together with a novel Transformer Parsing Module inserted in the dual-head decoder, which unifies transparent object and semantic scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Trans4Trans</head><p>Transformers adapted to vision tasks have achieved competitive performance compared to the CNN counterparts in recent years. Benefiting from the Multi-Head Self-Attention (MHSA) structure, ViT <ref type="bibr" target="#b57">[58]</ref> introduced the modeling ability in acquiring long-range dependencies, which is vital for dense prediction tasks, e.g. transparency-and semantic perception. SETR <ref type="bibr" target="#b58">[59]</ref> extended vision transformer to segmentation and validated the feasibility of performing MHSA on high-resolution inputs. In order to reduce the costly computation of full-resolution MHSA, Swin <ref type="bibr" target="#b63">[64]</ref> and CSWin <ref type="bibr" target="#b68">[69]</ref> limited the token region of the full attention by using shift operations or cross-shaped windows. However, the global context is slowly covered by stacking a large number of such regional self-attention blocks, and the receptive field is rather limited. In this work, we preserve the full-scale self-attention in the encoder for fasterenlarging receptive fields. To balance inference efficiency and segmentation accuracy in Trans4Trans, we consider: (1) to build upon a pyramidal transformer encoder with full-scale self-attention blocks; <ref type="bibr" target="#b1">(2)</ref> to lighten the decoder to enable deploying a dual-head model on portable GPUs; and (3) to maintain a symmetrical encoder-decoder structure for obtaining pyramidal feature representations.  Pyramidal Feature Representations. The whole architecture of the Trans4Trans model is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>  <ref type="bibr" target="#b70">[71]</ref> opt to modify the encoder while neglecting the decoder design. Yet, a robust decoder is critical for deploying a transformer model in real applications, especially for an assistive system. The whole decoder is composed of four transformer-based stages corresponding to the encoder, which is different to the single-scale decoder of ViT <ref type="bibr" target="#b57">[58]</ref> and CNN-headed decoder of Trans2Seg <ref type="bibr" target="#b23">[24]</ref>. To maintain the symmetrical pyramid features both in the encoder and the decoder, we propose the Transformer Parsing Module (TPM) to interpret the multi-scale features {F 1 , F 2 , F 3 , F 4 }, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>(a) and TPM is shown in detail in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>. A linear projection operation is used to translate the feature from the encoder into a preset embedding with a dimension C, which maintains the features channel-wise identical throughout the decoder. An MHSA module similar to the one in the encoder block is leveraged to obtain symmetrical features. An upsampling (resize) operation is used to preserve the same resolution H 4 ? W 4 ? C of different features between adjacent stages. As a result, the lower-resolution features with long-range contextual information are aggregated with the higher-resolution features with fine and local information in the lightweight hierarchical decoder. Multiple Decoders. Based on TPM, a full-attended and efficient decoder demands little computing resources and eases the integration with arbitrary encoder backbones, thus a segmentation transformer model is more flexible to be deployed on a portable hardware system. However, training a robust transformer model requires a large-scale dataset <ref type="bibr" target="#b57">[58]</ref>. To deal with the data-hunger problem and robustify segmentation in the wild, we design a double-head model to perform joint training on multiple datasets, e.g. general and transparent object segmentation datasets for our assistive system, or normal and adverse driving scene segmentation datasets for automated vehicles. The feature maps representation in two decoder heads are illustrated as <ref type="figure" target="#fig_0">Fig. 2(a)</ref>. Benefiting from our proposed TPM, the amount of GFLOPs and parameters of this dual-head structure is largely reduced compared to deploying two separate models. Also importantly, diverse features can be learned from various datasets. For real-world semantic segmentation, mounting multiple decoder heads robustifies the feature learned via the shared encoder and prevents overfitting. Meanwhile, the entire model is computationally efficient.</p><formula xml:id="formula_0">{F 1 1 , F 1 2 , F 1 3 , F 1 4 } and {F 2 1 , F 2 2 , F 2 3 , F 2 4 } in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Portable Assistive System</head><p>The entire wearable navigation assistance system is composed of a pair of smart vision glasses and a mobile GPU processor, e.g., a lightweight laptop or an NVIDIA AGX Xavier ( <ref type="figure">Fig. 1</ref>).</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the smart vision glasses have been integrated with a RealSense R200 <ref type="bibr" target="#b71">[72]</ref> RGB-D sensor to enable real-time acquisition of RGB and depth images at the resolution of 640?480, and a pair of bone-conduction earphones for delivering acoustic feedback to people with visual impairments. This is crucial as visually impaired people often rely on the sounds from the surroundings for determining the orientation and bone-conduction headphones will not block their ears when using the assistive system. The equipped RealSense R200 sensor uses a combination of active speckle projecting and passive stereo matching. With this design, it can work well both indoors and outdoors. In texture-less indoor scenes, the projected infrared speckles will augment the environments, which are beneficial for stereo matching algorithms (e.g., R200 leverages a straightforward correlation engine <ref type="bibr" target="#b71">[72]</ref>) to yield dense depth estimation. In sunny outdoor scenes, although many projected patterns would be overwhelmed by sunlight, the infrared components of natural light shine on the scene to form well-textured infrared image pairs, thus enabling robust depth sensing. In our assistive system, depth information is mainly used to assist the obstacle avoidance function, e.g., to prioritize near-range objects over mid-and long-range objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Algorithm</head><p>The algorithm and user interface of our assistive system with the dual-head Trans4Trans are described in Algorithm 1. System setting. According to the properties of the R200 camera sensor <ref type="bibr" target="#b71">[72]</ref>, the frame rate of RGB-D stream can be set up to 60. When the camera frame rate is set to 30 and the resolution is set to 640?480, the minimum effective distance of depth information is around 1.5m. However, it cannot cover the perception of objects in close range (&lt; 1.5m). To guarantee a timely data acquisition and widen the minimum effective range of depth information to 0.5m, we preset the frame rate of R200 to 60 and the resolution to 320?240. Based on this setting, the camera can obtain sufficiently-accurate depth information and cover near-range objects, which is critical for the assistive system. Once the system starts, it repeats the whole algorithm every n seconds. According to our experiments, the time interval setting as 1-2 seconds effectively prevents cognitive overload, especially in cases of complex scenes containing a large number of objects. Still, it is adjustable depending on the demands of users, e.g., a short interval for more feedback to explore unknown space. Within 2 seconds, our efficient Trans4Trans model can perform segmentation of approximately 20 frames. After aggregating multiple segmentation results, the final feedback of the system is more reliable against noises and perturbations. Obstacle avoidance. When moving indoors with limited space, the building materials and densely arranged objects will seriously hinder the flexibility of using the common white cane as an obstacle avoidance aid. In order to tackle the collision issue and balance indoor and outdoor scenarios, our Algorithm 1: Assistive system Data: RGB-D as X ? R H?W ?3 and D ? R H?W . Result: General segmentation G ? R H?W ?13 ;</p><p>Transparency T ? R H?W ?11 ; 1 initialize walkable rate: R l , R f , R r , parameters: ? obstacle , ? trans , ? walkable ; 2 while system start and each n seconds do <ref type="bibr" target="#b2">3</ref> RGB-D update and Trans4Trans segmentation: system presets the highest priority for obstacle avoidance. In contrast to the fixed and limited categories defined in an obstacle avoidance engine <ref type="bibr" target="#b18">[19]</ref>, we leave the obstacles as open-set and detect them based on the depth information D ? R H?W . In other words, if the average value of the depth information D is smaller than the preset distance threshold ? obstacle , the user will be immediately notified in the form of vibration. To minimize the uncertainty of vibrations and the cognitive load, only one single default threshold is set, instead of setting various vibration frequencies for different distances. Another purpose is to preclude the chaotic and lowconfidence segmentation from the less-textured images when users walk too close and face to the object surface, such as images of white wall or doors. According to our pre-tests and the minimum effective range (0.5m) of the R200 camera, we set the distance threshold as ? obstacle = 1m, and thereby the system can effectively detect open-set and near-range obstacles and output vibration as warnings.</p><formula xml:id="formula_1">4 G path ? R H?W , G object ? R H?W ?12 ; 5 T stuf f ? R H?W ?3 , T thing ? R H?W ?8 ; 6 partition {R l , R f , R r } ? G path ; 7 if D &lt; ? obstacle then 8 vibration as obstacle warning; 9 else if max{T i } ? T stuf f &gt; ? trans then 10 speech ? argmax{T i } ? T stuf f ; 11 else if max{R l , R f , R r } &gt; ? walkable then 12 speech ? argmax{R l , R f , R r } ? {lef t,</formula><p>(Transparent) object segmentation. After receiving the RGB image X ? R H?W ?3 , our efficient dual-head Trans4Trans model outputs two segmentation predictions, which are general object segmentation G ? R H?W ?13 and transparent object segmentation T ? R H?W ?11 , respectively. The specific object categories of each segmentation result will be introduced later in the dataset description. The general object segmentation is divided into G path of walkable path, i.e., floor class, and G object of other objects <ref type="bibr" target="#b22">[23]</ref>. Besides, the transparent object segmentation is divided into two disjoint sets as: T stuf f ? R H?W ?3 with {window, glass door, glass wall}, and T things ? R H?W ?8 with {shelf, jar/tank, freezer, eyeglass, cup, bowl, bottle, box} <ref type="bibr" target="#b23">[24]</ref>. To combine the segmentation results G path and T stuf f , we tend to preset a higher priority of prompts for the transparent objects. Specifi-cally, when the maximum segmented area of transparent stuff is greater than a preset threshold ? trans , its corresponding category is fed back in a speech form, before the one of other general objects. Based on our experiments, when the whole image area is 1.0, we set the ? trans = 0.5 to eliminate the effects of jitter-errors in segmentation. Walkable path detection. After achieving general object segmentation, the walkable mask G path is further partitioned into three regions as {lef t, f orward, right} directions for orientation assistance. The local ratio R is calculated by</p><formula xml:id="formula_2">G i path /A i image ,</formula><p>where i represents one of the three directions and A denotes the image area. As a result, the horizontally divided ratios are denoted as {R l , R f , R r } ? G path . Then, an intuitive and effective strategy is to prompt the direction that has the largest walkable area, only when the largest local ratio R is greater than the preset threshold ? walkable . In this case, we set a strict threshold ? walkable = 0.4 to ensure that the maximum area is safe enough and walkable for the user. According to our test, this orientation approach guarantees anti-veering in a straight path outdoors and indoors. Furthermore, it can accurately predict the best instantaneous turning direction during walking at an intersection, so as to constantly yield a safe direction suggestion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, with extensive experiments on multiple datasets, we verify the efficiency and robustness of the proposed Trans4Trans architecture for general-and transparent object segmentation, as well as driving scene segmentation. We first describe the datasets and settings in Sec. IV-A. Then, we quantitatively measure the accuracy of generaland transparent object segmentation desired for real-world navigation assistance (Sec. IV-B) as well as the robustness of driving scene segmentation in diverse conditions (Sec. IV-C). In Sec. IV-D, we assess the real-time performance and in Sec. IV-E, we investigate the importance of context priors for transparency perception with feature visualizations. Finally, in Sec. IV-F, we analyze qualitative segmentation results.   <ref type="bibr" target="#b72">[73]</ref> with power 0.9 in 100 epochs. AdamW <ref type="bibr" target="#b73">[74]</ref> is chosen as the optimizer with epsilon 1e?8 and weight decay 1e?4 and batch size is set as 4 on each of four 1080Ti GPUs. The experiments for ablating the effect of embedding channels are conducted on a single GPU. The images in the training and testing stages are resized in the resolution of 512?512 or 768?768 (will be specified) for the experiments, to maintain the shape of position embedding. For a fair comparison with the previous state-of-the-art Trans2Seg <ref type="bibr" target="#b23">[24]</ref>, some tricks such as OHEM, auxiliary and class-weighted losses are not applied in the experiments. We use mean Intersection over Union (mIoU) as the main evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy of General and Transparent Object Segmentation</head><p>In this subsection, we conduct analysis of the performance of Trans4Trans on different datasets and the performance of varied encoder-decoder structures realized by different combinations of CNN/transformer in detail to verify the efficacy of our approach. Then, computation complexity in GFLOPs of different approaches and segmentation accuracy are presented and compared with state-of-the-art methods. Results. The performances of different encoder-decoder model architectures are shown in <ref type="table" target="#tab_4">Table I</ref>. Compared with Trans2Seg <ref type="bibr" target="#b23">[24]</ref>, the encoder and decoder in our approach are both transformer-based. The single-and dual-head decoders are constructed with the TPM design. As shown in <ref type="table" target="#tab_4">Table I</ref>  When incorporating more general knowledge by learning jointly with supervision from Stanford2D3D dataset, dualhead Trans4Trans consistently improves the performance on Trans10K-v2 compared with Trans2Seg. More importantly, dual-head Trans4Trans highly alleviates the problem of overfitting and reduces false positives of transparent obstacle warning observed in our field tests, and thereby it is more suitable for real-world navigation perception. Overall, the superiority and efficiency of Trans4Trans are verified through the experiments on transparent-and general object segmentation. Combination of CNN / Transformer. As shown in <ref type="table" target="#tab_4">Table II</ref>, varied combinations of CNN-/Transformer-based encoder and decoder are compared, where FCN <ref type="bibr" target="#b49">[50]</ref> and OCNet <ref type="bibr" target="#b52">[53]</ref> are composed of only CNNs, whereas Trans2Seg is composed of CNN-based encoder and transformer-based decoder. In contrast, the architecture in the proposed Trans4Trans approach is a fully transformer-based encoder-decoder model. Our proposed approach outperforms both these aforementioned competitive networks such as OCNet and transformer-based encoder-decoder architectures such as PVT. In addition, our Trans4Trans keeps clearly smaller GFLOPs while being more accurate, demonstrating its suitability for efficient transparent object segmentation. Ablation of TPM channel. As TPM is one of our critical designs, the ablation study of different numbers of embedding channels applied in the Trans4Trans decoder is conducted, where the effects are illustrated in <ref type="table" target="#tab_4">Table III</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref>. It reveals that the larger channel number, the better performance, until 256. The drop at 512 indicates that the decoder overfits the encoded feature as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> and the computation complexity becomes exceedingly large with the increase of channel number for TPM. For the response time-critical wearable system, we adopt the smallest 64 channels when deploying dual-head Trans4Trans due to its highest efficiency and good segmentation performance. Yet, to pursue high accuracy on driving-scene datasets, we set TPM channel of Trans4Trans-T/-S/-M as {64, 128, 256}, respectively. Comparison to state-of-the-art models. As shown in Table IV, the performance of both accuracy-and efficiency-   oriented semantic segmentation approaches are compared according to <ref type="bibr" target="#b45">[46]</ref>. The superiority of Trans4Trans is further confirmed through the listed experimental results in <ref type="table" target="#tab_4">Table IV</ref>, compared with both CNNs and transformer-based approaches like Trans2Seg <ref type="bibr" target="#b23">[24]</ref>. Our medium Trans4Trans model outperforms the state-of-the-art method Trans2Seg by 2.99% in mIoU and 0.87% in Acc, while requiring much less GFLOPs. For category-wise accuracy, our Trans4Trans model achieves state-of-the-art performances in IoU on the classes background, jar or tank, window, door, cup, wall, bottle, and box, indicating the efficacy of transparent object segmentation of the proposed Trans4Trans architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Segmentation Robustness in Driving Scenes</head><p>Apart from segmenting general and transparent objects, we further verify our Trans4Trans model on driving scene datasets to show its potential for various ITS applications. Ablation of dual-head Trans4Trans. Five groups of results are shown in <ref type="table" target="#tab_11">Table V</ref>. Our TPM-based Trans4Trans trained at 512?512 resolution, illustrates better performance compared with PVT on Cityscapes and ACDC datasets focusing on driving scenes. On Cityscapes, Trans4Trans-M leveraging PVT encoder outperforms PVT-M by 5.25% and Trans4Trans-M leveraging PVTv2 as the encoder surpasses by 8.96% (with an additional 3.71% gain). They both utilize TPM/Single-head as the decoder in Trans4Trans, indicating the learning capacity of our proposed approach for driving scene understanding. On ACDC, our Trans4Trans-M leveraging PVT encoder outperforms PVT-M by 5.28% and the one leveraging PVTv2-M as the encoder exceeds by 8.05% (with an additional 2.77% boost) while utilizing TPM/Dual-head in the decoder architecture. Since ACDC is a dataset containing different adverse conditions, these results evidence that TPM/Dual-head has the better robustness under environment changes in driving scene segmentation, as it incorporates more generalized knowledge learned from diverse images in both datasets.        <ref type="bibr" target="#b25">[26]</ref>) and extreme accident (All-DADA <ref type="bibr" target="#b26">[27]</ref>) conditions. CS: Cityscapes <ref type="bibr" target="#b24">[25]</ref>. #GFLOPs are calculated with the input size of 768?768.</p><p>margins, and it achieves a similar score as SegFormer <ref type="bibr" target="#b64">[65]</ref> while being significantly more efficient. Segmentation in adverse conditions. As shown in <ref type="table" target="#tab_4">Table VII</ref>, we further test the performance our proposed Trans4Trans-M approach on ACDC <ref type="bibr" target="#b25">[26]</ref> and DADA-seg <ref type="bibr" target="#b26">[27]</ref> in adverse conditions and extreme accident scenes individually. The results of Trans4Trans are obtained via MMSegmentation with a resolution of 768 ? 768. The first three rows of models are trained on Cityscapes in normal and favorable conditions and tested on ACDC and DADA-seg datasets. Our Trans4Trans-M indicates higher performances of 55.7% and 27.7% in mIoU when compared with HRNet <ref type="bibr" target="#b81">[82]</ref> and DeepLabV3+ <ref type="bibr" target="#b47">[48]</ref>. Trans4Trans performs clearly better than them in various adverse conditions including foggy, nighttime, rainy, snowy, and extreme accident scenes, which demonstrates its high generalization capacity to these unseen domains. This is because with both transformer-based encoder and decoder, Trans4Trans can associate long-range visual concepts for robustly inferring semantics, despite local texture-and illumination changes in different scenarios like nighttime and accident scenes. Then, the following three rows of models are trained on ACDC and tested on ACDC and DADA-seg. Our Trans4Trans again indicates better overall performances with 75.2% and 32.4% on All-ACDC and -DADA. Finally, we train Trans4Trans by merging ACDC and Cityscapes which have the same 19 classes, and the learned model shows the best overall scores on All-ACDC and All-DADA with 76.3% and 39.2% in mIoU, respectively, illustrating that co-training on normal and adverse data can improve the performance of the model under both adverse and extreme accident conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Real-time Performance</head><p>To measure the inference speed of the different versions of our dual-head Trans4Trans model, 300 samples from the Trans10K-v2 test set with a batch size of 1 and a resolution of 512?512 are tested on three different GPUs, i.e., a mobile NVIDIA AGX Xavier in the MAXN mode, an NVIDIA GeForce MX350 on a lightweight laptop, and an RTX 2070 on a workstation. As shown in <ref type="table" target="#tab_4">Table VIII</ref>, the running time (latency) of our tiny Trans4Trans model on three GPUs are considerably lower than the other two versions, meanwhile the performances of the three models on both datasets are suitable for our system. In real applications, the more timely response of the navigation system is beneficial for assisting users with a similar prediction accuracy on each frame. Hence, the tiny version model is selected. The model is deployed on the lightweight laptop (with an MX350 GPU) to conduct the user study. Regarding the entire system, it takes 0.14ms (?0.11) for the image acquisition step, 101.5ms (?0.3) for semantic segmentation, and 74.47ms (?2.41) for the assistive algorithm. Thus, the entire system requires a total of 176ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Transparent Feature</head><p>Analysis of contexts and reflections. In order to ablate the impact of context information (such as door frames and walls) or reflection cues on the segmentation of transparent objects, we visualize the real-world segmentation results from different scales (six scales from 100% to 20%) of the original images in <ref type="figure" target="#fig_3">Fig. 5</ref>. The segmentation results are all generated by Trans4Trans. In <ref type="figure" target="#fig_3">Fig. 5(a)</ref>, among all six scales, even in the 20% case with less context, the segmentation of two overlapping doors are accurately obtained. In <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, the challenging sneeze guards with less frame cues are accurately segmented, since this sneeze guard has reflection characteristics as well. In the case of 40% ratio in <ref type="figure" target="#fig_3">Fig. 5(c</ref>  only a one-side outer frame and part of the reflection, it can still segment the area of the glass wall. However, in the 20% ratio, it is confused due to the tiny frame and absence of any reflections. The incomplete segmentation and errors in the latter two ratios of <ref type="figure" target="#fig_3">Fig. 5(d)</ref> are caused by the lack of texture information and reflections. Based on the analysis of these visualization results, two insights are provided: (1) The contextual information, e.g. the outer frame, is a vital factor for the segmentation of transparent objects; (2) The reflection characteristic of glass or transparent objects is also crucial. Benefiting from the symmetrical encoder-decoder architecture, our Trans4Trans model can robustly segment transparent objects even with diminishing context cues in most of the complex real-life scenes.</p><p>Features parsing. For semantic segmentation, the local feature from the low-level layer is often preserved for the fine prediction such as an exact boundary of the detected object, while the contextual feature from the high-level layer is for distinguishing the object category. To investigate the feature parsing capability of the TPM decoder, we visualize the interpreted feature maps from four stages of the decoder, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref> with real-world scenes. The results are generated by the single-head Trans4Trans trained on Trans10K-v2. We get two observations: <ref type="bibr" target="#b0">(1)</ref> In various low-and high-level stages, the parsed features contain both fine-grained and contextual information, thanks to our Trans4Trans architectural design which enables to capture long-range context priors from very first layers. For example, the fine feature in the stage 1_2 of the first row contains the instance-level information such as glass door. The contextual feature in stage 4 contains the precise boundary of the glass door as well. Both types of features are critical for the segmentation task; (2) In the same stage, the fine-grained feature is simultaneously reflected in the object and its surrounding. For example, the feature in stage 1_1 of the first row is more inclined to the surrounding, and the one in stage 1_2 is to the area of the glass door, and vice versa in the second row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Segmentation Analysis</head><p>Visualization of transparency segmentation. <ref type="figure">Fig. 7</ref> shows qualitative comparisons between our Trans4Trans-Tiny and the previous state-of-the-art approach Trans2Seg <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure">Fig. 7(a)</ref> illustrates some failed recognition cases of both models, but our model can yield a clearly better boundary distinction. <ref type="figure">Fig. 7(b)</ref> shows examples where our model predicts the correct label, but Trans2Seg is confused, indicating the reliable performance of our proposed approach. In <ref type="figure">Fig. 7</ref>(c)(d), it can be seen that our model is not only effective in detecting navigation-critical glass door and glass window, but can also predict more refined segmentation of small objects like jar/tank and glass cup.</p><p>We further perform field tests by navigating around the university campus and capturing real-world scenes with our smart vision glasses. The collected RGB-D images and corresponding predictions are shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. Meanwhile, the semantic segmentation runs on the mobile GPU processor ported with our efficient vision transformer model. We visualize multiple sets of predictions as shown in <ref type="figure" target="#fig_6">Fig. 8(c)(d)</ref>. The glass door in the first row captured at a moderate distance can be correctly distinguished from navigable areas, whereas in the other rows they are mis-classified as walkable paths by the general object segmentation model. As it can be seen, transparent surfaces are often texture-less and the infrared patterns projected by the glasses will transmit the glass regions, and thereby the depth information are often sparse, noisy, or even lost, making transparent objects a constant threat for 3D vision-based systems <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> designed for helping avoid obstacles. Only learning from a common segmentation dataset can help identify the walkable areas with predicted traversable classes, but this will miss the detection of transparent obstacles, and thereby renders conventional image segmentation-based systems <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref> less effective in these scenarios, which are omnipresent in real-world navigation. In contrast, our Trans4Trans accurately and completely segments those transparent objects, meanwhile covers general objects and walkable paths, and thereby it is ideally suitable for assistance systems.</p><p>Visualization of driving scene segmentation. As shown in <ref type="figure" target="#fig_7">Fig. 9</ref>, we visualize the predictions of Trans4Trans* trained on Cityscapes+ACDC, in comparison to DeepLabv3+ <ref type="bibr" target="#b47">[48]</ref>, HRNet <ref type="bibr" target="#b81">[82]</ref>, and our Trans4Trans models only trained on ACDC. It can be seen that DeepLabv3+ and HRNet often produce noisy segmentation results in complex real-world conditions, like the cars in challenging shadow and illumination situations (the first row). In adverse weather and illumination conditions, the previous methods also yield less precise and even fragmented semantics, like the trucks in foggy and rainy  scenes (the second and fourth rows) and the sidewalks in night and snowy scenes (the third and fifth rows). In extreme accident scenes, which are safety-critical for automated vehicles, existing state-of-the-art models cannot generate reliable predictions to be propagated to upper-level applications, as the close pedestrian is even completely recognized as road.</p><p>In contrast, Trans4Trans, which learns to gather long-range dependency from the very first layers, delivers more robust segmentation in various scenes, as it is less affected by local texture and illumination changes. Trans4Trans trained on both adverse and normal datasets further improves the performance, resulting sharp and fine-grained semantic segmentation, which is suitable for self-driving applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. USER STUDY</head><p>We conducted a qualitative study in order to "understand people's needs and the context within which [our] future technology might be used" <ref type="bibr" target="#b92">[93]</ref>. Another goal was to draw design conclusions for future works. Because system-level performance and efficiency have been evaluated in Sec. IV-D, we did not evaluate any additional objective metrics, but focus on user comments and suggestions on the system. Since the target user group is a very heterogeneous one, and participants with visual impairments are difficult to recruit, we decided to evaluate with experts in accessibility and assistive systems with focus on visual impairment. Methodology. The hardware consisted of the smart vision glasses and a backpack with a lightweight laptop and a battery pack inside. The system's battery life under these conditions was approximately 4 hours.</p><p>As mentioned in Algorithm 1 and Sec. III-C, three main functions are partitioned into four steps as mutually exclusive: (1) obstacle avoidance based on depth information (i.e. &lt; 1.0m) has the highest priority. (2) Three different types of transparent stuff (wall, door, and window) will be alerted via speeches. (3) Walkable path will be indicated in three different directions (left, forward, and right). (4) Other general objects and transparent things, listed in Sec. IV-A, will be fed back. Participants tried the system inside 2 buildings (one building is mostly glazed), and the blind expert (E1B) also walked with the system on a 700m route outdoors -see <ref type="figure" target="#fig_9">Fig. 10</ref>. The study lasted between 30-45 minutes (E5-E8) and 90 minutes (E1B). As Corona-protective measures, everyone wore FFP2 or surgery masks throughout the entire study and the prototype was disinfected several times.</p><p>After a short introduction, the participants put on the system and walked around the rooms, thinking out loud <ref type="bibr" target="#b93">[94]</ref>. At the end, demographics and NASA Raw Task Load Index (RTLX) <ref type="bibr" target="#b94">[95]</ref> questionnaires were filled in. Participants. In a first step, we evaluated with 5 participants <ref type="bibr" target="#b27">[28]</ref>, one of whom was an expert, and another one was expert and blind user at the same time. We subsequently repeated the experiment with 3 further sighted experts, and we only report here the aggregated results from the 5 experts: E1B (early blind expert), E5-E8 (sighted experts). When asked if they can see glass objects, E1B said he can sometimes see some light-dark contrasts, which allows him to perceive closed windows. Windows that open inside the room, however, are very dangerous, according to E1B, as one can get serious head injuries. All sighted participants said they can see glass objects, but some of them, like glass doors, glass walls or windows, can be challenging under particular conditions (E5). Cognitive load. The RTLX, averaged over the five expert participants, was 16.3 with a standard deviation of 8.1. The range is from 0 to 100, the lower the better. This score is enough to keep the user motivated, while not burdening too much <ref type="bibr" target="#b95">[96]</ref>. This score, however, must be critically interpreted, since it might not be representative for the users wearing the system in their daily activities. Instead, this score might reflect the cognitive load of the experts assessing the system, since this was their task, and not simulating user behavior.</p><p>Only the score of the blind participant is highly relevant for the cognitive load of users wearing the system. This score is 13.3, thus very close to the average, but being alone, it has hardly any statistical relevance. More studies will have to be performed in the future to assess the cognitive load of the users wearing the system. User comments. A thematic analysis <ref type="bibr" target="#b96">[97]</ref> performed on the comments made by the experts (both recorded and from the questionnaires) yielded the following insights: Functionality. All experts found the system useful and were impressed by its functionality, for instance, "for the first time, I had the feeling that artificial intelligence can be useful [.  The experts gave some important suggestions on subsequent system development, such as identifying more objects (E1B, E5), mounting a second camera to detect low-lying obstacles (E6), and hinting the directions of detected objects (E1B, E5-E7). Two experts (E5, E7) commented positively upon the free path detection, and mentioned that the obstacle detection should be improved. Most issues with the obstacle detection came from the 2seconds cycles (frame aggregation), which often caused a delay and delay inconsistencies (E1B, E5, E6, E7). To tackle this problem, it is desirable to further decrease the system response time. Regarding the suggestions from E5 and E8, adaptive feedback cycles for different functions can be implemented. For example, the feedback of obstacle detection should be given generally faster than for the other two functions. The default in this case could be for instance 1second instead of 2. Besides, the distances of detected objects are helpful for keeping social distances in COVID-19 pandemic times (E6, E8). Three experts (E5-E7) considered that this system is a nice complement to the white cane, but should not be used as alternative.</p><p>Hardware. The hardware was perceived as quite light weight (E5, E6), and in any case much better than previous prototypes (E1B) tried out by the experts in the past (at least three out of five had tried similar prototypes in the past). However, two experts (E1B, E5) considered the hardware still too big for a real-world deployment: "[use] a belt instead of a backpack [and] Bluetooth instead of cables for the glasses" (E1B), "ideally, it should run on a phone" (E5). The camera was perceived by E1B as very comfortable to wear, "although it is thick". Experts E5 and E6 commented positively on the system's battery life.</p><p>Interface. Four out of five experts thought the interface was very intuitive. Only E8 was neutral with respect to this. E6 liked "the object announcement. The acoustic signal is easy to follow + easy to understand". E1B thought that "the synthetic voice was very helpful, because it differentiates well from background noise". E8 thought that the speech output was appropriate for objects recognition, but for the other two functions, some alternatives would be better, in order to diminish the user's cognitive load. He suggested using vibrotactile feedback for obstacle avoidance and sonification for walkable path detection. One could modulate volume or intensity and use different patterns, while not changing the frequencies -for both vibration and sonification.</p><p>Context of use. Expert E7 thought the system is good for getting an overview of a new room, but not so good for known rooms. He also suggested implementing a couple of new functions, one to search for objects, and one for counting objects. Both E5 and E7 thought the system can be used for social distancing, but referred to two different functions of the system, namely obstacle detection and free path recognition. E5 suggested to use the system also for sighted people for warning when walking while looking at the phone.</p><p>Control. E7 thinks the user should be in full control of the system, like it is the case with the white cane: "I can't interact with the system (mute) -the white cane does what I want". Both E5 and E8 thought it is important to have the option to turn functions on and off, or switch to different modes (E8).</p><p>Conclusion. The functionality offered by the system so far can be of great use to people with visual impairments. The experts were positive about the system. Especially the object recognition was appreciated. Some improvements (such as conveying to the user the distance and direction of objects, reducing the delay for obstacle avoidance) were suggested. Also important, the users should be able to configure the system as much as possible and turn functions on and off as they need, or change the way things are conveyed (speech, sonification, vibration). Due to the heterogeneity of the user group, the configurability is a very important aspect. Augmented reality for partially sighted people. As transparent obstacles are usually a threat for people with low vision and even hardly distinguishable for sighted people in some confusing situations, our Trans4Trans model are further tested on a HoloLens 2 device by capturing real-world images around our computer vision laboratory. As displayed in <ref type="figure">Fig. 11</ref>, the challenging and omnipresent transparent objects like glass door, transparent wall, and glass window can be reliably and completely segmented, and the colored segmentation masks can be easily overlaid and naturally projected onto the original RGB images shot by the glasses for rendering augmented-or mixed reality. This field test on another glasses device reveals that our segmentation model is robust across cameras and the proposed Trasn4Trans framework can not only assist blind people, but can potentially help partially sighted people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we tackle the challenges of transparent object and semantic scene segmentation via Trans4Trans, an Input Segmentation Augmentation <ref type="figure">Fig. 11</ref>: Augmented results with a HoloLens 2 device. efficient transformer architecture with both transformer-based encoder and decoder. At the heart of our assistive system is Trans4Trans, which precisely segments general-and transparent objects with a Transformer Parsing Module (TPM) integrated in the dual-head structure. It achieves state-of-theart performances on Trans10K-v2 and Stanford2D3D datasets, while being swift and robust to support safety-critical navigation assistance. Considering the synergy between walkingand driving scene perception for improving traffic safety, Trans4Trans is further verified on driving scene segmentation benchmarks including Cityscapes (favorable conditions), ACDC (adverse conditions), and DADA-seg (extreme accident conditions), demonstrating its efficiency and robustness for real-world transportation applications.</p><p>The efficient vision transformer is ported in our wearable system with a pair of smart vision glasses designed to help visually impaired people travel and explore surrounding scenes, where transparent objects widely exist in the real life and impede their mobility. Despite a limited number of participants, an extensive set of analyses from a user study and various field tests evidences that the proposed assistive system is reliable and cognitive-load friendly.</p><p>In the future, we aim to address the issues and suggestions mentioned in the user study and improve the system continuously. We also intend to address indoor localization for assisting visually impaired people in their navigation tasks. We plan to further look into the challenging accident scene segmentation task via the lenses of curriculum-based and open compound domain adaptation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of (a) Trans4Trans model consists of shared encoder and dual decoders, while (b) and (c) are the general transformerbased encoder block and our proposed Transformer Parsing Module (TPM) for decoder, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Detailed illustration of components in the smart vision glasses designed for assisting visually impaired people. The main components: RealSense R200 camera and bone-conduction headphones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>mIoU and pixel accuracy curves under different numbers of embedding channels in TPM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of segmentation results from different cropped regions based on image center. Images of six scales from 100% to 20% are cropped from its original images and are separately segmented, in order to ablate the effect of image context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Network NVIDIA Xavier (ms) ? MX350 (ms) ? RTX 2070 (ms) ? Trans4Trans-M 115.9(?1.1) / 202.8(?1.1) 186.1(?0.3) / 243.2(?0.3) 22.9(?0.3) / 36.6(?0.8) Trans4Trans-S 95.3(?0.6) / 158.6(?1.8) 140.6(?0.3) / 188.4(?0.4) 17.1(?0.3) / 27.7(?0.5) Trans4Trans-T 75.8(?0.7) / 122.7(?0.7) 101.5(?0.3) / 141.7(?1.6) 12.8(?0.5) / 20.3(?0.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of feature maps, corresponding to four different stages in the TPM decoder. The two feature maps from the same stage (stage 1_1 and stage 1_2) indicate the global and local activated features of transparent objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Visualization of real-world scenes. From left to right are RGB and depth image, segmentation as walkable path by single-head model trained on Stanford2D3D, and as transparent objects (glass door or glass wall) corrected by our dual-head Trans4Trans model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative analysis on Cityscapes<ref type="bibr" target="#b24">[25]</ref> (Normal), ACDC<ref type="bibr" target="#b25">[26]</ref> (Fog, Night, Rain, and Snow), and DADA-seg<ref type="bibr" target="#b26">[27]</ref> (Accidental). The Trans4Trans* is trained on ACDC+Cityscapes, whereas other models are trained on ACDC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>..]. [I liked] how much it recognized correctly. [...] Systems react much better [than 10 years ago...]. I think it's just cool!" (E1B); Most positive comments are on the amount and type of objects recognized (E1B, E5, E7, E8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Incidences of participants using the system for navigation outdoors and indoors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Trans10K-v2 [24] has 10, 428 images, which are divided into 5, 000, 1, 000, and 4, 428 for training, validation, and testing, respectively. The H?W resolution of the images is 835?1, 113. There are 11 object categories marked as shelf, jar or tank, freezer, window, glass door, eyeglass, cup, wall, glass bow, water bottle, and storage box. Stanford2D3D [23] has 70, 496 images. The resolution of the images is 1, 080?1, 080. The 13 object categories are annotated as beam, board, bookcase, ceiling, chair, clutter, column, door, floor, sofa, table, wall, and window. Based on the fold-1 data splitting [23], the dataset is divided into the training set with Area 1-4 and Area 6, the validation set with Area 5a, and the test set with Area 5b. There are 52, 905, 6, 261, and 11, 332 images in the training-, validation-, and test set, respectively.</figDesc><table><row><cell>Network</cell><cell>Encoder</cell><cell>Decoder</cell><cell cols="2">GFLOPs MParams</cell><cell cols="2">Stanford2D3D Trans10K-v2</cell></row><row><cell>Trans2Seg-T [24]</cell><cell>PVT-T [67]</cell><cell></cell><cell>10.16</cell><cell>13.11</cell><cell>41.00</cell><cell>64.60</cell></row><row><cell>Trans2Seg-S [24]</cell><cell>PVT-S [67]</cell><cell>Transformer [24]</cell><cell>19.58</cell><cell>24.36</cell><cell>41.89</cell><cell>68.47</cell></row><row><cell>Trans2Seg-M [24]</cell><cell>PVT-M [67]</cell><cell></cell><cell>49.00</cell><cell>56.20</cell><cell>42.49</cell><cell>72.10</cell></row><row><cell>Trans4Trans-T</cell><cell>PVT-T [67]</cell><cell></cell><cell>10.45</cell><cell>12.71</cell><cell>41.28(+0.28)</cell><cell>68.63(+4.03)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVT-S [67]</cell><cell>TPM / Single-head</cell><cell>19.92</cell><cell>23.95</cell><cell>44.47(+3.04)</cell><cell>74.15(+5.68)</cell></row><row><cell>Trans4Trans-M</cell><cell>PVT-M [67]</cell><cell></cell><cell>34.38</cell><cell>43.65</cell><cell>45.73(+3.24)</cell><cell>75.14(+3.04)</cell></row><row><cell>Trans4Trans-T</cell><cell>PVT-T [67]</cell><cell></cell><cell>11.22</cell><cell>13.10</cell><cell>40.44(-0.56)</cell><cell>69.84(+5.24)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVT-S [67]</cell><cell>TPM / Dual-head</cell><cell>20.69</cell><cell>24.34</cell><cell>43.45(+1.56)</cell><cell>74.57(+6.10)</cell></row><row><cell>Trans4Trans-M</cell><cell>PVT-M [67]</cell><cell></cell><cell>35.17</cell><cell>44.04</cell><cell>45.15(+2.66)</cell><cell>74.98(+2.88)</cell></row></table><note>Cityscapes [25] is a street scene dataset captured in 50 different European cities under normal conditions. The dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I :</head><label>I</label><figDesc>Comparison with state-of-the-art methods on Stan-ford2D3D<ref type="bibr" target="#b22">[23]</ref> and Trans10K-v2<ref type="bibr" target="#b23">[24]</ref>. #MParams, #GFLOPs are calculated with the input size of 512?512. The embedding dimension of Trans4Trans is 64, while it is {128, 128, 256} for Trans2Seg-T/-S/-M decoder, following<ref type="bibr" target="#b23">[24]</ref>. comprises 2, 979 and 500 images for training and validation. The resolution of the images is 2, 048?1, 024 and the images are annotated with 19 categories.</figDesc><table /><note>ACDC [26] is a driving scene dataset captured under four adverse conditions, i.e., fog, night, rain, and snow. It contains 1, 600 training-and 406 validation images which are publicly available, and 2, 000 test images for benchmarking. The reso- lution of the images is 1, 920 ? 1, 080. DADA-seg [27] is a testing dataset captured in traffic acciden- tal scenes. The resolution of image is 1, 584 ? 660. It contains 313 images densely annotated with 19 categories, which are consistent with Cityscapes for benchmarking semantic segmentation. These extreme accident scenarios are utilized in our work together with ACDC to study the robustness of Trans4Trans in different adverse conditions. Implementation details. Our Trans4Trans model is imple- mented with CUDA 11.2 and PyTorch 1.8.0 with an initialized learning rate 1e ? 4 and scheduled by the poly strategy</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Effect of CNN/Transformer combination. Models are evaluated on the Trans10K-v2 dataset. #GFLOPs are calculated with the input size of 512?512.in all settings (T, S, and M are short for Tiny, Small, and Medium). Trans4Trans-M achieves the best performance in mIoU with 45.73% on Stanford2D3D dataset and 75.14% on Trans10K-v2 dataset, exceeding by more than 3% w.r.t.</figDesc><table><row><cell>Trans2Seg-M on the challenging transparent object segmenta-</cell></row><row><cell>tion benchmark. Meanwhile, compared to Trans2Seg-M, the</cell></row><row><cell>computational complexity in GFLOPs is much smaller in</cell></row><row><cell>our approach Trans4Trans-M, while Trans4Trans-Tiny and</cell></row><row><cell>Trans4Trans-Small also achieve better performance than the</cell></row><row><cell>corresponding Trans2Seg variant on Trans10K-v2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III :</head><label>III</label><figDesc>Effect of embedding channel in TPM. All tiny Trans4Trans are trained on Trans10K-v2 at 512?512 on one GPU.</figDesc><table /><note>Acc denotes pixel accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>44.24 42.25 50.36 65.23 60.00 43.88 53.81 47.47 44.64 48.99 67.88 63.80 55.08 58.86 46.27 39.47 33.06 58.87 59.45 43.22 44.87 41.39 43.42 61.97 69.48 61.65 54.89 63.47</figDesc><table><row><cell>Method</cell><cell cols="3">GFLOPs ? ACC ? mIoU ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Category IoU ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Background Shelf</cell><cell cols="4">Jar/Tank Freezer Window Door</cell><cell cols="2">Eyeglass Cup</cell><cell>Wall</cell><cell>Bowl</cell><cell>Bottle</cell><cell>Box</cell></row><row><cell>FPENet [75]</cell><cell>0.76</cell><cell>70.31</cell><cell>10.14</cell><cell>74.97</cell><cell>0.01</cell><cell>0.00</cell><cell>0.02</cell><cell>2.11</cell><cell>2.83</cell><cell>0.00</cell><cell cols="2">16.84 24.81</cell><cell>0.00</cell><cell>0.04</cell><cell>0.00</cell></row><row><cell>ESPNetv2 [76]</cell><cell>0.83</cell><cell>73.03</cell><cell>12.27</cell><cell>78.98</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>6.17</cell><cell>0.00</cell><cell cols="2">30.65 37.03</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ContextNet [77]</cell><cell>0.87</cell><cell>86.75</cell><cell>46.69</cell><cell>89.86</cell><cell>23.22</cell><cell>34.88</cell><cell cols="9">32.34 20.17</cell></row><row><cell>FastSCNN [78]</cell><cell>1.01</cell><cell>88.05</cell><cell>51.93</cell><cell>90.64</cell><cell>32.76</cell><cell>41.12</cell><cell cols="9">47.28 24.65</cell></row><row><cell>DFANet [79]</cell><cell>1.02</cell><cell>85.15</cell><cell>42.54</cell><cell>88.49</cell><cell>26.65</cell><cell>27.84</cell><cell cols="9">28.94 13.37</cell></row><row><cell>ENet [80]</cell><cell>2.09</cell><cell>71.67</cell><cell>8.50</cell><cell>79.74</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>22.25</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>DeepLabv3+MBv2 [81]</cell><cell>2.62</cell><cell>88.39</cell><cell>54.16</cell><cell>89.95</cell><cell>31.79</cell><cell>48.29</cell><cell cols="9">46.18 37.36</cell></row><row><cell>HRNet_w18 [82]</cell><cell>4.20</cell><cell>89.58</cell><cell>54.25</cell><cell>92.47</cell><cell>27.66</cell><cell>45.08</cell><cell cols="8">40.53 45.66 45.00 68.05 73.24 64.86 52.85 62.52</cell><cell>33.02</cell></row><row><cell>HarDNet [83]</cell><cell>4.42</cell><cell>90.19</cell><cell>56.19</cell><cell>92.87</cell><cell>34.62</cell><cell>47.50</cell><cell cols="8">42.40 49.78 49.19 62.33 72.93 68.32 58.14 65.33</cell><cell>30.90</cell></row><row><cell>DABNet [84]</cell><cell>5.18</cell><cell>77.43</cell><cell>15.27</cell><cell>81.19</cell><cell>0.00</cell><cell>0.09</cell><cell>0.00</cell><cell>4.10</cell><cell>10.49</cell><cell>0.00</cell><cell cols="2">36.18 42.83</cell><cell>0.00</cell><cell>8.30</cell><cell>0.00</cell></row><row><cell>LEDNet [85]</cell><cell>6.23</cell><cell>86.07</cell><cell>46.40</cell><cell>88.59</cell><cell>28.13</cell><cell>36.72</cell><cell cols="8">32.45 43.77 38.55 41.51 64.19 60.05 42.40 53.12</cell><cell>27.29</cell></row><row><cell>Trans4Trans-T</cell><cell>10.45</cell><cell>93.23</cell><cell>68.63</cell><cell>94.44</cell><cell>48.39</cell><cell>61.89</cell><cell cols="2">61.86 61.14</cell><cell>54.83</cell><cell>73.60</cell><cell>83.03</cell><cell>75.20</cell><cell>74.69</cell><cell>75.26</cell><cell>59.19</cell></row><row><cell>ICNet [86]</cell><cell>10.64</cell><cell>78.23</cell><cell>23.39</cell><cell>83.29</cell><cell>2.96</cell><cell>4.91</cell><cell>9.33</cell><cell cols="5">19.24 15.35 24.11 44.54 41.49</cell><cell>7.58</cell><cell>27.47</cell><cell>3.80</cell></row><row><cell>BiSeNet [73]</cell><cell>19.91</cell><cell>89.13</cell><cell>58.40</cell><cell>90.12</cell><cell>39.54</cell><cell>53.71</cell><cell cols="8">50.90 46.95 44.68 64.32 72.86 63.57 61.38 67.88</cell><cell>44.85</cell></row><row><cell>Trans4Trans-S</cell><cell>19.92</cell><cell>94.57</cell><cell>74.15</cell><cell>95.60</cell><cell>57.05</cell><cell>71.18</cell><cell cols="2">70.21 63.95</cell><cell>61.25</cell><cell>81.67</cell><cell>87.34</cell><cell>78.52</cell><cell>77.13</cell><cell>81.00</cell><cell>64.88</cell></row><row><cell>DenseASPP [87]</cell><cell>36.20</cell><cell>90.86</cell><cell>63.01</cell><cell>91.39</cell><cell>42.41</cell><cell>60.93</cell><cell cols="8">64.75 48.97 51.40 65.72 75.64 67.93 67.03 70.26</cell><cell>49.64</cell></row><row><cell>DeepLabv3+ [48]</cell><cell>37.98</cell><cell>92.75</cell><cell>68.87</cell><cell>93.82</cell><cell>51.29</cell><cell>64.65</cell><cell cols="8">65.71 55.26 57.19 77.06 81.89 72.64 70.81 77.44</cell><cell>58.63</cell></row><row><cell>FCN [50]</cell><cell>42.23</cell><cell>91.65</cell><cell>62.75</cell><cell>93.62</cell><cell>38.84</cell><cell>56.05</cell><cell cols="8">58.76 46.91 50.74 82.56 78.71 68.78 57.87 73.66</cell><cell>46.54</cell></row><row><cell>OCNet [53]</cell><cell>43.31</cell><cell>92.03</cell><cell>66.31</cell><cell>93.12</cell><cell>41.47</cell><cell>63.54</cell><cell cols="8">60.05 54.10 51.01 79.57 81.95 69.40 68.44 78.41</cell><cell>54.65</cell></row><row><cell>RefineNet [88]</cell><cell>44.56</cell><cell>87.99</cell><cell>58.18</cell><cell>90.63</cell><cell>30.62</cell><cell>53.17</cell><cell cols="8">55.95 42.72 46.59 70.85 76.01 62.91 57.05 70.34</cell><cell>41.32</cell></row><row><cell>Trans2Seg [24]</cell><cell>49.03</cell><cell>94.14</cell><cell>72.15</cell><cell>95.35</cell><cell>53.43</cell><cell>67.82</cell><cell cols="3">64.20 59.64 60.56</cell><cell>88.52</cell><cell cols="4">86.67 75.99 73.98 82.43</cell><cell>57.17</cell></row><row><cell>TransLab [46]</cell><cell>61.31</cell><cell>92.67</cell><cell>69.00</cell><cell>93.90</cell><cell>54.36</cell><cell>64.48</cell><cell cols="8">65.14 54.58 57.72 79.85 81.61 72.82 69.63 77.50</cell><cell>56.43</cell></row><row><cell>DUNet [89]</cell><cell>123.69</cell><cell>90.67</cell><cell>59.01</cell><cell>93.07</cell><cell>34.20</cell><cell>50.95</cell><cell cols="8">54.96 43.19 45.05 79.80 76.07 65.29 54.33 68.57</cell><cell>42.64</cell></row><row><cell>U-Net [90]</cell><cell>124.55</cell><cell>81.90</cell><cell>29.23</cell><cell>86.34</cell><cell>8.76</cell><cell>15.18</cell><cell cols="8">19.02 27.13 24.73 17.26 53.40 47.36 11.97 37.79</cell><cell>1.77</cell></row><row><cell>DANet [52]</cell><cell>198.00</cell><cell>92.70</cell><cell>68.81</cell><cell>93.69</cell><cell>47.69</cell><cell>66.05</cell><cell cols="8">70.18 53.01 56.15 77.73 82.89 72.24 72.18 77.87</cell><cell>56.06</cell></row><row><cell>PSPNet [51]</cell><cell>187.03</cell><cell>92.47</cell><cell>68.23</cell><cell>93.62</cell><cell>50.33</cell><cell>64.24</cell><cell cols="8">70.19 51.51 55.27 79.27 81.93 71.95 68.91 77.13</cell><cell>54.43</cell></row><row><cell>Trans4Trans-M</cell><cell>34.38</cell><cell>95.01</cell><cell>75.14</cell><cell>96.08</cell><cell>55.81</cell><cell>71.46</cell><cell cols="2">69.25 65.16</cell><cell>63.96</cell><cell>83.84</cell><cell>88.21</cell><cell>80.29</cell><cell>76.33</cell><cell>83.09</cell><cell>68.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IV :</head><label>IV</label><figDesc>Computation complexity in GFLOPs and category-wise accuracy evaluation and comparison with state-of-the-art semantic segmentation methods on the Trans10K-v2 dataset<ref type="bibr" target="#b23">[24]</ref>.</figDesc><table><row><cell>Network</cell><cell>Encoder</cell><cell>Decoder</cell><cell cols="2">GFLOPs MParams</cell><cell>Cityscapes</cell><cell>ACDC</cell></row><row><cell>PVT-T</cell><cell>PVT-T [67]</cell><cell></cell><cell>10.30</cell><cell>13.11</cell><cell>58.09</cell><cell>53.65</cell></row><row><cell>PVT-S</cell><cell>PVT-S [67]</cell><cell>Transformer [24]</cell><cell>19.77</cell><cell>24.35</cell><cell>59.68</cell><cell>57.13</cell></row><row><cell>PVT-M</cell><cell>PVT-M [67]</cell><cell></cell><cell>36.87</cell><cell>51.83</cell><cell>60.38</cell><cell>58.60</cell></row><row><cell>Trans4Trans-T</cell><cell>PVT-T [67]</cell><cell></cell><cell>10.45</cell><cell>12.71</cell><cell>60.41(+2.32)</cell><cell>54.37(+0.72)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVT-S [67]</cell><cell>TPM / Single-head</cell><cell>21.98</cell><cell>25.00</cell><cell>63.08(+3.40)</cell><cell>60.70(+3,57)</cell></row><row><cell cols="2">Trans4Trans-M PVT-M [67]</cell><cell></cell><cell>44.38</cell><cell>48.77</cell><cell>65.63(+5,25)</cell><cell>61.91(+3,31)</cell></row><row><cell>Trans4Trans-T</cell><cell>PVT-T [67]</cell><cell></cell><cell>11.23</cell><cell>13.10</cell><cell>57.42(-0.67)</cell><cell>56.36(+2.71)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVT-S [67]</cell><cell>TPM / Dual-head</cell><cell>24.82</cell><cell>26.45</cell><cell>62.39(+2.71)</cell><cell>62.14(+5.01)</cell></row><row><cell cols="2">Trans4Trans-M PVT-M [67]</cell><cell></cell><cell>55.16</cell><cell>54.28</cell><cell>63.00(+2.62)</cell><cell>63.88(+5.28)</cell></row><row><cell>Trans4Trans-T</cell><cell>PVTv2-B1 [68]</cell><cell></cell><cell>9.18</cell><cell>13.53</cell><cell>63.25(+5.16)</cell><cell>59.25(+5.60)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVTv2-B2 [68]</cell><cell>TPM / Single-head</cell><cell>19.27</cell><cell>25.62</cell><cell>67.28(+7.60)</cell><cell>64.61(+7.48)</cell></row><row><cell cols="2">Trans4Trans-M PVTv2-B3 [68]</cell><cell></cell><cell>41.89</cell><cell>49.55</cell><cell>69.34(+8.96)</cell><cell>65.92(+7.32)</cell></row><row><cell>Trans4Trans-T</cell><cell>PVTv2-B1 [68]</cell><cell></cell><cell>10.00</cell><cell>13.93</cell><cell>62.31(+4.22)</cell><cell>61.86(+8.21)</cell></row><row><cell>Trans4Trans-S</cell><cell>PVTv2-B2 [68]</cell><cell>TPM / Dual-head</cell><cell>22.17</cell><cell>27.08</cell><cell>65.98(+6.30)</cell><cell>64.83(+7.70)</cell></row><row><cell cols="2">Trans4Trans-M PVTv2-B3 [68]</cell><cell></cell><cell>52.77</cell><cell>55.09</cell><cell>69.05(+8.67)</cell><cell>66.65(+8.05)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V :Table</head><label>V</label><figDesc>Effectiveness of our model on driving scene datasets, e.g. Cityscapes<ref type="bibr" target="#b24">[25]</ref> and ACDC<ref type="bibr" target="#b25">[26]</ref>. The first group is selected as the baseline. The fourth and fifth groups further verify the flexibility of our TPM adapted to other backbones. #MParams, #GFLOPs are calculated with the input size of 512?512. The embedding dimensions of model-T, -S, and -M decoders are {64, 128, 256}.Segmentation in normal conditions. As shown inTable VI, experimental results of our proposed Trans4Trans approach trained with the input size of 768?768, together with state-ofthe-art semantic segmentation methods are presented 1 . Here, all the decoders for Trans4Trans are constructed as the singlehead decoder for the experiments on Cityscapes according to the best performance of single-head decoder illustrated in</figDesc><table><row><cell>Methods</cell><cell>Encoder</cell><cell cols="3">GFLOPs ? MParams ? mIoU (MS) ?</cell></row><row><cell>Fast-SCNN [78]</cell><cell>Fast-SCNN</cell><cell>2.07</cell><cell>1.46</cell><cell>72.65</cell></row><row><cell>CGNet [91]</cell><cell>CGNet-M3N21</cell><cell>7.72</cell><cell>0.50</cell><cell>64.80</cell></row><row><cell>Trans4Trans-T</cell><cell>PVTv2-B1 [68]</cell><cell>20.66</cell><cell>13.53</cell><cell>78.23</cell></row><row><cell>SegFormer-B1 [65]</cell><cell>MiT-B1</cell><cell>29.85</cell><cell>13.66</cell><cell>78.43</cell></row><row><cell>ERFNet [9]</cell><cell>ERFNet</cell><cell>30.22</cell><cell>2.07</cell><cell>72.10</cell></row><row><cell>Trans4Trans-S</cell><cell>PVTv2-B2 [68]</cell><cell>43.37</cell><cell>25.62</cell><cell>80.02</cell></row><row><cell>PSPNet [51]</cell><cell>MobileNetV2</cell><cell>119.09</cell><cell>13.72</cell><cell>70.20</cell></row><row><cell>PSPNet [51]</cell><cell>ResNet-18</cell><cell>119.27</cell><cell>12.77</cell><cell>76.90</cell></row><row><cell>SegFormer-B2 [65]</cell><cell>MiT-B2</cell><cell>127.86</cell><cell>27.33</cell><cell>80.46</cell></row><row><cell>SegFormer-B3 [65]</cell><cell>MiT-B3</cell><cell>160.78</cell><cell>47.18</cell><cell>81.50</cell></row><row><cell>DeepLabv3+ [48]</cell><cell>MobileNetv2</cell><cell>169.53</cell><cell>18.70</cell><cell>75.20</cell></row><row><cell>EMANet [92]</cell><cell>ResNet-50</cell><cell>379.00</cell><cell>42.09</cell><cell>80.49</cell></row><row><cell>PSPNet [51]</cell><cell>ResNet-50</cell><cell>401.51</cell><cell>48.98</cell><cell>79.96</cell></row><row><cell>DNL [55]</cell><cell>ResNet-50</cell><cell>449.73</cell><cell>50.02</cell><cell>80.70</cell></row><row><cell>EMANet [92]</cell><cell>ResNet-101</cell><cell>553.79</cell><cell>61.08</cell><cell>81.00</cell></row><row><cell>PSPNet [55]</cell><cell>ResNet-101</cell><cell>573.48</cell><cell>67.95</cell><cell>80.04</cell></row><row><cell>DNL [55]</cell><cell>ResNet-101</cell><cell>624.52</cell><cell>69.02</cell><cell>80.68</cell></row><row><cell>SETR-Naive [59]</cell><cell>ViT-L [58]</cell><cell>698.52</cell><cell>306.58</cell><cell>77.90</cell></row><row><cell>SETR-MLA [59]</cell><cell>ViT-L [58]</cell><cell>712.76</cell><cell>310.81</cell><cell>77.24</cell></row><row><cell>SETR-PUP [59]</cell><cell>ViT-L [58]</cell><cell>818.26</cell><cell>319.11</cell><cell>79.34</cell></row><row><cell>Trans4Trans-M</cell><cell>PVTv2-B3 [68]</cell><cell>94.25</cell><cell>49.55</cell><cell>81.54</cell></row></table><note>V on Cityscapes. Our Trans4Trans-M approach with PVTv2-B3 as encoder outperforms the others and achieves the best performance with an mIoU of 81.54% on Cityscapes, whose images are collected under normal weather and fa- vorable illumination conditions. Compared with the state-1 For a fair comparison, model weights are obtained by the same framework MMSegmentation: https://github.com/open-mmlab/mmsegmentation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of state-of-the-art models onCityscapes<ref type="bibr" target="#b24">[25]</ref>. Methods in the first group are designed for efficient semantic segmentation and have smaller #GFLOPs. #MParams, #GFLOPs are calculated with the input size of 768?768. "MS" means multi-scale testing.of-the-art methods such as SETR<ref type="bibr" target="#b58">[59]</ref> and PSPNet<ref type="bibr" target="#b50">[51]</ref>, our Trans4Trans approach shows smaller GFLOPs(94.25)   </figDesc><table><row><cell>100%</cell><cell>85%</cell><cell>70%</cell><cell>55%</cell><cell>40%</cell><cell>20%</cell><cell>100%</cell><cell>85%</cell><cell>70%</cell><cell>55%</cell><cell>40%</cell><cell>20%</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) Overlapping transparent doors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Transparent sneeze guards</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">(c) Glass walls</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d) Frosted doors</cell><cell></cell><cell></cell></row></table><note>and less parameters (49.55M ), which are relevant for fast inference in automated vehicles. Our TransTrans models with lighter encoder architectures indicated as Trans4Trans-T and Trans4Trans-S also show high scores of 78.23% and 80.02% in mIoU when leveraging PVTv2-B1 and PVTv2-B2 as the encoder. The lightest Trans4Trans outperforms state-of-the-art efficient networks FastSCNN [78] and ERFNet [9] by large</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VII :</head><label>VII</label><figDesc></figDesc><table /><note>Comparison of different semantic segmentation models on adverse (Fog, Night, Rain, Snow, and All-ACDC</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE VIII :</head><label>VIII</label><figDesc></figDesc><table /><note>Inference time (ms/frame) of dual-head Trans4Trans is tested in half-/single-precision on various GPUs at 512?512.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid detection of blind roads and crosswalks by using a lightweight semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mechatronic system to help visually impaired users during walking and running</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frontoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zingaretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Embedded multisensor system for safe point-to-point navigation of impaired users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frontoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zingaretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Descending step classification using time-of-flight sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stahlschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Camen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gavriilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kummert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>IV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>IV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Importance-aware semantic segmentation with efficient pyramidal context network for navigational assistant systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<editor>ITSC</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mobility-related accidents experienced by people with visual impairment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurniawan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AER Journal: Research and Practice in Visual Impairment and Blindness</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pray before you step out&quot;: Describing personal and situational blind navigation behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASSETS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PASS: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Omnisupervised omnidirectional semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Passive and Low Energy Cooling for the Built Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Butera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Glass architecture: is it sustainable</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Suitability evaluation of visual indicators on glass walls and doors for visually impaired people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Mech. Mater</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Don&apos;t hit me! Glass detection in real-world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Navigation assistance for the visually impaired using RGB-D sensor with range expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aladren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>L?pez-Nicol?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Syst. J</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Enabling independent navigation for visually impaired people through a wearable vision-based feedback system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Katzschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Giarr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">V-Eye: A vision-based navigation system for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><forename type="middle">F</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multim</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">HIDA: Towards holistic indoor understanding for the visually impaired via semantic instance segmentation with a wearable solid-state LiDAR sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning based wearable assistive system for visually impaired people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Closing the gap: Designing for the last-few-meters wayfinding problem for people with visual impairments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Fiannaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kneisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASSETS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Floor extraction and door detection for visually impaired guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berenguel-Baeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guerrero-Viu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bermudez-Cameo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>P?rez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
		<editor>ICARCV</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segmenting transparent object in the wild with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ISSAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic crosswalk scene understanding for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The semantic paintbrush: Interactive 3D mapping and recognition in large outdoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<editor>CHI</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic labeling for prosthetic vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Panoptic lintention network: Towards efficient navigational perception for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<editor>RCAR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unifying obstacle detection, recognition, and fusion based on millimeter wave radar and RGB-depth sensors for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Sci. Instrum</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Content-aware video analysis to guide visually impaired walking on the street</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yohannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<editor>IVIC</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Intersection perception through real-time semantic segmentation to assist navigation of visually impaired pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Long</surname></persName>
		</author>
		<editor>ROBIO</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Outdoor walking guide for the visually-impaired people based on semantic segmentation and depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Computer vision-based assistance system for the visually impaired using mobile edge artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Nivedha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bhandarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Smart guiding glasses for visually impaired people in indoor environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consumer Electron</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glass detection and recognition based on the fusion of ultrasonic sensor and RGB-D sensor for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving RealSense by fusing color stereo vision and infrared stereo vision for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICISS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simultaneous transparent and non-transparent object segmentation with multispectral scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Okazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takahata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep polarization cues for transparent object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rich context aggregation with reflection prior for glass surface detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Polarization-driven semantic segmentation via efficient attention-bridged fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Segmenting transparent objects in the wild,&quot; in ECCV</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">FakeMix augmentation improves transparent object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13279</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Enhanced boundary learning for glass-like object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15734</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequenceto-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fully transformer networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04108</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">PVTv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">CSWin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Twins: Revisiting spatial attention design in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Intel RealSense stereoscopic depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodfill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grunnet-Jepsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhowmik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Feature pyramid encoding network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">ESPNetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">ContextNet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">DFANet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">HarDNet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">DABNet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">LEDNet: A lightweight encoder-decoder network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">ICNet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">CGNet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Expectationmaximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Qualitative HCI research: Going behind the scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blandford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Furniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human-Centered Informatics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Using the think aloud method (cognitive labs) to evaluate test design for students with disabilities and english language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Bottsford-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>National Center on Educational Outcomes, University of Minnesota, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">NASA-Task load index (NASA-TLX); 20 years later</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Helping the blind to get through COVID-19: Social distancing assistant using real-time semantic segmentation on RGB-D video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Using thematic analysis in psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Qualitative Research in Psychology</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
