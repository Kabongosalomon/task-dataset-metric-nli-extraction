<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">READ: Large-Scale Neural Scene Rendering for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuopeng</forename><surname>Li</surname></persName>
							<email>lizhuopeng@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Zeyu</surname></persName>
							<affiliation key="aff2">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Alibaba</surname></persName>
							<affiliation key="aff2">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Group</forename><surname>China</surname></persName>
							<affiliation key="aff2">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<address>
									<country>Alibaba Group China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">READ: Large-Scale Neural Scene Rendering for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>China Figure 1: Given the input point clouds, our Autonomous Driving scene Render (READ) synthesizes photo-realistic driving scenes from different views, which is able to provide rich data for autonomous driving rather than images with a single view.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Github: https://githubcom/JOP-Lee/READ-Large-Scale-Neural- Scene-Rendering-for-Autonomous-Driving</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthesizing free-view photo-realistic images is an important task in multimedia. With the development of advanced driver assistance systems (ADAS) and their applications in autonomous vehicles, experimenting with different scenarios becomes a challenge. Although the photo-realistic street scenes can be synthesized by image-toimage translation methods, which cannot produce coherent scenes due to the lack of 3D information. In this paper, a large-scale neural rendering method is proposed to synthesize the autonomous driving scene (READ), which makes it possible to synthesize large-scale driving scenarios on a PC through a variety of sampling schemes. In order to represent driving scenarios, we propose an ? rendering network to learn neural descriptors from sparse point clouds. Our model can not only synthesize realistic driving scenes but also stitch and edit driving scenes. Experiments show that our model performs well in large-scale driving scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Synthesizing free-view photo-realistic images is an important task in multimedia <ref type="bibr" target="#b2">[3]</ref>. Especially, the synthetic large-scale street views are essential to a series of real-world applications, including autonomous driving <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, robot simulation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>, object detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, and image segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. As illustrated in <ref type="figure">Fig. 1</ref>, the objective of neural scene rendering is to synthesize the 3D scene from a moving camera, where the user can browse the street scenery from different views and conduct automatic driving simulation experiments. In addition, this can generate multi-view images to provide data for multimedia tasks.</p><p>With the development of autonomous driving, it is challenging to conduct experiments in various driving scenarios. Due to the complicated geographic locations, varying surroundings, and road conditions, it is usually difficult to simulate outdoor environments. Additionally, it is hard to model some unexpected traffic scenarios, such as car accidents, where the simulators can help to reduce the reality gap. However, the data generated by the widely used simulator like CARLA <ref type="bibr" target="#b5">[6]</ref> is far different from real world scenes using the conventional rendering pipeline.</p><p>The image-to-image translation-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> synthesize the street views with semantic labels by learning the mapping between source images and targets. Despite of generating the encouraging street scene, there exist some large artifacts and incoherent textures. Moreover, the synthesized image has only a single view that cannot provide the rich multi-view traffic conditions for autonomous vehicles. This hinders them from a large number of real world applications.</p><p>Recently, Neural Radiance Field (NeRF) based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> achieve the promising results in synthesizing the photo-realistic scenes with multi-view. As suggested in <ref type="bibr" target="#b4">[5]</ref>, they cannot produce reasonable results with only few input views, which typically happens in the driving scenario with the objects appearing in only a few frames. Moreover, the NeRF-based methods mainly render either the interiors or objects. They have difficulty synthesizing the large scale driving scenes with the complicated environment, where the large artifacts occur in the closed-up views and surroundings. To tackle this problem, NeRFW <ref type="bibr" target="#b15">[16]</ref> makes use of the additional depth and segmentation annotations to synthesize an outdoor building, which takes about two days with 8 GPU devices. Such a long reconstruction time is mainly due to the unnecessary sampling of the vast spaces.</p><p>Unlike the NeRF-based methods that purely depend on per-scene fitting, the neural rendering approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> can be effectively initialized via neural textures, which are stored as maps on top of the 3D mesh proxy. Similarly, NPBG <ref type="bibr" target="#b1">[2]</ref> learns neural descriptors from a raw point cloud to encode the local geometry and appearance, which avoids sample rays in empty scene space by the classical point clouds reflecting the geometry of the scene in real world. Moreover, ADOP <ref type="bibr" target="#b21">[22]</ref> improves NPBG by adding a differentiable camera model with a tone mapper, which introduces the formulation to better approximate the spatial gradient of pixel rasterization. In general, the point-based neural rendering method can synthesize a larger scene with fewer captured images by initializing the scene through three-dimensional point cloud information. Although neural rendering-based methods can synthesize the photo-realistic novel views in both indoor and outdoor scenes, it is still very challenging to deal with the large-scale driving scenarios due to the limitations on model capacity, as well as constraints on memory and computation. Additionally, it is difficult to render the photo-realistic views with rich buildings, lanes, and road signs, where the sparse point cloud data obtained from the few input images usually contain lots of holes.</p><p>In this paper, we propose an effective neural scene rendering approach, which makes it possible to synthesize the large-scale driving scenarios through efficient Monte Carlo sampling, screening of large-scale point clouds, and patch sampling. It is worth mentioning that our method synthesized large-scale driving scenarios with an average of two days of training on a PC with two RTX2070 GPUs. This greatly reduces the computational cost so that large-scale scene rendering can be achieved on affordable hardware. For sparse point clouds, we fill in the missing areas of point clouds by multi-scale feature fusion. To synthesize photo-realistic driving scene from sparse point clouds, we propose an ? network to filter neural descriptors through basic gate modules and fuse features of the same scale and different scales with different strategies. Through ? , our model can not only synthesize the realistic scenes but also edit and stitch scenes via neural descriptors. Moreover, we are able to update the specific areas and stitch them together with the original scene. Scene editing can be used to synthesize the diverse driving scene data from different views even for traffic emergencies.</p><p>The main contributions of this paper are summarized as: 1) Based on our neural rendering engine (READ), a large-scale driving simulation environment is constructed to generate realistic data for advanced driver assistance systems; 2) ? network is proposed to obtain a more realistic and detailed driving scenario, where multiple sampling strategies are introduced to enable synthesize the large-scale driving scenes; 3) Experiments on the KITTI benchmark <ref type="bibr" target="#b7">[8]</ref> and Brno Urban dataset <ref type="bibr" target="#b14">[15]</ref> show the good qualitative and quantitative results, where the driving scenes can be edited and stitched so as to synthesize larger and more diverse driving data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Image-to-image Translation</head><p>Many researchers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> employ image-to-image translation technique to synthesize photo-realistic street scenes. Gao et al. <ref type="bibr" target="#b6">[7]</ref> propose an unsupervised GAN-based framework, which adaptively synthesizes images from segmentation labels by considering the specific attributes of the task from segmentation labels to image synthesis. As in <ref type="bibr" target="#b6">[7]</ref>, Tang et al. <ref type="bibr" target="#b24">[25]</ref> present a dual-attention GAN that synthesizes the photo-realistic and semantically consistent images with fine detail from input layouts without the additional training overhead. To enhance the fidelity of images in the game, Richter et al. <ref type="bibr" target="#b20">[21]</ref> use the G-buffers generated in the rendering process of the game engine as the additional input signal to train the convolutional neural network, which is able to eliminate the disharmonious and unreasonable illusions generated by previous deep learning methods. Although the image-to-image translation method can synthesize the realistic street scene, it still cannot guarantee coherence in the scene transformation. Moreover, it can only synthesize the scene from a single view, whose results are far different from the real scene in terms of texture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Novel View Synthesis</head><p>Neural Radiance Fields <ref type="bibr" target="#b17">[18]</ref> become an important breakthrough for the novel view synthesis task, which is proposed to use a fully connected network of entire scenes optimized by differentiable volume rendering. Recently, there have been many variations of this method to render different objects, such as human <ref type="bibr" target="#b19">[20]</ref>, car <ref type="bibr" target="#b18">[19]</ref>, interior scene <ref type="bibr" target="#b28">[29]</ref>, and building <ref type="bibr" target="#b15">[16]</ref>. However, NeRF-based methods depend on per-scene fitting, it is hard to fit a large-scale driving scenario. NeRFW <ref type="bibr" target="#b15">[16]</ref> combines appearance embedding and decomposition of transient and static elements through uncertainty fields. Unfortunately, dynamic objects are ignored, which may lead to occlusions in the static scene. Moreover, synthesizing scenes as large as street view requires huge computing resources, which cannot be rendered in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scene Synthesis by Neural Rendering</head><p>Meshry et al. <ref type="bibr" target="#b16">[17]</ref> take a latent appearance vector and a semantic mask of the transient object's location as input, which render the scene's points into the deep frame buffer and learn these initial The input image is firstly aligned, and then the point cloud of the scene is obtained by matching feature points and dense construction. We rasterize points at several resolutions. Given the point cloud , the learnable neural descriptor , and the camera parameter , our presented ? rendering network synthesizes realistic driving scenes by filtering neural descriptors learned from the data and fusing the features from the same scale and different scales.</p><p>render mappings to the real photo. This requires a lot of semantic annotation and ignores the transient objects. By combining the traditional graphics pipelines with learnable components, Thies et al. <ref type="bibr" target="#b26">[27]</ref> introduce a new image composition paradigm, named Deferred Neural Rendering, where feature mapping of the target image is learned from UV-map through neural texture. Despite the promising results, it is time-consuming to obtain explicit surfaces with good quality from the point cloud.</p><p>The point-based neural rendering method employs point clouds as input to learn the scene representation. NPBG <ref type="bibr" target="#b1">[2]</ref> encodes the local geometric shapes and appearance by learning neural descriptors, which synthesizes high quality novel indoor views from point clouds. TRANSPR <ref type="bibr" target="#b12">[13]</ref> extends NPBG by augmenting point descriptors with alpha values and replacing Z-buffer rasterization with ray marching, it is able to synthesize semi-transparent parts of the scene. ADOP <ref type="bibr" target="#b21">[22]</ref> proposes a point-based differentiable neural rendering, where the parameters of all scenarios are optimized by designing the stages of the pipeline to be differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LARGE-SCALE NEURAL SCENE RENDER</head><p>Our proposed Neural Scene Render approach aims to synthesize the photo-realistic images from an arbitrary camera viewpoint by representing the driving scenes with point clouds. In this section, we first outline our proposed method. Secondly, multi sampling strategies of sparse point clouds are proposed to reduce the computational cost for large-scale driving scenes. Thirdly, ? is proposed to represent driving scenes with sparse point clouds and synthesize realistic driving scenarios. Finally, the driving scenes are edited and stitched to provide synthetic data for larger and richer driving scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a set of input images for a driving scene and the point cloud = { 1 , 2 , . . . , } with known camera parameters, our framework is capable of synthesizing the photo-realistic driving scenes from multiple views, as well as stitching and editing driving scenes. To this end, we propose an end-end large-scale neural scene render that synthesizes realistic images from sparse point clouds. Our framework is divided into the following three parts: rasterization, sampling with the sparse point cloud, ? rendering network. The overview of our proposed framework is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Sparse 3D point cloud can be obtained through the classic Structure-from-Motion and Multi-View Stereo pipelines, such as Agisoft Metashape <ref type="bibr" target="#b0">[1]</ref>. Each point is located at , which is associated with a neural descriptor vector encoding the local scene content. As in <ref type="bibr" target="#b1">[2]</ref>, each input 3D point in contains the position = { , , }, whose appearance feature is extracted by mapping the RGB value of image pixel to its corresponding 3D space. Neural descriptors = { 1 , 2 , . . . , } are calculated from the input point cloud, namely latent vectors representing local geometry and photometric properties. We update these features by propagating gradient to the input so that the features of the neural descriptor can be automatically learned from data. Given the camera's internal and external parameters, we can view the scene from different views by designing 8-dimensional neural descriptors to represent the RGB values. In the rasterization phase, images of size ? are captured by pinhole camera , we construct a pyramid of rasterized raw images { } =1 (T=4 in all our experiments), has the spatial size of 2 ? 2 , which is formed by assigning the neural descriptor of the point passing the depth test to each pixel. Then, it is projected onto the pixel under the full projection transformation of the camera. Essentially, the neural descriptors feature encodes the local 3D scene content around . The rendering network expresses a local 3D function that outputs the specific neural scene description ( , ) at , modeled by the neural point in its local frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling with Sparse Point Cloud</head><p>Synthesizing driving scenes with thousands of meters requires enormous computational power. Therefore, the key to our proposed approach is to reduce memory usage and improve training efficiency. Instead of fitting each scene separately, we employ the point clouds generated by the off-the-shelf 3D reconstruction software <ref type="bibr" target="#b0">[1]</ref> to initialize the geometry of the real world scene. As a huge amount of point cloud data consumes a lot of memory, it is still difficult to train. To tackle this critical issue, we take advantage of a sparse sampling strategy to generate sparse point clouds from 1/4 of the originally available pixels, resulting in only 25% of the total number of point clouds trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Screen out occluded point clouds.</head><p>To avoid updating the descriptors of the occluded points, we approximate the visibility of each point. We use the nearest rasterization scheme by constructing a Z-buffer, reserving only the points with the lowest Z-value at the pixel position. Neural descriptors avoid the points that are far from the camera of the current frame to better synthesize the scene. Thus, the computational cost of calculating occluded point cloud is reduced so that the training efficiency is greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Monte Carlo sampling.</head><p>Due to the different distribution of point clouds in the scene, there are abundant point clouds in the area with obvious features. For the area of the sky or dynamic objects, there are fewer corresponding regional point clouds due to the lack of obvious features or fewer feature points. To train effectively, we propose dynamic training strategies, which take advantage of the Monte Carlo method <ref type="bibr" target="#b23">[24]</ref> to sample a large amount of driving scene data. For the image set in the training phase , * ? arg Top ( ).</p><p>( ) is the synthetic quality of image , which is calculated by perceptual loss <ref type="bibr" target="#b10">[11]</ref> in our task. We employ the Top samples with the worst performance at each phase as training data. Through the training strategy of dynamic sampling, the model strengthens to learn the sparse region of the point cloud so that the overall training time is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Patch sampling.</head><p>Image resolution also plays a very important role in memory usage. To this end, we randomly divide the whole image into multiple patches through a sampling strategy, which can select the random patches with the size of ? ? according to the available GPU memory size. It is worth mentioning that the proportion of pixels in patch ? ?, to the whole image ? is less than 15% in our task. Given the intrinsic matrix as below</p><formula xml:id="formula_1">= ? ? ? ? ? ? 0 0 0 0 1 ? ? ? ? ? ? whee</formula><p>and represent the focal lengths of the and axes, respectively. ( , ) is the position of the principal point with respect to the image plane.</p><p>For each image in , the patch set is obtained by the following strategy to ensure that all areas in the scene can be trained:</p><formula xml:id="formula_2">( , ) ? ? , ? + ? , + ?<label>(2)</label></formula><p>where is zoom ratio. It shifts the patch ( ? , ? ) to enhance the synthetic quality of the scene from different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">?</head><p>The point cloud, especially ones from external reconstruction methods (e.g., Metashape <ref type="bibr" target="#b0">[1]</ref> or COLMAP <ref type="bibr" target="#b22">[23]</ref>), often has holes and outliers that degrade the rendering quality. Motivated by MIMO-UNet <ref type="bibr" target="#b3">[4]</ref>, rendering network ? is proposed to synthesize the novel view from sparse point clouds, which consists of three parts.</p><p>Given the sparse point cloud , the purpose of the rendering network is to learn reliable neural descriptors to represent scenes. However, neural descriptors learned from point clouds still have holes. To deal with this problem, we design a basic gate module to filter the neural descriptors of different scales, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>By taking into consideration the efficiency, we firstly employ 3 ? 3 convolution layers to extract the feature of neural descriptor . A mask is learned by the sigmoid function to filter the invalid values in the neural descriptor. The output is the value range of (0,1), which represents the importance of features in the neural descriptor. To improve the learning efficiency, we employ the ELU activation function for the neural descriptor. ? denotes the element-wise multiplication. We concatenate (?) the initial feature with the filtered one as a new feature. Finally, we use an additional 1 ? 1 convolution layer to further refine the concatenated features. In addition, Gate convolution <ref type="bibr" target="#b32">[33]</ref> is introduced to re-filter the fused features. 3.3.1 Fusing features at different scales. The lack of topology information in the point cloud leads to holes and bleeding. Given the feature of the neural descriptor with holes, as shown in the red box in <ref type="figure" target="#fig_2">Fig. 4</ref>. Although has a higher resolution with fine details, it still suffers from larger surface bleeding. For feature blocks in with no values, rough values can be obtained after average pooling. +1 has a low resolution, however, it is able to reduce surface bleeding. For the sparse point cloud, fusing features at two different scales still cannot completely fill the hole. Therefore, we suggest ? to fuse multi-scale features. In our model, we use neural descriptors of four scales in order to achieve the tradeoffs between efficiency and accuracy. Through fusing features at different scales, our proposed model learns the missing points in the sparse point cloud from the data, so as to synthesize a realistic novel view of the sky, distant objects, etc. Instead of using transpose convolution, we employ bilinear interpolation in upsampling phase. This is because transpose convolution is basically learnable upsampling, and learnable parameters incur the extra computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Fusing features at the same scale.</head><p>In our presented method, the feature of neural descriptor + is obtained from by an average pooling operation. The down-sampled feature of neural descriptor + concatenates itself with the last 1 ? 1 layer feature, which is used for detail enhancement to retain the lost information. At the same time, the feature with the size of 2 ? 2 is obtained by the neural descriptor of the gate module using the gate convolution. The fusion of and features at the same scale can make use of complementary information between features, where the fusion feature is = + ? , as shown in the red circle of <ref type="figure" target="#fig_2">Fig. 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scene Editing and Stitching</head><p>As our proposed model learns neural descriptors from point clouds to represent scenes, the scene can be edited by changing the trained neural descriptors. Therefore, we can move the cars, people, and houses in the scene at will. Or even simulate the extreme scenarios, like a car going the wrong way is about to crash.</p><p>As shown in <ref type="figure" target="#fig_3">Fig.5, {(</ref> </p><formula xml:id="formula_3">, , ) | ? ( , ), ? ( , ), ? (</formula><p>, )} represent the range of a car in the point clouds. Through back propagation, we employ a rendering network with learnable parameter to project all the neural descriptors onto the RGB image , = pr ( , , , ) ,</p><p>(3) where , is the projected 2D image, and pr denotes the projection and rasterization process. We synthesize the novel view of car ? , ? = pr ( ? , ? , , ) via changing its position of car ? = ( ? , ? , ? ). By taking advantage of scene editing, we can not only move objects in the scene, but also remove dynamic objects, so as to obtain more diverse driving scenes, as shown in <ref type="figure" target="#fig_5">Fig.7</ref>.</p><p>To account for the large scale driving scene, we propose a scene stitching method that is able to concatenate multiple scenes and update a block locally. For coordinates at the boundary of scene 1 ( 1 , 1 , 1 ), the boundary coordinates of scene 2 ( 2 , 2 , 2 ) need to be stitched. We firstly rotate the point clouds ( 1 , 2 ) of the two scenes so that they are aligned on a coordinate system at the boundary. The feature descriptors ( 1 , 2 ) represent the texture of their scenes after being trained by our rendering network. Then, 1 and 2 are stitched at the boundary to update the scene. The new scene is ( 1 , 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Loss function.</head><p>Perceptual loss <ref type="bibr" target="#b10">[11]</ref>, also known as VGG loss, can effectively reflect the image quality of perception more than other loss functions. Thus, we employ the perceptual loss function to prevent smoothing the high-frequency details while encouraging color preservation. Specifically, we compute the perceptual loss between the synthetic novel view and ground truth image , which is calculated by a pretrained VGG layer ? as follows:</p><formula xml:id="formula_4">( , ) = ?? =1 (? ( ) ? ? ( ( , , ))) ,<label>(4)</label></formula><p>where denotes the randomly cropped patches. Given point cloud and camera parameters , our driving scene render learns the neural descriptors and network parameters .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>To fairly compare the qualitative and quantitative results of various methods, we conduct the experiments on Nvidia GeForce RTX 2070 GPU and evaluate our proposed approach on the two datasets for autonomous driving. To reduce memory usage to load the point cloud of the entire large-scale scene, all comparison methods use the sparse point cloud optimized by our method as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>KITTI Dataset KITTI Dataset KITTI Dataset <ref type="bibr" target="#b7">[8]</ref>: KITTI is a large data set of real driving scenarios, which contains rich scenes. We mainly conducted experiments in three different cases, namely Residential, Road and City, covering 3724, 996, and 1335 meters, respectively. Due to the overlapping parts, we finally adopted 3560 frames, 819 frames, and 1584 frames in Residential, Road, and City scenes as the testbed. We evaluated every 10 frames (e.g., frame 0, 10, 20...) by following the training and testing split of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. The rest of the frames are used for training. To demonstrate the effectiveness of our method, we conducted the more challenging experiment by discarding 5 test frames before and after every 100 frames as test frames. As the car is driving at a fast speed, losing 10 frames may lose a lot of scene information, especially at corners.</p><p>Brno Urban Dataset Brno Urban Dataset Brno Urban Dataset <ref type="bibr" target="#b14">[15]</ref>: Compared to KITTI's single-view trajectory, the Brno Urban Dataset contains four views, including left, right, left-front and right-front. In this paper, we use 1641 frames of driving images in our experiments. The left view, the left-front view, and the right view are used as a set of data using similar evaluation criteria as KITTI.  <ref type="bibr" target="#b24">[25]</ref>, NRW <ref type="bibr" target="#b16">[17]</ref>, NPBG <ref type="bibr" target="#b1">[2]</ref> and ADOP <ref type="bibr" target="#b21">[22]</ref>, our approach performs the best in cases of pedestrians, vehicles, sky, buildings and road signs. Please zoom in for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on the KITTI Testing Set</head><p>Since point clouds from external reconstruction methods such as MetaShape <ref type="bibr" target="#b0">[1]</ref> often contain holes and outliers, the quality of rendering is usually degraded. Moreover, the sparse point cloud as input brings a great challenge to scene synthesis.</p><p>To demonstrate the efficacy of our method, we compare it against the recent image-to-image translation based and neural point-based approaches, including DAGAN <ref type="bibr" target="#b24">[25]</ref>, NRW <ref type="bibr" target="#b16">[17]</ref>, NPBG <ref type="bibr" target="#b1">[2]</ref> and ADOP <ref type="bibr" target="#b21">[22]</ref>, which have achieved promising results in outdoor scene synthesis. Followed by the above methods, we employ Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) and perceptual loss (VGG loss) as the evaluation metrics. To facilitate the fair comparison, we also adopt the perception metric and learned Perceptual Image Patch Similarity (LPIPS) in our evaluation.</p><p>Tang et al. <ref type="bibr" target="#b24">[25]</ref> propose a Novel Dual Attention GAN (DAGAN) algorithm, which can effectively model semantic attention at spatial and channel dimensions, thus improving the feature representation ability of semantic images. The GAN-based method can synthesize reasonable semantic images, however, there is a gap with the real scene texture, and its performance is weak in various metrics.</p><p>NRW <ref type="bibr" target="#b16">[17]</ref> renders the points to the deep frame buffer and learns the mapping from the initial rendering to the actual photo by training the neural network. Besides point clouds, camera views, and paired poses, NRW needs the extra depth maps for supervision, which are obtained by Agisoft Metashape <ref type="bibr" target="#b0">[1]</ref>. Although having obtained good metrics in KITTI dataset, it does not perform well in the texture with details, as shown in <ref type="figure" target="#fig_4">Fig.6</ref>.</p><p>NPBG <ref type="bibr" target="#b1">[2]</ref> is one of the baseline methods. To facilitate fair comparisons, we use the same parameters, such as the number of neural descriptors and the number of layers of the network structure, etc. We evaluate the effectiveness of our method in detail by comparing ablation with NPBG.</p><p>ADOP <ref type="bibr" target="#b21">[22]</ref> proposes the differentiable render that can optimize the parameters of camera pose, intrinsic, or texture color. However, it has complicated scene parameters of pipeline and is a two-stage rendering network, per image exposure and per image white balance needs to be manually adjusted. ADOP has difficulty in sparse point clouds, which tends to synthesize the blurred images in the area of point clouds with holes.</p><p>In our proposed approach, we initialize the driving scene through point cloud in four datasets and use neural descriptors to encode the geometry and appearance. At each iteration, we sample ten target patches with the size of 256 ? 256 in KITTI Dataset. Due to the high image resolution of the Brno Urban Dataset, the patch with the size of 336 ? 336 is used for training. In Monte Carlo sampling, we set the sampling ratio to 80%. <ref type="table" target="#tab_0">Table 1</ref> shows the results of our method. It can be seen that our proposed approach is significantly better than the previous method on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on the Brno Urban Testing Set</head><p>Unlike the KITTI dataset, the Brno Urban dataset is very challenging with three views.</p><p>In the evaluation, we test the left side view, left front side view, and right side view, separately. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our proposed approach is significantly better than the other methods while DA-GAN obtains slightly better LPIPS metrics in the 100 frame test. The results of NRW's side view are similar to ours. This is due to the faster car speed and the narrower side view, resulting in significantly different images between frames. As the distributions of training and test sets are different, the methods based on point cloud rendering are seriously affected. NRW relies on additional segmentation annotations to synthesize the unfamiliar scenes, and the image is projected onto the point cloud to initialize the deep buffer. The method using the GAN generator can obtain reasonable images for the missing regions of the point cloud. Therefore, DA-GAN and NRW achieve a slight improvement in the case of fewer input images. However, such methods are limited by synthesizing novel scenes from different views. In addition, ADOP uses a differentiable rendering pipeline to align camera images to point clouds, the right side view synthesized results are similar to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In the ablation experiment, we examine the effectiveness of each module, and more results are given in the supplementary materials. For a fair comparison, we have added the sampling strategy proposed in Section 3.2.3 to the NPBG as our baseline, namely sampling NPBG, as shown at the first line in <ref type="table" target="#tab_2">Table 3</ref>. Then, we gradually add each module mentioned in Section 3 and evaluate them in the KITTI Road scenario. In contrast to sampling NPBG, our proposed basic gate module can effectively filter the invalid values in neural descriptors, which obtains significant improvement in PSNR, SSIM, and other metrics. By fusing the features at the same scale (Same), the texture of the scene is enhanced with fine details. The fusion of different scale features modules (Differ) can effectively fill the value of neural descriptors close to zero. All metrics are greatly improved over the baseline. We also study the influence of different loss functions. It can be observed that combining 1 , PSNR Loss and VGG Loss can improve the SSIM index slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Driving Scene Editing</head><p>Editing the driving scenarios not only provides more synthetic data for Advanced Driver Assistance Systems, but also simulates the rare traffic conditions in daily life, i.e., a car driving illegally on the wrong side of the road. Moreover, our proposed approach can remove the dynamic objects in the scene so that data collection staff do not need to worry about the impact of complex traffic and vehicles on the restoration of the real scene. This provides convenience for data collection. Additionally, the panoramic view can be synthesized through our method. The larger field-of-view provides more street view information for Driver Assistance Systems, which makes it easier to observe the surrounding environment and deal with the emergencies in a timely manner, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. More results are presented in the supplementary materials. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Driving Scene Stitching</head><p>By taking advantage of scene stitching, our model is able to synthesize the larger driving scenes and update local areas with obvious changes in road conditions. This not only enables to deal with the driving area at a larger scale, but also divides the large-scale scene into small parts for efficient training in parallel. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, we stitch two KITTI residential scenes, which share the same rendering network and learn the texture from the corresponding part, respectively. <ref type="table" target="#tab_3">Table 4</ref> shows the comparison results on the stitched scenario. It can be seen that decomposing large scenes into small parts can achieve better results. This indicates the effectiveness of our presented stitching method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS</head><p>We propose a multi-scale rendering method that synthesizes the photo-realistic driving scenes from sparse point clouds. However, for images that differ greatly from the training views, for example, in the right view of the Brno Urban data set, 10 frames near the test frame are discarded. As shown in <ref type="figure">Fig.8</ref>. neural rendering-based   methods are difficult to synthesize the scene with few observations, resulting in blur. In addition, misalignment on point clouds affects the rendering results. In the future, we will consider using point </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposed an efficient neural scene rendering approach to autonomous driving, which makes it possible to synthesize largescale scenarios on a PC through a variety of sampling schemes. We presented an ? rendering network to filter the neural descriptors through basic gate modules, and fused features at the same scale and different scales with different strategies. Our proposed READ: Large-Scale Neural Scene Rendering for Autonomous Driving , , <ref type="figure">Figure 8</ref>: Failure cases.</p><p>approach not only synthesized the photo-realistic views, but also edited and stitched the driving scenes. This enables to generate various photo-realistic images to train and test the autonomous driving system. The encouraging experimental results showed that our proposed approach significantly outperforms the alternative methods both qualitatively and quantitatively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our proposed large-scale neural scene Render (READ) for Autonomous Driving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Basic gate module. Neural descriptors learned from sparse point clouds can effectively screen out invalid values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Feature fusion module. Part 1 fuses the features at the same scale, which takes advantage of the complementary information of same scale. Part 2 learns missing points in neural descriptors by fusing the features at different scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The scene stitching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparative results of novel view synthesis on the Residential, Road, City scenes from the KITTI benchmark, and a multiple view scene from the Brno Urban dataset. Comparing to DAGAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Example results of scene editing. We can move and remove the cars in different views. A panorama with larger view can be synthesized by changing the camera parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation of novel view synthesis on three scenes from the KITTI dataset. Ours) 695.3 695.3 695.3 17.70 17.70 17.70 0.2875 0.2875 0.2875 0.5963 0.5963 0.5963 573.5 573.5 573.5 20.26 20.26 20.26 0.2408 0.2408 0.2408 0.6238 0.6238 0.6238 673.2 673.2 673.2 18.35 18.35 18.35 0.2529 0.2529 0.2529 0.6412 0.6412 0.6412 Ours) 454.9 454.9 454.9 22.09 22.09 22.09 0.1755 0.1755 0.1755 0.7242 0.7242 0.7242 368.2 368.2 368.2 24.29 24.29 24.29 0.1465 0.1465 0.1465 0.7402 0.7402 0.7402 391.1 391.1 391.1 23.48 23.48 23.48 0.1321 0.1321 0.1321 0.7871 0.7871 0.7871</figDesc><table><row><cell></cell><cell>KITTI Residential</cell><cell>KITTI Road</cell><cell>KITTI City</cell></row><row><cell></cell><cell cols="3">VGG? PSNR? LPIPS? SSIM? VGG? PSNR? LPIPS? SSIM? VGG? PSNR? LPIPS? SSIM?</cell></row><row><cell></cell><cell></cell><cell>Test every 100 frames (w/ discard)</cell></row><row><cell cols="4">DAGAN [25] 1241.3 11.18 0.4968 0.3081 929.0 15.33 0.3570 0.4135 1301.3 10.74 0.4949 0.3014</cell></row><row><cell>NRW [17]</cell><cell cols="3">923.0 15.70 0.3874 0.4748 860.8 17.01 0.3343 0.4311 1007.0 15.66 0.3847 0.4361</cell></row><row><cell>NPBG [2]</cell><cell cols="3">924.7 14.98 0.4426 0.4733 791.4 17.63 0.3680 0.5080 994.5 14.97 0.4384 0.4518</cell></row><row><cell>ADOP [22]</cell><cell cols="3">900.5 14.89 0.3590 0.4734 785.9 17.56 0.3275 0.4701 910.6 15.67 0.3497 0.4774</cell></row><row><cell cols="3">READ (Test every 10 frames (w/o discard)</cell></row><row><cell cols="4">DAGAN [25] 1031.2 14.27 0.3800 0.4337 847.2 16.84 0.2916 0.4638 1128.8 13.40 0.3971 0.3845</cell></row><row><cell>NRW [17]</cell><cell cols="3">767.4 18.43 0.3197 0.5476 748.0 18.58 0.2809 0.4996 823.7 18.02 0.3102 0.5682</cell></row><row><cell>NPBG [2]</cell><cell cols="3">621.2 19.32 0.2584 0.6316 597.3 20.25 0.2517 0.5919 632.8 19.58 0.2480 0.6277</cell></row><row><cell>ADOP [22]</cell><cell cols="3">610.8 19.07 0.2116 0.5659 577.7 19.67 0.2150 0.5554 560.9 20.08 0.1825 0.6234</cell></row><row><cell>READ (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation of novel view synthesis on Brno Urban Dataset. LPIPS? SSIM? VGG? PSNR? LPIPS? SSIM? VGG? PSNR? LPIPS? SSIM? PSNR? LPIPS? Test every 100 frames(w/ discard) DAGAN [25] 1055.5 13.93 0.3960 0.3960 0.3960 0.3705 754.9 16.95 0.3078 0.5234 1105.1 11.84 0.5323 0.5323 0.5323 0.3561 14.24 0.4120 NRW [17] 919.2 15.25 0.4435 0.4397 712.7 17.87 0.4063 0.5513 949.5 13.49 0.5790 0.4405 15.54 0.4762 NPBG [2] 1002.3 13.14 0.5242 0.3978 724.5 17.13 0.4098 0.5596 1024.4 12.22 0.6634 0.4333 14.17 0.5325 ADOP [22] 997.1 14.08 0.4373 0.3915 683.6 18.24 0.3150 0.5618 1091.2 13.21 0.5531 0.3803 15.18 0.4352 READ (ours) 842.0 842.0 842.0 15.28 15.28 15.28 0.3992 0.4752 0.4752 0.4752 523.9 523.9 523.9 20.51 20.51 20.51 0.2467 0.2467 0.2467 0.6713 0.6713 0.6713 928.0 928.0 928.0 13.88 13.88 13.88 0.5464 0.4533 0.4533 0.4533 16.56 16.56 16.56 0.3974 0.3974 0.3974 Test every 10 frames(w/o discard) DAGAN [25] 851.4 16.67 0.2822 0.4766 657.6 19.08 0.2445 0.5662 1041.6 13.14 0.4514 0.3805 16.30 0.3260 NRW [17] 735.0 18.64 0.3199 0.5422 619.6 19.74 0.3125 0.6062 864.6 16.05 0.4631 0.4749 18.14 0.3651 NPBG [2] 659.4 18.56 0.3112 0.5849 531.6 20.30 0.2705 0.6773 813.1 16.00 0.4424 0.5093 18.28 0.3414 ADOP [22] 634.0 19.19 0.2414 0.5927 520.6 20.83 0.2189 0.6633 807.1 16.51 0.3636 0.5009 18.84 0.2746 READ (ours) 459.8 459.8 459.8 21.79 21.79 21.79 0.1905 0.1905 0.1905 0.7067 0.7067 0.7067 341.1 341.1 341.1 24.85 24.85 24.85 0.1513 0.1513 0.1513 0.7836 0.7836 0.7836 663.6 663.6 663.6 18.44 18.44 18.44 0.3065 0.3065 0.3065 0.5771 0.5771 0.5771 21.69 21.69 21.69 0.2161 0.2161 0.2161</figDesc><table><row><cell>Left side view</cell><cell>Left front side view</cell><cell>Right side view</cell><cell>Total</cell></row><row><cell>VGG? PSNR?</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our method on KITTI road dataset.</figDesc><table><row><cell>Gate</cell><cell>Same</cell><cell>Differ</cell><cell>1</cell><cell cols="2">VGG? PSNR? LPIPS ? SSIM?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>572.1</cell><cell>20.63</cell><cell>0.2359 0.6109</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>477.6</cell><cell>22.29</cell><cell>0.1893 0.6883</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>392.1</cell><cell>23.76</cell><cell>0.1477 0.7205</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>368.2 368.2 368.2</cell><cell>24.29 24.29 24.29</cell><cell>0.1465 0.1465 0.1465 0.7402</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>401.3</cell><cell>24.16</cell><cell>0.1865 0.7487</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>383.3</cell><cell>23.96</cell><cell>0.1506 0.7325</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>401.1</cell><cell>24.19</cell><cell>0.1863 0.7490 0.7490 0.7490</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of scene stitching on KITTI dataset. / stitching 429.3 22.58 0.1625 0.7392 clouds scanned by LiDAR sensor as training data so as to reduce the reconstruction errors.</figDesc><table><row><cell>Method</cell><cell>VGG? PSNR? LPIPS? SSIM?</cell></row><row><cell>READ</cell><cell>454.9 22.08 0.1755 0.7242</cell></row><row><cell>READ w</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Agisoft: Metashape software</title>
		<imprint>
			<date type="published" when="2019-05-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural point-based graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kara-Ali</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Kolos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimedia for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Ching</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo-Won</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Pyo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depthsupervised NeRF: Fewer Views and Faster Training for Free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangle</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lab2pix: Label-adaptive generative adversarial network for unsupervised image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3734" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-End Video Object Detection with Spatial-Temporal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1507" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel scheduling for cyber-physical systems: Analysis and case study on a self-driving car</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoseung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Lakshmanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE international conference on cyber-physical systems</title>
		<meeting>the ACM/IEEE international conference on cyber-physical systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TRANSPR: Transparency ray-accumulating neural 3D scene point renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Kolos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sevastopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1167" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Event-based vision enhanced: A joint detection framework in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1396" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brno Urban Dataset-The New Data for Self-Driving Agents and Mapping Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ligocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludek</forename><surname>Zalud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3284" to="3290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nerf in the wild: Neural radiance fields for unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duckworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7210" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural rerendering in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Meshry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin-Brualla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6878" to="6887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11453" to="11464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot Neural Human Performance Rendering from Sparse RGBD Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haimin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="938" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Abu Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04619</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Enhancing photorealism enhancement. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>R?ckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06635</idno>
		<title level="m">Adop: Approximate differentiable one-pixel point rendering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Monte Carlo sampling methods. Handbooks in operations research and management science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="353" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dual attention gans for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local classspecific and global image-level generative adversarial networks for semanticguided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7870" to="7879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deferred neural rendering: Image synthesis using neural textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2021. iButter: Neural Interactive Bullet Time Generator for Human Free-viewpoint Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ibrnet: Learning multi-view image-based rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computer vision and natural language processing: recent approaches in multimedia and robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peratham</forename><surname>Wiriyathammabhum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imitating arbitrary talking style for realistic audio-driven talking face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishun</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1478" to="1486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SRNet: Spatial Relation Network for Efficient Single-stage Instance Segmentation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mooi Choo</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Auto-msfnet: Search multi-scale fusion network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth quality-inspired feature manipulation for efficient RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="731" to="740" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
