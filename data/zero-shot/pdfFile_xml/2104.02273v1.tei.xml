<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
							<email>jiahao@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches for multi-view multi-person 3D pose estimation explicitly establish cross-view correspondences to group 2D pose detections from multiple camera views and solve for the 3D pose estimation for each person. Establishing cross-view correspondences is challenging in multi-person scenes, and incorrect correspondences will lead to sub-optimal performance for the multi-stage pipeline. In this work, we present our multi-view 3D pose estimation approach based on plane sweep stereo to jointly address the cross-view fusion and 3D pose reconstruction in a single shot. Specifically, we propose to perform depth regression for each joint of each 2D pose in a target camera view. Cross-view consistency constraints are implicitly enforced by multiple reference camera views via the plane sweep algorithm to facilitate accurate depth regression. We adopt a coarse-to-fine scheme to first regress the person-level depth followed by a per-person joint-level relative depth estimation. 3D poses are obtained from a simple back-projection given the estimated depths. We evaluate our approach on benchmark datasets where it outperforms previous state-of-the-arts while being remarkably efficient. Our code is available at the project website. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation has been an active research area in the field of computer vision due to its large number of real-world applications such as human-computer interaction, virtual and augmented reality, camera surveillance, etc. However, 3D human pose estimation for multiple persons from monocular images is an ill-posed and challenging problem due to both the loss of depth information and severe occlusions under a single camera viewpoint. On the other hand, multi-view images captured by multiple cameras provide complementary information of the scene that can be used to effectively alleviate projective ambiguities.</p><p>Unlike its multi-view single person <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26]</ref> 1 https://github.com/jiahaoLjh/PlaneSweepPose</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Back Projection Projection</head><p>Reference View 1 Reference View 2 <ref type="figure">Figure 1</ref>: Our method is based on plane sweep stereo to regress depths for 2D pose detections. 2D poses are backprojected to successive depth planes and warped to reference views for consistency measurement which is utilized for depth regression.</p><p>counterpart, the fusion of information from multi-view images with multiple persons is more challenging since the identity of the 2D poses from each camera view is unknown. Previous works such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref> address this problem in three steps. The 2D poses are first estimated for each camera view independently. Subsequently, the 2D poses from different views that correspond to the same person are identified and grouped together. Finally, the 3D pose of each person is estimated with triangulation or optimization-based pictorial structure models using the set of grouped 2D pose detections from multiple views.</p><p>The establishment of cross-view correspondences is critical for multi-view multi-person 3D pose estimation. Traditional methods use either greedy matching approach <ref type="bibr" target="#b10">[11]</ref> for fast inference speed, or optimization-based approach <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> for better global consistency. Recently, Voxel-Pose <ref type="bibr" target="#b25">[25]</ref> is proposed to jointly solve the challenging crossview matching and 3D pose estimation problems in an object-detection paradigm. Instead of explicitly searching for 2D pose correspondences, VoxelPose projects the 2D pose heatmaps from multiple views to a common 3D space, and performs both 3D pose detection and estimation in the 3D volumetric space. The 3D object detection formulation avoids the explicit cross-view matching step, thus effectively reduces the impact from incorrectly established cross-view correspondences. Despite its effectiveness, several limitations exist for the object-detection-based pipeline: 1) Prior knowledge of the common 3D space dimension according to the multi-camera settings is needed to define the volumetric space for 3D object detection. 2)</p><p>The back-projection of the 2D pose detections on each 3D voxel is not scalable to larger scenes. 3) 3D convolution that is applied to all voxel locations incurs unnecessary heavy computations, especially for large sparse scenes.</p><p>In this work, we present our plane-sweep-based approach for multi-view multi-person 3D pose estimation. Our approach avoids explicit cross-view matching and aggregates multiple views for 3D pose estimation in a single shot. Specifically, we build our framework upon the concept of plane sweep stereo <ref type="bibr" target="#b5">[6]</ref> to estimate the depth for each joint of each person in a target camera view. As illustrated in <ref type="figure">Figure 1</ref>, 2D poses are first back-projected to successive virtual depth planes, and then warped to the respective reference camera views. We measure the cross-view consistency at each depth level, which is then used to regress the depths from standard convolutional neural networks. Our depth regression adopts a two-stage coarse-to-fine scheme. Personlevel depth is first estimated for each 2D pose. Joint-level relative depth with respect to the person-level depth within a much smaller depth range is then regressed for each joint. The two stages can be trained together in an end-to-end manner. During inference, we obtain the 3D poses by backprojecting the 2D poses with the estimated depths. Multiple 3D poses of the same person from different views can be easily merged via a simple distance-based clustering.</p><p>We evaluate our plane-sweep-based framework on three benchmark datasets, i.e., the Campus and the Shelf datasets, and CMU Panoptic dataset, where we outperform existing state-of-the-arts. In addition to the removal of explicit cross-view matching and triangulation compared to the traditional three-step approaches, our method is also more efficient compared to VoxelPose in two aspects: 1) In contrast to VoxelPose that builds voxels in the 3D space, we leverage on the plane sweep algorithm that is proportional to only the number of virtual depth planes. 2) Instead of performing 3D convolution on all voxel locations, we utilize the much faster 1D convolutions for each 2D pose. Furthermore, our method is more generablizable to scenarios with no prior knowledge of the multi-camera settings since only the range of virtual depth planes needs to be pre-defined for each camera view.</p><p>Our contributions in this work are:</p><p>? We present a plane-sweep-based approach to perform multi-view multi-person 3D pose estimation without the need for explicit cross-view matching.</p><p>? Our approach outperforms existing state-of-the-arts on benchmark datasets, while being much more efficient compared to existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review the related works that utilize multiple camera views for 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-view Single-person</head><p>3D human pose estimation from 2D images is an illposed problem due to the loss of depth information in the process of camera projection. Exploiting multi-view images is an effective way to alleviate projective ambiguities since multiple camera viewpoints provide complementary information of the 3D scene. Extensive research has been done for the single-person 3D pose estimation task under the multi-view setting. Qiu et al. <ref type="bibr" target="#b20">[20]</ref> propose to fuse multiple views in the feature space with the epipolar geometry <ref type="bibr" target="#b8">[9]</ref> for more accurate 2D pose estimates. A recursive pictorial structure model is used to reconstruct the 3D pose from the multi-view 2D detections. The idea of explicit fusion is also adopted in <ref type="bibr" target="#b21">[21]</ref> by Remelli et al. They transform the latent features with the known camera extrinsics into a canonical 3D space, where the transformed features from multiple views are then stacked together for joint reasoning. Iskakov et al. <ref type="bibr" target="#b16">[16]</ref> present a learnable triangulation method that learns per-view confidence weights for the standard triangulation <ref type="bibr" target="#b8">[9]</ref>, and a volumetric-based method that aggregates multi-view images and performs the 3D pose estimation in a 3D volumetric space. Weakly-supervised approach <ref type="bibr" target="#b15">[15]</ref> and meta-learning approach <ref type="bibr" target="#b26">[26]</ref> have also been proposed to utilize the multi-view settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-view Multi-person</head><p>Single-person 3D pose estimation with multi-camera settings has achieved satisfying results on benchmark datasets such as Human3.6M <ref type="bibr" target="#b13">[14]</ref>. However, it is much more challenging for the multi-person case. The key difficulty lies in the cross-view matching since the identity of 2D poses from each view is unknown. Early approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref> create a common state space shared by all persons via triangulation of corresponding body joints in pairs of the camera views. A 3D pictorial structure is defined as a graphical model with unary and pairwise potentials, and 3D poses are obtained from inference on the graph with the loopy belief propagation algorithm <ref type="bibr" target="#b3">[4]</ref>. Recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">18</ref>] adopt a multistage pipeline for the multi-view multi-person 3D pose estimation task. The pipeline consists of a cross-view matching step to group 2D poses from different views that correspond to the same person, and a 3D pose estimation step to reconstruct the 3D pose from the clustered 2D poses for each CNN CNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V i r t u a l D e p t h P l a n e s</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person Depth</head><p>Regressor</p><formula xml:id="formula_0">! Joint Depth Regressor ! ("#$) + Score Matrix ? ? !?# Score Matrix ? ? ! ("#$) ?# (a) (b) Figure 2:</formula><p>Overview of our approach. 2D pose estimation is first performed for each camera view. We then use the plane sweep algorithm to aggregate the cross-view consistency score for the target person highlighted with jet colormap. Personlevel depth is regressed first in (a). Joint-level relative depth is then estimated in (b) and combined with the person-level depth to reconstruct the 3D pose.</p><p>person. Kadkhodamohammadi et al. <ref type="bibr" target="#b18">[18]</ref> propose to compute a distance between each pair of 2D poses from different views based on the epipolar constraints, and then find the cross-view correspondences with the lowest distance. Instead of directly performing triangulation, the matched 2D poses from all camera views are stacked together and passed into a regression neural network to estimate the 3D pose. Dong et al. <ref type="bibr" target="#b6">[7]</ref> enhance the cross-view consistency with appearance features. They utilize a person Re-ID network <ref type="bibr" target="#b27">[27]</ref> to get the appearance features for each person. These features are then used to compute the appearancebased distance. They also formulate a convex optimization problem to solve for the optimal correspondence matrix, and use a rank constraint to enforce cycle-consistency. Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose to match cross-view 2D poses by applying the epipolar constraints on feet joints instead of the entire 2D pose. They perform bipartite matching for each pair of views on the pairwise affinities defined on feet joints. A maximum a posteriori (MAP) estimator is adopted for the 3D pose reconstruction. Huang et al. <ref type="bibr" target="#b10">[11]</ref> propose a greedy bottom-up matching approach for 2D pose grouping. Candidate 3D poses are first obtained from triangulation of each pair of 2D poses. These candidate 3D poses form a 3D pose subspace that are then used with a distance-based greedy clustering approach to group the cross-view poses.</p><p>Triangulation with learnable weights inspired by <ref type="bibr" target="#b16">[16]</ref> is applied for each group to obtain the 3D pose estimates.</p><p>The aforementioned methods are multi-stage pipelines, where incorrect correspondences can cause large errors in the subsequent 3D pose estimation step. A recent work, VoxelPose <ref type="bibr" target="#b25">[25]</ref>, presents a novel pipeline that avoids the explicit cross-view matching and performs 3D pose estimation directly from the multi-view input. This work is inspired by the volumetric approach presented in <ref type="bibr" target="#b16">[16]</ref> that generates 3D volumes from 2D detections. To identify multiple persons in the common 3D volumetric space, VoxelPose utilizes a 3D object detection formulation to localize each 3D pose, followed by a per-person 3D pose estimation. VoxelPose shows promising results since cross-view consistency is implicitly enforced in the 3D pose estimation. However, the 3D convolution used on the volumetric space is computationally expensive, thus not scalable for larger scenes.</p><p>In this work, we present our multi-view 3D pose estimation approach. Inspired by plane sweep stereo <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> for dense depth regression, our approach utilizes a pose-aware geometric consistency metric to aggregate multi-view information and performs depth regression for 2D poses without explicitly establishing correspondences. Our approach demonstrates higher 3D pose estimation precision, while being much more efficient compared to previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Our task is to estimate the 3D poses for all persons in a common 3D space from multi-view images captured by a set of synchronized and calibrated cameras. The overview of our framework is shown in <ref type="figure">Figure 2</ref>. We first perform 2D pose estimation for each camera view independently using a top-down multi-person pose estimation approach, e.g., HR-Net <ref type="bibr" target="#b23">[23]</ref>. Subsequently, we perform depth regression for each candidate 2D pose with J joints under a target camera view by utilizing 2D pose detections from multiple reference views. Finally, the 3D poses can be reconstructed from back-projections of the candidate 2D poses with the estimated depths. In this section, we present our multi-view depth regression approach based on plane sweep stereo. A coarse person-level depth regression module is introduced first in Section 3.1, followed by a per-person joint-level relative depth regression module in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Person-Level Depth Regression</head><p>Our framework is inspired by the plane sweep stereo for dense depth estimation. The basic idea of plane sweep stereo is to back-project the target view image to a set of successive virtual depth planes, and then warp these projections to the reference view images so that photometric consistency can be measured to determine the depth of each target view pixel. We adopt the concept of plane sweep in our framework for the person-and joint-level depth regression. In contrast to the dense depth estimation in the standard plane sweep stereo that relies on photometric consistency, we measure a pose-aware geometric consistency for the depth regression of 2D human poses instead. In this section, we present a person-level depth regression module to coarsely localize the depth for each candidate 2D pose. The person-level depth is defined to be the depth of the center hip joint of each person in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multi-View Score Aggregation</head><p>We define a set of D virtual depth planes equally spaced in [d min , d max ] to represent depths in the target camera coordinate frames. We set [d min , d max ] such that the depth range is reasonably large to cover the common 3D space shared by multiple cameras. We empirically set D = 64 depth planes from experiments (c.f . Section 4) in our implementation.</p><p>A candidate 2D pose p in the target view is first backprojected to a virtual depth plane d, and then followed by a projection to a reference view. The projected 2D pose is denoted as q (d) . We then search for the nearest 2D pose r (d) from the set of candidate 2D poses {p } in the reference view by:</p><formula xml:id="formula_1">r (d) = arg min p J j=1 ? (p j , q (d) j ),<label>(1)</label></formula><p>where the function ? (?, ?) measures the distance between two joints in the reference image plane. Subsequently, we generate a score matrix S ? R D?J for the target pose p.</p><p>The score of joint j at depth d measures the alignment of the projected pose q (d) with the matched reference view pose r (d) at joint j. It is computed as:</p><formula xml:id="formula_2">S d,j = exp ? ? (r (d) j , q (d) j ) 2 2 ? ? 2 .<label>(2)</label></formula><p>A small distance between joint r results in a high score of S d,j . This indicates a higher chance for the depth of joint p j to be around d. ? is a hyper-parameter to control the width of the bell curve. <ref type="figure">Figure 2(a)</ref> shows an example of the score matrix. It is a measurement of the pose-aware cross-view geometric consistency and is used for the subsequent depth regression. In cases when multiple reference views are available, we fuse the score matrices computed from all reference views via a weighted averaging, where the confidence of the matched 2D pose r (d) in each reference view obtained from the 2D pose estimator is used as the weight.</p><p>Remark: Note that back-projecting all joints of a 2D pose to the same depth plane and projecting the "flat" 3D pose to the reference view for pose matching is an approximation. However, it does not affect the retrieval of the nearest pose from the reference view in most scenarios and works sufficiently well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Depth Regression</head><p>We treat the score matrix S ? R D?J of a target pose p as a 1D-signal of length D with J feature channels, and utilize a 1D Convolutional Neural Network (1D-CNN) to map it into a depth vector D ? R D . As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(a), we use a simple architecture with residual links that is sufficient to coarsely estimate the person-level depth.</p><p>A soft-argmax operation can be applied on the output depth vector D to obtain the scalar depth valued:</p><formula xml:id="formula_3">d = D i=1 d i ? D i ,<label>(3)</label></formula><p>where d i is the depth of the i th depth layer. Despite its effectiveness on the single-person case <ref type="bibr" target="#b24">[24]</ref>, soft-argmax operation assumes uni-modality of the input distribution, which can fail in multi-person scenarios. To overcome this limitation, we propose to use an adapted "local" soft-argmax instead:  </p><formula xml:id="formula_4">d = i +??1 i=i d i ? D i i +??1 i=i D i , where i = arg max i i +??1 i=i D i .<label>(4)</label></formula><formula xml:id="formula_5">128 ? ()* (c) Dilated Conv ? ? !?# ? ? ! = (%&amp;') ? ? ! ("#$) ?# (%&amp;') ? ? ! ("#$) ?#</formula><p>Remark: Although we aggregate the scores at each depth layer independently, the scores at successive depth layers exhibit smooth variation (see the score matrix in <ref type="figure">Figure 2</ref>(a) for an illustration). 1D-CNN across the depth dimension effectively aggregates local features over all J joints at successive depth layers, facilitating the regression of the coarse person-level depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint-Level Relative Depth Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Score Aggregation</head><p>After the coarse localization of each 2D pose with the regressed person-level depth, we adopt a fine-grained jointlevel relative depth regression module to estimate the perjoint relative depth with respect to the person-level depth.</p><p>Similar to the person-level depth regression module in Section 3.1.1, a score matrix S (rel) is aggregated from the reference views for each joint j at each relative depth layer d (rel) :</p><formula xml:id="formula_7">S (rel) d (rel) ,j = exp ? ? (r (d+d (rel) ) j , q (d+d (rel) ) j ) 2 2 ? ? 2 . (6)</formula><p>The key difference is that we use a different set of D (rel) virtual depth planes in the range of [?1000, +1000]mm, which is sufficient to cover the depth range of arbitrary pose variation. D (rel) is also set to 64 in our implementation. Note thatd is the estimated person-level depth from Equation 4. During training, we use the ground truth person-level depth d * in place ofd in Equation 6 to stabilize the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Depth Regression</head><p>We use another 1D-CNN to regress the per-joint relative depth from the score matrix S (rel) . The network structure is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). Since the joint-level depth planes compactly surround each target person, it is expected to see more widely-spread peaks in the score matrix (See <ref type="figure">Figure  2(b)</ref> for an illustration of the score matrix). Consequently, compared to the person-level depth regression, a larger receptive field along the depth dimension is needed for the joint-reasoning of the depths for all body joints. To this end, we use a series of 1D dilated convolutions to effectively increase the receptive field as illustrated in <ref type="figure" target="#fig_1">Figure   3</ref>(c). The output of the network is the relative depth matrix D (rel) ? R D (rel) ?J . The relative depth of each joint is obtained by the standard soft-argmax operation:</p><formula xml:id="formula_8">d (rel) j = D (rel) i=1 d (rel) i ? D (rel) i,j ,<label>(7)</label></formula><p>where d (rel) i is the relative depth of the i th depth layer. Similarly, the L 1 loss between the regressed relative depthd (rel) and the ground truth relative depth d (rel) * is minimized:</p><formula xml:id="formula_9">L joint = p j ||d(p) (rel) j ? d * (p) (rel) j || 1 .<label>(8)</label></formula><p>During inference, the absolute depth of each joint is com-</p><formula xml:id="formula_10">puted by:d (abs) j =d +d (rel) j ,<label>(9)</label></formula><p>which is then used to back-project the 2D pose to produce the final 3D pose estimate.</p><p>Remark: Our framework adopts a coarse-to-fine scheme to decouple the task into a person-level depth regression and a per-person joint-level relative depth regression. The benefit of utilizing a two-stage scheme is that we can reduce the computational cost by using a sparse set of virtual depth layers in the first stage for a coarse depth regression from a larger depth range. Since the joint-level relative depth regression is able to compensate for small person-level depth offsets, the person-level depth in the first stage does not need to be very precise to achieve an accurate final 3D pose estimation. The two-stage practice is also widely used in object detection pipelines such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Details</head><p>In view of the limited availability of multi-view multiperson 3D pose annotations, we follow the practice in <ref type="bibr" target="#b25">[25]</ref> to use synthesized data in training both the person-level and joint-level depth regression modules. Specifically, we utilize 3D pose skeletons from a MoCap dataset and randomly place them in a pre-defined 3D space. The 3D poses are projected to 2D poses under each camera view which serve as the input to our depth regression modules. We randomly perturb the image coordinates of 2D poses in different views to simulate the scenario of in-precise 2D pose estimation. Confidence score is assigned to each 2D joint based on the level of random perturbation.</p><p>During inference, we use the 2D pose estimator HRNet <ref type="bibr" target="#b23">[23]</ref> to obtain candidate 2D poses. We take each camera view as the target view in turn to generate 3D pose estimates given the depth estimation under that particular view. 3D poses from all camera views are fused into the same global coordinate space. Duplicates can be effectively removed by clustering and averaging nearby 3D poses given a distance threshold.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Campus <ref type="bibr" target="#b0">[1]</ref>. The Campus dataset captures an outdoor environment with three persons interacting with each other using three cameras. Due to the incomplete annotation of 3D ground truth poses, we directly use HRNet <ref type="bibr" target="#b23">[23]</ref> pre-trained on COCO <ref type="bibr" target="#b19">[19]</ref> to estimate the 2D poses and train our depth regression modules with synthesized 3D MoCap poses. We follow previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">25]</ref> and perform evaluation on the test set frames: 350-470, 650-750.</p><p>Shelf <ref type="bibr" target="#b0">[1]</ref>. The Shelf dataset captures an indoor environment with four persons interacting with each other using five cameras. Similar to the Campus dataset, we use pretrained HRNet to estimate 2D poses and only train our depth regression modules with synthesized data. We follow previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">25]</ref> in evaluating only three of the four persons on the test set frames: 300-600 since one person is occluded in majority of the frames.</p><p>CMU Panoptic <ref type="bibr" target="#b17">[17]</ref> The dataset captures an indoor environment with multiple actors performing social activities. Following <ref type="bibr" target="#b25">[25]</ref>, we use HRNet pre-trained on COCO and fine-tuned on Panoptic to obtain 2D poses. We use the same set of training and testing sequences captured by the same set of HD cameras <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23)</ref> as in <ref type="bibr" target="#b25">[25]</ref> for evaluation.</p><p>Evaluation metrics. Following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">25]</ref>, we use the Percentage of Correctly estimated Parts (PCP) to evaluate the accuracy of the estimated 3D poses for the Campus and the Shelf datasets. Specifically, the closest estimated 3D pose is selected to evaluate the correctness of each body part for each ground truth pose. To better understand the performance of the person-level depth regression module, we also evaluate the recall rate of the person-level depth at various error thresholds. Since previous works share no common evaluation protocol on the Panoptic dataset, we follow the evaluation process in VoxelPose <ref type="bibr" target="#b25">[25]</ref> and report the Average Precision (AP) and Mean Per Joint Position Error (MPJPE).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>"Local" Soft-argmax. We first justify the use of a "local" soft-argmax operation (c.f . Equation 4). A direct comparison between using a standard soft-argmax and our "local" soft-argmax is shown in <ref type="table" target="#tab_2">Table 1</ref>. The "local" soft-argmax is able to focus on the mode with the highest response without being interfered by other modes in a multi-modality distribution. From <ref type="table" target="#tab_2">Table 1</ref>, we see obvious improvement when using our "local" soft-argmax operation on both the Campus and the Shelf datasets. The performance gap is large especially for the Campus dataset, where each camera view typically consists of 2 to 3 persons.</p><p>We then conduct ablation studies to evaluate our approach under various settings. The performance of 3D pose estimation on the Shelf dataset measured in PCP is reported in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Number of virtual depth planes. We use D = 64 person-level depth planes and D (rel) = 64 joint-level depth planes in our implementation by default. In this ablation study, we examine the results when fewer depth planes are used in each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Campus (fps) Shelf (fps)</head><p>VoxelPose <ref type="bibr" target="#b25">[25]</ref> 5.5 3.0 Ours 110.0 42.8 <ref type="table">Table 3</ref>: Comparison of inference speed (frames per second) with <ref type="bibr" target="#b25">[25]</ref> on the Campus and the Shelf datasets.</p><p>We first compare the person-level depth regression performance by evaluating the recall rate of the center hip joint with respect to various distance thresholds. The results of D = 16 and 64 are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Using more virtual depth planes in general increases the person-level depth estimation precision. <ref type="table" target="#tab_4">Table 2</ref>(a) and (b) also show that the 3D pose estimation accuracy drops by 0.4% when reducing D from 64 to 16.</p><p>The impact from using fewer depth layers is small in the person-level stage since regressing a coarse person-level depth is sufficient for rough 3D localization. In comparison, the joint-level stage requires more precise depth regression in order to estimate the 3D pose accurately. <ref type="table" target="#tab_4">Table 2</ref>(a) and (c) show the comparison between using 16 and 64 jointlevel depth planes. Reducing D (rel) from 64 to 16 leads to a larger 0.7% performance drop, which is due to that using fewer depth planes increases the quantization error. <ref type="table" target="#tab_4">Table 2</ref>(a) and (d)-(f), we compare the performance of our approach when different number of cameras is used. The performance drops with reducing number of cameras. This is as expected since the indoor dataset exhibits severe occlusions, which can be ambiguous even for multi-camera settings. Nonetheless, our approach still achieves over 92% accuracy in the extreme cases of using only two cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of cameras. In</head><p>Generalization to different camera settings. In this setting, we use randomly sampled camera viewpoints during training and use the 5 cameras from the dataset for evaluation. We can see from the result in <ref type="table" target="#tab_4">Table 2</ref>(g) that our method is able to perform equally well compared to (a) when trained and evaluated on different sets of cameras. This demonstrates the generalization ability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inference Speed</head><p>We compare the computational efficiency of our work with VoxelPose <ref type="bibr" target="#b25">[25]</ref> since both works address the multiview 3D pose estimation problem in a learning-based framework without explicit cross-view matching. We perform inference for both methods using the same set of 2D pose estimates on a single GTX 1080 Ti graphics card. The inference runtime for the evaluation on both the Campus and the Shelf datasets is reported in <ref type="table">Table 3</ref>. Note that the runtime for 2D pose estimation is not included. Our method achieves a frame rate that is up to 20x faster than the  <ref type="table">Table 4</ref>: Comparison of PCP with existing multi-view multi-person 3D pose estimation methods on the Campus and the Shelf datasets.   VoxelPose method, which can be used to fully support realtime inference. Our advantage in computational efficiency is mainly due to our method focusing on only the target 2D poses and uses more computationally affordable 1D convolutions. In contrast, VoxelPose performs expensive 3D convolutions for each voxel in the 3D space that incurs unnecessary computations in regions without any person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to the State-of-the-arts</head><p>We report the 3D pose estimation accuracy on the Campus and the Shelf datasets in <ref type="table">Table 4</ref>. The average accuracy improves from 96.7% to 97.0% on the Campus dataset, and from 97.4% to 97.9% on the Shelf dataset. <ref type="table" target="#tab_7">Table 5</ref> further shows that our method decreases the error by ?1mm on Panoptic dataset when compared to VoxelPose <ref type="bibr" target="#b25">[25]</ref>. Our method shows decent performance improvement in addition to being able to implicitly solve the challenging multi-view matching problem neatly. Note that among the existing works shown in <ref type="table">Table 4</ref>, VoxelPose <ref type="bibr" target="#b25">[25]</ref> and our method utilize only geometric consistency based on multi-view epipolar geometry. Although photometric consistency is not considered, robust performance can still be achieved with 2D poses from the current top-performing 2D pose estimator. Examples of qualitative results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present our plane-sweep-based approach to regress 2D pose depths for the task of multiview multi-person 3D pose estimation. Our method uses the plane sweep algorithm to aggregate multi-view information based on a pose-aware geometric consistency and effectively estimates the depths for each 2D pose in a target camera view without explicitly establishing cross-view correspondences. Depth regression is performed in a coarse-tofine scheme, where we first regress the person-level depth followed by the joint-level relative depth estimation. Our framework is computationally more efficient and shows superior performance compared to previous state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Network structure of (a) the person-level depth regression network, (b) the joint-level depth regression network, and (c) the dilated convolution module used in (b). Each block "Conv -k" consists of a 1D convolution with kernel size k followed by the batch normalization<ref type="bibr" target="#b12">[13]</ref> and ReLU operations. The numbers in A ? B denote the channel size. Residual links are used in both networks.A window of size ? slides over D to search for the window with the largest response from D. Standard soft-argmax is then computed within the window to obtaind. We use a window size ? = D/4 = 16 in our implementation. Note that when the window size is equal to the length of the input signal, i.e., ? = D, Equation 4 degenerates to the standard soft-argmax operation.The network is trained by minimizing the L 1 loss between the regressed depthd and the ground truth personlevel depth d * over all 2D poses {p} in the target view: L pose = p ||d(p) ? d * (p)|| 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of person-level recall when different number of depth planes is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the Shelf dataset. (Left) Images with projections of the estimated 3D poses in each camera view. (Right) 3D pose estimates in solid lines and 3D ground truth poses in dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of PCP between the soft-argmax operations used in the person-level depth regression on the Campus and the Shelf datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the Shelf dataset. D and D (rel) are the number of depth layers in the person-level and jointlevel depth regression modules, respectively. + means different sets of cameras are used for training and evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Method AP 25 AP 50 AP 100 AP 150 MPJPE VoxelPose [25] 83.59 98.33 99.76 99.91 17.68mm Ours 92.12 98.96 99.81 99.84 16.75mm</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with [25] on CMU Panoptic dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
	<note>Slobodan Ilic, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gim Hee Lee, and Gregory Chirikjian. Multi-person 3d pose estimation in crowded scenes based on multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multiimage matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="358" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Sanaei</surname></persName>
		</author>
		<title level="m">Multiple human 3d pose estimation from multiview images. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="15573" to="15601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end dynamic matching network for multiview multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzhentao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Traish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard Yi Da</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dpsnet: End-to-end deep plane sweep stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weaklysupervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5243" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7718" to="7727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="190" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolrahim</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lightweight multi-view 3d pose estimation through camera-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6040" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voxelpose: Towards multi-camera 3d human pose estimation in wild environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Metafuse: A pre-trained fusion model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongchang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13686" to="13695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
