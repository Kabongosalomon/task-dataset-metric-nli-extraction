<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2022 TAPEX: TABLE PRE-TRAINING VIA LEARNING A NEURAL SQL EXECUTOR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<email>qian.liu@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University, ? Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
							<email>bei.chen@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Ziyadi</surname></persName>
							<email>morteza.ziyadi@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
							<email>zeqi.lin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Azure AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2022 TAPEX: TABLE PRE-TRAINING VIA LEARNING A NEURAL SQL EXECUTOR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pretraining can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and highquality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained language models (LMs) such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> and BART <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref> have hit a success on a range of free-form natural language (NL) tasks. By learning from a large amount of unstructured textual data, these models have demonstrated surprising capabilities in understanding NL sentences. Inspired by this huge success, researchers have attempted to extend pre-training to structured tabular data <ref type="bibr">(Herzig et al., 2020;</ref><ref type="bibr" target="#b40">Yin et al., 2020;</ref><ref type="bibr" target="#b42">Yu et al., 2021a;</ref><ref type="bibr" target="#b38">Wang et al., 2021b;</ref><ref type="bibr" target="#b5">Deng et al., 2020;</ref><ref type="bibr" target="#b28">Shi et al., 2021a)</ref>. However, different from free-form NL sentences, tabular data often contains rich and meaningful structural information, for which existing pre-training approaches designed for unstructured data are not well suited.</p><p>To apply pre-training techniques on structured tabular data, there exist two key challenges: (i) where to obtain a large-scale pre-training corpus with high quality, and (ii) how to design an efficient pretraining task for table pre-training. For the first challenge, existing works generally collect parallel data including NL sentences and tables as the pre-training corpus, since downstream tasks often involve a joint reasoning over both free-form NL sentences and tables. They either crawled tables and their surrounding NL sentences from the Web <ref type="bibr">(Herzig et al., 2020;</ref><ref type="bibr" target="#b40">Yin et al., 2020;</ref><ref type="bibr" target="#b6">Deng et al., 2021)</ref>, or synthesized NL sentences on available tables <ref type="bibr" target="#b42">(Yu et al., 2021a;</ref><ref type="bibr" target="#b28">Shi et al., 2021a)</ref>. However, as pointed by <ref type="bibr" target="#b40">Yin et al. (2020)</ref>, the raw data mined from the Web is extremely noisy and requires complicated heuristics to clean. Conversely, the synthesis method is easier to control the data quality, but it usually requires experts to write hundreds of templates, which is both costly and often lacking diversity. Regarding the pre-training task, existing works often employ different variants of Masked Language Modeling (MLM) <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> to guide LMs to learn better representations of Input: Greece held its last Summer Olympics in which year? <ref type="table">[Table]</ref> Output: 2004 <ref type="bibr">Fine-tuning</ref> Fine-tuned LM for Downstream Task <ref type="figure">Figure 1</ref>: The schematic overview of our method. For the sake of brevity, the table content in the input is simplified with the symbol <ref type="table">[Table]</ref>. tabular data. For example, TAPAS <ref type="bibr">(Herzig et al., 2020)</ref> used MLM with whole word masking, and TABERT <ref type="bibr" target="#b40">(Yin et al., 2020)</ref> proposed Masked Column Prediction (MCP) to encourage the model to recover the names and data types of masked columns. Despite their success, they still largely treat tabular data as a structural format of text, which leads to the need of an extremely large corpus for their table pre-training. All of these hinder the progress of table pre-training.</p><p>In this paper, we present a novel execution-centric table pre-training approach TAPEX (TAble Pretraining via EXecution). It addresses the above challenges and achieves efficient table pre-training via approximating the structural reasoning process of formal languages over tables. The structural reasoning process is associated with the executability of tables, i.e., tables are inherently capable of supporting various reasoning operations (e.g., summing over a column in the table). In particular, TAPEX approximates the structural reasoning process of SQL queries by pre-training LMs to mimic the behavior of a SQL execution engine on tables. As shown in <ref type="figure">Figure 1</ref>, by sampling executable SQL queries over tables, TAPEX first synthesizes a large-scale pre-training corpus. Then it continues pre-training a language model to output the execution results of these SQL queries, which are obtained from the SQL execution engine. Since the diversity of SQL queries can be systematically guaranteed, we can easily synthesize a diverse, large-scale, and high-quality pre-training corpus.</p><p>Our key insight is that if a language model can be pre-trained to faithfully "execute" SQL queries and produce correct results, it should have a deep understanding of tables. Thus, the execution pre-training task could be more efficient in understanding tables and reasoning over tables. To our knowledge, TAPEX is the first one to explore table pre-training via synthetic executable programs.</p><p>TAPEX is conceptually simple and easy to implement. In this paper, we regard the pre-training as a sequence generation task and employ an encoder-decoder model. Specifically, we employ the pre-trained encoder-decoder language model BART <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref> as the backbone. Furthermore, we examine the effectiveness of TAPEX via two fundamental downstream tasks: table-based question answering (TableQA) and table-based fact verification (TableFV). To enable fine-tuning of downstream tasks to take full advantage of TAPEX, we reformulate these tasks using the encoderdecoder sequence generation paradigm. We evaluate TAPEX using four well-known benchmark datasets. Experimental results clearly demonstrate that TAPEX can bring significant and consistent improvements on these datasets. For example, TAPEX obtains an absolute improvement of 19.5% over BART in the WIKITABLEQUESTIONS dataset. Furthermore, TAPEX yields strong results even with a small pre-training corpus, demonstrating its high efficiency. Finally, TAPEX achieves new state-of-the-art results on all experimental benchmarks, outperforming previous approaches by a large margin, including complicated table pre-training approaches with several heuristics in data processing. We will make our code, model, and data publicly available to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FINE-TUNING ON DOWNSTREAM TASKS</head><p>Before diving into the details of our proposed table pre-training, we start by describing how to tackle downstream task fine-tuning with the encoder-decoder sequence generation paradigm. In this section, we first present the background of two <ref type="table">fundamental table related downstream tasks: table-</ref>based question answering (TableQA) and table-based fact verification (TableFV). Then we elaborate on our generative fine-tuning method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning</head><p>Who is the other person who is 24 years old besides Reyna Royo ?  <ref type="figure">Figure 2</ref>: The illustration of the fine-tuning procedure in our method. During fine-tuning, we feed the concatenation of an NL sentence and its corresponding table taken from the downstream task to the model, and train it to output the answer (e.g., "Marisela Moreno Montero").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DOWNSTREAM TASK FORMULATION</head><p>As mentioned in ? 1, downstream tasks always involve joint reasoning over free-form NL sentences and tables. Therefore, examples of downstream tasks generally contain an NL sentence x and a (semi-)structured table T as the model input. Each NL sentence consists of K tokens as</p><formula xml:id="formula_0">x = x 1 , x 2 , ? ? ?, x K , while each table T consists of M rows {r i } M i=1 , in which each row r i contains N cell values {s i,j } N j=1</formula><p>. Each cell s i,j includes a list of tokens and corresponds to a table header c j . As for the output, there are variations among different tasks. In this paper, we focus on TableQA and TableFV. TableQA aims to retrieve table content to answer the user's question, and thus its output is either a list of cell values or number(s) calculated over the selected table region by aggregation functions (e.g., SUM). It is worth noting that for semi-structured tables, the answer may not be exactly table cell values, but their normalized forms (e.g., from 2k to 2,000), which makes downstream tasks more challenging <ref type="bibr" target="#b24">(Oguz et al., 2020)</ref>. As for TableFV, the output is a binary decision entailed or refused, indicating whether the NL sentence follows the fact indicated by the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GENERATIVE FINE-TUNING</head><p>In this section, we present a generative approach for downstream task fine-tuning. Unlike previous works, we model both TableQA and TableFV as sequence generation tasks and leverage generative LMs to generate the output autoregressively. Taking TableQA as an example, given an NL question, our method generates the answer by decoding it in a word-by-word fashion.</p><p>Architecture Our method theoretically applies for any LM as long as it can generate sequence, such as GPT3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> and UniLM <ref type="bibr" target="#b1">(Bao et al., 2020)</ref>. In our experiments, we implemented our method based on BART <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref>, a widely used pre-trained encoder-decoder model. BART follows a standard sequence-to-sequence Transformer architecture <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>, with modifying ReLU activation functions to GeLU. It is pre-trained via corrupting sentences (i.e., randomly sampling length-variable spans and masking each one with a single [MASK] token) and then optimizing a reconstruction loss. As for the number of layers, we employ the BART Large configuration in our experiments, i.e., 12 layers are used in both the encoder and the decoder.</p><p>Model Input As illustrated in <ref type="figure">Figure 2</ref>, the input contains an NL sentence and its corresponding table. Encoding the NL sentence is relatively straightforward, while encoding the table is non-trivial since it exhibits underlying structures. In practice, we flatten the table into a sequence so that it can be fed directly into the model. By inserting several special tokens to indicate the table boundaries, a flattened table can be represented as T * = [HEAD], c 1 , ? ? ?, c N , [ROW], 1, r 1 , [ROW], 2, r 2 , ? ? ?, r M . Here <ref type="bibr">[HEAD]</ref> and <ref type="bibr">[ROW]</ref> are special tokens indicating the region of table headers and rows respectively, and the number after <ref type="bibr">[ROW]</ref> is used to indicate the row index. Notably, we also separate headers or cells in different columns using a vertical bar | . Finally, we prefix the flattened table T * with the NL sentence x and feed them into the model encoder.</p><p>Model Output With attending on the encoder, the decoder is responsible for modeling the outputs of both TableQA and TableFV. For TableQA, the output is the concatenation of the answer(s) separated by commas, and the decoder generates it autoregressively. In this way, our model can readily support (almost) all operators and their compositions in TableQA. For TableFV, as BART does for sequence classification tasks <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref>, the same input is fed into both the encoder and decoder, and a binary classifier upon the hidden state of the last token in the decoder is used for the output. Notably, our method can be easily extended to other table related tasks in a similar way. <ref type="figure">Figure 3</ref>: The illustration of the pre-training procedure in our method. During pre-training, we feed the concatenation of a sampled SQL query and a sampled table to the model, and train it to output the corresponding execution result (e.g., "Pairs").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training</head><p>Fine-Tuning Strategy Since our approach can perform various downstream tasks on the same architecture, it can easily perform multi-task learning. Therefore, we explore two ways of finetuning, one for vanilla fine-tuning and the other for multi-task fine-tuning. The former is to fine-tune the model on each individual downstream task. The latter is inspired by TAPAS <ref type="bibr">(Herzig et al., 2020)</ref> and T5 <ref type="bibr" target="#b27">(Raffel et al., 2020)</ref>, which first fine-tunes the model on related or similar intermediate downstream tasks and then continues to fine-tune it on the target downstream task.</p><p>Discussion Our approach comes with several advantages: (i) Flexibility: due to the powerful expressiveness of encoder-decoder models, our approach can readily adapt to (almost) any kind of output. (ii) Conveniency: our approach does not require any modification (e.g., table-specific masking) on pre-trained LMs, and can be trained in an end-to-end manner. (iii) Transferability: since we formulate downstream tasks as sequence generation tasks, which allows different tasks to share the same training protocol, it is easy to perform multi-task fine-tuning for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TABLE PRE-TRAINING VIA EXECUTION</head><p>As mentioned in ? 1, TAPEX achieves efficient table pre-training by training LMs to mimic the behavior of a SQL execution engine. In this section, we illustrate how to conduct table pre-training from two aspects: the pre-training task and the pre-training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PRE-TRAINING TASK</head><p>Following the MLM task in NL pre-training, existing works usually use reconstruction tasks for table pre-training. They generally take corrupted tables and NL sentences as input and try to recover the corrupted parts, in order to strengthen the linking between NL sentences and tables. While these pre-training tasks perform well, they tend to be less efficient since they usually require an extremely large pre-training corpus.</p><p>To design efficient tasks for table pre-training, we argue that the key lies in the executability of tables. That is to say, structured tables enable us to perform discrete operations on them via programming languages such as SQL queries, while unstructured text does not. Taking this into account, TAPEX adopts SQL execution as the only pre-training task. As illustrated in <ref type="figure">Figure 3</ref>, the pre-training of TAPEX is similar to the procedure of the above generative fine-tuning. Given an executable SQL query and a table T , TAPEX first concatenates the SQL query and the flattened table T * to feed into the model encoder. Then it obtains the query's execution result through an off-the-shelf SQL executor (e.g., MySQL) to serve as the supervision for the model decoder. Intuitively, the pretraining procedure is to encourage a language model to be a neural SQL executor. We believe that if a language model can be trained to faithfully "execute" SQL queries and produce correct results, then it should have a deep understanding of tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PRE-TRAINING CORPUS</head><p>Synthesizing the pre-training corpus is very important for table pre-training. Generally, there are two key factors: the table source and the SQL query sampling strategy.     <ref type="formula">(2020)</ref>, we choose publicly available semistructured tables as the table source. However, rather than requiring millions of raw tables in <ref type="bibr" target="#b40">(Yin et al., 2020)</ref>, TAPEX works well even with only a few thousand tables. Therefore, instead of fetching noisy tables from the Web and then heuristically filtering them, we pick high-quality tables right from existing public datasets. Concretely, we randomly select nearly 1, 500 tables from the training set of WIKITABLEQUESTIONS <ref type="bibr" target="#b26">(Pasupat &amp; Liang, 2015)</ref> as the table source for our pre-training corpus. Notice that there is no overlap between the tables used in our pre-training and the tables used in the dev and test sets of all downstream tasks, so there is no data leakage problem.</p><p>Query Sampling Regarding the sampling of diverse SQL queries, there are various choices in the literature. We can either sample SQL queries according to a probabilistic context-free grammar , or instantiate SQL templates over different tables <ref type="bibr" target="#b47">(Zhong et al., 2020a)</ref>. In our experiments, we follow the latter, where SQL templates are automatically extracted from the SQUALL dataset <ref type="bibr" target="#b31">(Shi et al., 2020b</ref>). An example SQL template is: SELECT num1 WHERE text1 = val1, where num1 and text1 correspond to a numeric column and a text column respectively, and val1 refers to one of the cell values with respect to the column text1. Given a SQL template, at each instantiation, we uniformly sample headers and cell values from a sampled table to fill the template, forming a concrete SQL query. Notably, SQL queries that execute with empty results are discarded, because empty results do not reflect much information about the executability of tables. This way, we can obtain a large-scale pre-training corpus with high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate TAPEX on different downstream tasks to verify its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Evaluation</head><p>We evaluate the performance of our approach on weakly-supervised WikiSQL (WIKISQL-WEAK) <ref type="bibr" target="#b46">(Zhong et al., 2017)</ref>, WIKITABLEQUESTIONS <ref type="bibr" target="#b26">(Pasupat &amp; Liang, 2015)</ref>, SQA <ref type="bibr" target="#b14">(Iyyer et al., 2017)</ref>, and TABFACT . Compared to WIKISQL-WEAK, which only requires filtering and optionally aggregating on table cell values, WIKITABLE-QUESTIONS requires more complicated reasoning capabilities. SQA is a conversational benchmark, which requires our approach to model the conversational context. Datset details can be found in Appendix A. For TableQA datasets, the evaluation metric is denotation accuracy, which checks whether the predicted answer(s) is equal to the ground-truth answer(s). It is worth noting that we evaluate our approach on WIKISQL-WEAK with answer annotations provided by TAPAS <ref type="bibr">(Herzig et al., 2020)</ref>, since nearly 2% of answers obtained from the official evaluation script are incorrect. For TABFACT, the evaluation metric is accuracy, which is calculated using the percentage of correct prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our approach based on fairseq <ref type="bibr" target="#b25">(Ott et al., 2019)</ref>. During pre-training, we synthesize up to 5 million pairs of SQL queries and their execution results for   TAPEX. In the following, unless specified explicitly, all the experimental results are by default evaluated under the 5 million setting. Our pre-training procedure runs up to 50, 000 steps with a batch size of 256. It takes about 36 hours on 8 Tesla V100 GPUs to finish the pre-training. The best pre-training checkpoint is selected based on the loss on the validation set. For all downstream datasets, the fine-tuning procedure runs up to 20, 000 steps with a batch size of 128. For both pretraining and fine-tuning, the learning rate is 3?10 ?5 .  <ref type="table" target="#tab_6">Table 3</ref> and <ref type="table" target="#tab_7">Table 4</ref> summarize the experimental results of various models on WIKISQL-WEAK, WIKITABLEQUESTIONS, SQA and TABFACT respectively. For both dev and test sets of all datasets, we report the median performance of our approach for five random runs. <ref type="table" target="#tab_2">Table 1</ref>, TAPEX outperforms all the baselines by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAIN RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WIKISQL-WEAK As shown in</head><p>On the test set of WIKISQL-WEAK, TAPEX registers a denotation accuracy of 89.5%, which is 3.7% higher than BART and 2.3% higher than the previous best performance. This is significant since the previous best model has already utilized the execution-guided decoding. In short, TAPEX achieves a new state-of-the-art result on the well-known benchmark WIKISQL-WEAK.</p><p>WIKITABLEQUESTIONS On the more challenging WIKITABLEQUESTIONS, TAPEX also achieves a new state-of-the-art denotation accuracy of 57.5%, surpassing the previous best system by 4.8% <ref type="table" target="#tab_3">(Table 2)</ref>. Meanwhile, we find that BART alone can only reach the denotation accuracy of 38.0%, much worse than the performance of previous pre-training models. We conjecture that the performance degradation could be attributed to the relatively small amount of training data in WIK-ITABLEQUESTIONS, which makes the adaptation of BART to tabular structures more challenging.  <ref type="figure">Figure 4</ref>: The visualization results of attention weights from other tokens to the cell "adrian lewis". Intuitively, the darker the color, the more closely the word is associated with "adrian lewis".</p><p>However, TAPEX delivers a dramatic improvement of 19.5% over BART, indicating that in the low data regime, the improvements introduced by TAPEX are often more significant.</p><p>SQA <ref type="table" target="#tab_6">Table 3</ref> presents the performance of various models on the test set of SQA, where TAPEX again obtains a new state-of-the-art denotation accuracy in terms of both the conversation level (48.4%) and the sentence level (74.5%). This improvement is also a surprise to us since SQA is a conversational dataset while our pre-training task is context-free. Meanwhile, the substantial improvements of TAPEX over BART on SQA continues to verify the same observation that TAPEX alleviates the low resource issue.</p><p>TABFACT Beyond TableQA, TAPEX also excels at TableFV. As shown in <ref type="table" target="#tab_7">Table 4</ref>, TAPEX achieves new state-of-the-art results on all subsets of TABFACT. For example, it surpasses the previous best system by 4.0% on Test complex . The result shows that TAPEX endows BART with generic table understanding capabilities, which could be adapted to different downstream tasks, regardless of whether these tasks are highly similar to the TAPEX pre-training task or not.</p><p>Overall Results Experimental results on four datasets show that TAPEX can broadly improve the model ability on understanding tables, especially in the low data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MULTI-TASK RESULTS</head><p>As discussed in ? 2.2, our approach can easily perform multi-task learning, thereby conferring benefits to downstream tasks. To verify it, we conducted multi-task fine-tuning experiments and obtained the following findings: (1) when initialized by BART, multi-task fine-tuning boosts the performance of the target task significantly;</p><p>(2) when initialized by TAPEX, the gain of multi-task fine-tuning tends to be marginal, suggesting that most of the "skills" (loosely speaking) gained by multi-task learning can be acquired by our table pre-training. Detailed results can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS</head><p>In this section, we carefully analyze our approach in terms of various aspects. Besides, we perform an exploratory analysis to provide more insights for future work, which can be found in Appendix C.   <ref type="figure">Figure 4</ref>, TAPEX seems to focus more on the row and the header where a cell corresponds to. Taking the example from <ref type="figure">Figure 4</ref>, the attention weights imply that "adrian lewis" is closely associated with the first column "player" and the entire third row, which are the positions of "adrian lewis" in the structured table.    lyzed them in <ref type="table" target="#tab_13">Table 5</ref>. One can find that TAPEX significantly boosts the performance on all operators, implying that it does enhance BART's capabilities for joint reasoning over text and tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQL Execution by</head><p>The Scale of Pre-training Corpus <ref type="figure" target="#fig_0">Figure 5</ref> illustrates downstream performance with different scales of the pre-training corpus. It can be seen that even if our pre-training corpus is synthetic, scaling up the pre-training corpus generally brings positive effects. The observation is analogous to the one in language modeling <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>: the larger the pre-training corpus, the better the downstream performance. By the comparison across different datasets, we can find that for simple tasks like WIKISQL-WEAK, the gains by scaling up pre-training corpus become marginal, while they remain non-trivial for complex tasks like TABFACT. Meanwhile, both downstream datasets in the low data regime show a positive trend by increasing the pre-training corpus. Conclusively, the scale matters when the downstream task is difficult, or the downstream dataset is relatively small.</p><p>The Efficiency of Pre-training As mentioned in ? 1, the pre-training efficiency of existing table pre-training approaches is relatively low, as they usually require an extremely large corpus. Therefore, taking WIKITABLEQUESTIONS as an example, we compare the pre-training efficiency of TAPEX with TAPAS <ref type="bibr">(Herzig et al., 2020)</ref>, TABERT <ref type="bibr" target="#b40">(Yin et al., 2020)</ref> and GRAPPA <ref type="bibr" target="#b42">(Yu et al., 2021a)</ref>. It is worth noting that part of the pre-training corpus for GRAPPA comes from humanannotated, high-quality parallel data. As shown in <ref type="figure" target="#fig_1">Figure 6</ref>, TAPEX can yield very promising performance when using a much smaller pre-training corpus, indicating that our proposed SQL execution pre-training task is more efficient than other table pre-training tasks.</p><p>Limitations The first limitation of our approach is that it cannot ideally handle large tables. As mentioned above, we employ the table flattening technique to represent a table. It works well when the table is relatively small, but it becomes infeasible when the table is too large to fit in memory. In practice, we can compress tables by removing some unrelated rows or columns, which would decrease downstream performance. The second limitation is that the task of text-to-SQL cannot benefit from our proposed table pre-training. We have tried to apply TAPEX for a text-to-SQL task, where the input remains the same and the output converts to SQL. However, TAPEX does not show a significant advantage over BART. We attribute this to two factors: first, our synthetic pre-training corpus does not contribute to grounding, one of the most important factors for semantic parsing ; second, table reasoning capabilities (e.g., aggregate) learned by TAPEX may not be necessary for SQL generation. For example, a model could still understand an NL phrase "total" as the aggregation function "sum", even though it is unaware of the mathematical meaning of "sum".  <ref type="bibr" target="#b40">(Yin et al., 2020;</ref><ref type="bibr">Herzig et al., 2020)</ref>, leveraged human-annotated parallel NL-table datasets for pre-training <ref type="bibr" target="#b6">(Deng et al., 2021;</ref><ref type="bibr" target="#b42">Yu et al., 2021a)</ref>, or synthesized a NL-table corpus using human-written templates <ref type="bibr" target="#b42">(Yu et al., 2021a;</ref><ref type="bibr" target="#b8">Eisenschlos et al., 2020)</ref>. Our work is different from theirs because we are the first to use pure synthetic SQL-table data for table pre-training, which allows us to automatically synthesize a diverse, large-scale, and high-quality pre-training corpus. As for the pre-training task, existing works proposed several pretraining tasks, such as Mask Column Prediction <ref type="bibr" target="#b40">(Yin et al., 2020)</ref>, Multi-choice Cloze at the Cell Level <ref type="bibr" target="#b38">(Wang et al., 2021b)</ref> and Structure Grounding <ref type="bibr" target="#b6">(Deng et al., 2021)</ref>. Different from all of them, we present a novel SQL execution task to perform table pre-training. <ref type="table">Table and</ref> Text As our experiments are mainly on TableQA and TableFV, our work is also closely related to previous methods for these tasks. For TableQA, previous works almost formulate it as a weakly semantic parsing task <ref type="bibr" target="#b16">(Liang et al., 2018;</ref><ref type="bibr" target="#b34">Wang et al., 2019a;</ref>, which always employ reinforcement learning to optimize semantic parsers over tables. Although these parsers produce logic forms (e.g., SQL), they have difficulties in training due to the large search space and the presence of spurious programs <ref type="bibr" target="#b9">(Goldman et al., 2018)</ref>. In addition, another promising line of work has emerged in recent advances <ref type="bibr" target="#b21">(Mueller et al., 2019;</ref><ref type="bibr">Herzig et al., 2020)</ref>, which aims at answering NL sentences without logical forms. This line of work predicts answer(s) by selecting cell values and optionally applying an aggregation operator to them. They can be easily trained, but their modeling ability is limited. For example, it is hard to support compound aggregation operators such as max(Year) -min(Year). What makes our approach different from these works is that we employ generative models to handle TableQA and can enjoy the end-toend training and flexibility simultaneously. For TableFV, previous works usually employ specialized architectures with limited scalability <ref type="bibr" target="#b29">(Shi et al., 2020a;</ref>. For example, <ref type="bibr" target="#b48">Zhong et al. (2020b)</ref> leveraged a graph construction mechanism, a semantic parser, and a semantic composition model to capture the connections among the NL sentence and the table. While the approach works well for TableFV, it is not easily applied to other table-related tasks. Compared with them, our approach works well for a variety of downstream tasks in the same architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Understanding on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present TAPEX, an execution-centric table pre-training approach whose corpus is automatically synthesized via sampling SQL queries and their execution results. TAPEX addresses the data scarcity challenge in table pre-training by learning a neural SQL executor on a diverse, large-scale, and high-quality synthetic corpus. Experimental results on four downstream datasets demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. Our work opens the way to exploit structured data by pre-training on synthetic executable programs, which is conceptually simple and has great potential to be extended to other research areas (e.g., knowledge base).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank all the anonymous reviewers for their constructive feedback. The first author Qian is supported by the Academic Excellence Foundation of Beihang University for PhD Students.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>In this work, we present a novel pre-training approach for tabular data, which approximates the structural reasoning process of formal languages over tables to achieve efficient table pre-training. Different from previous works which employ web crawling to construct a large-scale NL-table corpus for pre-training, our pre-training corpus is synthesized via sampling SQL queries and their execution results on public tables. Compared with previous works, our pre-training corpus is more controllable with high-quality. For example, compared with TABERT which crawls 26 million noisy tables from the Web, our approach adopts 1, 500 high-quality tables from public datasets, which greatly alleviates the potential privacy and bias issues raised by web crawling. We evaluate our approach on two fundamental <ref type="table">table-related tasks: table-based question answering and table-based fact verification.</ref> The former enables non-expert users to query databases without learning programming languages, while the latter helps users to verify whether a textual hypothesis is valid based on given tabular evidence. Experimental results on four well-known benchmark datasets show that our approach achieves new state-of-the-art results on all of them, especially in the low data regime.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DOWNSTREAM DATASETS</head><p>The dataset statistics are shown in <ref type="table" target="#tab_16">Table 6</ref>, while <ref type="table" target="#tab_17">Table 7</ref> show example inputs and outputs for our model. Note that SQA is a conversation benchmark, and we directly concatenate the history and i-th question as the "sentence" part (x) in the input, as done in . <ref type="table" target="#tab_19">Table 8</ref> presents the full experimental results on multi-task fine-tuning mentioned in ? 2.2. Note that we chose WIKISQL-WEAK and TABFACT as the transfer source because their training data are relatively rich.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MULTI-TASK RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPLORATORY ANALYSIS</head><p>In this section, we perform an exploratory analysis to provide more insights for future work. Concretely, we explore two interesting research questions: (1) How does the difficulty of SQL queries     Fine-Grained Analysis To understand the impact from a fine-grained perspective, we divide questions from the WIKITABLEQUESTIONS dev set into the same four levels of difficulty, with the help of SQL query annotation for WIKITABLEQUESTIONS questions provided by SQUALL. All finegrained experimental results are presented in <ref type="figure" target="#fig_3">Figure 8</ref>. We can see that with the addition of harder SQL queries, the performance on questions at the same difficulty level are greatly improved. For example, the addition of Medium level SQL queries boosts the performance of Medium-level questions from 38.2% (? Easy) to 56.2% (? Medium), which is in line with expectations. More encouragingly, adding simpler SQL queries can even improve performance on harder questions. For example, compared to BART, the ? Medium pre-training leads to an impressive improvement of up to 13.1% in the performance of Hard-level questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 IMPACT OF NATURAL LANGUAGE IN PRE-TRAINING</head><p>Natural Language Generation Intuitively, compared to SQL queries, using NL sentences in pretraining is better for downstream tasks since the pre-training objective is nearly the same as the fine-tuning objective. However, it is non-trivial to obtain a fluent NL sentence which faithfully reflects the semantics of a SQL query. In this experiment, we follow <ref type="bibr" target="#b47">Zhong et al. (2020a)</ref> to train a SQL-to-NL model and employ the model to translate SQL queries from the pre-training corpus into NL sentences. Concretely, our SQL-to-NL model is based on BART-Large <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref> and trained on the SQUALL dataset <ref type="bibr" target="#b31">(Shi et al., 2020b)</ref>, which contains nearly 9, 000 SQL-NL pairs. Then we apply the well-trained SQL-to-NL model to the pre-training corpus of TAPEX (0.5 Million) and obtain a NL pre-training corpus of the same size. By manually analyzing 100 sampled translated NL sentences, we are surprised to find that all NL sentences are fluent, and nearly 68% of them SELECT COUNT (*) WHERE Result = "won" AND Year &gt; 1987 How many times did they win after 1987?</p><p>SELECT MAX (Chart Position) -MIN (Chart Position) WHERE Release date = "july 21, 1995"</p><p>What is the difference between the chart position of july 21, 1995 and the chart position of july 22, 1995?</p><p>SELECT Nation WHERE Nation != "Japan" AND Gold = (SELECT Gold WHERE Nation = "Japan" ) Which other countries had the same number of gold medals as Japan?</p><p>SELECT Incumbent Electoral History GROUP BY Incumbent Electoral History ORDER BY COUNT ( * ) DESC LIMIT 1</p><p>Who has held the office the most?  are faithful to the semantics of the corresponding SQL queries. <ref type="table" target="#tab_2">Table 10</ref> presents some sampled SQL queries and their corresponding translated NL sentences. After obtaining the NL pre-training corpus, we follow the same pre-training and fine-tuning procedures as TAPEX to leverage it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>We compare the performance of all downstream tasks between TAPEX with. SQL and TAPEX with. NL in <ref type="table" target="#tab_2">Table 11</ref>. Surprisingly, the performance of TAPEX with. NL is comparable or even worse than the one of TAPEX with. SQL. For example, compared to using SQL queries in pre-training, using NL sentences causes a drop of 1.4% on WIKITABLE-QUESTIONS. We attribute such drop to the fact that the translated NL sentences contain some noise.</p><p>Taking the second row in <ref type="table" target="#tab_2">Table 11</ref> as an example, the translated NL sentence includes extra information such as "in the 1989 major league baseball draft", which may interfere with the pre-training. D FINE-GRAINED ANALYSIS OF SQL EXECUTION <ref type="figure">Figure 9</ref> provides a fine-grained analysis of the SQL execution accuracies for each operator type.  <ref type="figure">Figure 9</ref>: The fine-grained statistics of typical operators, example SQLs, operator percentage and their execution accuracies on the held-out 20, 000 SQL queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>The illustration of downstream tasks performance with different scales of pre-training corpus. Scaling up the pre-training corpus of TAPEX generally brings positive effects across datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>The amount of pre-training corpus vs. denotation accuracy on WIKITABLE-QUESTIONS dev set. TAPEX surpasses existing table pre-training approaches with a much smaller corpus, showing its high efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>= "Tikamgarh" AND Agg = 0 Hard SELECT (SELECT COUNT( Distinct Area)) &gt;= 5 SELECT COUNT (*) WHERE Result = "won" AND Year &gt; 1987 SELECT Driver WHERE Manufacturer = "t-bird" ORDER BY Pos ASC LIMIT 1 Extra Hard SELECT COUNT ( * ) WHERE Position = 1 AND Notes = "110 m hurdles" AND Year &gt; 2008 SELECT Nation WHERE Nation != "Japan" AND Gold = (SELECT Gold WHERE Nation = "Japan" ) SELECT Tournament WHERE Tournament IN ("oldsmar", "los angeles") GROUP BY Tournament ORDER BY COUNT ( * ) DESC LIMIT 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>The fine-grained performance of different SQL difficulty levels in pre-training on different question difficulty levels from WIKITABLEQUESTIONS dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[HEAD] Contestant | Age | Hometown [ROW] 1 Reyna Royo ?</figDesc><table><row><cell>Contestant Reyna Royo</cell><cell>Age 24</cell><cell>Hometown Panama City</cell><cell>Marisela Moreno Montero</cell><cell>supervise</cell><cell>Model</cell></row><row><cell>? Marisela Moreno Montero</cell><cell>? 24</cell><cell>? Panama City</cell><cell>flatten</cell><cell></cell><cell></cell></row><row><cell>Patricia De Le?n</cell><cell>19</cell><cell>Panama City</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">take its related table</cell><cell cols="2">take an NL question and its answer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell></row><row><cell cols="2">Previous Systems</cell><cell></cell></row><row><cell>Pasupat &amp; Liang (2015)</cell><cell>37.0</cell><cell>37.1</cell></row><row><cell cols="2">Neelakantan et al. (2016) 34.1</cell><cell>34.2</cell></row><row><cell>Zhang et al. (2017)</cell><cell>40.6</cell><cell>43.7</cell></row><row><cell>Liang et al. (2018)</cell><cell>42.7</cell><cell>43.8</cell></row><row><cell>Dasigi et al. (2019)</cell><cell>43.1</cell><cell>44.3</cell></row><row><cell>Agarwal et al. (2019)</cell><cell>43.2</cell><cell>44.1</cell></row><row><cell>Wang et al. (2019b)</cell><cell>43.7</cell><cell>44.5</cell></row><row><cell cols="2">Pre-trained Language Models</cell><cell></cell></row><row><cell>Herzig et al. (2020)</cell><cell>-</cell><cell>48.8</cell></row><row><cell>Yin et al. (2020)</cell><cell>53.0</cell><cell>52.3</cell></row><row><cell>Yu et al. (2021a)</cell><cell>51.9</cell><cell>52.7</cell></row><row><cell>BART</cell><cell>37.2</cell><cell>38.0</cell></row><row><cell>TAPEX</cell><cell>57.0</cell><cell>57.5</cell></row></table><note>Denotation accuracies on WIKISQL- WEAK. Execution-Guided Decoding is proposed to leverage execution results of SQL queries during in- ference (Wang et al., 2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Denotation accuracies on WIK-</figDesc><table /><note>ITABLEQUESTIONS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table Source</head><label>Source</label><figDesc></figDesc><table /><note>Following previous work by Yin et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Denotation accuracies on SQA test set. ALL is the denotation accuracy over all sentences, SEQ the denotation accuracy over all conversations, and Q i the denotation accuracy of the i-th sentence in a conversation.</figDesc><table><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell><cell cols="3">Test simple Testcomplex Testsmall</cell></row><row><cell></cell><cell cols="3">Pre-trained Language Models</cell><cell></cell><cell></cell></row><row><cell>Chen et al. (2020)</cell><cell>66.1</cell><cell>65.1</cell><cell>79.1</cell><cell>58.2</cell><cell>68.1</cell></row><row><cell>Zhong et al. (2020b)</cell><cell>71.8</cell><cell>71.7</cell><cell>85.4</cell><cell>65.1</cell><cell>74.3</cell></row><row><cell>Shi et al. (2020a)</cell><cell>72.5</cell><cell>72.3</cell><cell>85.9</cell><cell>65.7</cell><cell>74.2</cell></row><row><cell>Zhang et al. (2020)</cell><cell>73.3</cell><cell>73.2</cell><cell>85.5</cell><cell>67.2</cell><cell>-</cell></row><row><cell>Yang et al. (2020)</cell><cell>74.9</cell><cell>74.4</cell><cell>88.3</cell><cell>67.6</cell><cell>76.2</cell></row><row><cell cols="2">Eisenschlos et al. (2020) 81.0</cell><cell>81.0</cell><cell>92.3</cell><cell>75.6</cell><cell>83.9</cell></row><row><cell>BART</cell><cell>81.2</cell><cell>80.8</cell><cell>90.7</cell><cell>76.0</cell><cell>82.5</cell></row><row><cell>TAPEX</cell><cell cols="3">84.6 84.2 93.9</cell><cell>79.6</cell><cell>85.9</cell></row><row><cell>Human Performance</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Accuracies on TABFACT, including the Human Performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 ,</head><label>1</label><figDesc></figDesc><table /><note>Table 2,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Who are the only players listed that played in 2011 ?[HEAD]  player | year | round | result | opponent [ROW] 1 ray mond van bar ne ve ld | 2009 | quarter -final | won | j elle k la as en [ROW] 2 ray mond van bar ne ve ld | 2010 | 2 nd round | won | bre nd an d olan [ROW] 3 ad rian le w is | 2011 | final | won | g ary and erson</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Pre-training In order to understand how well TAPEX performs SQL execution after pre-training, we analyze its performance on nearly 20, 000 held-out SQL queries over unseen tables. Overall, the SQL execution accuracy is relatively high, as TAPEX correctly "executes" 89.6% of the SQL queries 1 . In particular, TAPEX performs better on Filter, Aggregate and Superlative operators, indicating that it is highly accurate in table cell selection and table aggregating. Regarding Arithmetic and Comparative operators, TAPEX also does a good job, demonstrating its numerical reasoning skill on tables. To summarize, TAPEX has learned to be a neural SQL executor with good selection, aggregating and numerical capabilities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table Understanding by</head><label>Understanding</label><figDesc>Pre-training To provide insight on if TAPEX helps downstream tasks understand tables better, we visualize and analyze the self-attention of TAPEX (without fine-tuning) on sampled WIKITABLEQUESTIONS examples. As shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table Reasoning</head><label>Reasoning</label><figDesc></figDesc><table><row><cell>Operator</cell><cell>Example Question</cell><cell>BART</cell><cell>TAPEX</cell></row><row><cell>Select</cell><cell>What is the years won for each team?</cell><cell>41.3%</cell><cell>64.8% (+23.5%)</cell></row><row><cell>Filter</cell><cell>How long did Taiki Tsuchiya last?</cell><cell>40.1%</cell><cell>65.7% (+25.6%)</cell></row><row><cell>Aggregate</cell><cell>What is the amount of matches drawn?</cell><cell>26.9 %</cell><cell>57.4% (+30.5%)</cell></row><row><cell>Superlative</cell><cell>What was the last Baekje Temple?</cell><cell>46.3 %</cell><cell>64.3% (+18.0%)</cell></row><row><cell>Arithmetic</cell><cell>What is the difference between White</cell><cell>33.1 %</cell><cell>53.5% (+20.4%)</cell></row><row><cell></cell><cell>voters and Black voters in 1948?</cell><cell></cell><cell></cell></row><row><cell cols="2">Comparative Besides Tiger Woods, what other player</cell><cell>30.0 %</cell><cell>55.9% (+25.9%)</cell></row><row><cell></cell><cell>won between 2007 and 2009?</cell><cell></cell><cell></cell></row><row><cell>Group</cell><cell cols="2">What was score for each winning game? 49.5 %</cell><cell>66.7% (+17.2%)</cell></row></table><note>by Pre-training To understand if TAPEX can improve table reasoning, we com- pare the performance of TAPEX to BART on 500 randomly selected questions and manually ana-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>The most common operators in the randomly selected 500 questions from WIKITABLE-QUESTIONS dev set. Listed are, the operator, the example question with the operator semantic (i.e., the colorful spans), the performance of BART and TAPEX on the operator.</figDesc><table><row><cell></cell><cell cols="5">WIKITABLEQUESTIONS SQA TABFACT WIKISQL-WEAK</cell></row><row><cell>Task Performance (%)</cell><cell>40 60 80 100</cell><cell cols="3">48.6 65.3 82.8 88.1 54.2 68.9 83.6 88.8 56.1 70.2 83.8 89.2</cell><cell>57</cell><cell>70.3 84.6 89.1</cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell>0.5</cell><cell>1.0</cell><cell>5.0</cell></row><row><cell></cell><cell></cell><cell cols="4">Amount of Pretraining Corpus (Millions)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table Pre -</head><label>Pre</label><figDesc>training The work most related to ours is table pre-training whose key factors include the pre-training corpus and the pre-training task. As for the pre-training corpus, most of previous works almost collect NL-table data to perform table pre-training. They either mined a large corpus of tables and their NL sentence contexts</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Experimental dataset statistics. Baltimore Orioles | RHP | Louisiana State University [ROW] 2 : Tyler Houston | Atlanta Braves | C | Valley HS (Las Vegas, NV) . . . : year | date | driver | team | manufacturer | laps | -| race time | average speed (mph) [ROW] 1 : 1990 | july 15 | tommy ellis | john jackson | buick | 300 | 317.4 (510.805) | 3:41:58 | 85.797 [ROW] 2 : 1990 | october 14 | rick mast | ag dillard motorsports | buick | 250 | 264.5 (425.671) | 2:44:37 | 94.45 . . .</figDesc><table><row><cell>Dataset</cell><cell>Example Input</cell><cell>Example Output</cell></row><row><cell>WIKISQL-WEAK</cell><cell>How many CFL teams are from York College?</cell><cell>2</cell></row><row><cell></cell><cell>[HEAD] : pick # | CFL team Player | Position | Col-</cell><cell></cell></row><row><cell></cell><cell>lege [ROW] 1 : 27 | hamilton tiger-cats | connor</cell><cell></cell></row><row><cell></cell><cell>healey | db | wilfrid laurier [ROW] 2 : 28 | calgary</cell><cell></cell></row><row><cell></cell><cell>stampeders | anthony forgione | ol | york . . .</cell><cell></cell></row><row><cell cols="2">WIKITABLEQUESTIONS Which album released by the band schnell fenster pro-</cell><cell>The Sound Of Trees</cell></row><row><cell></cell><cell>duced the most singles appearing on the australian</cell><cell></cell></row><row><cell></cell><cell>peak chart? [HEAD] : Year | Title | Peak Chart Posi-</cell><cell></cell></row><row><cell></cell><cell>tions AUS | Peak Chart Positions NZ | Album [ROW]</cell><cell></cell></row><row><cell></cell><cell>1 : 1988 | "whisper" | 58 | 42 | the sound of trees</cell><cell></cell></row><row><cell></cell><cell>[ROW] 2 : 1988 | "love-hate relationship" | 81 | 46 |</cell><cell></cell></row><row><cell></cell><cell>The Sound Of Trees . . .</cell><cell></cell></row><row><cell>SQA</cell><cell cols="2">where are the players from? which player went to louisiana state university? [HEAD] : Pick | Player | Team | Position | School [ROW] 1 : 1 | Ben McDon-ald | Ben McDonald</cell></row><row><cell>TABFACT</cell><cell cols="2">On june 26th, 2010 kyle busch drove a total of 211.6 miles at an average speed of 110.673 miles per hour. [HEAD] 1 (Yes)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>The example inputs and outputs for our model on experimental datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 :</head><label>8</label><figDesc>Experimental results (denotation accuracy) of multi-task fine-tuning on the Target dev set. Source ? Target means first fine-tuning on Source and then fine-tuning on Target.</figDesc><table><row><cell>Difficulty</cell><cell>Example SQL Query</cell></row><row><cell></cell><cell>SELECT Date</cell></row><row><cell>Easy</cell><cell>SELECT COUNT (Canal)</cell></row><row><cell></cell><cell>SELECT Name WHERE Age &gt;= 28</cell></row></table><note>Medium SELECT Region ORDER BY ID DESC LIMIT 1 SELECT COUNT (Tornadoes) WHERE Date = 1965 SELECT District WHERE District !</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 :</head><label>9</label><figDesc>Four SQL query difficulty levels and their corresponding example SQL queries. Million). ? Medium means that we only use SQL query templates with a difficulty level less than or equal to Medium when synthesizing its pre-training corpus. Notably, ? Extra Hard is equivalent to using all SQL query templates.</figDesc><table><row><cell>in pre-training impact the performance of downstream tasks? (2) Would it be better to use natural</cell></row><row><cell>language sentences instead of SQL queries during pre-training?</cell></row></table><note>BART ? Easy ? Medium ? Hard ? Extra Hard SQL Difficulty Level in Pre-training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>SELECT Name WHERE Age &gt;= 28Who is at least 28 years old?SELECT MAX (Pick#)What was the last pick in the 1989 major league baseball draft?</figDesc><table><row><cell>SQL Query</cell><cell>Translated NL Sentence</cell><cell>Faithfulness</cell></row><row><cell>SELECT Driver ORDER BY Pos DESC LIMIT 1</cell><cell>What driver came in last place?</cell><cell></cell></row><row><cell>SELECT COUNT (Competition) WHERE Notes != 100</cell><cell>How many competitions have no notes?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 10 :</head><label>10</label><figDesc>The sampled SQL queries, their corresponding NL sentences translated by our SQL-to-NL model, and the faithfulness of the NL sentences.</figDesc><table><row><cell>Setting</cell><cell cols="4">WIKISQL-WEAK WIKITABLEQUESTIONS SQA TABFACT</cell></row><row><cell>TAPEX with. SQL</cell><cell>88.8</cell><cell>54.2</cell><cell>68.9</cell><cell>83.6</cell></row><row><cell>TAPEX with. NL</cell><cell>87.5</cell><cell>52.8</cell><cell>68.7</cell><cell>83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 11 :</head><label>11</label><figDesc>The downstream performance on dev sets of TAPEX with the SQL and the NL pre-training corpus. The NL corpus is obtained via translating the SQL corpus using our SQL-to-NL model, and they share the same amount of examples (0.5 Million).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The full analysis about SQL execution can be found in Appendix D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/forward/sql-parser</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to generalize from sparse and underspecified rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkeJRhNYDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative search for weakly supervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1273</idno>
		<ptr target="https://aclanthology.org/N19-1273" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2669" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TURL: table understanding through representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.5555/3430915.3442430</idno>
		<ptr target="http://www.vldb.org/pvldb/vol14/p307-deng.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-grounded pretraining for text-to-SQL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Hassan</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.105</idno>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.105" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1337" to="1350" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding tables with intermediate pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.27</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.27" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="281" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic parsing with abstract examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Latcinnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Nave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1168</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-1168" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1809" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic parsing by learning from mistakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.findings-emnlp" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="2603" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using database rule for weak supervised text-to-sql generation. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonglei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huilin</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th</title>
		<meeting>the 58th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.398" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1167</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory augmented policy optimization for program synthesis and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A split-and-recombine approach for follow-up query analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1535</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1535" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5316" to="5326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How far are we from effective context modeling? an exploratory study on semantic parsing in context twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Awakening latent grounding from pretrained language models for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.100</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.100" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08" />
			<biblScope unit="page" from="1174" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A discrete hard EM approach for weakly supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1284</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1284" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="2851" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Answering conversational questions on structured data without logical forms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1603</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1603" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5902" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.04834" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry2YOrcge" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14610</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
		<ptr target="https://aclanthology.org/N19-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
		<ptr target="https://aclanthology.org/P15-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning contextual representations for semantic parsing with generation-augmented pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Hanbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/17627" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13806" to="13814" />
		</imprint>
	</monogr>
	<note>C?cero Nogueira dos Santos, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learn to combine linguistic and symbolic information for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.coling-main.466" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="5335" to="5346" />
		</imprint>
	</monogr>
	<note>ternational Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Logic-level evidence retrieval and graph-based verification network for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.16" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the potential of lexico-logical alignments for semantic parsing to SQL queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.167</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.167" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1849" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge-aware conversational semantic parsing over web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning semantic parsers from denotations with latent structured alignments and abstract programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1391</idno>
		<ptr target="https://aclanthology.org/D19-1391" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="3774" to="3785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to synthesize data for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.220</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2760" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust text-to-sql generation with execution-guided decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Tatwawadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<idno>abs/1807.03100</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1176</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="19" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TUTA: tree-based transformers for generally structured table pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467434</idno>
		<ptr target="https://doi.org/10.1145/3447548" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>Feida Zhu, Beng Chin Ooi, and Chunyan Miao</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Program enhanced fact verification with verbalization and graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno>doi: 10. 18653/v1/2020.emnlp-main.628</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.628" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="7810" to="7825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.745</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.745" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="8413" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1425</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1425" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grappa: Grammar-augmented pre-training for table semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=kyaIeYj4zZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Score: Pretraining for context representation in conversational semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Hassan</forename><surname>Awadallah</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oyZxhRI2RiE" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Table fact verification with structure-aware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.126</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.126" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1624" to="1629" />
		</imprint>
	</monogr>
	<note>Online, November 2020</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Macro grammars and holistic triggering for efficient semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1125</idno>
		<ptr target="https://www.aclweb.org/anthology/D17-1125" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grounded adaptation for zero-shot executable semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.558</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.558" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="6869" to="6882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LogicalFactChecker: Leveraging logical operations for fact checking with graph module network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.539</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.539" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="6053" to="6065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">C.1 IMPACT OF SQL QUERY DIFFICULTY IN PRE-TRAINING</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SELECT), or a table schema (i.e., a header or a cell value). In practice, we obtain elements of SQL queries via an off-the-shelf SQL parser 2 , which returns a stream of SQL elements for each SQL query. Empirically, we categorize SQL queries with ? 6 elements into Easy, &gt; 6 and ? 14 elements into Medium, &gt; 14 and ? 20 elements into Hard, and the rest into Extra Hard. Example SQL queries of different difficulty levels can be found in Table 9. Based on the SQL difficulty criteria, we divide the templates from SQUALL (Shi et al., 2020b) into four levels of difficulty and gradually add them to the construction of the pre-training corpus from Easy-level ( ? Easy) to Extra-Hard-level (? Extra Hard)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">we suppose that the difficulty of a SQL query can be measured by the number of SQL elements. An element can be either a SQL keyword</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SQL Difficulty Criteria. Notably, to avoid the effect of the scale of pre-training, we maintain the same amount of examples for the above pre-training corpus</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">As can be seen, it is helpful to add harder SQL queries to the pre-training corpus in most cases. For example, compared to ? Easy, ? Medium achieves consistent improvements on the performance of downstream tasks (e.g., 10.6% on WIKITABLEQUESTIONS). Meanwhile, we also notice that the impact of the difficulty of SQL queries becomes less significant after the Medium-level</title>
	</analytic>
	<monogr>
		<title level="m">On the TABFACT dataset</title>
		<imprint/>
	</monogr>
	<note>Downstream Performance The experimental results are shown in Figure 7. involving Extra-Hard-level SQL queries in pre-training even slightly hurts the performance</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
