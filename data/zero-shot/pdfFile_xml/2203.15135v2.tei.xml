<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filler Word Detection and Classification: A Dataset and Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Zhu</surname></persName>
							<email>ge.zhu@rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Pablo</forename><surname>Caceres</surname></persName>
							<email>caceres@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
							<email>salamon@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Filler Word Detection and Classification: A Dataset and Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: filler word detection</term>
					<term>speech disfluency</term>
					<term>keyword spotting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Filler words such as 'uh' or 'um'  are sounds or words people use to signal they are pausing to think. Finding and removing filler words from recordings is a common and tedious task in media editing. Automatically detecting and classifying filler words could greatly aid in this task, but few studies have been published on this problem to date. A key reason is the absence of a dataset with annotated filler words for model training and evaluation. In this work, we present a novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K annotations of other sounds that commonly occur in podcasts such as breaths, laughter, and word repetitions. We propose a pipeline that leverages VAD and ASR to detect filler candidates and a classifier to distinguish between filler word types. We evaluate our proposed pipeline on PodcastFillers, compare to several baselines, and present a detailed ablation study. In particular, we evaluate the importance of using ASR and how it compares to a transcription-free approach resembling keyword spotting. We show that our pipeline obtains state-of-the-art results, and that leveraging ASR strongly outperforms a keyword spotting approach. We make PodcastFillers publicly available, in the hope that our work serves as a benchmark for future research. Index Terms: filler word detection, speech disfluency, keyword spotting 1 podcastfillers.github.io</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech disfluencies, such as filler words, stuttering, repetitions and corrections, are common in spontaneous speech <ref type="bibr" target="#b0">[1]</ref>. Of all disfluencies, filler words, especially 'uh's and 'um's, are the most common <ref type="bibr" target="#b1">[2]</ref>. For content creators working on, e.g., podcasts or video interviews, manually finding and editing filler words in video and audio recordings requires significant time and effort. Automatically detecting filler words accurately has the potential to significantly speed up speech content creation workflows. Such a filler word detection system must be able to both localize filler words in time and classify them correctly.</p><p>Previous work has focused on detecting and removing speech disfluencies from text transcripts <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, some also incorporating acoustic features <ref type="bibr" target="#b6">[7]</ref>. In some cases, the transcripts are produced via Automatic Speech Recognition (ASR) <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. In this scenario it is up to the ASR to transcribe the filler words, which requires training an ad-hoc ASR with filler words in its vocabulary. This is computationally intensive and challenging since ASR systems are often trained on spoken text corpora which do not contain any filler words, and thus cannot detect them reliably. Furthermore, adding a new filler word to the vocabulary would require re-training the ASR model.</p><p>More recently, several data driven methods have been proposed to detect speech disfluencies directly from audio in telephone conversations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and naturalistic recordings [13-*This work was performed during an internship at Adobe Research. <ref type="bibr" target="#b15">15</ref>]. <ref type="bibr">Sheikh et al.</ref> proposed StutterNet <ref type="bibr" target="#b13">[14]</ref>, a time-delay neural network (TDNN) to classify repetitions, blocks, prolongations and interjections, the latter being another term for filler words. They used the UCLASS dataset <ref type="bibr" target="#b16">[16]</ref>, which is designed for stutter classification. This poses some challenges, since the dataset was recorded in a controlled environment, and exclusively contains speech by people who stutter. This, along with the small size of the dataset (?4K sentences), means it is unclear whether the results would generalize to recordings of spontaneous speech more broadly. To tackle this issue, Kourkounakis et al. <ref type="bibr" target="#b15">[15]</ref> expanded the training data by creating a synthesized stutter dataset, LibriStutter, by inserting repetitions or interjections in between non-stuttered speech from a subset of LibriSpeech <ref type="bibr" target="#b17">[17]</ref>. The interjections, however, were taken from UCLASS, presenting the same aforementioned challenge. Salamin et al. <ref type="bibr" target="#b18">[18]</ref> trained Hidden Markov Models to segment laughter, fillers, speech and silence from spontaneous speech on the SSPNet Vocalization Corpus, which is also a small-scale dataset. Lea et al. <ref type="bibr" target="#b11">[12]</ref> created a large speech disfluency dataset, SEP-28K, from podcasts with people who stutter, and used it to build a stutter detector. As before, there is a generalization challenge given the audio data are specific to people who stutter. Also, the dataset is annotated at the clip level, so it does not provide precise timestamps for filler words and cannot be used to evaluate detection accuracy at a fine temporal resolution.</p><p>The closest study to our work is by Das et al. <ref type="bibr" target="#b0">[1]</ref>, who proposed a disfluency repair system aiming at removing filler words and long pauses. They trained a convolutional recurrent neural network (CRNN) for filler word segmentation (detection and classification) applied directly to audio recordings. They used two speech datasets, Switchboard speech data <ref type="bibr" target="#b19">[19]</ref> with transcripts and Automanner <ref type="bibr" target="#b20">[20]</ref>. A key limitation of the approach noted by the authors is that it was unable to distinguish filler words such as 'uh' or 'um' from real parts-of-speech, returning false-positives for actual words that sound similar to or contain filler words, such as "um-brella". Also, Kaushik et al. <ref type="bibr" target="#b12">[13]</ref> found that the mismatch between training on telephony speech and testing on naturalistic recordings hurts filler classification accuracy. The test set for evaluating the methods presented by Das et al. <ref type="bibr" target="#b0">[1]</ref> only contains 20 speech samples, once again making it hard to draw generalizable conclusions.</p><p>In this paper, we address the data scarcity challenge for filler word detection by creating the largest annotated dataset of filler words published to date, PodcastFillers, which we make publicly available online 1 . We propose an efficient workflow for generating annotation candidates in continuous speech recordings that leverages a robust Voice Activity Detection (VAD) model and an off-the-shelf ASR, and annotate over 85K filler word candidates. The resulting dataset spans 145 hours of speech from over 350 speakers coming from 199 public podcast episodes, and has 35K annotated filler words and 50K annotations of other speech events that are common in podcasts  such as laughter, breaths, and repetitions. It also includes the ASR transcriptions we obtained for all the episodes. Using Pod-castFillers, we train a filler classifier similar to a keyword spotting approach, and present an end-to-end pipeline that leverages VAD, ASR, and the classifier to perform filler word detection and classification. We compare our proposed pipeline to two baselines and show that it yields state-of the-art results. We hope it serves as a robust benchmark for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The PodcastFillers Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Podcast curation</head><p>We manually curated 199 gender-balanced, English-language podcast episodes from SoundCloud 2 , totaling 145 hours of speech audio from over 350 speakers sampled at 44.1 kHz. We searched for episodes using the 24 topic categories defined in <ref type="bibr" target="#b21">[21]</ref> to include a variety of topics and styles, and selected episodes from different shows or shows with guest speakers to ensure a diversity of speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Filler word annotation pipeline</head><p>Listening to the entire dataset to label fillers would be highly inefficient. Instead, we propose an annotation pipeline that leverages a commercial ASR system 3 and a VAD model to generate filler candidates for annotation, depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>As previously noted, ASR systems typically do not transcribe non-lexical fillers in spontaneous speech. We make use of this drawback to identify possible filler word locations: nonlexical fillers such as 'uh' and 'um' will trigger the VAD model, but appear as silent gaps in the ASR output. These regions where VAD activates but ASR does not are candidate locations for fillers. Using this approach we identified 85K candidates in PodcastFillers. Since the candidates may contain other sounds such as breaths, laughter, music, or even words (due to ASR errors), they require manual verification, which we obtained via crowdsourcing using a custom-built annotation interface.</p><p>Voice Activity Detection (VAD) model and data: For detecting candidate fillers, we need a VAD model that outputs predictions at a fine temporal resolution (100 Hz) to precisely locate the temporal boundaries of speech regions. It also needs to be robust to various background and foreground noises in podcasts such as music and non-speech sounds (e.g., fan noise). <ref type="bibr" target="#b1">2</ref> www.soundcloud.com 3 www.speechmatics.com</p><p>We achieve this fine temporal resolution by computing input acoustic features at a 10 ms hop size, such that we can slide the trained model over the audio at this temporal resolution. To ensure robustness, we need to combine a generalizable ML model with a varied training set containing various background and foreground noises at different signal-to-noise ratios (SNR). We create a new labeled speech dataset for VAD by programatically combining recordings of clean speech with music and noise using the Scaper soundscape mixing software <ref type="bibr" target="#b22">[22]</ref>.</p><p>We generate frame-level (10 ms) VAD annotations by computing the audio amplitude from clean speech recordings sourced from the Librispeech-100 <ref type="bibr" target="#b17">[17]</ref> and VCTK <ref type="bibr" target="#b23">[23]</ref> datasets, labeling regions below a 19 dB threshold relative to the peak amplitude of the normalized signal as silent. Then we programmatically mix the clean speech clips with background music and environmental sound from the strongly labeled subset <ref type="bibr" target="#b24">[24]</ref> of AudioSet <ref type="bibr" target="#b25">[25]</ref>. To test our VAD model, we keep a disjoint test set of audio source material using 8% of the speakers in VCTK, the test partition of Librispeech, and 5% of sound events from the AudioSet subset. We generate 300,000 training mixtures from the training source material and 10,000 test mixtures from the test source material. Scaper allows us to control the SNR range and distribution in the mixtures. Preliminary experiments showed the performance of the VAD model, in terms of producing filler candidates when combined with ASR, was sensitive to the SNR range. We empirically settled on a speech SNR range of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">22]</ref> dB relative to background noise, [?3, 17] dB relative to foreground noise and [?6, 14] dB relative to music.</p><p>We compute log-scaled mel-spectrograms (log-mel) as input to the VAD model using Librosa <ref type="bibr" target="#b26">[26]</ref>. We use 64 mel bins, and a purposely short window of 25ms and a hop size of 10ms, to support inference at a high temporal resolution. We adopt a Convolutional Recurrent Neural Netowrk (CRNN) architecture that has been shown to be robust for VAD in complex environments with noise <ref type="bibr" target="#b27">[27]</ref>, but remove the recurrent layer to improve run-time performance. We found this change does not impact model accuracy. Our trained VAD model obtained Precision/Recall of 0.93/0.92 respectively on the test split of our mixed dataset. Once our VAD model was trained, we used it in combination with the ASR to produce filler candidates. To minimize the chance of missing soft fillers, we set a lenient VAD activation threshold of 0.1 (as opposed to the standard 0.5 out of [0, 1]). We found the majority of candidates to have a duration in the 150-400 ms range. For candidates shorter than 150 ms it was hard to determine by ear whether they were actual filler words or other sounds, so we decided to remove them prior to labeling. Similarly, we removed candidates longer than 2 s, which were rare and not representative of our target use case.</p><p>Labeling filler candidates: Based on an initial audition of a sample of candidates, we identified a set of filler and nonfiller classes for our labeling task. The labels, along with the final number of annotations per label (in parentheses), are: For fillers, 'uh' (17907), 'um' (17078), 'you know' (668), 'like' (157), and 'other' (315). 'Like' and 'you know' occurred rarely in our candidate set, when the ASR failed to transcribe them. For non-fillers, 'laughter' (6623), 'breath' (8288), 'agreement sound' (3755, e.g., 'mmm' or 'uh-huh'), 'regular words' (12709), 'repetitions' (9024), 'simultaneous speakers' (1484), 'music' (5060) and 'noise' (2735). The first three non-filler labels represent voice sounds that aren't fillers. The next three are caused by ASR errors or intentional omissions, and the final two are caused by VAD false-positives.</p><p>The candidates were presented to crowd workers for annotation. Each filler candidate was positioned at time 3 sec inside a 5 sec clip (for context), and highlighted in the interface. Annotators had to determine whether the highlighted candidate was a filler word or not, and based on that select one of the five filler labels or eight non-filler labels. Each candidate was annotated by two people, or three when the first two disagreed. Out of all candidates labeled 'uh' or 'um' in the dataset, 98.4% and 96.4% respectively had at least two annotators agree on the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Filler Detection Pipeline</head><p>We propose a filler detection pipeline with two variants: the first leverages ASR, while the second does not, which is relevant for deployment scenarios where ASR is not available. The pipeline is depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>. In the first stage, the input audio is passed through the VAD model to find voice regions. The first pipeline variant also runs the audio through ASR to discard regions with transcribed words. The second variant passes straight to the next stage. In the second stage, the remaining candidate time regions are passed through a classification model that produces labeled events with a start time, end time, and a label. Moving forward, we shall refer to the first pipeline as AVC-FillerNet (for ASR + VAD + Classifier), and the second as VC-FillerNet (no ASR). By skipping the ASR, VC-FillerNet is computationally lighter, but runs the risk of detecting parts of actual words as fillers <ref type="bibr" target="#b0">[1]</ref>. Our goal is to train a robust multi-class classifier to detect fillers given a short snippet of audio. Given the label distribution in PodcastFillers, we opted to discard labels with 3K or less annotations and consolidate other labels, producing five new labels, each with ample training data: 'filler' ('uh'+'um'), 'words' ('regular words' and 'repetitions'), 'laughter', 'music', and 'breath'. Ultimately, we only care about the detection accuracy for the 'filler' class, and we expect this consolidation to lead to a more robust classifier. In Section 5 we also evaluate our ability to classify 'uh' and 'um' as two separate classes.</p><p>We use wav2vec <ref type="bibr" target="#b28">[28]</ref> embeddings computed with a 10 ms hop size as input to the model. Wav2vec was pretrained on over 960 hours of speech, providing a robust representation for classification of speech-like sounds. During training we apply time and "frequency" masking to the embeddings via SpecAugment <ref type="bibr" target="#b29">[29]</ref>, and optimize a cross entropy loss.</p><p>Since our goal is to detect specific short utterances in an audio stream, the task can be viewed as a keyword-spotting (KWS) problem where our keyword is the joint set of 'uh' and 'um'. With this in mind, we adapt a lightweight KWS model backbone architecture, TC-ResNet8 <ref type="bibr" target="#b30">[30]</ref>, for efficient classification. TC-ResNet8 only has around 100k parameters, mak-ing it suitable for low-latency inference. It applies 1D convolutions along the temporal axis and spans the entire frequency range in every layer, achieving strong performance even with a small number of layers. Because the filler candidates in AVC-FillerNet are normally short segments, we can train an event classifier to directly predict the event label for the entire input segment. On the contrary, for VC-FillerNet, the filler candidates are usually long sequences of voice, so we train a frame classifier to predict frame-level labels at a fine temporal resolution, e.g., every 100 ms. To get frame-level predictions, we adapt the TC-ResNet8 backbone by adding an LSTM layer. Similar to Filler-CRNN <ref type="bibr" target="#b0">[1]</ref>, we then group contiguous frames with the same predicted label into an event. The final output of both the event-level classifier and frame-level classifier are discrete events with a start time, end time, and a label. We compare the two approaches as part of our ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data split and training</head><p>For our experiments, we split the PodcastFillers dataset into train, validation, and test sets with 173, 6, 20 episodes respectively, while ensuring each subset remains gender-balanced. The audio is downsampled from 44.1 kHz to 16 kHz for computational efficiency. We train our proposed models on the training set, tune hyper-parameters and the VAD threshold on the validation set, and report performance on the test set. To train the event classifier, AVC-FillerNet, we use 1 s input clips with the labeled filler candidate placed at the center of the clip. The model produces a single prediction for the input, which is compared to the ground truth label. To train the frame classifier, VC-FillerNet, we use 1 s input clips where the filler candidate can appear anywhere in the clip. The model produces per-frame <ref type="bibr" target="#b9">(10)</ref> predictions that are compared to the ground truth events (which have start/end times) frame-by-frame during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare our systems with two strong baselines: a neuralnetwork-based method, Filler-CRNN <ref type="bibr" target="#b0">[1]</ref>, and a forced-alignerbased method, Gentle <ref type="bibr" target="#b31">[31]</ref>. The input to Filler-CRNN is logmel with 128 bins computed from 1 s clips. The original Filler-CRNN architecture yielded weak performance in our experiments, so we fine tune it (number of layers, kernel size, pooling) on PodcastFillers for a stronger baseline. With Gentle, we first apply pre-trained Kaldi <ref type="bibr" target="#b32">[32]</ref> acoustic models developed on the Fisher English corpus <ref type="bibr" target="#b33">[33]</ref> to generate syllable tokens. We compare the tokens with the ASR transcript: filler words are detected if the inconsistent regions between the two are re-aligned by inserting 'um' or 'uh' into the transcript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation metrics</head><p>We compute segment-based and event-based metrics (Precision, Recall, F1) using sed eval <ref type="bibr" target="#b34">[34]</ref> for the 'filler' class, to evaluate detection accuracy and localization accuracy respectively. Segment-based metrics map the system output and ground truth to a fixed time grid for comparison. Event-based metrics compare the estimated sound events and the ground truth events directly. A predicted event is considered a true positive if it overlaps with a ground truth event that has the same label and its onset and offset are within a threshold (slack) from the reference event's onset and offset (200 ms in this work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations</head><p>We run ablation studies to understand the impact of each of the two stages of our proposed pipeline, VAD and the filler classifier. For VAD, we vary the activation threshold from 0.1-0.9: the lower the threshold the more candidates will be passed to the second stage. For the classifier, we compare different input features (log-mel with 64 bins and wav2vec) and architectures (event classifier and frame classifier). We evaluate both the AVC-FillerNet and VC-FillerNet pipelines in all ablations. The output events from each model are converted to frame-level likelihoods, which are compared to the reference annotations to produce Precision-Recall (PR) curves, which elucidate the trade-off between precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation studies</head><p>We start by analyzing the influence of the VAD activation threshold, using the PodcastFillers validation set, and wav2vec as the input feature. The results are shown in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. The best PR-curve is obtained using the lowest threshold (0.1), and as we increase the threshold there is a notable decrease in recall. Interestingly, the precision remains consistent regardless of the VAD activation threshold. This suggests that the filler classifier is robust at rejecting false positives with lower VAD likelihoods, and so a low VAD threshold maximizes recall by ensuring we do not miss soft filler words, without compromising on precision.</p><p>Following this, we fix the VAD threshold to 0.1 in the second ablation study where we compare different input features and classifier backbones, shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. Wav2vec consistently outperforms log-mel as the input feature, confirming that this model, trained on a large speech corpus, yields a discriminative representation for filler word classification. For the AVC-FillerNet pipeline, the event classifier is only marginally better than the frame classifier. In contrast, since the VC-FillerNet pipeline cannot leverage ASR to determine the precise timing of filler candidates, the frame classifier outperforms the event classifier in this pipeline due to its superior temporal accuracy.</p><p>Most importantly, we see that across both ablation studies, AVC-FillerNet clearly outperforms VC-FillerNet. By leveraging ASR, AVC-FillerNet produces tight temporal boundaries around filler candidates, and dramatically reduces the number of candidates passed to the classifier. This reduces the chances of producing false positives, leading to a boost in precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to baselines</head><p>We compare AVC-FillerNet and VC-FillerNet with two baselines, Filler-CRNN and Gentle (described in Sec 4.2), and report results the PodcastFillers test set as shown in Tab. 1. For our pipelines, we use the optimal VAD threshold of 0.1 as determined by the ablation study on the validation set. We see that AVC-FillerNet significantly outperforms all the other systems for all metrics. Gentle yields higher precision than VC-FillerNet and Filler-CRNN, but has the lowest recall among all the systems. We speculate this may be improved by leveraging acoustic models trained with filler words in Gentle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Filler detection &amp; classification with fine granularity</head><p>Finally, we evaluate our systems' ability to separately detect 'uh' and 'um' filler words. We re-train the classifier with the  We see that 'um' is easier to classify, especially for VC-FillerNet. We speculate that this is because 'um's are typically longer than 'uh's, providing the classifier with more signal to leverage for inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work we presented PodcastFillers, a large dataset of podcasts with annotated filler words. The dataset was created by boostrapping a VAD model and a commercial ASR system to generate filler candidates that were annotated via crowdsourcing. We proposed ASR-based and ASR-free filler detection and classification pipelines, AVC-FillerNet and VC-FillerNet. Our experiments showed that AVC-FillerNet achieves state-of-theart results, significantly outperforming existing filler word detection systems, and that leveraging ASR outperforms a keyword spotting approach for filler word detection. Through ablation studies, we evaluated the impact of our design choices on system performance. We hope the PodcastFillers dataset, our proposed filler detection and classification pipeline, and our experimental results serve as a benchmark for future research.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I</head><label></label><figDesc>want a flight to Boston on Friday.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Filler word candidate generation pipeline: nonlinguistic filler word candidates are identified at times where VAD is activated while ASR is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Proposed two-stage filler detection and classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Frame level P/R vs (a) VAD threshold and (b) classifier backbone. 'event' stands for event classifier and 'frame' for frame classifier, solid lines are AVC-FillerNet, dashed lines are VC-FillerNet. Dotted position is the classifier threshold at 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a)Figure 5 :Figure 6 :</head><label>56</label><figDesc>Training set (b) Validation set (c) Test set (d) Entire dataset Pitch distribution of filler candidates in the (a) train, (b) validation, (c) test sets of PodcastFillers, and (d) the entire dataset. 2D ConvBlock (C=32, k=3?3) L P Pooling (p=4, k=2?4) (a) Voice activity detection (VAD) network architecture used in both AVC-FillerNet and VC-FillerNet. (b) filler classifier network architecture used for AVC-FillerNet (left fork) and VC-FillerNet (right fork).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>a) wav2vec, coarse labels (b) log-mel, coarse labels (c) wav2vec, granular labels (d) log-mel, granular labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrices for the filler classifier used in AVC-FillerNet for different input features and target labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Segment-and event-based evaluation results (%) for our proposed systems and baselines for filler word detection.</figDesc><table><row><cell>System</cell><cell>P</cell><cell>Segment R</cell><cell>F1</cell><cell>P</cell><cell>Event R</cell><cell>F1</cell></row><row><cell cols="7">AVC-FillerNet (Ours) 93.0 95.4 94.2 91.7 94.0 92.8</cell></row><row><cell cols="7">VC-FillerNet (Ours) 71.6 71.0 71.3 66.0 76.9 71.0</cell></row><row><cell>Filler-CRNN [1]</cell><cell cols="6">56.4 70.3 62.6 37.5 78.3 50.7</cell></row><row><cell>Gentle [31]</cell><cell cols="6">78.4 64.8 71.0 77.0 64.9 70.4</cell></row></table><note>two labels as separate classes, and evaluate our systems on the PodcastFillers test set, as shown in Tab. 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Segment-and event-based F1 measure (%) results for separately detecting 'uh' and 'um' with our proposed systems.</figDesc><table><row><cell>System</cell><cell cols="4">'Um' Segment Event Segment Event 'Uh'</cell></row><row><cell>AVC-FillerNet</cell><cell>92.5</cell><cell>91.0</cell><cell>85.0</cell><cell>84.3</cell></row><row><cell>VC-FillerNet</cell><cell>75.2</cell><cell>75.9</cell><cell>57.0</cell><cell>57.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">J.W. Kim, J. Salamon, P. Li, and J.P. Bello, "CREPE: A convolutional representation for pitch estimation," in ICASSP, 2018.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide additional information about the annotation interface we built to label PodcastFillers (Appx. A), the distribution of voice pitch in the dataset (Appx. B), and the model architectures used for our VAD and filler classification models (Appx. C). In Appx. D we present confusion matrices produced by the filler classification model, to provide further insight into the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Annotation interface</head><p>To annotate the PodcastFillers dataset, we custom-built the annotation interface depicted in <ref type="figure">Fig. 4</ref>. Annotators were presented with an audio player with a waveform visualization of a 5 s audio clip, with a filler word candidate at time 3 s highlighted in yellow ( <ref type="figure">Fig. 4(a)</ref>). Annotators were first asked if the yellow region contains a filler word or not. Based on their answer, a second list of options appeared for them to label the fine-grained filler or non-filler type, depicted in <ref type="figure">Fig. 4</ref>(b) and <ref type="figure">Fig. 4</ref>(c) respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pitch distribution</head><p>When curating PodcastFillers dataset, we ensured the pitch (f0) of the voices in the dataset spans the range of adult speech pitch from 60 Hz to 300 Hz. We used CREPE 4 for pitch estimation, and measure pitch in cents relative to 55 Hz. The pitch distribution for all filler candidates in the dataset is depicted in <ref type="figure">Fig. 5(d)</ref>, with a mode roughly at the center of the pitch range, around 170 Hz. The dataset contains roughly equally decreasing numbers of samples as we move away from the mode toward higher and lower voices. The train, validation and test splits, depicted in subplots (a), (b), (c) respectively, exhibit a similar pitch distribution for filler candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model architectures for VAD and the filler word classifier</head><p>The network architecture for the VAD model described in Section 2, adapted from <ref type="bibr" target="#b27">[27]</ref>, is depicted in <ref type="figure">Fig. 6(a)</ref>. The two variants of the architecture used for the filler classifier described in Section 3, a TCResNet-8 <ref type="bibr" target="#b30">[30]</ref>, are depicted in <ref type="figure">Fig. 6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Confusion matrices for the filler word classifier</head><p>In <ref type="figure">Fig. 7</ref> we depict the confusion matrices produced by the filler classification model used in the AVC-FillerNet pipeline when evaluated on 1 s filler candidate clips from the PodcastFillers test set. We use either wav2vec or log-mel as the input feature to the model, and predict either a coarse label set with 5 labels where 'uh' and 'um' are grouped into a 'filler' label, or a granular label set with 6 classes where 'uh' and 'um' are treated as separate labels. For the coarse labels, the greatest source of confusion is between 'filler' and 'words', which makes sense given their acoustic similarity. For the granular labels, 'uh' is confused with words more often than 'um' is. We conjecture the additional 'm' in the latter makes it easier to classify. For both label sets, using wav2vec as the input feature reduces the confusion between filler words and regular words compared to using log-mel as the input feature.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Increase apparent public speaking fluency by speech augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilkrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6890" to="6894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disfluencies as extra-propositional indicators of cognitive processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Womack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Alm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Calvelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pelz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on extrapropositional aspects of meaning in computational linguistics</title>
		<meeting>the workshop on extrapropositional aspects of meaning in computational linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Noisy bilstm-based models for disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4230" to="4234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semisupervised disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disfluency detection using auto-correlational neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2018)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP2018)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4610" to="4619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9193" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Giving attention to the unexpected: Using prosody innovations in disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="86" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disfluency detection with a semi-markov model and prosodic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segmentation and disfluency removal for conversational speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using Clinician Annotations to Improve Automatic Speech Recognition of Stuttered Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lunsford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcmillin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yaruss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2651" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Paralinguistic event detection from speech using probabilistic time-series smoothing and masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="173" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sep-28k: A dataset for stuttering event detection from podcasts with people who stutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6798" to="6802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Laughter and filler detection in naturalistic audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>International Speech and Communication Association</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stutternet: Stuttering detection using time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 29th European Signal Processing Conference (EUSIPCO)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="426" to="430" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fluentnet: End-toend detection of stuttered speech disfluencies with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kourkounakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hajavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2986" to="2999" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The university college london archive of stuttered speech (uclass)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bartrip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language, and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="556" to="569" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic detection of laughter and fillers in spontaneous mobile phone conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polychroniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4282" to="4287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automanner: An automated interface for making public speakers aware of their mannerisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tiet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Intelligent User Interfaces</title>
		<meeting>the 21st International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="385" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2021</title>
		<meeting>Interspeech 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3670" to="3674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaper: A library for soundscape synthesis and augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macconnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="344" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The benefit of temporally-strong labels in audio event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Voice Activity Detection in the Wild via Weakly Supervised Sound Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3665" to="3669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised Pre-Training for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3465" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Temporal convolution for real-time keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gentle: A robust yet lenient forced aligner built on kaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ochshorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Max</forename><surname>Hawkins</surname></persName>
		</author>
		<ptr target="https://lowerquality.com/gentle/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The fisher corpus: A resource for the next generations of speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
