<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
							<email>yihong.chen@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London, London</settlement>
									<country>United Kingdom, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Minervini@cs Ac</forename><surname>Ucl</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<email>s.riedel@cs.ucl.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London, London</settlement>
									<country>United Kingdom, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
							<email>p.stenetorp@cs.ucl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Automated Knowledge Base Construction (2021) Conference paper</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning good representations on multi-relational graphs is essential to knowledge base completion (KBC). In this paper, we propose a new self-supervised training objective for multirelational graph representation learning, via simply incorporating relation prediction into the commonly used 1vsAll objective. The new training objective contains not only terms for predicting the subject and object of a given triple, but also a term for predicting the relation type. We analyse how this new objective impacts multi-relational learning in KBC: experiments on a variety of datasets and models show that relation prediction can significantly improve entity ranking, the most widely used evaluation task for KBC, yielding a 6.1% increase in MRR and 9.9% increase in Hits@1 on FB15k-237 as well as a 3.1% increase in MRR and 3.4% in Hits@1 on Aristo-v4. Moreover, we observe that the proposed objective is especially effective on highly multi-relational datasets, i.e. datasets with a large number of predicates, and generates better representations when larger embedding sizes are used. Code will be available at https://github.com/facebookresearch/ssl-relation-prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aiming at completing missing entries, Knowledge Base Completion (KBC), also known as Link Prediction, plays a crucial role in constructing large-scale knowledge graphs <ref type="bibr" target="#b14">[Nickel et al., 2016</ref><ref type="bibr" target="#b6">, Ji et al., 2020</ref><ref type="bibr" target="#b10">, Li et al., 2020</ref>. Over the past years, most of the research on KBC has been focusing on Knowledge Graph Embedding models, which learn representations for all entities and relations in a Knowledge Graph, and use them for scoring whether an edge exists or not <ref type="bibr" target="#b14">[Nickel et al., 2016]</ref>. Numerous models and architectural innovations have been proposed in the literature, including but not limited to translation-based models <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>, latent factorisation models <ref type="bibr" target="#b13">[Nickel et al., 2011</ref><ref type="bibr" target="#b19">, Trouillon et al., 2016</ref><ref type="bibr" target="#b0">, Balazevic et al., 2019</ref>, and neural network-based models <ref type="bibr" target="#b2">[Dettmers et al., 2018</ref><ref type="bibr" target="#b16">, Schlichtkrull et al., 2018</ref><ref type="bibr" target="#b21">, Xu et al., 2020</ref>.</p><p>Other more recent research has been making complementary efforts on analysing the evaluation procedures for these KBC models. For instance,  call for standardisation of evaluation protocols; <ref type="bibr" target="#b7">Kadlec et al. [2017]</ref>, <ref type="bibr" target="#b15">Ruffinelli et al. [2020]</ref> and <ref type="bibr" target="#b5">Jain et al. [2020]</ref> highlight the importance of training strategies and show that careful hyper-parameter tuning can produce more accurate results than adopting more elaborate model architectures; <ref type="bibr" target="#b9">Lacroix et al. [2018]</ref> suggests that a simple model can produce state-of-the-art results when its training objective is properly selected.</p><p>Taking inspiration from these findings, this paper explores relation prediction: a simple auxiliary training objective that significantly improves multi-relational graph representation learning across several KBC models. Aside from training models to predict the subject and object entities for triples in a Knowledge Graph, we also train them to predict relation types, leading to a self-supervised training objective. Intuitively, this approach is akin to using a masked language model-like training objective <ref type="bibr" target="#b3">[Devlin et al., 2019]</ref> instead of the commonly used auto-regressive training objective for KBC. In our experiments, we find that the new auxiliary training objective significantly improves downstream link prediction accuracy.</p><p>Empirical evaluations on various models and datasets support the effectiveness of our new training objective: the largest improvements were observed on ComplEx-N3 <ref type="bibr" target="#b19">[Trouillon et al., 2016]</ref> and CP-N3 <ref type="bibr" target="#b9">[Lacroix et al., 2018]</ref> with embedding sizes between 2K and 4K, providing up to 9.9% boost in Hits@1 and 6.1% boost in MRR on FB15k-237 with negligible computational overhead.</p><p>We further experiment on datasets with varying numbers of predicates and find that relation prediction helps more when the dataset is highly multi-relational, i.e. contains a larger number of predicates. Moreover, our qualitative analysis demonstrates improved prediction of some MANY-TO-MANY <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref> predicates and more diversified relation representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>A Knowledge Graph G ? E ? R ? E contains a set of subject-predicate-object s, p, o triples, where each triple represents a relationship of type p ? R between the subject s ? E and the object o ? E of the triple. Here, E and R denote the set of all entities and relation types, respectively.</p><p>Knowledge Graph Embedding Models A Knowledge Graph Embedding (KGE) model, also referred to as neural link predictor, is a differentiable model where entities in E and relation types in R are represented in a continuous embedding space, and the likelihood of a link between two entities is a function of their representations. More formally, KGE models are defined by a parametric scoring function ? ? : E ? R ? E ? R, with parameters ? that, given a triple s, p, o , produces the likelihood that entities s and o are related by the relationship p.</p><p>Scoring Functions KGE models can be characterised by their scoring function ? ? . For example, in TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>, the score of a triple s, p, o is given by ? ? (s, p, o) = ? s + p ? o 2 , where s, p, o ? R k denote the embedding representations of s, p, and o, respectively. In Dist-Mult <ref type="bibr" target="#b22">[Yang et al., 2015]</ref>, the scoring function is defined as</p><formula xml:id="formula_0">? ? (s, p, o) = s, p, o = k i=1 s i p i o i ,<label>where</label></formula><p>? , ? , ? denotes the tri-linear dot product. Canonical Tensor Decomposition <ref type="bibr">[CP, Hitchcock, 1927</ref>] is similar to DistMult, with the difference that each entity x has two representations, x s ? R k and x o ? R k , depending on whether it is being used as a subject or object: ? ? (s, p, o) = s s , p, o o . In RESCAL <ref type="bibr" target="#b13">[Nickel et al., 2011]</ref>, the scoring function is a bilinear model given by ? ? (s, p, o) = s Po, where s, o ? R k is the embedding representation of s and p, and P ? R k?k is the representation of p. Note that DistMult is equivalent to RESCAL if P is constrained to be diagonal. Another variation of this model is ComplEx <ref type="bibr" target="#b19">[Trouillon et al., 2016]</ref>, where the embedding representations of s, p, and o are complex vectors -i.e. s, p, o ? C k -and the scoring function is given by ? ? (s, p, o) = ( s, p, o ), where (x) represents the real part of x, and x denotes the complex conjugate of x. In TuckER <ref type="bibr" target="#b0">[Balazevic et al., 2019]</ref>, the scoring function is defined as ? ? (s, p, o) = W ? 1 s ? 2 p ? 3 o, where W ? R ks?kp?ko is a three-way tensor of parameters, and s ? R ks , p ? R kp , and o ? R ko are the embedding representations of s, p, and o. In this work, we mainly focus on DistMult, CP, ComplEx, and TuckER, due to their effectiveness on several link prediction benchmarks <ref type="bibr" target="#b15">[Ruffinelli et al., 2020</ref><ref type="bibr" target="#b5">, Jain et al., 2020</ref>.</p><p>Training Objectives Another dimension for characterising KGE models is their training objective. Early tensor factorisation models such as RESCAL and CP were trained to minimise the reconstruction error of the whole adjacency tensor <ref type="bibr" target="#b13">[Nickel et al., 2011]</ref>. To scale to larger Knowledge Graphs, subsequent approaches such as <ref type="bibr" target="#b1">Bordes et al. [2013]</ref> and <ref type="bibr" target="#b22">Yang et al. [2015]</ref> simplified the training objective by using negative sampling: for each training triple, a corruption process generates a batch of negative examples by corrupting the subject and object of the triple, and the model is trained by increasing the score of the training triple while decreasing the score of its corruptions. This approach was later extended by <ref type="bibr" target="#b2">Dettmers et al. [2018]</ref> where, given a subject s and a predicate p, the task of predicting the correct objects is cast as a |E|-dimensional multi-label classification task, where each label corresponds to a distinct object and multiple labels can be assigned to the (s, p) pair. This approach is referred to as KvsAll by <ref type="bibr" target="#b15">Ruffinelli et al. [2020]</ref>. Another extension was proposed by <ref type="bibr" target="#b9">Lacroix et al. [2018]</ref> where, given a subject s and an object p, the task of predicting the correct object o in the training triple is cast as a |E|-dimensional multi-class classification task, where each class corresponds to a distinct object and only one class can be assigned to the (s, p) pair. This is referred to as 1vsAll by <ref type="bibr" target="#b15">Ruffinelli et al. [2020]</ref>.</p><p>Note that, for factorisation-based models like DistMult, ComplEx, and CP, KvsAll and 1vsAll objectives can be computed efficiently on GPUs <ref type="bibr" target="#b9">[Lacroix et al., 2018</ref><ref type="bibr" target="#b5">, Jain et al., 2020</ref>. For example for DistMult, the score of all triples with subject s and predicate p can be computed via E(s p), where denotes the element-wise product, and E ? R |E|?k is the entity embedding matrix. In this work, we follow <ref type="bibr" target="#b9">Lacroix et al. [2018]</ref> and adopt the 1vsAll loss, so as to be able to compare with their results, and since <ref type="bibr" target="#b15">Ruffinelli et al. [2020]</ref> showed that they produce similar results in terms of downstream link prediction accuracy.</p><p>Recent work on standardised evaluation protocols for KBC models  and their systematic evaluation <ref type="bibr" target="#b7">[Kadlec et al., 2017</ref><ref type="bibr" target="#b12">, Mohamed et al., 2019</ref><ref type="bibr" target="#b5">, Jain et al., 2020</ref><ref type="bibr" target="#b15">, Ruffinelli et al., 2020</ref> shows that latent factorisation based models such as RESCAL, ComplEx, and CP are very competitive when their hyper-parameters are tuned properly <ref type="bibr" target="#b7">[Kadlec et al., 2017</ref><ref type="bibr" target="#b15">, Ruffinelli et al., 2020</ref>. In this work, we show that using relation prediction as an auxiliary training task can further improve their downstream link prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Relation Prediction as An Auxiliary Training Objective</head><p>In what is referred to as the 1vsAll setting <ref type="bibr" target="#b15">[Ruffinelli et al., 2020]</ref>, KBC models are trained using a self-supervised training objective by maximising the conditional likelihood of the subject s (resp. object o) of training triples, given the predicate and the object o (resp. subject s). More formally, KBC models are trained by maximising the following objective:</p><formula xml:id="formula_1">arg max ??? s,p,o ?G [log P ? (s | p, o) + log P ? (o | s, p)] with log P ? (o | s, p) = ? ? (s, p, o) ? log o ?E exp ? ? (s, p, o ) log P ? (s | p, o) = ? ? (s, p, o) ? log s ?E exp ? ? (s , p, o) ,<label>(1)</label></formula><p>where ? ? ? are the model parameters, including entity and relation embeddings, and ? ? is a scoring function parameterised by ?. Such an objective limits predicting positions in the training objective to either the first (s) or the last (o) element of the triple.</p><p>In this work, we propose relation prediction as an auxiliary task for training KBC models. The new training objective not only contains terms for predicting the subject and the object of the triple log P (s | p, o) and log P (o | s, p) in Equation (1) -but also an objective log P (p | s, o) for predicting the relation type p:</p><formula xml:id="formula_2">arg max ??? s,p,o ?G [log P ? (s | p, o) + log P ? (o | s, p) + ? log P ? (p | s, o)] with log P ? (p | s, o) = ? ? (s, p, o) ? log p ?R exp ? ? (s, p , o) ,<label>(2)</label></formula><p>where ? ? R + is a user-specified hyper-parameter that determines the contribution of the relation prediction objective; we assume ? = 1 unless specified otherwise. This new training objective adds very little overhead to the training process, and can be easily added to existing KBC implementations; PyTorch examples are included in Appendix A. Compared to conventional approaches, relation prediction can help the model learn to further distinguish among different predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical Study</head><p>In this section, we conduct several experiments to verify the effectiveness of incorporating relation prediction as an auxiliary training objective. We are interested in the following research questions:</p><p>RQ1: How does the new training objective impact the results of downstream knowledge base completion tasks across different datasets? How does the number of relation types on the datasets affect the new training objective?</p><p>RQ2: How does the new training objective impact different models? Does it benefit all the models uniformly, or it particularly helps some of them?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ3: Does the new training objective produce better entity and relation representations?</head><p>Datasets We use Nations, UMLS, and Kinship from <ref type="bibr" target="#b8">Kok and Domingos [2007]</ref>, WN18RR <ref type="bibr" target="#b2">[Dettmers et al., 2018]</ref>, and FB15k-237 <ref type="bibr" target="#b18">[Toutanova et al., 2015]</ref>, which are commonly used in the KBC literature. As these datasets contain a relatively small number of predicates, we also experiment with Aristo-v4, the 4-th version of Aristo Tuple KB <ref type="bibr" target="#b11">[Mishra et al., 2017]</ref>, which has more than 1.6k predicates. Since Aristo-v4 has no standardised splits for KBC, we randomly sample 20k triples for test and 20k for validation. <ref type="table">Table 1</ref> summarises the statistics of these datasets. Metrics Entity ranking is the most commonly used evaluation protocol for knowledge base completion. For a given query (s, p, ?) or (?, p, o), all the candidate entities are ranked based on the scores produced by the models, and the resulting ordering is used to compute the rank of the true answer. We use the standard filtered Mean Reciprocal Rank (MRR) and Hits@K (Hit ratios of the top-K ranked results), with K ? {1, 3, 10}, as metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>We use several competitive and reproducible <ref type="bibr" target="#b15">[Ruffinelli et al., 2020</ref> models: RESCAL <ref type="bibr" target="#b13">[Nickel et al., 2011]</ref>, ComplEx <ref type="bibr" target="#b19">[Trouillon et al., 2016]</ref>, CP <ref type="bibr" target="#b9">[Lacroix et al., 2018]</ref>, and TuckER <ref type="bibr" target="#b0">[Balazevic et al., 2019]</ref>. To ensure fairness in various comparisons, we did an extensive tuning of hyper-parameters using the validation sets, which consists of 41,316 training runs in total. For the main results on all the datasets, we tuned ? using grid-search. For the ablation experiments on the number of predicates and for different choices of models, we set ? to 1 for simplicity. Details regarding the hyper-parameter sweeps can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RQ1: Impacts of Relation Prediction on Different Datasets</head><p>How does the proposed training objective impact knowledge base completion on different datasets?</p><p>To answer this research question, we compare the performance of training with relation prediction and training without relation prediction on several popular KBC datasets. For the smaller datasets (Kinship, Nations and UMLS), we selected the best one from RESCAL, ComplEx, CP, and TuckER. For larger datasets (WN18RR, FB15k-237, and Aristo-v4), due to a limited computation budget, we used ComplEx, which outperformed other models in our preliminary experiments. <ref type="table" target="#tab_2">Table 2</ref> summarises the results on the smaller datasets, where indicates training with relation prediction while indicates training without relation prediction. We can observe that relation prediction brings a 2% -4% improvement in MRR and Hits@1, as well as keeping a competitive Hits@3 and Hits@10. <ref type="table" target="#tab_3">Table 3</ref> summarises the results on the larger datasets. Including relation prediction as an auxiliary training objective brings a consistent improvement on the 3 datasets with respect to all metrics, except for Hits@10 on WN18RR. Particularly, relation prediction leads to increases of 6.1% in MRR, 9.9% in Hits@1, 6.1% in Hits@3 on FB15k-237 and 3.1% in MRR, 3.4% in Hits@1, 3.8% in Hits@3 on Aristo-v4. Compared to WN18RR, we observe a larger improvement on FB15k-237 and Aristo-v4. One potential reason is that on FB15k-237 there is a more diverse set of predicates (|R| = 237) and Aristo-v4 (|R| = 1605) than in WN18RR (|R| = 11). The number of predicates |R| on WN18RR is comparatively small, and the model gains more from distinguishing different entities than distinguishing relations. In other words, using lower values for ? (the weight of the    we report the corresponding statistics -i.e. the sum of ranks of positive differences -and the p-value as (statistics, p-value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">SIGNIFICANCE TESTING</head><p>In order to show that the improvements brought by relation perturbation are significant, we run the experiments with 5 random seeds and perform Wilcoxon signed-rank test <ref type="bibr" target="#b20">[Wilcoxon, 1992]</ref> over the metrics obtained with and without relation prediction. The test is performed as follows. First, we computed the differences between results obtained with ComplEx trained with and without relation prediction. The null hypothesis is that the median of the differences is negative. As previously discussed, relation prediction brings different impacts to WN18RR, FB15k-237, and Aristo-v4. Since one of the biggest differences among these datasets is the number of different predicates |R| (1, 605 for Aristo-v4 and 237 for FB15k-237, while only 11 for WN18RR), we would like to determine the impact of perturbing relations with various |R|. In order to achieve this, we construct a series of datasets with different |R| by sampling triples containing a subset of predicates from FB15k-237. For example, to construct a dataset with only 5 predicates, we first sampled 5 predicates from the set of 237 predicates and then extracted triples containing these 5 predicates as the new dataset. In total, we have datasets with |R| ? <ref type="bibr">[5,</ref><ref type="bibr">25,</ref><ref type="bibr">50,</ref><ref type="bibr">100,</ref><ref type="bibr">150,</ref><ref type="bibr">200]</ref> predicates. To address the noise introduced in predicate sampling during datasets construction, we experimented with 3 random seeds. For convenience, we set the weight of relation prediction ? to 1 and performed a similar grid-search over the regularisation and other hyper-parameters to ensure that the models were regularised and trained appropriately with the different amounts of training and test data points.</p><p>Results are summarised in <ref type="figure">Figure 1</ref>. As shown in the right portion of <ref type="figure">Figure 1</ref>, predicting relations helps datasets with more predicates, resulting in a 2%-4% boost in MRR, Hits@1, and Hits@3. For datasets with fewer than 50 predicates, there is considerable fluctuation in the relative change as shown in the left portion of the figure -but a clear downward trend. These results verify our hypothesis that relation prediction brings benefits to datasets with a larger number of predicates. Note that we did not tune the weight of relation prediction objective ? (and fixed it to 1), and this choice might have been sub-optimal on datasets with a fewer number of predicates.    For simplicity, we set the weight of relation prediction ? to 1. As shown in <ref type="table" target="#tab_7">Table 5</ref>, including relation prediction as an auxiliary training objective brings consistent improvement to all models. Notably, up to a 4.4% and a 6.6% increase in Hits@1 can be observed respectively for CP and ComplEx. For TuckER and RESCAL, the improvements brought by relation perturbation are relatively small. This may be due to the fact that we had to use smaller embedding sizes for TuckER and RESCAL, since these models are known to suffer from scalability problems when used with larger embedding sizes. We include the ablation on embedding sizes of the models in Section 4.2.1. As for the computational cost, in our experiments, adopting the new loss only added an average 2% increase in training time per epoch, though it might require more epochs to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ABLATIONS ON EMBEDDING SIZES</head><p>In our experiments, increasing the embedding size of the model leads to better performance. However, there might exist a saturating point where larger embedding sizes stop boosting the performance. We are interested in how perturbing relations will impact the saturating point and which embedding sizes benefit most from it. <ref type="figure" target="#fig_1">Figure 2</ref> shows the relationship between the embedding size and the MRR for CP on FB15k-237. At small embedding sizes, perturbing relations makes little difference. However, it does help CP with larger embedding sizes and delays the saturating point. As we can see, the slope of the blue curve is larger than the red one, which bends little between an embedding size of1,000 and an embedding size of 4,000. We can thus observe that perturbing relations leaves more headroom to improve the model by increasing embedding sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RQ3: Qualitative Analysis of the Learned Entity and Relation Representations</head><p>In our experiments, we observe that relation prediction improves the link prediction accuracy for MANY-TO-MANY predicates, which are known to be difficult for KBC models <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>. <ref type="table" target="#tab_8">Table 6</ref> lists the top 10 predicates that benefit most from relation prediction. We rank the predicates   <ref type="figure">, ? and (?, p, o)</ref> that are improved most by relation prediction. We can see that relation prediction helps the queries like "Where was film Magic Mike released?", "Where was Paramount Pictures founded?", "Which person appear in the film The Dictator 2012?", "Which places are located in UK?" and "Which award did Vera Drake win?".</p><p>To intuitively understand why it helps with these predicates, we ran t-SNE over the learned entity and predicate representations. Reciprocal predicates are also included in the t-SNE visualisations. We set the embedding size to 1000, and use N3 regularisation. Hyper-parameters were chosen based on the validation MRR. We run t-SNE for 5000 steps with 50 as perplexity. As we can see from <ref type="figure" target="#fig_2">Figure 3</ref>, there are more predicate clusters in the t-SNE visualisation for relation prediction compared to without relation prediction. This demonstrates relation prediction helps the model distinguish between different predicates: Most predicates are separated from the entities (the pink region) while some predicates with similar semantics or subject-object contexts form a cluster (the red region); There are also a few predicates, which are not close to their predicate counterparts but instead close to highly related entities (the green region). <ref type="table">Table 7</ref> lists 3 example predicates for each region. Though there can be information loss during the process of projecting high-dimensional embedding vectors into 2-dimensional space, we hope this visualisation will help illustrate how relation prediction helps to learn more diversified predicate representations.</p><p>Pink Region /base/schemastaging/organization extra/phone number./base/schemastaging/phone sandbox/contact category /location/statistical region/places exported to./location/imports and exports/exported to /sports/sports league/teams./sports/sports league participation/team Red Region /people/person/nationality /people/person/religion /soccer/football team/current roster./sports/sports team roster/position Green Region /education/educational institution/students graduates./education/education/student /common/topic/webpage./common/webpage/category /education/educational institution/students graduates./education/education/major field of study <ref type="table">Table 7</ref>: Three example predicates in each region of the t-SNE plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>In this paper, we propose to use a new self-supervised objective for training KBC models -by simply incorporating relation prediction into the commonly used 1vsAll objective. In our experiments, we show that adding such a simple learning objective is significantly helpful to various KBC models. It brings up to 9.9% boost in Hits@1 for ComplEx trained on FB15k-237, even though the evaluation task of entity ranking might seem irrelevant to relation prediction.</p><p>Our work suggests a worthwhile direction towards devising relation-aware self-supervised objectives for KBC. In this paper, we mainly focus on simple factorisation-based models. Future work will consider analysing the proposed objective for more complex KBC models, such as graph neural network-based KBC models, and on more datasets. Another interesting future work direction is analysing the proposed auxiliary objective on more downstream applications besides link prediction, and evaluate whether it can be used to learn useful multi-relational graph representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Hyper-parameters Sweeps</head><p>In this section, we summarise all the hyper-parameters used in our experiments. We used Tesla P100 and Tesla V100 GPUs to run the experiments. We implemented each model by PyTorch. Our codebase is based on https://github.com/facebookresearch/kbc.   <ref type="table">Table 9</ref>: Best hyper-parameter configuration and the corresponding Validation MRR on the smaller datasets. d stands for embedding size, d r stands for a separate embedding size of relations, lr is the learning rate, bsz is the batch size, reg is the regularisation strength, ? is the weighting of relation prediction -NA means "not applicable".</p><p>For all small datasets (Kinship, Nations, UMLS), we trained RESCAL, ComplEx, CP and TuckER with Adagrad optimiser and N3 regularisation for at most 400 epochs. Reciprocal triples were included since they are reported to be helpful <ref type="bibr" target="#b2">[Dettmers et al., 2018</ref><ref type="bibr" target="#b9">, Lacroix et al., 2018</ref>. We did grid searches over hyper-parameter combinations and chose the best configuration for each dataset based on validation MRR. We listed the grids of hyper-parameter search in <ref type="table" target="#tab_10">Table 8</ref> and report the best-searched configuration in <ref type="table">Table 9</ref>. As for the balancing between relation prediction and entity prediction, we searched the weight of relation prediction over {4, 2, 0.5, 0.25, 0.125}.</p><p>B.1.2 WN18RR, FB15K-237, AND ARISTO-V4</p><p>For all datasets, we trained ComplEx with N3 regularizer and Adagrad optimiser and N3 regularisation for at most 400 epochs. Reciprocal triples were included since they are reported to be helpful <ref type="bibr" target="#b2">[Dettmers et al., 2018</ref><ref type="bibr" target="#b9">, Lacroix et al., 2018</ref>. As for the weight of relation prediction, we searched over different zones on different datasets. For WN18RR, we searched the weight of relation prediction over [0.005, 0.001, 0.05, 0.1, 0.5, 1]. For FB15k-237, we searched over <ref type="bibr">[0.125, 0.25, 0.5, 1, 2, 4]</ref>.</p><p>For Aristo-v4, we searched over <ref type="bibr">[0.125, 0.25, 0.5, 1, 2, 4]</ref>. We did grid searches over hyper-parameter combinations and chose the best configuration for each dataset based on validation MRR. We report the grids for each dataset in <ref type="table" target="#tab_12">Table 10</ref>, and the best found configuration in <ref type="table" target="#tab_13">Table 11</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyper-parameter Ranges of Relation Prediction Across Models</head><p>We experiment with each model on FB15k-237. Note that the original TucKER <ref type="bibr" target="#b0">[Balazevic et al., 2019]</ref> includes some training strategies which are not used in CP, ComplEx and TuckER, like dropout, learning rate decay etc. However, for a fair comparison of how relation prediction affects each model, we trained all the models conditioned on similar settings with Adagrad optimizer and N3 regularisation for at most 400 epochs. We did grid searches and selected the best hyperparameter configurations according to validation MRR. We set the weight of relation prediction to 1 in this experiment. <ref type="table" target="#tab_2">Table 12</ref> lists the grid of the shared hyper-parameters. For RESCAL, the regularisation over predicate matrices can be normalised over the rank to achieve better results. Also F2 regularisation empirically performed better than N3 regulariser for RESCAL. For TuckER, the ranks for predicate and entity are different. <ref type="table" target="#tab_3">Table 13</ref> lists the best hyper-parameter configuration found by our search.        <ref type="table" target="#tab_7">Table 15</ref>: Top 20 (s, p, o) test triples, based on their increase of the left-hand-side (i.e. on the task of predicting s given p and o) reciprocal rank after we introduce the relation prediction auxiliary objective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Hits@1 versus embedding size for CP on FB15k-237, each point represents a model trained with some specific embedding size with (blue) / -out (red) perturbing relations. The smallest embedding size is 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>: t-SNE visualisations for ComplEx embeddings, trained with relation prediction (left) and without relation prediction (right). Red points and blue points correspond to predicates and entities respectively. Dashed boxes highlight different clusters. /ice hockey/hockey team/current roster./sports/sports team roster/position /sports/sports team/roster./baseball/baseball roster position/position /location/country/second level divisions /tv/tv producer/programs produced./tv/tv producer term/program /olympics/olympic sport/athletes./olympics/olympic athlete affiliation/olympics /award/award winning work/awards won./award/award honor/honored for /music/instrument/family /olympics/olympic games/sports /base/biblioness/bibs location/state /soccer/football team/current roster./soccer/football roster position/position</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Appendix A. Code Snippet for Relation Prediction as an Auxiliary Training Objective Figure 4 demonstrates how to add relation prediction to the existing implementation of ComplEx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Relation Prediction for ComplEx, the red region shows the lines related to using relation prediction as an auxiliary training task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>MRR versus Rank for CP on FB15k-237. Appendix C. Additional Results C.1 More Metrics for Ablation on Rank</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 (</head><label>5</label><figDesc>MRR), Figure 6 (Hits@3) and Figure 7 (Hits@10) show the additional metric for the experiments ablating ranks. Blue indicates training with relation prediction while red indicates training without prediction. The range of the rank is[25, 50, 100, 500, 1000, 2000, 3000, 4000]    C.2 Top 20 Queries That Are Improved Most by Relation Prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dataset statistics, where |E| and |R| indicate the numbers of entities and predicates.</figDesc><table><row><cell>Dataset</cell><cell>|E|</cell><cell>|R|</cell><cell cols="3">#Train #Validation #Test</cell></row><row><cell>Nations</cell><cell>14</cell><cell>55</cell><cell>1,592</cell><cell>100</cell><cell>301</cell></row><row><cell>UMLS</cell><cell>135</cell><cell>46</cell><cell>5,216</cell><cell>652</cell><cell>661</cell></row><row><cell>Kinship</cell><cell>104</cell><cell>25</cell><cell>8,544</cell><cell>1,068</cell><cell>1,074</cell></row><row><cell cols="2">WN18RR 40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell cols="4">FB15k-237 27,395 237 272,115</cell><cell>17,535</cell><cell>20,466</cell></row><row><cell cols="4">Aristo-v4 44,950 1,605 242,594</cell><cell>20,000</cell><cell>20,000</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Dataset Entity Prediction Relation Prediction MRR Hits@1 Hits@3 Hits@10</figDesc><table><row><cell>Kinship</cell><cell>0.920</cell><cell>0.867</cell><cell>0.970</cell><cell>0.990</cell></row><row><cell></cell><cell>0.897</cell><cell>0.835</cell><cell>0.955</cell><cell>0.987</cell></row><row><cell></cell><cell>0.916</cell><cell>0.866</cell><cell>0.964</cell><cell>0.988</cell></row><row><cell>Nations</cell><cell>0.686</cell><cell>0.493</cell><cell>0.871</cell><cell>0.998</cell></row><row><cell></cell><cell>0.813</cell><cell>0.701</cell><cell>0.915</cell><cell>1.000</cell></row><row><cell></cell><cell>0.827</cell><cell>0.726</cell><cell>0.915</cell><cell>0.998</cell></row><row><cell>UMLS</cell><cell>0.863</cell><cell>0.795</cell><cell>0.914</cell><cell>0.979</cell></row><row><cell></cell><cell>0.960</cell><cell>0.930</cell><cell>0.991</cell><cell>0.998</cell></row><row><cell></cell><cell>0.971</cell><cell>0.954</cell><cell>0.986</cell><cell>0.997</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test performance comparison on Kinship, Nations, and UMLS. We conducted an extensive hyper-parameter search over 4 different models, namely RESCAL, ComplEx, CP, and TuckER, where the model is also treated as a hyper-parameter. Including relation prediction as an auxiliary training objective on these three datasets helps in terms of test MRR and Hits@1, while keeping competitive test Hits@3 and Hits@10. More details on the hyperparameter selection process are available in Appendix B.1.1.</figDesc><table><row><cell cols="5">Dataset Entity Prediction Relation Prediction MRR Hits@1 Hits@3 Hits@10</cell></row><row><cell>WN18RR</cell><cell>0.258</cell><cell>0.212</cell><cell>0.290</cell><cell>0.339</cell></row><row><cell></cell><cell>0.487</cell><cell>0.441</cell><cell>0.501</cell><cell>0.580</cell></row><row><cell></cell><cell>0.488</cell><cell>0.443</cell><cell>0.505</cell><cell>0.578</cell></row><row><cell>FB15K-237</cell><cell>0.263</cell><cell>0.187</cell><cell>0.287</cell><cell>0.411</cell></row><row><cell></cell><cell>0.366</cell><cell>0.271</cell><cell>0.401</cell><cell>0.557</cell></row><row><cell></cell><cell>0.388</cell><cell>0.298</cell><cell>0.425</cell><cell>0.568</cell></row><row><cell>Aristo-v4</cell><cell>0.169</cell><cell>0.120</cell><cell>0.177</cell><cell>0.267</cell></row><row><cell></cell><cell>0.301</cell><cell>0.232</cell><cell>0.324</cell><cell>0.438</cell></row><row><cell></cell><cell>0.311</cell><cell>0.240</cell><cell>0.336</cell><cell>0.447</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test performance comparison on WN18RR, FB15k-237, and Aristo-v4 using ComplEx. Including relation prediction as an auxiliary training objective brings consistent improvements across the three datasets on all metrics except Hits@10 on WN18RR. On FB15k-237 and Aristo-v4, adding relation prediction yields larger improvements in downstream link prediction tasks. More details on the hyper-parameter selection process are available in Appendix B.1.2. relation prediction objective) is more suitable for datasets with fewer predicates but a large number of entities. We include ablations on |R| in Section 4.1.2. WN18RR (15.0, 0.03125) (15.0, 0.03125) (15.0, 0.03125) (3.0, 0.76740) FB15k-237 (15.0, 0.03125) (15.0, 0.03125) (15.0, 0.03125) (15.0, 0.03125)</figDesc><table><row><cell>Dataset</cell><cell>MRR</cell><cell>Hits@1</cell><cell>Hits@3</cell><cell>Hits@10</cell></row><row><cell cols="5">Aristo-v4 (15.0, 0.03125) (15.0, 0.03125) (15.0, 0.03125) (15.0, 0.03125)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Wilcoxon signed-rank test for ComplEx-N3 on several datasets. For each dataset and metric,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>summarises the result. We can observe that almost all p-values are about 0.03, which means we can reject the null hypothesis at a confidence level of about 97%. The new training objective that incorporates relation prediction as an auxiliary training objective significantly improves the performance of KBC models except for Hits@10 on WN18RR.4.1.2 ABLATION ON THE NUMBER OF PREDICATES</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Test performance comparison on FB15k-237 across 4 different models -CP, ComplEx, RESCAL, and TuckER. We set the weight of relation prediction to 1 and performed a grid search over hyper-parameters. More details are available in the appendix. While relation prediction seems to help all 4 models, it brings more benefit to CP and ComplEx compared to TuckER and RESCAL.</figDesc><table /><note>4.2 RQ2: Impacts of Relation Prediction on Different KBC Models For measuring how does relation prediction influences the downstream accuracy of KBC models, we run experiments on FB15k-237 with several models -namely ComplEx, CP, TuckER, and RESCAL.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Top 10 predicates that are improved most by relation prediction.by averaging the associated MRR of (s, p, ?) and (?, p, o) queries.Table 14 and Table 15list the top 20 queries of (s, p</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter Search Different KBC Models on Small Datasets (Kinship, Nations, UMLS); d stands for embedding size, d r stands for a separate embedding size of relations, lr is the learning rate, bsz is the batch size, and reg is the regularisation strength.</figDesc><table><row><cell>Dataset</cell><cell>Relation Prediction Entity Prediction Model</cell><cell>d</cell><cell>d r</cell><cell cols="2">lr bsz</cell><cell>reg</cell><cell>? Dev MRR</cell></row><row><cell>KINSHIP</cell><cell>Tucker</cell><cell cols="3">200 100 0.10</cell><cell>10</cell><cell>0.1</cell><cell>NA 0.919581</cell></row><row><cell></cell><cell>CP</cell><cell cols="3">2000 NA 0.10</cell><cell cols="2">50 0.01</cell><cell>NA 0.897429</cell></row><row><cell></cell><cell>CP</cell><cell cols="3">2000 NA 0.10</cell><cell cols="3">50 0.05 4.000 0.918323</cell></row><row><cell>NATIONS</cell><cell>Tucker</cell><cell>200</cell><cell cols="2">50 0.01</cell><cell>10</cell><cell>0.1</cell><cell>NA 0.686010</cell></row><row><cell></cell><cell>CP</cell><cell cols="3">2000 NA 0.01</cell><cell cols="2">10 0.01</cell><cell>NA 0.855388</cell></row><row><cell></cell><cell>TuckER</cell><cell>200</cell><cell cols="2">25 0.01</cell><cell cols="3">10 0.10 0.250 0.865352</cell></row><row><cell>UMLS</cell><cell>CP</cell><cell cols="2">1000 NA</cell><cell cols="3">0.1 500 0.01</cell><cell>NA 0.863008</cell></row><row><cell></cell><cell cols="4">ComplEx 1000 NA 0.10</cell><cell cols="2">10 0.00</cell><cell>NA 0.967626</cell></row><row><cell></cell><cell cols="4">ComplEx 1000 NA 0.01</cell><cell cols="3">10 0.00 0.500 0.971612</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameter Search for Vanila Relation Perturbation over ComplEx on Different Datasets d stands for embedding size. lr is the learning rate. bsz is the batch size. reg is the regularization strength. ? is the weighting of relation prediction.</figDesc><table><row><cell>Dataset</cell><cell>Relation Prediction Entity Prediction</cell><cell>d</cell><cell>lr</cell><cell>bsz</cell><cell>reg</cell><cell>? Dev MRR</cell></row><row><cell>WN18RR</cell><cell></cell><cell cols="2">1000 0.10</cell><cell>500</cell><cell>0.5</cell><cell>NA 0.257945</cell></row><row><cell></cell><cell></cell><cell cols="2">1000 0.10</cell><cell>100</cell><cell>0.10</cell><cell>NA 0.488083</cell></row><row><cell></cell><cell></cell><cell cols="2">1000 0.10</cell><cell>100</cell><cell cols="2">0.10 0.050 0.490053</cell></row><row><cell>FB15k-237</cell><cell></cell><cell cols="4">1000 0.10 1000 0.0005</cell><cell>NA 0.262888</cell></row><row><cell></cell><cell></cell><cell cols="2">1000 0.10</cell><cell>100</cell><cell>0.05</cell><cell>NA 0.372305</cell></row><row><cell></cell><cell></cell><cell cols="3">1000 0.10 1000</cell><cell cols="2">0.05 4.000 0.393722</cell></row><row><cell>Aristo-v4</cell><cell></cell><cell cols="3">1500 0.10 1000</cell><cell>0.01</cell><cell>NA 0.168700</cell></row><row><cell></cell><cell></cell><cell cols="2">1500 0.01</cell><cell>500</cell><cell>0.01</cell><cell>NA 0.307076</cell></row><row><cell></cell><cell></cell><cell cols="2">1500 0.10</cell><cell>100</cell><cell cols="2">0.05 0.125 0.314443</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Best hyper-parameter configurations and the corresponding validation MRR for ComplEx across datasets with Weighted Relation Perturbation. d stands for embedding size, lr is the learning rate, bsz is the batch size, reg is the regularisation strength, ? is the weighting of relation prediction. NA indicates not applicable.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Hyper-parameter Search Different KBC Models on FB15k-237; d stands for embedding size, d r stands for a separate embedding size of relations, lr is the learning rate, bsz is the batch size, reg is the regularisation strength.</figDesc><table><row><cell>Model</cell><cell>Relation Prediction d or (d, d r )</cell><cell>lr</cell><cell>bsz</cell><cell>reg Dev MRR</cell></row><row><cell>RESCAL</cell><cell>512</cell><cell>0.1</cell><cell cols="2">500 0.00 0.365384</cell></row><row><cell></cell><cell>512</cell><cell>0.1</cell><cell cols="2">100 0.00 0.366789</cell></row><row><cell>ComplEx</cell><cell>1000</cell><cell>0.1</cell><cell cols="2">100 0.05 0.372305</cell></row><row><cell></cell><cell>1000</cell><cell cols="3">0.1 1000 0.05 0.387133</cell></row><row><cell>CP</cell><cell>4000</cell><cell>0.1</cell><cell cols="2">100 0.05 0.364245</cell></row><row><cell></cell><cell>4000</cell><cell cols="3">0.1 1000 0.05 0.372408</cell></row><row><cell>TuckER</cell><cell cols="2">(1000, 100) 0.1</cell><cell cols="2">100 0.10 0.358857</cell></row><row><cell></cell><cell cols="2">(1000, 100) 0.1</cell><cell cols="2">100 0.50 0.359932</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Best hyper-parameter configuration and the corresponding validation MRR on FB15k-237 across models; for simplicity, we set the weighting ? to 1. d stands for embedding size, d r stands for a separate embedding size of relations, lr is the learning rate, bsz is the batch size, reg is the regularisation strength.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14</head><label>14</label><figDesc>shows the top 20 queries of (?, p, o) form that are improved most by relation prediction whileTable 15shows the top 20 queries of (s, p, ?) form that are improved most by relation prediction.</figDesc><table><row><cell></cell><cell>0.40</cell></row><row><cell></cell><cell>0.39</cell></row><row><cell></cell><cell>0.38</cell></row><row><cell>Hits@3</cell><cell>0.36 0.37</cell></row><row><cell></cell><cell>0.35</cell></row><row><cell></cell><cell>0.34</cell></row><row><cell></cell><cell>0.33</cell></row><row><cell></cell><cell>0</cell><cell>500 1000 1500 2000 2500 3000 3500 4000 Embedding Size</cell></row><row><cell></cell><cell cols="2">Figure 6: Hits@3 versus Rank for CP on FB15k-237.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Top 20 (s, p, o)  test triples, based on their increase of the right-hand-side (i.e. on the task of predicting o given p and s) reciprocal rank after we introduce the relation prediction auxiliary objective.</figDesc><table><row><cell>Subject</cell><cell>Predicate</cell><cell>Object</cell><cell>? 1/Rank</cell></row><row><cell>Midfielder</cell><cell>/soccer/football team/current roster</cell><cell>Wimbledon F.C.</cell><cell>0.990</cell></row><row><cell></cell><cell>/soccer/football roster position/position</cell><cell></cell><cell></cell></row><row><cell>Forward (association football)</cell><cell cols="2">/soccer/football team/current roster /sports/sports team roster/position Iraq national football team</cell><cell>0.989</cell></row><row><cell>United States</cell><cell>/film/film/release date s</cell><cell>Cleopatra (1963 film)</cell><cell>0.975</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>Critics' Choice Movie Award for Best Act-</cell><cell>/award/award nominee/award nominations</cell><cell>Matt Damon</cell><cell>0.975</cell></row><row><cell>ing Ensemble</cell><cell>/award/award nomination/award</cell><cell></cell><cell></cell></row><row><cell>Female</cell><cell>/people/person/gender</cell><cell>Grey DeLisle</cell><cell>0.968</cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell cols="2">New Year's Eve (2011 film) 0.967</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>United States dollar</cell><cell>/location/statistical region/rent50 2</cell><cell>Anchorage, Alaska</cell><cell>0.966</cell></row><row><cell></cell><cell>/measurement unit/dated money value/currency</cell><cell></cell><cell></cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>Killing Them Softly</cell><cell>0.962</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>Glendale, California</cell><cell>/people/person/spouse s /people/marriage/location of ceremony</cell><cell>Jane Wyman</cell><cell>0.962</cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>ParaNorman</cell><cell>0.962</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>Streaming media</cell><cell>/film/film/distributors</cell><cell>Pulp Fiction</cell><cell>0.960</cell></row><row><cell></cell><cell>/film/film film distributor relationship/film distribution medium</cell><cell></cell><cell></cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>Magic Mike</cell><cell>0.958</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>United States dollar</cell><cell>/location/statistical region/rent50 2</cell><cell>Napa County, California</cell><cell>0.952</cell></row><row><cell></cell><cell>/measurement unit/dated money value/currency</cell><cell></cell><cell></cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>Rock of Ages (2012 film)</cell><cell>0.952</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>Contact (1997 American</cell><cell>0.950</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell>film)</cell><cell></cell></row><row><cell>Streaming media</cell><cell>/film/film/distributors</cell><cell>American History X</cell><cell>0.950</cell></row><row><cell></cell><cell>/film/film film distributor relationship/film distribution medium</cell><cell></cell><cell></cell></row><row><cell>Los Angeles</cell><cell>/organization/organization/place founded</cell><cell>Paramount Pictures</cell><cell>0.947</cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>L.A. Confidential (film)</cell><cell>0.941</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row><row><cell>Psychological thriller</cell><cell>/film/film/genre</cell><cell>Family Plot</cell><cell>0.941</cell></row><row><cell>United Kingdom</cell><cell>/film/film/release date s</cell><cell>Moneyball (film)</cell><cell>0.938</cell></row><row><cell></cell><cell>/film/film regional release date/film release region</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the reviewers for their constructive feedback. We would also want to thank all the other members in UCL NLP for their support throughout COVID. PM and PS are supported by the European Union's Horizon 2020 research and innovation programme under grant agreement no. 875160.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Knowledge base completion: Baseline strikes back (again). ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388</idno>
		<title level="m">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-2609</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-2609" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<editor>Phil Blunsom, Antoine Bordes, Kyunghyun Cho, Shay B. Cohen, Chris Dyer, Edward Grefenstette, Karl Moritz Hermann, Laura Rimell, Jason Weston, and Scott Yih</editor>
		<meeting>the 2nd Workshop on Representation Learning for NLP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/lacroix18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on approaches and applications of knowledge representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkui</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="312" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-targeted, high precision knowledge extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Loss functions in knowledge graph embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?t</forename><surname>Sameh K Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Nov??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emir</forename><surname>Vandenbussche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu?oz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DL4KG2019-Workshop on Deep Learning for Knowledge Graphs</title>
		<meeting>DL4KG2019-Workshop on Deep Learning for Knowledge Graphs</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A reevaluation of knowledge graph completion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.489/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1174</idno>
		<ptr target="https://www.aclweb.org/anthology/D15-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamically pruned message passing networks for large-scale knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkeuAhVKvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
