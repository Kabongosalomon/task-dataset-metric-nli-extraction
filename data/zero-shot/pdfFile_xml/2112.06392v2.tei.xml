<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Overlooked Classifier in Human-Object Interaction Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft ?</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Overlooked Classifier in Human-Object Interaction Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-Object Interaction</term>
					<term>Action Recognition</term>
					<term>Scene Un- derstanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) recognition is challenging due to two factors: (1) significant imbalance across classes and (2) requiring multiple labels per image. This paper shows that these two challenges can be effectively addressed by improving the classifier with the backbone architecture untouched. Firstly, we encode the semantic correlation among classes into the classification head by initializing the weights with language embeddings of HOIs. As a result, the performance is boosted significantly, especially for the few-shot subset. Secondly, we propose a new loss named LSE-Sign to enhance multi-label learning on a longtailed dataset. Our simple yet effective method enables detection-free HOI classification, outperforming the state-of-the-arts that require object detection and human pose by a clear margin. Moreover, we transfer the classification model to instance-level HOI detection by connecting it with an off-the-shelf object detector. We achieve state-of-the-art without additional fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human-Object Interaction (HOI) recognition has drawn significant interest for its essential role in scene understanding. HOI recognition aims to retrieve multiple &lt;verb, object&gt; pairs (e.g. , "hold, apple") in the image. Image-level HOI classification is challenging for two reasons. First, the datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref> are long-tailed and have severe imbalance across classes. Second, it has multiple labels per image and the positions of the HOIs are unknown. Most of the recent work focuses on leveraging the object and keypoint detection while overlooking the classifier. We discover that the classifier is critical for both HOI classification and detection.</p><p>A crucial distinction between HOI recognition and conventional image classification is that the HOI classes are combinations of verbs and objects, so there is a stronger semantic correlation between the classes. For example, the average number of classes sharing the same verb is 5.1 in the HICO <ref type="bibr" target="#b4">[5]</ref> dataset, and the average number of classes sharing the same object category is 7.5. However, the effective use of such semantic cues is still underexplored in existing methods. make, vase make, hot dog make, cake make, pizza cook, pizza make, sandwich cook, sandwich eat, sandwich eat, pizza <ref type="figure">Fig. 1</ref>. t-SNE visualization of BERT-encoded HOI embeddings. Each point represents an HOI class. Left: zoomed view of a cluster. HOIs with closer semantic meanings (e.g. , &lt;make, pizza&gt;, &lt;cook, pizza&gt;) locate closely. Right: 600 HOI classes from the HICO dataset <ref type="bibr" target="#b4">[5]</ref> colored by the verb. The figure shows a clear clustering pattern, which captures the semantic structure. We use these embeddings as weight initialization Some work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51]</ref> classify the objects and verbs independently. This idea considerably reduces the number of classes to be predicted but is prone to polysemy because the same verb can have different meanings in different HOIs (e.g. , "make, pizza", "make, vase"). As one step forward, we use language models to generate text embeddings of HOI classes and use them to initialize the weight vectors in the linear classification layer. This approach, later referred to as Language Embedding Initialization, offers superior performance compared with the widely used Random Initialization methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, particularly in the few-shot cases. The rationale lies in the fact that the weight vectors in the linear classifier can be viewed as the proxy (feature center) of each class <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>. When using language embeddings as weight initialization, the initial positions of proxies in the vector space are no longer random but are determined by their semantic structure. This configuration eases the multi-label training process as the vision feature from the backbone is guided to move toward the semantic feature center, where HOIs that frequently co-exist in an image also closely distribute (see <ref type="figure">Fig. 1</ref>). Furthermore, language embeddings provide proper weight initialization for few-shot classes, of which the weight vectors in the classifier cannot be properly learned with just a few samples.</p><p>Existing studies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b8">9]</ref> use weighted sigmoid cross-entropy loss <ref type="bibr" target="#b38">[39]</ref> to handle the positive/negative imbalance per class. This is superior to cross-entropy loss but is less effective when the class is extremely imbalanced (20.1% of the classes in HICO has ? 5 positive samples). We propose a new loss function named Log-Sum-Exp Sign (LSE-Sign) loss to enhance such a multi-label learning scenario on long-tailed datasets. We design the loss function so that its gradient form is a softmax function of losses from all classes. The softmax function amplifies the loss from poorly classified classes and suppresses the loss from well-classified classes. This idea facilitates multi-label learning and balances the loss over all classes dynamically. Please note that the LSE-Sign loss is different from focal loss <ref type="bibr" target="#b32">[33]</ref> in two ways: 1). it considers the relative difficulty among all classes while focal loss operates on each class, and 2) focal loss requires tuning two hyper-parameters.</p><p>Though a detection-free baseline using ImageNet-1K pre-trained ViT-B/32 as the backbone experiences a severe performance degradation compared with the detection-assisted state-of-the-arts (from 47.1 to 37.8 mAP), Language Embedding Initialization using BERT successfully boosts it to 50.6 mAP. The applicability generalizes to other types of language models, including SimCSE <ref type="bibr" target="#b11">[12]</ref> and CLIP <ref type="bibr" target="#b40">[41]</ref>. LSE-Sign loss provides an average of 3.1 mAP improvement in eight experiments <ref type="table" target="#tab_5">(Table 6</ref>). Based upon these two findings, we outperform existing detection-assisted approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref> by a clear margin. With a ResNet101 backbone, our method achieves 53.6 mAP, surpassing the state-of-the-art <ref type="bibr" target="#b29">[30]</ref> that uses both object and human keypoint detections by 6.5 mAP. With a ViT-B/16 backbone, our method achieves 65.6 mAP. On few-shot subsets, we achieve 52.7 mAP on 1-shot and 56.9 mAP on 5-shot classes.</p><p>On the instance level, we decouple the HOI detection task to localization and regional HOI classification. The advantage of being detection-free allows us to apply the classification model on a regional human-object box pair produced by any off-the-shelf detector. With a vision transformer backbone, we convert bounding boxes into an attention mask so that the CLS token can only attend to the region of interest. Without additional training under instance-level supervision, it achieves state-of-the-art (SOTA) performance (32.35 mAP) on the HICO-DET dataset, outperforming the SOTA <ref type="bibr" target="#b49">[50]</ref> which is trained on HICO-DET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>HOI Classification: This is an image-level multi-label classification task, where HOI labels are given without specifying the positions of the human and object. Existing work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref> successfully leverage object detectors to locate humans and objects in the first stage and then infer interactions between human-object pairs. Though the performance gets improved over time, the pipelines are becoming more and more complicated. Since the HOI labels are not grounded to specific human-object pairs, Multiple Instance Learning (MIL) <ref type="bibr" target="#b37">[38]</ref> is required in these work to enable training. Girdhar et al. <ref type="bibr" target="#b12">[13]</ref> removes MIL by utilizing human pose guided attention maps. PaStaNet <ref type="bibr" target="#b30">[31]</ref> and HAKE <ref type="bibr" target="#b29">[30]</ref> achieve performance boost by utilizing PaSta, which are action classes on the body part level (e.g. , "right hand: hold something"), and require additional annotations to train. In contrast, this paper shifts focus to a simplified detection-free solution. Unlike existing work, our perspective is to leverage the semantic correlations among HOI classes and design a suitable loss function. The proposed method emphasizes the overlooked classification head and complements previous studies.</p><p>HOI Detection: The detection task requires a pair of human-object boxes and the corresponding HOI classes, and instance-level supervision is provided. Existing work can be categorised into three streams. Two-stage methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51]</ref> require object detection prior to HOI classification to extract regional features. Graph neural networks are often used <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49]</ref> to classify the verbs between humanobject pairs. For training, two-stage methods usually initialize their backbone from a pre-trained object detector. One-stage methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">48]</ref> mostly execute object detection and HOI detection in parallel and match them afterwards. Recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> achieve end-to-end HOI detection based on DETR <ref type="bibr" target="#b2">[3]</ref> and benefit from the wider perception field of transformers <ref type="bibr" target="#b45">[46]</ref>.</p><p>Language Models: Natural language is occasionally used in related work as statistical priors or additional features. DRG <ref type="bibr" target="#b9">[10]</ref> and HAKE <ref type="bibr" target="#b29">[30]</ref> use BERTgenerated embeddings of object class or part-level actions (PaSta) as features. BERT <ref type="bibr" target="#b6">[7]</ref> is trained in an unsupervised manner by predicting masked tokens in the sequence (i.e., masked language modeling). The BERT embedding space is not isotropic, which could harm the performance in downstream tasks. SimCSE <ref type="bibr" target="#b11">[12]</ref> and other methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref> improve the sentence embedding of BERT. CLIP <ref type="bibr" target="#b40">[41]</ref> and ALIGN <ref type="bibr" target="#b20">[21]</ref> jointly train a language model and a vision encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce the implementation of Language Embedding Initialization and the LSE-Sign loss. The two ideas complement the classifier while being agnostic to the backbone architecture. Our DEFR method has a simple pipeline (shown in <ref type="figure">Fig. 2</ref>), including a vision transformer <ref type="bibr" target="#b7">[8]</ref> as the backbone and a linear layer as the classifier. The classifier is initialized with language embeddings instead of the conventional random initialization methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Embedding Initialization</head><p>HOI classes have strong semantic correlations as they are combinations of verbs and objects. Language models can effectively encode the structure of HOI classes, which can be hard for the image backbone to learn along (see <ref type="figure">Fig. 1</ref>). Based on this observation, we use language embedding to initialize the linear classification layer. By doing so, the semantic cues are encoded into the classifier, which guides training and provides proper weight initialization for few-shot classes in the classifier. This is referred to as Language Embedding initialization.</p><p>Specifically, we first convert the HOI classes (e.g. , "&lt; ride, bicycle &gt;") to prompts (e.g. , "a person riding a bicycle") and then generate language embeddings with a language model. The embeddings are normalized to be consistent in the scale, and then used as the initial weight in the linear classification layer. The bias of the linear classifier is zero-initialized. During training, the output logit for the i th class is the dot product of the image feature and w i , a row vector in the classifier's weight w, plus bias (see <ref type="figure">Fig. 2</ref>). Since dot product is the unnormalized cosine similarity, w i is often considered as the proxy for a given class <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>In our scenario, language embedding initialization using BERT fires a performance boost of 12.8 mAP for an ImageNet-1K pre-trained ViT-B/32 backbone. More importantly, the applicability is agnostic to backbone architectures (ViT and ResNet <ref type="bibr" target="#b17">[18]</ref>), pre-training, and the language model being used. The gain is maximized when the language model is jointly trained with the image encoder (e.g. , CLIP). <ref type="figure" target="#fig_1">Fig. 3</ref> visualizes the linear classifier's weight vectors per class before (left-column) and after fine-tuning with both CLIP (middle-column) and ImageNet-1K (right-column) pre-trained backbones. The weight vectors initialized with language embeddings maintain clustered after fine-tuning, meaning the initialization is closer to optimal. However, this structure is difficult to learn when fine-tuned with the random initialization, resulting in lower performance (shown in <ref type="figure" target="#fig_1">Fig. 3</ref> bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Log-Sum-Exp Sign (LSE-Sign) Loss</head><p>We propose a log-sum-exp sign (LSE-Sign) loss function to facilitate multi-class learning, as an image usually contains multiple interactions (e.g. , cut carrot, hold carrot, peel carrot). Let x denote a feature vector from the backbone and y = {y 1 , y 2 , . . . , y C } denote multiple class labels, where C is the number of HOI classes, and y i ? {1, ?1}, indicating the positive and negative labels, respectively. We first pass the ? scaled vision feature to the last linear layer as:</p><formula xml:id="formula_0">s i = ?x T w i + b i<label>(1)</label></formula><p>where w i is the i th row in the weight matrix in the linear classifier, corresponding to the i th HOI class. It is initialized with a normalized text embedding. ? is a scalar hyper-parameter controlling the range (the effect is ablated in <ref type="table" target="#tab_7">Table 8</ref>).   the label y i controls the sign. The overall loss is defined as the log-sum-exp function of ?y i s i as follows:</p><formula xml:id="formula_1">L = log 1 + C i=1 e ?yisi<label>(2)</label></formula><p>where the constant term 1 in the log function sets the zero lower bound for the loss. Log-sum-exp is a smooth approximation of the maximum function, and its gradient is the softmax function as follows:</p><formula xml:id="formula_2">?L ?s i = ?y i e ?yisi 1 + C j=1 e ?yj sj<label>(3)</label></formula><p>= ?1 i?pos e ?si + 1 i?neg e si 1 + i?pos e ?si + i?neg e si</p><p>where 1 i?pos is a Dirac delta function that returns 1 if the i th class is positive and 0 otherwise. Compared to the binary cross entropy loss and focal loss that consider each class separately, LSE-Sign loss considers the dependency across classes as the magnitude of the gradients are normalized over all classes and distributed by a softmax function. This facilitates multi-label learning on an long-tailed dataset as it encourages learning of classes with larger loss values and suppresses the learning of classes with smaller loss values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HOI Detection</head><p>We discover that a strong HOI classification model trained on image-level labels can aid HOI detection effectively. We connect our frozen classification model with an off-the-shelf object detector to recognize the regional HOI. The two models are trained separately and when combined as a whole, they achieve state-of-the-art performance without additional fine-tuning on instance-level supervision.</p><p>To connect the classifier with the detector, we convert the detected bounding boxes to self-attention masks, ?. The last transformer layer in the ViT backbone consumes ? so that the CLS token attends only to the specified region of interest:</p><formula xml:id="formula_4">Attention(Q, K, V ) = sof tmax(? + QK T ? d k )V<label>(5)</label></formula><p>Equation <ref type="formula" target="#formula_4">(5)</ref> is the Attention [37] function of a transformer layer. ? is a binary mask converted from the bounding boxes. ? i,j equals ?? if i is the CLS token and j a patch outside the given bounding boxes, and 0 otherwise. d k is the dimension of Q, K and V . Therefore, the CLS token in the last layer attends only to the region of interest specified by the pair of human and object bounding boxes. Object probabilities are multiplied to corresponding HOI classes. By doing so, we treat HOI detection as a special case of HOI classification, as in this task, DEFR classifies only the regional HOI. This pipeline requires no training on instance-level HOI annotation thanks to the properly learned feature in the classification task (see <ref type="figure" target="#fig_2">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relationships with Existing Work</head><p>Existing work on image-level HOI classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49]</ref> successfully leverage object detectors and human pose to improve the feature for HOI classification. In contrast, we emphasizes on improving the classification head, which is underexplored in the field. Our method achieves a simplified detection-free pipeline and is complementary to existing studies. On HOI detection, we tackle the problem by reusing the classification model on the instance-level. As the detector is trained separately, our method is easy to leverage future advancements on object detection. Our approach decouples object detection from HOI recognition, which is different from existing work that contains a detector as a module in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed DEFR on both HOI classification and HOI detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation for HOI Classification</head><p>Datasets: For image-level HOI classification, we conduct experiments on two commonly used datasets: HICO and MPII. The HICO dataset <ref type="bibr" target="#b4">[5]</ref>  with only one of 393 interaction classes. We follow <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b8">9]</ref> to report performance on the validation set that contains 6,987 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training:</head><p>The backbone is ViT-B/32, and we investigate three different strategies to pre-train the backbone at resolution 224.</p><p>1. Image classification task on ImageNet-1K or ImageNet-21K referred to as CLS1K and CLS21K, respectively. 2. Masked language modeling task motivated by <ref type="bibr" target="#b24">[25]</ref>, where the network input includes both an image and the associated text. Both modalities are fully attended to each other in each transformer block. The pre-training datasets are Google Conceptual Captions <ref type="bibr" target="#b42">[43]</ref>, SBU <ref type="bibr" target="#b39">[40]</ref>, COCO <ref type="bibr" target="#b33">[34]</ref> and Visual Genome <ref type="bibr" target="#b26">[27]</ref>. This is referred to as MLM. 3. Image-text contrastive learning task based on CLIP <ref type="bibr" target="#b40">[41]</ref>. The image encoder is jointly trained with a text encoder and the two modalities are contrasted on the encoder output representations. Only the image encoder is used as the backbone. Here, we directly use the released CLIP model 3 , and reuse the term CLIP to refer to the image encoder as pre-training.</p><p>Fine-tuning: All models have a ViT-B/32 backbone fine-tuned at resolution 672 with the AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer without weight decay. We use a batch size of 128 on 8 V100 GPUs. We set the base learning rate as 1.5e-5 for CLIP pre-trained backbones and 1e-4 otherwise, and use cosine scheduling with warm restarts <ref type="bibr" target="#b35">[36]</ref> every 5 epochs. The best model is fine-tuned for 10 epochs. Data augmentation of random color jittering, horizontal flipping, and resized cropping is used. To reduce class imbalance, we adopt over-sampling so that each class has at least 40 samples per epoch.</p><p>Improved Classifier: <ref type="table" target="#tab_2">Table 1</ref> shows the path from baseline to DEFR. The baseline achieves 37.8 mAP, much lower than the detection-assisted approaches. The LSE-Sign loss gains 6.3 mAP, and language embedding initialization adds another 9.4 mAP with BERT as the language model. We found the performance is further improved when the language model is jointly trained with the image backbone (i.e., using CLIP, to 60.5 mAP). This demonstrates that our proposed approaches are effective and complementary mechanisms for HOI recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparision on HOI Classification</head><p>HICO dataset: <ref type="table">Table 2</ref> compares our detection-free method (DEFR) with the prior works that need assistance from object detection or human keypoint detection. Our method achieves significantly better accuracy without detection assist. Specifically, DEFR achieves 65.6 mAP on the HICO, gaining over the state-of-the-art HAKE [30] by 18.5 mAP.</p><p>Few-shot analysis: Our model outperforms existing methods considerably in few-shot subsets on HICO, by leveraging language embedding initialization and LSE-Sign loss. As shown in <ref type="table">Table 3</ref>, DEFR achieves 52.7 mAP in one-shot classes, only 20% lower than the all-class performance.</p><p>MPII dataset: we evaluate HOI classification on the MPII dataset in addition to HICO. As shown in <ref type="table">Table 4</ref>, our model achieves 43.6 mAP with the same <ref type="table">Table 4</ref>. Comparison on the MPII dataset. We additionally apply the proposed approach on ResNet101 backbone for fair comparision. We follow <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9]</ref>  ResNet101 backbone without detection assist. This proves that our proposed approach is effective on ResNet architectures as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>Several ablations were performed that focus on key components of DEFR: (a) the image backbone, (b) language embedding initialization and (c) the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training:</head><p>We evaluate three different pre-training tasks for the backbone: (a) image classification (CLS1K/CLS21K), (b) masked language modeling (MLM), and (c) image-text contrastive learning (CLIP). <ref type="table">Table 5</ref> (the first column) shows the results for these pre-training tasks using random initialization in the linear classifier.</p><p>Classifier initialization: <ref type="table">Table 5</ref> compares the conventional random initialization and language embedding initialization. Clearly, the language embedding initialization with both language models provides a consistent improvement over all pre-training methods. It is worth noting that ImageNet pre-trained models (CLS1K/CLS21K) gain 10+ points from language embedding initialization.</p><p>Loss function: The choice of loss function is vital in fine-tuning. Previous works use binary cross entropy (BCE) loss and treat HOI recognition as a set  of binary classification problems. <ref type="table" target="#tab_5">Table 6</ref> shows that the proposed LSE-Sign loss outperforms BCE loss on all four differently pre-trained backbones. <ref type="table" target="#tab_6">Table 7</ref> compares LSE-Sign loss with other alternatives: binary cross entropy (BCE) loss, weighted BCE loss and focal loss <ref type="bibr" target="#b32">[33]</ref> on our model. Weighted cross entropy loss is intended to impose a weight on the loss of each class so that each category is balanced among the positive samples and negative samples. It is not effective if the dataset is severely long-tailed. Focal loss <ref type="bibr" target="#b32">[33]</ref> reduces the weight of the massive negative samples, but requires manual tuning of two hyper-parameters. Our LSE-Sign loss outperforms all others by a clear margin.</p><p>Scalar ? in Eq. (1): LSE-Sign loss has a scalar ? which controls the magnitude of output per class s i . It is needed as the initial weights w i are normalized embeddings. ? performs like the temperature parameter of softmax. A small ? makes softmax distribution close to uniform and a large ? makes the softmax close to one-hot. The best trade-off is achieved when ? = 100, see <ref type="table" target="#tab_7">Table 8</ref>. Backbone architecture: We train DEFR with different backbone architectures on HICO as in <ref type="table">Table 9</ref>. We see all backbones outperform the current SotA <ref type="bibr" target="#b29">[30]</ref> by using our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison on HOI Detection</head><p>Dataset: We evaluate HOI detection performance on the HICO-DET dataset <ref type="bibr" target="#b3">[4]</ref>. HICO-DET contains 117,871 annotated human-object pairs in the training set and 33,405 in the test set. HICO-DET and HICO share the same images and classes, but HICO-DET provides bounding box localized HOI annotations.</p><p>Evaluation metric: Following existing studies, we use the standard evaluation protocol <ref type="bibr" target="#b3">[4]</ref>. Each positive prediction should have the correct HOI class together with a pair of human-object bounding boxes with IoU greater than 0.5 in reference to ground truth. Similar to the classification task, the average precision (AP) of each HOI class is separately computed and then averaged (mAP).</p><p>Results: <ref type="table" target="#tab_2">Table 10</ref> compares the HOI detection performance on the HOI-DET dataset. Detectors used in this table are fine-tuned on the HICO-DET dataset. We use the detector from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51]</ref>. Though DEFR is never trained under instance-level HOI supervision from HICO-DET, with stronger backbones, our method outperforms existing SotA that are trained on HICO-DET. Specifically, on rare sets (classes with less than 10 samples), our method outperforms current studies by a clear margin, which is consistent with our image-level classification results. Different from previous works, our method showcases the strong correlation between HOI-Classification and HOI-Detection in a very simple format that HOI-Detection is a special <ref type="table" target="#tab_2">Table 10</ref>. HOI Detection performance on HICO-DET <ref type="bibr" target="#b3">[4]</ref>. Full: full set of 600 HOI classes, Rare: a subset of 138 HOI classes that have less than 10 training samples, Non-rare: the rest 462 classes. We use the same detector as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51]</ref>. Our method is not fine-tuned on HICO-DET, but with stronger backbones, we outperform existing work trained on HICO-DET</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Full Rare Non-rare case of HOI-Classification with additional input of bounding boxes of human and objects. Note that the detected boxes are obtained from an object detector trained separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we discover that classifier is critical for both HOI classification and detection, though overlooked in existing studies. Our method builds upon two findings. Firstly, we show that the structure of HOI classes can be effectively leveraged by using language embedding as classifier initialization. Secondly, we propose the LSE-Sign loss to facilitate multi-label learning on a long-tailed dataset. A combination of DEFR and an object detector achieves the state-of-theart HOI detection without additional fine-tuning. Our proposed detection-free method simplifies the pipeline and achieves higher accuracy than the detectionassisted counterparts. We hope that our work opens up a new direction for HOI recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The losses of both positive and negative classes can be unified into e ?yisi , where Init. (Fine-tuned) CLIP + Random Init. (Fine-tuned) ImageNet-1K + Embedding Init. (Fine-tuned) ImageNet-1K + Random Init. (Fine-tuned)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>t-SNE visualization of classifier weights. Left Column: same asFig. 1but the language model is CLIP. Middle Column: the weight vectors (proxies) of 600 HOI classes after fine-tuning, when CLIP pre-trained ViT-B/32 backbone is used. The top and bottom use language embedding initialization and random initialization, respectively. Right Column: the weight vectors (class proxies) after fine-tuning, when ImageNet-1K pre-trained ViT-B/32 backbone is used. The top and bottom use text embedding and random initialization for the linear classifier, respectively. Text embeddings are clearly clustered before fine-tuning, and the overall structure changes slightly after fine-tuning. However, this clustered structure is not clearly learned when fine-tuned with random initialization, reflected in lower model performance (see mAP results in the plots)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of CLS attention map. First row: the input image. Second row: the attention map of the CLIP pre-trained backbone without fine-tuning on HICO. Third row: the attention map of DEFR fine-tuned on HICO. After fine-tuning, the attention activates more on the HOI related objects and activates less elsewhere. The feature for HOI learned in the classification task aids HOI detection effectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>contains 600 HOI categories which have 117 unique verbs and 80 COCO [34] object classes. Each image may contain multiple HOI classes and multiple human-object pairs. The training set has 38, 116 images and test set has 9, 658 images. Following existing methods, we randomly reserve 10% images from the training set for validation and report performance on the test set. The MPII dataset [1] contains 15,205 training images and 5,708 test images. Unlike HICO, each image is labeled The gain of individual components evaluated on HICO. The baseline uses ImageNet-1K pre-trained ViT-B/32 as the backbone, random initialization for the classifier, and binary cross entropy loss. Language embeddings generated by BERT are used as classifier's weight initialization</figDesc><table><row><cell>Pre-training</cell><cell>LSE-Sign Loss</cell><cell>Embedding Initialization</cell><cell>mAP</cell></row><row><cell>Baseline CLS1K</cell><cell></cell><cell></cell><cell>37.8</cell></row><row><cell>CLS1K</cell><cell>?</cell><cell></cell><cell>44.1</cell></row><row><cell>CLS1K</cell><cell></cell><cell>? BERT</cell><cell>50.6</cell></row><row><cell>CLS1K</cell><cell>?</cell><cell>? BERT</cell><cell>53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with the state of the art on HICO. Dependencies (additional input) required by existing methods include Bbox: object detection, Pose: human keypoints, PaSta<ref type="bibr" target="#b30">[31]</ref>: additional training data of part level actions. Few-shot performance evaluated on HICO. Few@i means classes that the number of training images is i. The number of HOI classes for Few@1, 5, 10 are 49, 125 and 162, respectively. Methods can be different in the backbone and detection assists</figDesc><table><row><cell>We report the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Binary Cross Entropy (BCE) Loss vs. LSE-Sign Loss evaluated on HICO for four differently pre-trained models. The classifier is initialized with CLIP text embeddings</figDesc><table><row><cell>Pre-training</cell><cell></cell><cell>Loss Function</cell></row><row><cell></cell><cell>BCE Loss</cell><cell>LSE-Sign Loss</cell></row><row><cell></cell><cell cols="2">BERT Embedding Initialization</cell></row><row><cell>CLS1K</cell><cell>50.6</cell><cell>53.5 (+2.9)</cell></row><row><cell>CLS21K</cell><cell>51.0</cell><cell>53.9 (+2.9)</cell></row><row><cell>MLM</cell><cell>45.8</cell><cell>47.0 (+1.2)</cell></row><row><cell>CLIP</cell><cell>44.4</cell><cell>51.0 (+6.6)</cell></row><row><cell></cell><cell cols="2">CLIP Embedding Initialization</cell></row><row><cell>CLS1K</cell><cell>51.5</cell><cell>54.7 (+3.2)</cell></row><row><cell>CLS21K</cell><cell>50.0</cell><cell>55.1 (+5.1)</cell></row><row><cell>MLM</cell><cell>46.6</cell><cell>47.1 (+0.5)</cell></row><row><cell>CLIP</cell><cell>57.9</cell><cell>60.5 (+2.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison between LSE-Sign loss and other loss functions evaluated on HICO. We change the loss function of our best model with ViT-B/32 backbone and language embedding initialization. Weighted BCE is the binary cross entropy loss weighted by positive-negative-ratio per-class. Focal loss uses ?=2 and ?=0.25 as recommended in<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell>Loss function</cell><cell>mAP</cell></row><row><cell>Weighted BCE</cell><cell>54.7</cell></row><row><cell>BCE</cell><cell>57.9</cell></row><row><cell>Focal Loss</cell><cell>53.2</cell></row><row><cell>LSE-Sign Loss (ours)</cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Ablation of scalar ? in Eq. (1) evaluated on HICO dataset. The highest accuracy is achieved at ? = 100. The backbone is CLIP pre-trained ViT-B/32, the classifier is embedding initialized and LSE-Sign loss is used for fine-tuningTable 9. Ablation of backbone architecture on HICO dataset. We apply our method on different ResNet and ViT backbone architectures. The pre-training of the backbone, language embedding initialization and loss function stay the same as our best model. DEFR method with ResNet-50 achieves 49.7 mAP, still outperforms the SotA [30] by 2.6 mAP</figDesc><table><row><cell>?</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>300</cell><cell>500</cell></row><row><cell>mAP</cell><cell>60.4</cell><cell>60.5</cell><cell>59.1</cell><cell>57.2</cell><cell>53.0</cell></row><row><cell></cell><cell cols="2">DEFR Backbone</cell><cell></cell><cell>mAP</cell><cell></cell></row><row><cell></cell><cell>ResNet-50</cell><cell></cell><cell></cell><cell>49.7</cell><cell></cell></row><row><cell></cell><cell>ResNet-101</cell><cell></cell><cell></cell><cell>53.6</cell><cell></cell></row><row><cell></cell><cell>ViT-B/32</cell><cell></cell><cell></cell><cell>60.5</cell><cell></cell></row><row><cell></cell><cell>ViT-B/16</cell><cell></cell><cell></cell><cell>65.6</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/openai/CLIP</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
	<note>ieee winter conference on applications of computer vision (wacv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="9004" to="9013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ican: Instance-centric attention network for humanobject interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting and recognizing humanobject interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r* cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1080" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Affordance transfer learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards realtime human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hotr: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="718" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05864</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06539</idno>
		<title level="m">Hake: Human activity knowledge engine</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="248" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="414" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems pp</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1143" to="1151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<title level="m">Whitening sentence representations for better semantics and faster retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="10410" to="10419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05077</idno>
		<title level="m">Mining the benefits of two-stage and one-stage hoi detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatially conditioned graphs for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="13319" to="13327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Glance and gaze: Inferring action-aware points for one-stage human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="13234" to="13243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11825" to="11834" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
