<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
						</author>
						<title level="a" type="main">Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a Collaboration of Experts (CoE) framework to assemble the expertise of multiple networks towards a common goal. Each expert is an individual network with expertise on a unique portion of the dataset, contributing to the collective capacity. Given a sample, delegator selects an expert and simultaneously outputs a rough prediction to trigger potential early termination. For each model in CoE, we propose a novel training algorithm with two major components: weight generation module (WGM) and label generation module (LGM). It fulfills the co-adaptation of experts and delegator. WGM partitions the training data into portions based on delegator via solving a balanced transportation problem, then impels each expert to focus on one portion by reweighting the losses. LGM generates the label to constitute the loss of delegator for expert selection. CoE achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy with 194M FLOPs. Combined with PWLU and CondConv, CoE further boosts the accuracy to 80.0% with only 100M FLOPs for the first time. Furthermore, experiment results on the translation task also demonstrate the strong generalizability of CoE. CoE is hardware-friendly, yielding a 3?6x acceleration compared with existing conditional computation approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>From simple systems to complicated ones, the accomplishment of various tasks relies on the collaboration of multiple individuals. Similarly, a wise combination of models with different properties could yield improved performance on a specific task compared to only deploying one individual 1 Huawei, Beijing, China. Correspondence to: Yikang Zhang &lt;zhangyikang7@huawei.com&gt;.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><p>model. There are many approaches for model collaboration, among which ensemble learning <ref type="bibr" target="#b14">(Hansen &amp; Salamon, 1990;</ref><ref type="bibr" target="#b32">Wen et al., 2020;</ref><ref type="bibr" target="#b33">Wenzel et al., 2020</ref>) is a popular one. Ensemble learning uses a consensus scheme to decide the collective result by vote. However, it requires multiple forward passes, leading to a significant runtime cost. MIMO <ref type="bibr" target="#b15">(Havasi et al., 2021</ref>) draws inspiration from model sparsity <ref type="bibr" target="#b10">(Frankle &amp; Carbin, 2019)</ref> and tries to ensemble several subnetworks within one regular network. It only needs one single forward pass of the regular network but is incompatible with compact models. Conditional computation methods <ref type="bibr" target="#b5">(Cheng et al., 2020;</ref><ref type="bibr" target="#b36">Yan et al., 2015)</ref> adopt the delegation scheme for model collaboration, conditionally assigning one or several, rather than all models, to make the prediction. Some recently proposed conditional computation methods <ref type="bibr" target="#b37">(Yang et al., 2019;</ref><ref type="bibr" target="#b41">Zhang et al., 2021)</ref> have achieved remarkable performance based on dynamic convolution. Nonetheless, they usually have high memory access cost (MAC) and a low degree of parallelism, increasing the real latency <ref type="bibr" target="#b23">(Ma et al., 2018)</ref>.</p><p>Motivated by this, we propose the Collaboration of Experts (CoE) framework to both eliminate the need for multiple forward passes and keep hardware-friendly. CoE consists of one delegator and multiple experts. Firstly, delegator gives a rough prediction and makes the expert selection. If the rough prediction is unreliable, the selected expert will make the refined prediction. Otherwise, the procedure will be early terminated to save FLOPs. Moreover, we only need to load the selected expert into memory, thus keeping the ratio of MAC to FLOPs as a constant. By contrast, dynamic convolution methods  need to load a large number of parameters, namely basis models or experts, to synthesize the input-dependent ones. It enlarges MAC and reduces the degree of parallelism, resulting in a significant deceleration. To make each model in CoE play its role, we propose a novel training algorithm (as shown in <ref type="figure" target="#fig_0">Fig.1</ref>) which consists of two major components: weight generation module (WGM) and label generation module (LGM). LGM generates the label (selection label) to constitute the loss of delegator for expert selection. Selection label is a one-hot vector, indicating the suitable expert for each given input. It is obtained by solv- and L 3 T are the expert losses, which are reweighted by WGM with weights W1, W2 and W3.</p><p>LGM generates the label to constitute the loss LS for expert selection. LP is the cross-entropy loss for the rough prediction of delegator. LGM WGM ?</p><p>LGM generates the selection label which indicates the suitable expert to supervise the delegator for expert selection. WGM partitions the training data into portions based on delegator and impel each expert to focus on one portion by reweighting the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate the selection label</head><p>Reweight the expert losses <ref type="figure">Figure 2</ref>. The co-adaptation of experts and delegator.</p><p>LGM generates the selection label which indicates the suitable expert to supervise delegator for expert selection. WGM partitions the training data into portions based on delegator and impels each expert to focus on one portion by reweighting the losses.</p><p>ing a balanced transportation problem (BTP, <ref type="bibr" target="#b28">Shore 1970)</ref>. Based on delegator, WGM partition the training data into portions by maximizing the summed selection probability via solving BTP as well. Then expert losses are reweighted so that each expert can focus on one portion. As shown in <ref type="figure">Fig.2</ref>, this fulfills the co-adaptation of experts and delegator. The co-adaptation manner makes CoE generalize well to the validation set. Due to the random initialization of experts, selection labels are irregular in the early training stage. Nonetheless, delegator tends to learn generalizable patterns first, since networks learn gradually more complex hypotheses during training <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>. Therefore, WGM can partition the training data into portions based on generalizable patterns with delegator as the bridge. It makes selection labels more regular in return, thus delegator avoids overfitting to the irregular labels.</p><p>We conduct the main experiments on ImageNet classification task. CoE achieves 78.2/80.7% top-1 accuracy with only 100/194M FLOPs, while the accuracy for ensembled models <ref type="bibr" target="#b14">(Hansen &amp; Salamon, 1990</ref>) is only 79.6% with 920M FLOPs. Compared with the widely-used gate-valuebased optimization method <ref type="bibr" target="#b9">Fedus et al., 2021)</ref>, our proposed training algorithm improves 1.2% accuracy for CoE, indicating the training effectiveness. Compared with dynamic network approaches, CoE is more hardware-friendly. It not only outperforms the SOTA dynamic method BasisNet which achieves 80.0% accuracy with 198M FLOPs <ref type="bibr" target="#b41">(Zhang et al., 2021)</ref>, but also accomplishes a 3.1x speedup on hardware. Besides, CoE can be equipped with CondConv and further improve the accuracy to 79.2/81.5% with 102/214M FLOPs. Moreover, we further boost the accuracy to 80.0% with only 100M FLOPs for the first time by using PWLU activation function <ref type="bibr">(Zhou et al., 2021)</ref>. Experiment results on the translation task also demonstrate the strong generalizability of CoE.</p><p>The contributions of this paper can be summarized as follows:</p><p>? We propose a collaboration framework named Collaboration of Experts (CoE). The core advantage of it is the inference efficiency. Compared with other conditional computation methods, CoE has low memory access cost and a high degree of parallelism, which are two important factors for real latency.</p><p>? We present a novel optimization strategy for CoE that fulfills the co-adaptation of experts and delegator. Experiment results demonstrate its superiority over the widely-used gate-value-based optimization method.</p><p>? CoE updates the state-of-the-art on ImageNet for mobile setting, achieving 80.7% top-1 accuracy on Ima-geNet with less than 200M FLOPs for the first time as far as we know.</p><p>? Since the expert selection is done at the model level, CoE can take advantage of existing techniques like conditional convolution and PWLU activation function to push the performance to a new level, namely, 80.0% accuracy on ImageNet with only 100M FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Ensemble Learning and Model Selection</head><p>Ensemble learning <ref type="bibr" target="#b14">(Hansen &amp; Salamon, 1990</ref>) aims at combining the predictions from several models to get a more robust one. Some recently proposed literatures <ref type="bibr" target="#b32">(Wen et al., 2020;</ref><ref type="bibr" target="#b33">Wenzel et al., 2020)</ref> demonstrate that significant gains can be achieved with negligible additional parameters compared to the original model. However, these methods still require multiple (typically, 4-10) forward passes for prediction, leading to a significant runtime cost. Differently, CoE utilizes a delegator to select only one expert for the refined prediction, thus at most two forward passes are needed. MIMO <ref type="bibr" target="#b15">(Havasi et al., 2021</ref>) draws inspiration from model sparsity <ref type="bibr" target="#b10">(Frankle &amp; Carbin, 2019)</ref> and holds the view that multiple independent subnetworks can be concurrently trained within one regular network because of the heavy parameter redundancy. Therefore, those subnetworks can be ensembled with a single forward pass of the regular model. However, MIMO cannot be applied to compact models which have already been pruned or the ones constructed by AutoML methods <ref type="bibr" target="#b2">(Cai et al., 2020;</ref><ref type="bibr">Zhong et al., 2018)</ref>. It is because these models have few redundant parameters. By contrast, CoE is free from the compactness of experts since expert selection is done at the model level. Recently, some works about model selection are proposed <ref type="bibr" target="#b22">(Li et al., 2021b;</ref><ref type="bibr" target="#b38">You et al., 2021)</ref>. These methods are concerned with ranking a number of pre-trained models and finding the one transfers best to a downstream task of interest. Therefore, they select models task-wisely. By contrast, CoE aims at improving the task performance via selecting the most suitable expert for each sample instance-wisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamic Networks</head><p>Dynamic networks achieve high performance with low computation cost by conditionally varying the network parameters <ref type="bibr" target="#b37">Yang et al., 2019)</ref> or network architectures . HD-CNN <ref type="bibr" target="#b36">(Yan et al., 2015)</ref> and HydraNet <ref type="bibr" target="#b25">(Mullapudi et al., 2018)</ref> select branches based on the category, they cluster all categories into n groups, where n is the number of branches. While CoE learns the model selection pattern automatically, it can be based on any property, rather than limited to the category. MoE  and Switch Transformer <ref type="bibr" target="#b9">(Fedus et al., 2021</ref>) select experts at the layer level with a router. The output feature of each expert will be scaled with the gate-value predicted by router, thus router becomes trainable. This gate-value-based optimization manner is heuristic while CoE trains the delegator more reasonably. Since expert selection for CoE is done across models, we can use protocols like True Class Probability (TCP, <ref type="bibr" target="#b6">Corbi?re et al. 2019)</ref> to measure the suitability of each expert without bias. Based on expert suitabilities, the labels to supervise delegator for expert selection can be obtained. Additionally, CoE takes more advantage of conditional computation as the whole network is selected, rather than only some particular layers. The recently proposed Dynamic Convolution methods <ref type="bibr" target="#b37">Yang et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref> share the similar idea and achieve remarkable performance with low FLOPs but high latency. It is because these methods need to load many basis models or experts to synthesize the dynamic parameters, causing high MAC and low degree of parallelism <ref type="bibr" target="#b23">(Ma et al., 2018)</ref>. By contrast, CoE only needs to load the selected expert into memory, avoiding these problems. Finally yet importantly, batch processing is an important method to enhance the degree of parallelism. Because of the inputdependent parameters <ref type="bibr" target="#b41">Zhang et al., 2021)</ref> or architectures , these methods cannot process samples in batch. Differently, CoE supports batch processing because the number of experts is limited and each one of them corresponds to many test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>CoE consists of a delegator and n experts, a total of n + 1 individual neural networks. Given a sample, delegator will select an expert and simultaneously output a rough prediction to trigger potential early termination. Since the inference of delegator is conducted all the time, we prefer to make delegator more lightweight than expert. Delegator consists of three modules: feature extractor, task predictor and expert selector as shown in <ref type="figure">Fig.3</ref>. Based on the feature derived from feature extractor, task predictor and expert selector output the probabilities for classification and expert selection respectively. In the following subsections, we will describe the inference procedure and training strategy of CoE comprehensively. The number of samples and experts are denoted as m and n.  <ref type="figure">Figure 3</ref>. The architecture of delegator. Task predictor consists of one fully connected layer and a SoftMax layer. Expert selector consists of two fully connected layers with the hidden dim as 100, followed by a SoftMax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inference Procedure</head><p>CoE firstly uses delegator to obtain the rough prediction and determine the selected expert for each sample. Afterward, Maximum Class Probability (MCP, <ref type="bibr" target="#b6">Corbi?re et al. 2019</ref>) of rough prediction is calculated. It is the probability of predicted class. Then, the final recognition results for samples with MCP larger than a given threshold ? are derived from the rough predictions. Other samples are partitioned into n groups based on which expert is selected. Subsequently, batch processing can be conducted within each group to obtain refined predictions. This procedure is shown in <ref type="figure">Fig.4</ref>. The averaged FLOPs/Instance of CoE ranges from F D to F D + F E by varying ? from 0 to 1, F D and F E are FLOPs of delegator and experts. Therefore, the value of ? is directly determined by target FLOPs.</p><formula xml:id="formula_0">Delegator ? ? ? ? Expert 1 Expert 2 Expert Inputs Expert Selection ( ? ) ( ? ) ( ? ) ( ) MCP1 ? MCP ? MCP1 &lt; ? MCP ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early terminated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rough Predictions:</head><p>Refined Predictions: <ref type="figure">Figure 4</ref>. The inference procedure of CoE. MCP is the probability of predicted class. ? is the threshold for early termination.</p><formula xml:id="formula_1">MCP &lt; ? MCP &lt; ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Generation Module (LGM)</head><p>Since expert selection for CoE is done across models, we can measure the suitability of each expert without bias. Based on this, labels to supervise delegator for expert selection can be obtained. This label-based training method for delegator is more reasonable than the widely-used gate-value-based method <ref type="bibr" target="#b9">Fedus et al., 2021)</ref> which enables the training of routing function by using its predicted gate-value to scale the output of each expert. Next, we firstly introduce how to measure the suitability of each expert, then illustrate how to obtain the selection label.</p><p>Model accuracy can be measured by True Class Probability (TCP, <ref type="bibr" target="#b6">Corbi?re et al. 2019)</ref>:</p><formula xml:id="formula_2">TCP j,k = P (Y = y j |x j , Expert k ),<label>(1)</label></formula><p>where, x j is the j-th sample, Y and y j are the predicted and true class. But accuracy is not the only factor for suitability. For example, when models are of different sizes, the larger one usually has a higher TCP. But it may not be more suitable, due to the large inference cost. Given that our concern is not the optimization of network architecture, we can suppose no expert is superior to others (No Superiority Assumption, NSA). Motivated by NSA, we leverage the standardized TCP as the metric for sutability:</p><formula xml:id="formula_3">S j,k = TCP j,k ? M ean(TCP :,k ) Std(TCP :,k ) ,<label>(2)</label></formula><p>where, Mean(TCP :,k ) and Std(TCP :,k ) are mean value and standard deviation for TCPs of Expert k on m samples.</p><p>Selection labels can be denoted by a binary matrix L m?n , where each row is a selection label. According to NSA, each expert should be assigned the same number of samples, thus the sum of each colum vector of L m?n should be same, i.e. j L j,k = m n for k = 1, ..., n. Therefore, L m?n can be obtatined by maximizing j,k S j,k * L j,k :</p><formula xml:id="formula_4">min j,k ?S j,k * L j,k s.t. L j,k ? {0, 1}, k L j,k = 1, j L j,k = m n (3)</formula><p>This problem can be modeled as the balanced transportation problem (BTP, Shore 1970), where each sample is a supply source with a supply of one, each expert is a demand source with a demand of m/n. ?S j,k is the per-unit transportation cost from the j-th supply source to the k-th demand source. We solve this problem via Vogel approximation method (VAM, Shore 1970) as introduced in Appendix A.1, which is a short-cut approach to invariably obtain a good solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weight Generation Module (WGM)</head><p>To maximize the collective capacity of CoE, the dataset needs to be partitioned into portions then each expert only focuses on one portion. This is achieved by WGM which reweights the losses of experts. The partition can be indicated by an assignment matrix A m?n , with one-hot row vectors. A j,k = 1 means the j-th sample x j is assigned to the k-th expert, thus the loss weight for Expert k gets larger than other experts on x j .</p><p>A naive partition can be based on expert suitability, namely, partitioning the training data with selection labels L m?n . However, it results in a poor generalization to delegator as shown in <ref type="figure">Fig.5a</ref>. Assuming Expert k is suitable on a sample x j , thus A j,k = L j,k = 1. Due to A j,k = 1, the loss weight for Expert k gets larger than other experts on x j , making Expert k more suitable in return. Therefore selection labels cannot be updated and the irregularity for them caused by random initialization will be preserved. Consequently, delegator gradually overfits to irregular labels, yielding a poor generalization. This is also verified in Section 4.4, where the expert-suitability-based partition results in a terrible performance for CoE W GM .</p><p>Since networks learn gradually more complex hypotheses during training <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>, delegator tends to learn generalizable patterns first. Therefore, the partition can be based on generalizable patterns with delegator as the bridge, namely, partition based on the output of delegator. In this way, selection labels get more regular in return due to the reweighting of expert losses. As shown in <ref type="figure">Fig.5b</ref>, delegator avoids overfitting to the irregular labels.</p><p>Delegator outputs a probability matrix P m?n ? R m?n , Demo with two experts for training. The training samples are denoted as green triangles or pink circles, where pink circle indicates Expert 1 is more suitable for it than Expert 2 . During training, which expert is more suitable may change for any sample, we mark it with a dashed bounding rectangle. As mentioned in the main text, the training data need to be partitioned into to two portions then each expert only focus on one portion. We use pink background to represent the portion for Expert 1 and green background to represent the portion for Expert 2 . Delegator is trained with selection labels that indicate the suitable expert, thus it learns hypothesis to select the suitable expert. The hypothesis delegator learns is denoted as red dashed curve.</p><p>(a) Partition the training data based on expert suitability, which makes delegator overfit to irregular selection labels. Demo with two experts for training. The training samples are denoted as green triangles or pink circles, where pink circle indicates Expert 1 is more suitable for it than Expert 2 . During training, which expert is more suitable may change for any sample, we mark it with a dashed bounding square. As mentioned in the main text, the training data need to be partitioned into to two portions then each expert only focus on one portion. We use pink background to represent the portion for Expert 1 and green background to represent the portion for Expert 2 . Delegator is trained with selection labels that indicate the suitable expert, thus it learns hypothesis to select the suitable expert. The hypothesis delegator learns is denoted as red dashed curve.</p><p>(b) Partition the training data based on delegator, which fulfills the co-adaptation of experts and delegator, avoiding the overfitting problem of delegator. <ref type="figure">Figure 5</ref>. Demo with two experts of the training process. The training samples are denoted as green triangles or pink circles, where pink circle indicates Expert 1 is more suitable for it than Expert 2 . During training, which expert is more suitable may change, we mark it with a dashed bounding square. As mentioned, the training data need to be partitioned into two portions then each expert only focuses on one portion. We use pink background to represent the portion for Expert 1 and the green one for Expert 2 . Delegator learns hypothesis to select expert, it is denoted as red dashed curve. whose element P j,k ? [0, 1] represents the probability of selecting the k-th expert on the j-th sample. As analyzed above, it is better to partition the training data based on P m?n , thus A m?n is obtained by maximizing j,k P j,k * A j,k . Moreover, according to NSA, the number of samples assigned to each expert should be same, i.e. j A j,k = m/n. Thus, A m?n is optimized by:</p><formula xml:id="formula_5">min j,k ?P j,k * A j,k s.t. A j,k ? {0, 1}, k A j,k = 1, j A j,k = m n (4)</formula><p>This problem can also be modeled as BTP, and solved via VAM as described in section 3.2. In the early training stage, the models in CoE are underfitted. Thus we cannot trust A m?n and need to make the gap between loss weights for different experts smaller to warm up. We achieve this by smoothing A m?n to A m?n with Eq.5, where ? grows linearly from 0.2 to 0.8 with the training going on,</p><formula xml:id="formula_6">A j,k = ? + 1?? n , if A j,k = 1 1?? n , if A j,k = 0 .<label>(5)</label></formula><p>Finally, the output of WGM (i.e. W m?n ) is obtained by</p><formula xml:id="formula_7">normalizing A m?n with the coefficient Z = j A j,k = m n : W j,k = A j,k Z . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Details</head><p>The training framework of CoE is shown in <ref type="figure" target="#fig_0">Fig.1</ref>, which consists of three major losses: L P , L T and L S .</p><p>L P is the cross-entropy loss for the rough prediction of delegator. To avoid the repeated training of delegator, we use L P to train the feature extractor and task predictor ( <ref type="figure">Fig.3</ref>) first of all. Then these two modules are fixed, only expert selector and experts are jointly optimized with L T otal = ? * L S + L T , ? is set as 0.8 in this paper.</p><p>L S is used to optimize the expert selector. Based on the selection label L j,: , we can get the cross-entropy loss L j S for the j-th sample. Because the final recognition result of CoE is not always sensitive to expert selection, {L j S |j = 1, . . . , m} should be attached various importance. For example, when experts have similar suitabilities (Eq.2) on the j-th sample, expert selection will have little influence to final performance of CoE, therefore the weight for L j S gets smaller. Considering the similarity of suitabilities can be measured by the standard deviation Std(S j,: ), we set the loss weight for L j S as v j = Std(Sj,:)</p><p>i Std(Si,:) . Finally,</p><formula xml:id="formula_8">L S = j v j * L j S .<label>(7)</label></formula><p>L T is used to optimize the experts. Based on the class labels of m samples, we can get m ? n cross-entropy losses {L j,k T |j = 1, . . . , m; k = 1, . . . , n}, where L j,k T is the crossentropy loss for the k-th expert on the j-th sample. Then L T is obtained by the weighted sum of L j,k T with weights W j,k output by WGM:</p><formula xml:id="formula_9">L T = j,k W j,k * L j,k T .<label>(8)</label></formula><p>We use either four or sixteen experts in this paper. When using sixteen experts, we decompose the task into four subtasks, each of which involves four experts as described in Appendix A.2. This reduces the memory cost for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct the main experiments on ImageNet classification task. After comparing with some popular efficient models, we verify the superiority of CoE over the existing model collaboration methods: model ensemble and category-based model selection. Afterward, the effectiveness of training strategy is analyzed by the comparison with the widelyused gate-value-based training method and the elaborated ablations. Moreover, we try to generalize CoE to the translation task and re-evaluate CoE using Reassessed Labels (ReaL) <ref type="bibr" target="#b1">(Beyer et al., 2020)</ref>. Finally, we try to analyze the reasonability of learned expert selection patterns. Statistics on referenced baselines in section 4.2.1&amp;4.2.2 are directly cited from original papers, others are implemented with the following setting unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We conduct experiments with two settings: CoE-Small and CoE-Large. For CoE-Small, we take TinyNet-E <ref type="bibr" target="#b13">(Han et al., 2020b)</ref> with 24M FLOPs as the feature extractor of delegator by removing the last fully connected layer. We use OFA-110 <ref type="bibr" target="#b2">(Cai et al., 2020)</ref> with 110M FLOPs as the expert. For CoE-Large, MobileNetV3-Small <ref type="bibr" target="#b17">(Howard et al., 2019)</ref> with 56M FLOPs is adopted to construct the delegator by analogy. We use OFA-230 with 230M FLOPs as the expert. We have also tried to introduce CondConv  and PWLU activation fuction (Zhou et al., 2021) to achieve the extreme performance. More details are illustrated in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">ACCURACY AND COMPUTATION COST</head><p>By varying the threshold ? of early termination, the accuracy curves for CoE are obtained. We show them in <ref type="figure" target="#fig_2">Fig. 6</ref>, then pick out a point from each curve to compare with some efficient models in <ref type="table" target="#tab_3">Table 1</ref>. CoE achieves 78.2% and 80.7% accuracy with 16 experts and the averaged FLOPs/Instance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">INFERENCE SPEED AND MEMORY COST</head><p>The core advantage of CoE is the inference efficiency. To verify this advantage, we analyze the inference latency on hardware. The experiments are conducted on CPU platform (Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz) with PyTorch version as 1.8.0. We report the averaged latency on the ImageNet validation set in <ref type="table" target="#tab_4">Table 2</ref>. We notice the discrepancy between FLOPs and real speed. For example, OFA-230 has 1.6x FLOPs compared with GhostNet 1.0x, but the speed is 1.2x faster. Moreover, this discrepancy can be enlarged by CondConv. CondConv-EfficientNet-B0 has similar FLOPs with the original EfficientNet-B0, but the speed is 1.7x slower. BasisNet synthesizes the dynamic parameters all at once, rather than the "layer by layer" fashion like CondConv, thus is more efficient. However, it still needs to load a large number of parameters for this synthesis, which brings a large MAC. This is why CoE (16 experts) can reduce 14.09% latency than BasisNet even when the mini-batch size is one. Finally yet importantly, BasisNet and CondConv do not support batch processing, while CoE (16 experts) can take advantage of it to further achieve a 3.1/6.1x speedup compared with them. We analyze the memory cost from two perspectives: the number of parameters and MAC. As can be seen from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">COMPARISON WITH MODEL ENSEMBLE</head><p>We train four OFA-230 models with different initialization seeds as shown in <ref type="table">Table 3</ref>. The random initialization usually causes minor variety in accuracy for ImageNet classifica-tion but leads models to fall into different local minima, yielding the diversity of output. This diversity enables the ensembled models to achieve an improvement of 1.6% in accuracy. We adopt the naive ensemble here, i.e. averaging the output of each model. As shown in <ref type="bibr" target="#b32">Wen et al. (2020)</ref>; <ref type="bibr" target="#b3">Chen et al. (2019)</ref>, though simple, naive ensemble has a competitive performance in terms of accuracy. However, CoE still achieves 1.1% higher accuracy than it. Recently proposed ensemble methods <ref type="bibr" target="#b15">(Havasi et al., 2021)</ref> mainly focus on reducing the computation cost with a little drop of accuracy than naive ensemble, but the computation cost is always larger than the one of a base model. By contrast, CoE reduces the FLOPs to 0.84? of the base model, indicating the superiority of CoE in terms of FLOPs as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">COMPARISON WITH CATEGORY-BASED METHOD</head><p>HD-CNN and HydraNets select branches based on the category. Despite their methods are originally designed to select a specific block, we apply them to the model level.</p><p>To select expert based on category, the categories should be partitioned into n groups, where n is the number of experts. We try two schemes: random partition and clustering-based partition. Then, expert can be selected according to the rough prediction of delegator. During the training procedure, we also reweight losses of each expert based on the assignment matrix A m?n with Eq.5&amp;6. Here, A m?n is obtained directly based on the rough prediction. The results with 4 experts are shown in <ref type="table">Table 4</ref>, demonstrating a better collaboration pattern is learned by CoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">COMPARISON WITH GATE-VALUE-BASED TRAINING METHOD</head><p>MoE and Switch Transformer adopt the gate-value-based training method for routing function. They enable the training of router by using its predicted gate-value to scale the output of each expert. This optimization manner is heuristic while CoE trains the delegator more reasonably. Since expert selection for CoE is done across models, we can measure the suitability of each expert without bias. Thereafter, selection labels can be obtained to supervise delegator. Despite the gate-value-based method is originally designed to select a specific layer, we apply it to the model level. We compare with both the soft gate-value and the hard gatevalue. For hard gate-value, it is a one-hot vector generated by replacing the softmax in expert selector <ref type="figure">(Fig.3)</ref> with gumbel softmax <ref type="bibr" target="#b19">(Jang et al., 2017)</ref>. The results with 4 experts are shown in <ref type="table" target="#tab_6">Table 5</ref>, where CoE achieves better performance, indicating the effectiveness of our training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies for CoE</head><p>We have conducted elaborated ablations, including ablations for each element of our proposed training method,  ? CoE W GM ? : Remove the " j A j,k = m/n" constraint in Eq.4 and abandon the smoothing of A m?n (Eq.5). Thus WGM neglects the NSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? CoE W GM ?</head><p>: Abandon the progressive sharpening for A m?n in WGM. Specifically, set ? in Eq.5 as a constant 0.8, instead of linearly increasing it.</p><p>? CoE SR : Abandon the reweighting for losses of expert selection (L j S ), namely set v j in Eq.7 as a constant 1 m .</p><p>The results for those CoE versions with the CoE-Large setting and 4 experts are shown in <ref type="table" target="#tab_7">Table 6</ref>, which demonstrates the significance of each element of the training method. ? ? ? <ref type="figure">Figure 8</ref>. The architecture of CoE-Transformer. ? is a constant token, whose feature is used for expert selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis of the Generalization</head><p>To verify the generalizability, we conduct two extra experiments: generalizing CoE to translation task and using Reassessed Labels (ReaL) <ref type="bibr" target="#b1">(Beyer et al., 2020)</ref> to re-evaluate CoE. We mainly introduce the first one here, another one are discussed in Appendix B.3.</p><p>To generalize CoE to translation task, we build a CoE-Transformer model based on Transformer (base model) <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. CoE-Transformer has four decoders, given a sentence, one decoder will be selected to decode the features extracted by encoder. To select the decoder, an extra constant token ? is added at the beginning of each sentence, whose feature extracted by encoder is input to the expert selector ( <ref type="figure">Fig.3)</ref> for expert selection. During training, the TCP of a sentence is obtained by averaging the TCPs of each token. The architecture of CoE-Transformer are shown in <ref type="figure">Fig. 8</ref>.</p><p>Following <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Ott et al., 2019)</ref>, CoE-Transformer is trained on the standard WMT 2014 English-German dataset. As mentioned, an extra token will be added to this vocabulary. We adopt the same training and evaluating setting as <ref type="bibr" target="#b26">(Ott et al., 2019)</ref>, more details are shown in Appendix B.4. From <ref type="table" target="#tab_8">Table 7</ref> we can see, CoE-Transformer outperforms Transformer (base model) by a large margin and achieves similar performance as Transformer (big) with much less MAC and parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis of the Learned Expert Selection Patterns</head><p>We also conduct experiments to analyze the expert selection patterns of CoE and find them quite reasonable. When experts have different architectures, the delegator tends to assign easy samples to smaller experts and complex samples to heavier experts. When experts share the same architecture, delegator learns the expert selection patterns automatically, it can be based on any property (e.g. whether humans are contained), rather than limited to the category. We introduce the experiment when experts have various architectures here, more details are illustrated in Appendix B.5.</p><p>Considering TCP <ref type="bibr" target="#b6">(Corbi?re et al., 2019)</ref> measures the complexity of a given sample if the inference model is fixed, namely, the more complex is the sample, the smaller TCP will be. We can analyze the relationship between sample complexity and expert selection. We take four architectures searched via OFA <ref type="bibr" target="#b2">(Cai et al., 2020)</ref> as the experts, i.e. OFA-110, OFA-163, OFA-230 and OFA-595. OFA-xx indicates the FLOPs is xx. The delegator is also MobileNetV3-small as described in section 4.1. We obtain the TCP value for each sample based on the delegator. We count the selection probability for each expert at different TCP values on the validation set. As shown in <ref type="figure" target="#fig_4">Fig.9</ref>, the selection probability for heavier model increases with the input sample getting more complex (with the decrease of TCP). It demonstrates that CoE can learn reasonable expert selection patterns automatically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a CoE framework to pool together the expertise of multiple networks towards a common aim. Experiments in this paper demonstrate the superiority of CoE on both accuracy and real speed. We also analyze the collaboration patterns and find them have interpretability. In the future, CoE will be extended to the trillion parameters level. Meanwhile, we will try to implement CoE to more tasks and verify its compatibility with quantification and other technologies.</p><p>Besides, CoE has the potential to solve problems of lifelong learning by updating experts. In weight generation module (WGM) and label generation module (LGM), we need to solve the balanced transportation problem (BTP, <ref type="bibr" target="#b28">(Shore, 1970)</ref>) via Vogel approximation method (VAM, <ref type="bibr" target="#b28">(Shore, 1970)</ref>). We will introduce it in this section with the number of samples and experts as m and n respectively.</p><p>The BTP involved in WGM and LGM has m supply sources, each of which is denoted as Silo j with a supply of one, as well as n demand sources, each of which is denoted as M ill k with a demand of m n . C j,k is the per-unit transportation cost from Silo j to M ill k . Specifically, C j,k = ?P j,k in WGM and C j,k = ?S j,k in LGM. To make it clear, we illustrate this algorithm with a toy example, where the problem is simplified as <ref type="figure" target="#fig_0">Fig.10 (a)</ref> with m = 4, n = 2. In the first step, we calculate the penalty cost pc rowj for each row and pc col k for each column of the tableau in <ref type="figure" target="#fig_0">Fig.10 (a)</ref>. Penalty cost is determined by subtracting the lowest unit cost in the row (column) from the next lowest unit cost. The penalty costs of the respective rows and columns have been marked in red color for clarity in <ref type="figure" target="#fig_0">Fig.10 (b)</ref>. Since the third row has the largest penalty cost (pc row3 =11) and C 3,1 is the lowest unit cost of that row, Silo 3 is allocated to M ill 1 , i.e. A 3,: = [1, 0] in WGM or L 3,: = [1, 0] in LGM. Then the corresponding row should be crossed out and the demand of M ill 1 should minus one, if this results in a zero demand, the first column will be crossed out as well. After adjusting penalty cost for each row and column, the tableau becomes <ref type="figure" target="#fig_0">Fig.10 (c)</ref>, where the changed values are marked in orange. The described procedure will be looped until no rows remained.</p><p>Considering the calculation of pc col k is much more time-consuming compared with pc rowj because m n in WGM and LGM, we modify VAM by only seeking lowest penalty cost among {pc row1 , ..., pc rowm }. We find this modification makes VAM more efficient while keeps the superiority of the solution. It is because the mechanics of VAM makes it meaningful to take pc col k into account only when the demand of M ill k is one, which rarely happens. Thus, we adopt this modification to promote efficiency in this paper.  <ref type="figure" target="#fig_0">Figure 10</ref>. The Vogel approximation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. A Strategy for Task Decomposition</head><p>To fulfill task decomposition, we introduce a new module to delegator, named subtask selector as shown in <ref type="figure" target="#fig_0">Fig.11</ref>. The subtask selector is used to allocate the input samples into different subtasks. The expert selector outputs sixteen probabilities, which are partitioned into four groups as well. For each subtask, only one group of probabilities is visible. The experts within each subtask and the corresponding weights of the expert selector are jointly optimized. As for the feature extractor, task predictor, and subtask selector, their weights directly derive from the delegator trained with the setting of four experts and then fixed. During this procedure, the weights of subtask selector derive from the expert selector. To combine with CondConv, we replace the convolutions within each inverted residual block of the experts with CondConv (expert_num = 4). To take advantage of PWLU, we replace all activation layers except those that have tiny input feature maps as illustrated in <ref type="bibr">Zhou et al. (2021)</ref>. Models are trained using SGD optimizer with 0.9 momentum. We use a mini-batch size of 4096, and a weight decay of 0.00002. Cosine learning rate decay is adopted and the number of training iterations is 313000. We use the augment policy searched by <ref type="bibr" target="#b7">Cubuk et al. (2019)</ref> as well (fixed auto-augment). Similar as BasisNet, we use knowledge distillation with EfficientNet-B2 <ref type="bibr" target="#b29">(Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b34">Xie et al., 2020)</ref> as the teacher. The learning rate is 0.8/1.6 for CoE-Small/Large and dropout rate is 0.2. The stochastic depth <ref type="bibr" target="#b18">(Huang et al., 2016)</ref> is used except for TinyNet-E with a survival probability of 0.8. We think only when the overfitting problem is solved can task accuracy reflect model capacity exactly. Because this paper is concerned with improving the model capacity with limited computation cost, we use knowledge distillation, fixed auto-augment and stochastic depth to overcome the overfitting problem. Nonetheless, we also conduct ablations for them as shown in Appendix B. Knowledge distillation (KD), auto-augment (AA) and stochastic depth (SD) are widely-used strategies to overcome the overfitting problem. We think only when the overfitting problem is solved can task accuracy reflect model capacity exactly. Because this paper is concerned with improving the model capacity with limited computation cost, we use these strategies. Nonetheless, we conduct ablations for them in this section. We adopt the CoE-Large setting with 4 experts. Results are shown in  The original OFA-230 has 78.0% top-1 accuracy with 230M FLOPs. We can introduce a MobileNetV3-Small to conduct early termination. By varying the threshold, we get a series of accuracies and FLOPs as shown in <ref type="figure" target="#fig_0">Fig.12</ref>. It can seen that the accuracy becomes 78.0% with 220M FLOPs. This indicates the computation cost brought by MobileNetV3-Small is eliminated via early termination strategy. Inspired by this, we expect to eliminate the computation cost brought by delegator via early termination as well. It does reduce the computation cost by 60/66M FLOPs, demonstrating the effectiveness of early termination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Re-evaluation with Reassessed Labels</head><p>As described in paper <ref type="bibr" target="#b1">(Beyer et al., 2020)</ref>, the validation set labels have a set of deficiencies that makes the recent progress on ImageNet classification benchmark suffer from overfitting to the artifacts. To verify the generalization, we use the Reassessed Labels (ReaL) <ref type="bibr" target="#b1">(Beyer et al., 2020)</ref> to re-evaluate our method. The results are shown in <ref type="table" target="#tab_3">Table 10</ref>. It can be seen that our method still has a remarkable performance, achieving higher accuracy than the compared methods with significantly smaller FLOPs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Experiment details for the translation task</head><p>Following <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Ott et al., 2019)</ref>, CoE-Transformer is trained on the standard WMT 2014 English-German dataset, which has a shared source-target vocabulary of about 37000 tokens. As mentioned, an extra token will be added to this vocabulary. The default training setting is identical with the one described in <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>, except for the batch size and learning rate becoming larger following <ref type="bibr" target="#b26">(Ott et al., 2019)</ref>. Moreover, the parameter ? in Eq.5 grows linearly from 0.1 to 0.4 with the training going on. We report BLEU on news2014 with a beam width of 4 and length penalty of 0.6 based on a single model obtained by averaging the last 5 checkpoints following <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Ott et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Analysis of the Learned Expert Selection Patterns</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5.1. EXPERT SELECTION PATTERNS WHEN EXPERTS SHARE THE SAME ARCHITECTURE</head><p>We have analyzed the selection patterns when experts have different architectures, here we focus on the case that all experts share the same architecture. We adopt the CoE-Large setting with four experts.</p><p>Considering many works <ref type="bibr" target="#b36">(Yan et al., 2015;</ref><ref type="bibr" target="#b25">Mullapudi et al., 2018)</ref> select branches based on the category, we firstly experiment to observe the relationship between selection patterns and rough prediction of the delegator on ImageNet validation set. Based on the predicted class of rough prediction, the validation set can be partitioned into 1000 subsets. Then we calculate the probabilities to select each expert within each subset and get 1000 probability vectors. After clustering, we plot the probability vectors on <ref type="figure" target="#fig_0">Fig.13</ref>, where each column represents a probability vector. It can be seen that samples with the same rough prediction class are assigned to different experts. Therefore, we can conclude that the expert is not always selected based on category.</p><p>Besides, we further make qualitative analysis on the ImageNet validation set and find some interesting patterns. For example, we find that images predicted as "meat market" are most likely to be assigned to the fourth expert if humans are contained. We show those images in <ref type="figure" target="#fig_0">Fig.14.</ref> It can be seen, 27 images are assigned to the fourth expert, among which 22 images contain humans with a ratio of 81.5%. By contrast, among the 32 images assigned to other experts, only 7 images contain humans with a ratio of 21.9%. This indicates CoE learns the expert selection patterns automatically, it can be based on properties other than the category. Samples assigned to the first expert:</p><p>Samples assigned to the second expert:</p><p>Samples assigned to the third expert:</p><p>Samples assigned to the fourth expert:</p><p>: Humans are contained.</p><p>: Humans are not contained. <ref type="figure" target="#fig_0">Figure 14</ref>. Images that are predicted as 'meat market' by the delegator. They are partitioned into four groups based on which expert is selected. The red border indicates humans are contained, green border indicates humans are not contained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Training framework of CoE with three experts. L 1 T , L 2 T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Top-1 Accuracy v.s. FLOPs for CoE on ImageNet. "CC" means CondConv and 'PWLU' is an activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Accuracy v.s. training cost for OFA-230 and CoE-Large (4 experts). "xx ep." means the number of training epochs is "xx".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Selection probabilities for each expert at different TCP values. The selection probability for heavier model increases with the input sample getting more complex (with the decrease of TCP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Tableau of the transportation problem (b) Penalty costs of respective rows and columns (c) Tableau after allocating 3 to Mill 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>Accuracy v.s. FLOPs. "ET" means Early Termination and "CC" indicates CondConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 .</head><label>13</label><figDesc>Selection probabilities for each expert. The horizontal axis indicates the rough prediction class. There are a total of 1000 columns, each one represents a probability vector for selecting experts. Probability vectors are clustered for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Compare CoE with some efficient models on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="2">FLOPs TOP-1 Acc</cell></row><row><cell>WeightNet (2020)</cell><cell>141M</cell><cell>72.4%</cell></row><row><cell>DS-MBNet-M  ? ? (2021a)</cell><cell>319M</cell><cell>72.8%</cell></row><row><cell>GhostNet 1.0x (2020a)</cell><cell>141M</cell><cell>73.9%</cell></row><row><cell>MobileNetV3-Large (2019)</cell><cell>219M</cell><cell>75.2%</cell></row><row><cell>OFA-230 (2019)</cell><cell>230M</cell><cell>76.9%</cell></row><row><cell>TinyNet-A (2020b)</cell><cell>339M</cell><cell>77.7%</cell></row><row><cell>CondConv-EfficientNet-B0 (2019)</cell><cell>413M</cell><cell>78.3%</cell></row><row><cell>GFNet (2020)</cell><cell>400M</cell><cell>78.5%</cell></row><row><cell>CoE-Small</cell><cell>100M</cell><cell>78.2%</cell></row><row><cell>CoE-Small + CondConv</cell><cell>102M</cell><cell>79.2%</cell></row><row><cell>CoE-Small + CondConv + PWLU</cell><cell>100M</cell><cell>80.0%</cell></row><row><cell>BasisNet (2021)</cell><cell>198M</cell><cell>80.0%</cell></row><row><cell>OFA-595 (2019)</cell><cell>595M</cell><cell>80.0%</cell></row><row><cell>EfficientNet-B2 (2019)</cell><cell>1.0B</cell><cell>80.1%</cell></row><row><cell>EfficientNet-B1(Noisy Student) (2020)</cell><cell>700M</cell><cell>80.2%</cell></row><row><cell>BasisNet (2021)</cell><cell>290M</cell><cell>80.3%</cell></row><row><cell>FBNetV3-C (2020)</cell><cell>557M</cell><cell>80.5%</cell></row><row><cell>BasisNet + CondConv (2019)</cell><cell>308M</cell><cell>80.5%</cell></row><row><cell>CoE-Large</cell><cell>194M</cell><cell>80.7%</cell></row><row><cell>CoE-Large + CondConv</cell><cell>214M</cell><cell>81.5%</cell></row></table><note>are 100M and 194M respectively. Compared with OFA, CoE reduces the FLOPs from 230M to 100M and from 595M to 194M, with better top-1 accuracy. Though dynamic networks like GFNet, CondConv and BasisNet are more ef- ficient than traditional networks, CoE still has significantly higher accuracy with smaller FLOPs. Compared with these approaches, CoE improves the accuracy by 2.2/2.4/0.7% re- spectively. When combined with CondConv, CoE achieves 79.2% and 81.5% accuracy with only 102M and 214M FLOPs, indicating that CoE is complementary to dynamic networks like CondConv. On the contrary, as CondConv and BasisNet share similar essence, namely using a group of basis to dynamically synthesize the input-dependent con- volution kernel, the combination of them only arouses little collaborative benefit with the accuracy as 80.5%. Moreover, CoE achieves 80.0% accuracy with 100M FLOPs for the first time by further using PWLU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, the</cell></row></table><note>2x training cost. To verify whether the improve- ment still exists with similar training cost, we get a series of accuracies by varying the number of training epochs as shown in Fig.7, where 32 GPUs (Tesla-V100-PCIe-16GB) are used. It is seen that CoE boosts the performance from 78.3% to 79.9% even when the training cost is similar.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>CPU latency and memory cost for different methods.</figDesc><table><row><cell></cell><cell>Models</cell><cell></cell><cell cols="2">CPU Latency/Instance (ms) Batchsize=1 Batchsize=64</cell><cell>FLOPs</cell><cell>MAC</cell><cell>Params Accuracy</cell></row><row><cell cols="2">MobileNetV3-Small</cell><cell></cell><cell>14.77</cell><cell>4.18</cell><cell>56M</cell><cell>2.5M</cell><cell>2.5M</cell><cell>67.4%</cell></row><row><cell cols="2">GhostNet 1.0x</cell><cell></cell><cell>39.91</cell><cell>16.50</cell><cell>141M</cell><cell>5.2M</cell><cell>5.2M</cell><cell>73.9%</cell></row><row><cell cols="2">TinyNet-B</cell><cell></cell><cell>34.58</cell><cell>19.44</cell><cell>202M</cell><cell>3.7M</cell><cell>3.7M</cell><cell>75.0%</cell></row><row><cell cols="2">MobileNetV3-Large</cell><cell></cell><cell>31.55</cell><cell>18.43</cell><cell>219M</cell><cell>5.4M</cell><cell>5.4M</cell><cell>75.2%</cell></row><row><cell cols="2">GhostNet 1.3x</cell><cell></cell><cell>43.94</cell><cell>29.70</cell><cell>226M</cell><cell>7.3M</cell><cell>7.3M</cell><cell>75.7%</cell></row><row><cell cols="2">OFA-230</cell><cell></cell><cell>33.52</cell><cell>15.21</cell><cell>230M</cell><cell>5.8M</cell><cell>5.8M</cell><cell>76.9%</cell></row><row><cell cols="2">EfficientNet-B0</cell><cell></cell><cell>49.12</cell><cell>35.21</cell><cell>391M</cell><cell>5.3M</cell><cell>5.3M</cell><cell>77.2%</cell></row><row><cell cols="2">TinyNet-A</cell><cell></cell><cell>45.76</cell><cell>23.71</cell><cell>339M</cell><cell>5.1M</cell><cell>5.1M</cell><cell>77.7%</cell></row><row><cell cols="3">CondConv-EfficientNet-B0</cell><cell>81.81</cell><cell>-</cell><cell>413M</cell><cell cols="2">24.0M 24.0M</cell><cell>78.3%</cell></row><row><cell cols="2">BasisNet</cell><cell></cell><cell>40.61</cell><cell>-</cell><cell>198M</cell><cell cols="2">24.9M 24.9M</cell><cell>80.0%</cell></row><row><cell cols="3">CoE-Large (4 experts)</cell><cell>38.67</cell><cell>15.02</cell><cell>220M</cell><cell>6.6M</cell><cell>25.7M</cell><cell>79.9%</cell></row><row><cell cols="3">CoE-Large (16 experts)</cell><cell>34.89</cell><cell>13.30</cell><cell>194M</cell><cell>6.0M</cell><cell>95.3M</cell><cell>80.7%</cell></row><row><cell cols="4">Table 3. Comparison with model ensembe.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell>FLOPs</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Seed1</cell><cell>230M</cell><cell>78.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Seed2</cell><cell>230M</cell><cell>78.0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OFA-230</cell><cell>Seed3</cell><cell>230M</cell><cell>78.1%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Seed4</cell><cell>230M</cell><cell>78.0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ensemble</cell><cell>920M</cell><cell>79.6%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoE-Large</cell><cell>4 Experts 16 Experts</cell><cell>194M 194M</cell><cell>79.8% 80.7%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 4. Comparison with category-based selection. "RP" and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">"CBP" means Random Partition and Clustering-Based Partition.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">FLOPs Top-1 Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Category-Based</cell><cell>RP CBP</cell><cell>220M 220M</cell><cell>78.3% 77.5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoE-Large</cell><cell>-</cell><cell>220M</cell><cell>79.9%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ablations for the training tricks, ablations for expert number</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and early termination. We mainly introduce the ablations</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">for our proposed training method here, others are discussed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in Appendix B.2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CoE consists of 2 major components: LGM and WGM.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Apart from directly removing one component, we also try</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to alter some elements inside them. We propose several</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">modified versions of CoE for ablation as below:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">? CoE LGM : Remove LGM from CoE. Thus, CoE col-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">lapses to a single expert with delegator to trigger the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison with gate-value-based training method.</figDesc><table><row><cell cols="2">Method</cell><cell cols="2">FLOPs Top-1 Acc.</cell></row><row><cell>Gate-Value-Based</cell><cell>Soft Gate-Value Hard Gate-Value</cell><cell>220M 220M</cell><cell>78.7% 78.9%</cell></row><row><cell>CoE-Large</cell><cell>-</cell><cell>220M</cell><cell>79.9%</cell></row></table><note>? CoE LGM : Abandon the refining of suitability criterion (Eq.2) and remove the " j L j,k = m/n"constraint in Eq.3. So that L m?n neglects the No Superiority Assumption (NSA).? CoE W GM : Remove WGM from CoE. Thus, losses of experts have identical weights for each sample.? CoE W GM : WGM partitions the training data based on expert suitability, thus the assignment matrix A m?n in WGM equals to the output matrix L m?n of LGM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Ablations for each component of the training method.</figDesc><table><row><cell>Method</cell><cell cols="2">Experts FLOPs</cell><cell>Acc.</cell></row><row><cell>CoE-Large</cell><cell>4</cell><cell>220M</cell><cell>79.9%</cell></row><row><cell>CoE-Large LGM</cell><cell>4</cell><cell>220M</cell><cell>78.0%</cell></row><row><cell>CoE-Large LGM</cell><cell>4</cell><cell>220M</cell><cell>79.4%</cell></row><row><cell>CoE-Large W GM</cell><cell>4</cell><cell>220M</cell><cell>78.1%</cell></row><row><cell>CoE-Large W GM</cell><cell>4</cell><cell>220M</cell><cell>77.0%</cell></row><row><cell>CoE-Large W GM ?</cell><cell>4</cell><cell>220M</cell><cell>79.2%</cell></row><row><cell>CoE-Large W GM ?</cell><cell>4</cell><cell>220M</cell><cell>79.4%</cell></row><row><cell>CoE-Large SR</cell><cell>4</cell><cell>220M</cell><cell>79.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>The BLEU scores on newstest2014 (English-to-German).</figDesc><table><row><cell>Model</cell><cell>MAC</cell><cell>Parameters</cell><cell>BLEU</cell></row><row><cell>Transformer (base)</cell><cell>62.4M</cell><cell>62.4M</cell><cell>28.1 (2019)</cell></row><row><cell>Transformer (big)</cell><cell>213.0M</cell><cell>213.0M</cell><cell>29.3 (2019)</cell></row><row><cell>CoE-Transformer</cell><cell>62.5M</cell><cell>138.2M</cell><cell>29.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Zhong, Z.,Yan, J., Wu, W., Shao, J., and Liu, C.-L. Practical block-wise neural network architecture generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Zhou, Y., Zhu, Z., and Zhong, Z. Learning specialized activation functions with the piecewise linear unit. CoRR, abs/2104.03693, 2021. A. Extra Details for Method A.1. Introduction of Vogel Approximation Method (VAM)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>We find KD extremely important for CoE, it may indicate CoE is easy to be overfitted. In addition, SD decreases the accuracy of CoE. By removing SD, CoE-Large (4 experts) boosts the accuracy from 79.9% to 80.2%. Perhaps, it is because SD makes the capacity of delegator and each expert too tiny (Gontijo-Lopes et al., 2021).B.2.2. EFFECT OF EXPERT NUMBERWe analyze the number of experts in this section, including 1, 4, and 16 experts. The results are shown inTable 9. Using one expert brings little improvement compared with the original model. When increasing the number of experts, the accuracy becomes 1.9% better with four experts and 2.9% better with sixteen experts. It demonstrates CoE can make full use of multiple experts, leading to a large collaborative benefit. What's more, the accuracy also reaches 79.9% by combining CondConv with OFA-230. In this manner, CoE can further enhance the accuracy to 80.8/81.5% with 4/16 experts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Ablation study for the training tricks. "KD", "AA" and "SD" denotes knowledge distillation, auto-augment and stochastic depth respectively. Comparison among different number of experts. "CC" indicates CondConv.</figDesc><table><row><cell cols="6">KD AA SD Experts FLOPs TOP-1 Acc</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>4</cell><cell>220M</cell><cell>79.9%</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>4</cell><cell>220M</cell><cell>80.2%</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>4</cell><cell>220M</cell><cell>79.4%</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>4</cell><cell>220M</cell><cell>76.2%</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>4</cell><cell>220M</cell><cell>79.7%</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>4</cell><cell>220M</cell><cell>76.3%</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>4</cell><cell>220M</cell><cell>75.2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>220M</cell><cell>75.1%</cell></row><row><cell>Method</cell><cell></cell><cell>Experts</cell><cell></cell><cell>FLOPs</cell><cell>Acc.</cell></row><row><cell>OFA-230</cell><cell></cell><cell>-</cell><cell></cell><cell>230M</cell><cell>78.0%</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>220M</cell><cell>78.0%</cell></row><row><cell>CoE-Large</cell><cell></cell><cell>4</cell><cell></cell><cell>220M</cell><cell>79.9%</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell></cell><cell>220M</cell><cell>80.9%</cell></row><row><cell>CC-OFA-230</cell><cell></cell><cell>-</cell><cell></cell><cell>242M</cell><cell>79.9%</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell>214M</cell><cell>79.9%</cell></row><row><cell cols="2">CoE-Large + CC</cell><cell>4</cell><cell></cell><cell>214M</cell><cell>80.8%</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell></cell><cell>214M</cell><cell>81.5%</cell></row><row><cell>B.2.3. EFFECT OF EARLY TERMINATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 .</head><label>10</label><figDesc>ReaL and original top-1 accuracy. "CC" means CondConv.</figDesc><table><row><cell>Method</cell><cell cols="3">FLOPs ReaL Acc. Ori. Acc.</cell></row><row><cell>OFA-595 (Cai et al., 2020)</cell><cell>595M</cell><cell>86.0%</cell><cell>80.0%</cell></row><row><cell>S4L MOAM (Zhai et al., 2019)</cell><cell>4B</cell><cell>86.6%</cell><cell>80.3%</cell></row><row><cell>ResNeXt-101 (Xie et al., 2017)</cell><cell>16B</cell><cell>85.2%</cell><cell>79.2%</cell></row><row><cell>ResNet-152 (He et al., 2016)</cell><cell>11B</cell><cell>84.8%</cell><cell>78.2%</cell></row><row><cell>CoE-Large</cell><cell>194M</cell><cell>86.5%</cell><cell>80.7%</cell></row><row><cell>CoE-Large + CC</cell><cell>214M</cell><cell>86.9%</cell><cell>81.5%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v70/arpit17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Are we done with imagenet? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oncefor-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HylxE1HKwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Embedding complementary deep networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instanas: Instance-aware neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3577" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Addressing failure prediction by learning model confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corbi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar-Hen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02049</idno>
		<title level="m">Joint architecture-recipe search using neural acquisition function</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tradeoffs in data augmentation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ZcKPWuhG6wy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghostnet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00165</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Model rubik&apos;s cube: Twisting resolution, depth and width for tinynets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="993" to="1001" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training independent subnetworks for robust prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OGg9XnKxFAH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic slimmable network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="8607" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking neural checkpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2663" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Revisiting the design space of weight networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weightnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hydranets: Specialized dynamic architectures for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The transportation problem and the vogel approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Shore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="441" to="457" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need" />
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glance and focus: a dynamic approach to reducing spatial redundancy in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1963bd5135521d623f6c29e6b1174975-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2432" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sklf1yrYDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenatton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13570</idno>
		<title level="m">Hyperparameter ensembles for robustness and uncertainty quantification</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Condconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Practical assessment of pre-trained models for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logme</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/you21b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Meila, M. and Zhang, T.</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01097</idno>
		<title level="m">Dynamic graph: Learning instance-aware connectivity for neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basisnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03014</idno>
		<title level="m">Two-stage model synthesis for efficient inference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10694</idno>
		<title level="m">Dynamic convolution for accelerating convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
