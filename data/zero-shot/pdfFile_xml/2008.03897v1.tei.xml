<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature descriptor matching is a critical step is many computer vision applications such as image stitching, image retrieval and visual localization. However, it is often affected by many practical factors which will degrade its performance. Among these factors, illumination variations are the most influential one, and especially no previous descriptor learning works focus on dealing with this problem. In this paper, we propose IF-Net, aimed to generate a robust and generic descriptor under crucial illumination changes conditions. We find out not only the kind of training data important but also the order it is presented. To this end, we investigate several dataset scheduling methods and propose a separation training scheme to improve the matching accuracy. Further, we propose a ROI loss and hard-positive mining strategy along with the training scheme, which can strengthen the ability of generated descriptor dealing with large illumination change conditions. We evaluate our approach on public patch matching benchmark and achieve the best results compared with several state-of-the-arts methods. To show the practicality, we further evaluate IF-Net on the task of visual localization under large illumination changes scenes, and achieves the best localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Scene</head><p>Easy-level Scene Hard-level Scene Figure 4. Two examples from the proposed Illumination Matching Dataset (IMD). Easy-level scene contains only minor illumination changes while hard-level scene includes severe diversities on both illumination and viewpoint aspects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Designing powerful local feature descriptor is a fundamental and critical problem in many computer vision applications such as image retrieval <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>, camera localization <ref type="bibr" target="#b14">[15]</ref>, wide baseline stereo <ref type="bibr" target="#b22">[23]</ref>, and structurefrom-motion (SfM) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. The representation of these local features must be capable to describe a variety of physical aspects, such as the variations in scale, viewpoint or illumination. These works can be divided into two categories: hand-crafted features and learned features using Convolutional Neural Network (CNN).</p><p>In hard-crafted features, the well-known SIFT <ref type="bibr" target="#b17">[18]</ref> improves accuracy in feature matching, opening a new age in the field of feature description. <ref type="bibr" target="#b8">[9]</ref> uses a binary method to generate a robust feature descriptor, while others <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> explore to improve the performance by using an alternative distance metrics.</p><p>In learning-based features, it can be further divided to two different kinds based on whether or not the metric layer is used. With the metric layer, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> replace traditional handcrafted methods to generate a more robust descriptor. However, the drawback of using a metric layer is it cannot perform the Nearest Neighbor Search (NNS). Since this kind of method usually treat the matching task only as a binary classification problem. On the other hand, the CNN architecture of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref> do not contain any metric layer, and the results are not limited to binary classification which can make the matching accuracy better. To this end, CNN architecture without the use of metric layer has become the mainstream in learning-based feature descriptors.</p><p>Despite the recent notable achievements by using CNN, the performance of current state-of-the-art learning-based descriptors is found out to be somewhat limited on standard benchmarks <ref type="bibr" target="#b0">[1]</ref>. As shown in <ref type="figure">Figure 1</ref>(a), current standard benchmark dataset is either focus on scenes with only viewpoint changes or only minor illumination changes. This phenomenon often limits the generalizable ability of current learned feature descriptors, since most of the descriptors are focused only on learning how to handle scenes with viewpoint, orientation and scale variance. However, when feature matching is used as the pre-step of many real-life applications such as image retrieval or camera localization, where it usually happens that two images are taken in different weathers or more extreme, one is taken during the day while the other is taken at night, matching image pairs with significant illumination diversity become an extremely important issue.</p><p>To make feature matching feasible under large illumination diversity, we propose IF-Net, an Illumination-invariant Feature Network. Since the quality of training data and training strategy are crucial for the success of descriptor learning, and in order to make IF-Net capable of overcoming the effect caused by illumination changes, we propose an AMOS-crop dataset, which is a patch-correspondence dataset with very large illumination variations. However, instead of directly merging AMOS-crop with commonly used datasets,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IF-Net: An Illumination-invariant Feature Network</head><p>Po-Heng Chen, Zhao-Xu Luo, Zu-Kuan Huang, Chun Yang, Kuan-Wen Chen* (a) (b) <ref type="figure">Figure 1</ref>: (a) Current benchmark focus on geometry-related scenes and with only small illumination variations. (b) Our work can tackle both geometry and illumination-related scenes by generating robust and generic descriptors.</p><p>we evaluate the influence of dataset schedules. Interestingly, we find out directly combining all data into the training process leads to an inferior result, and this observation is consistent with <ref type="bibr" target="#b9">[10]</ref>. Due to this reason, a new dataset scheduling methods named separation training scheme is proposed. The proposed training scheme not only can make IF-Net handle viewpoint changes scenarios, but also can largely improve the matching accuracy under significant illumination diversity, as shown in <ref type="figure">Figure 1</ref></p><formula xml:id="formula_0">(b).</formula><p>Along with the training scheme, a new training loss function named Reduce-Outlier-Inlier (ROI) loss is also designed to train IF-Net. The proposed ROI loss can enhance the ability of IF-Net to resist data noise, which can stably converge the network and generate robust descriptors. Furthermore, in order to make anchor patch not only have better robustness to negative patch, but also to positive patch, a hard-positive mining strategy is proposed. As mentioned before, current benchmark dataset <ref type="bibr" target="#b0">[1]</ref> often focus on evaluating on either viewpoint or scale changes scenarios. To show the effect of IF-Net with the proposed training scheme and loss function under severe illumination changes, we further propose an Image Matching Dataset (IMD). IMD is an evaluation dataset contains 54 worldwide attraction scenarios, each of which is categorized as either easy or hard, based on the level of illumination and viewpoint changes. As the proposed IF-Net achieves the best results not only on public benchmark dataset <ref type="bibr" target="#b0">[1]</ref>, but also on the proposed IMD, we believe IF-Net can also perform well on the task of camera localization under severe illumination diversity. To verify the statement, we show IF-Net actually achieves state-of-the-art results on single-shot localization compared to previous work <ref type="bibr" target="#b21">[22]</ref>.</p><p>In summary, the contributions of this paper has four folds:</p><p>? High quality training data and strategy is crucial for the success of descriptor learning. We find out that not only the kind of data is important but also the order in which it is presented. To respond to this phenomenon, a new dataset scheduling method named separation training scheme is proposed, which can significantly improve the matching accuracy and descriptor generalizable ability.</p><p>? A new training loss function named ROI loss is designed along with the proposed separation training scheme. The ROI loss not only strengthen the matching accuracy on geometry-related scenes (e.g. Hpathces benchmark), but also make CNN more capable of handling scenes with large illumination diversity (e.g. proposed IMD).</p><p>? In order to make anchor patch not only have a better robustness to negative patch, but also to positive patch, a hard-positive mining strategy is proposed. This strategy can more effectively widen the descriptor distance between (anchor, positive) and (anchor, negative), improving the discriminative ability of the network.</p><p>? We present two datasets for both training and evaluation, named AMOS-crop dataset and Image Matching Dataset, respectively. AMOS-crop dataset is a large collection of corresponding feature patches under a variety of illumination conditions from fixed cameras. IMD contains 54 world attractions with large illumination diversity, which can be used as the baseline to evaluate the ability of illumination invariance. The proposed datasets are available at http://covis.cs.nctu.edu.tw/IF-Net/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>The works on designing feature descriptors has gradually moved from handcrafted to learning-based methods. For classical handcrafted feature descriptors, please refer to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> for a detail overview. Here, we foucs on the review of descriptor learning, ranging from traditional method to popular CNN-based methods. Traditional descriptor learning. Early efforts to learn descriptors are not limited to any particular machine learning approach, thus many unique works are proposed at that time.</p><p>In contrast to SIFT <ref type="bibr" target="#b17">[18]</ref>, PCA-SIFT <ref type="bibr" target="#b13">[14]</ref> does not directly use smoothed weighted histograms to gradient patch, but replace it with Principal Components Analysis (PCA). To achieve better performance, Simonyan et al. <ref type="bibr" target="#b28">[29]</ref> and Brown et al. <ref type="bibr" target="#b2">[3]</ref> focus on the learning of pooling region and non-linear transforms with dimensionality reduction, respectively. Not only in the field of float descriptors, there are also some works focus on learned binary descriptors. To ensure each bit in the binary descriptor can achieve low variance for intra-class and high variance for inter-class, BOLD <ref type="bibr" target="#b1">[2]</ref> is proposed for adaptive online selection of binary intensity tests. Another work of binary descriptors is RMGD <ref type="bibr" target="#b6">[7]</ref>. They combine an extended Adaboost bit selection with the proposed spatial ring-region based pooling method for intensity tests. All the mentioned works have one thing in common, which is the descriptor starts learning from low level features such as patch gradient or binary intensity test. Learning from low level features will make these works suffer from information loss. CNN based descriptor learning. At the beginning, Siamese network is the main stream and the most commonly used architecture in CNN based descriptor, and a fully connected layer is usually used as the metric network to improved performance. MatchNet <ref type="bibr" target="#b7">[8]</ref> is a typical Siamese network with metric layer. They show a great potential of CNN based descriptor learning by significantly improve previous results. Based on MatchNet, Zagoruyko et al. <ref type="bibr" target="#b32">[33]</ref> attempt different kinds of network structure and propose a central-surround method to boost matching accuracy. By proposing a global loss function, and combine with metric layer, Kumar et al. <ref type="bibr" target="#b16">[17]</ref> achieve the best performance on Brown dataset <ref type="bibr" target="#b2">[3]</ref>. Despite metric network improves the matching accuracy, it also limits the versatility of the deep network (i.e. cannot perform nearest neighbor search).</p><p>To tackle this problem, later proposed CNN-based descriptors do not adopt metric layer into the model archit ecture. Simo-Serra et al. propose DeepDesc <ref type="bibr" target="#b27">[28]</ref>, a CNN without metric layer with aggressive mining strategy to select hard patches. L2-Net <ref type="bibr" target="#b30">[31]</ref> yields significant improvements by modifying CNN architecture with proposed novel loss functions and progressive sampling strategy. Built on the basis of L2-Net, Mishchuk et al. <ref type="bibr" target="#b21">[22]</ref> propose HardNet, a CNN network trained with a distance matrix to find hardnegative pairs and achieve state-of-the-art performance on public benchmark dataset <ref type="bibr" target="#b0">[1]</ref>. Recently, Luo et al. additionally consider image geometry information and cross-modality contextual property into training process, and propose GeoDesc <ref type="bibr" target="#b19">[20]</ref> and ContextDesc <ref type="bibr" target="#b18">[19]</ref>, respectively. However, both of the work not only consider patch as the network input, but also take the keypoint position information into account to train the descriptors, which is different from our approach and <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>. Though mentioned works lead CNN-based descriptor to achieve significant improvements, they mainly focus on generating viewpoint-invariance and scale-invariance descriptors. However, illuminationinvariance is also a non-negligible effect in the field of feature matching, but none of them take this effect into account. So our work aim to generate a descriptor meets both illumination-invariance and viewpoint-invariance, which seeks to achieve improvements not only in feature matching, but also in real-life applications such as camera localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET SCHEDULING</head><p>The quality of training data is crucial for the success of supervised learning. In this section, we first briefly introduce the proposed AMOS-crop dataset, and then investigate the matching performance depending on the presented training data. Interestingly, we find out the outcome is consistence with Ilg et al. <ref type="bibr" target="#b9">[10]</ref>: During the training process, not only the kind of training data will influence the results, but also the order in which it is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The AMOS-crop Dataset</head><p>The PhotoSynth (PS) dataset <ref type="bibr" target="#b23">[24]</ref> is currently the largest training dataset for feature matching. It includes more than six million patch correspondences, and is collected automatically based on SIFT and structure-from-motion (SfM) technique. Limited to their process for dataset construction, PS dataset can only collect patches with little illumination diversity. However, the quality of training data will directly influence the performance of network in supervised learning. In order to collect patch correspondences with large illumination diversity, we take advantage of AMOS dataset <ref type="bibr" target="#b10">[11]</ref> ( <ref type="figure" target="#fig_0">Figure  2(a)</ref>). AMOS dataset is a collection of time-lapse photographs, containing images taken by fixed cameras at day and night for several months. Due to the fixed camera property, we construct AMOS-crop dataset, consisting by large amounts of feature correspondences under different weather and illumination conditions. To ensure the diversity of AMOScrop dataset, we select 200 frames in each scene. To detect feature points in every frame, we apply a commonly used and stable detectors such as SIFT, SURF and ORB. By leveraging the property of fixed camera position, we can easily verify whether the detected features are the same or not by their pixel coordinates. Finally, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b), we generate 1,380,724 patch correspondences with size 64x64 in 35 different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Scheduling Method</head><p>We adopt the network architecture introduced by Tian et al. <ref type="bibr" target="#b30">[31]</ref> as our backbone model for testing different dataset schedules. The model is trained on both PS dataset and proposed AMOS-crop dataset. However, instead of directly merge both datasets for training, we investigate various dataset scheduling methods by considering different dataset training order and training mechanisms.</p><p>To train IF-Net on different datasets, we investigate 3 different training schedules. "#$%&amp; is the basic schedule to train IF-Net for 50 epochs. Apart from "#$%&amp; , a schedule for fine-tuning '%() is also investigate. The last one $#*)+#,%-( is carefully designed relative to the first two schedules. In $#*)+#,%-( , it first input PS and AMOS-crop datasets into a shared-weight IF-Net <ref type="figure">(Figure 3)</ref>, and then update the parameters of the model by considering the gradients caused by both datasets. Different schedules and their results on HPatches benchmark are listed in <ref type="table" target="#tab_0">Table 1</ref>. The results lead to the following observations: The order of training data with different properties matters. Though AMOS-crop dataset provides scenarios with illumination diversity, however, merging with PS dataset to train IF-Net lead to worse results. Even for the schedules with fine-tuning, the performance is still not as expected. The results indicate the importance of training data schedules when learning a generic concept with deep networks. Separation training scheme outperforms mixed all data together. By just using the proposed separation training scheme, we can improve IF-Net by almost 10% on HPatches benchmark. We conjecture that in order to let both datasets contribute equally influence to IF-Net, it's critical to first separate their descriptor generation process, then update the network by considering both gradients until back-propagation. Besides, we did not use specialized training sets for special scenarios. The above observation is in consistent with <ref type="bibr" target="#b9">[10]</ref>, where the dataset schedules are important for training a deep model. In the rest of the paper, we adopt IF-Net trained under $#*)+#,) as our default structure since it achieves the best results on public benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ILLUMINATION-INVARIANT FEATURE NETWORK</head><p>In this section, we will introduce the proposed methods for training IF-Net. <ref type="figure">Figure 3</ref> shows the model architecture of deep network.   <ref type="bibr" target="#b0">[1]</ref>. With the proposed separation training scheme, IF-Net achieves the best mean average precision (mAP) on the matching task. Dataset</p><formula xml:id="formula_1">Mixed 0.31 - - PhotoSynth 0.45 - - AMOS-crop 0.21 - - PS?AMOS-crop - 0.29 - AMOS-crop?PS - 0.34 - Separation - - 0.48</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hard-Positive Mining</head><p>The conventional triplet margin loss is used to increase the distance between matching and non-matching pairs, as shown in Eq. (1):</p><formula xml:id="formula_2">= ( + ( , ) ? ( , ), 0),<label>(1)</label></formula><p>where d is the Euclidean distance, a, p and n represent anchor, positive and negative patch, respectively. Here positive means correct matching pair (i.e. correspondence) with anchor, while negative means the opposite. m is a self-defined threshold to control the distance between positive and negative patches. As most of the existing datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref> belong to geometryrelated samples, current work <ref type="bibr" target="#b21">[22]</ref> only need to apply hardnegative mining to search hard-negative patches for training. However, for large illumination variation, considering only hard-negative patches is not enough. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, some matching pairs may appear quite different. How to sample a robust matching pair to improve network's discriminative ability becomes a critical and inevitable issue.</p><p>To solve this issue, we propose hard-positive mining strategy to strengthen network's ability. At First, a batch of matching pairs = =&gt; % , % A BC = 1. . , = 1. . J are generated, where A, P represent anchor and positive patch respectively. % and % A are the 2D points from different images j but represents the same 3D point i in the world (i.e., a matching pair). M is the number of randomly sampled positive patch corresponding to the anchor. Then, ? ( + 1) patches are forwarded to IF-Net to generate corresponding descriptors. In order to perform hard-positive mining strategy, an L2 pairwise distance matrix D of size ? is calculated. Each row in D represents the L2 distance between one anchor patch and all of its corresponding matching pairs. In the process of hard-positive mining, we eventually select a positive patch with the farthest distance to the anchor, and serves as the correspondence matching pairs. As for the negative patch (i.e. non-matching pairs to the anchor), we follow <ref type="bibr" target="#b21">[22]</ref> for the same sampling strategy.</p><p>By adopting the proposed hard-positive mining, the triplet margin loss from Eq. (1) are modified as Eq. (2):</p><formula xml:id="formula_3">= 1 L ( + ( ( % , % A ) ( %NO ? P ( % , Q ), &gt; Q , % A BR , 0),<label>(2)</label></formula><p>where a, p and n share the same definition as Eq. <ref type="formula" target="#formula_2">(1)</ref>, however, with additional subscription for hard-positive mining. In the subscript, i represents i'th training data in a batch and ? [1, ], ? for both ( % , Q ) and &gt; Q , % A B . After the mining strategy, n triplet training data ( % , % A , Q ) is formed to train the network. With the proposed hard-positive mining, the network not only can make negative patch further away from anchor, but also can make hard-positive patch be closer to anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reduce-Outlier-Inlier (ROI) Loss</head><p>Adopting the proposed mining strategy can strengthen the discriminative ability of the network. However, there still exist discrepancy between the training data in the distribution of descriptor space. This phenomenon will make the training process difficult to converge to a local minima value. To address this issue, we further propose a Reduce-Outlier-Inlier (ROI) loss, which can make the training process more stable with improved matching results. In the following, we assume all data have already been applied hard-positive mining and the strategy in <ref type="bibr" target="#b21">[22]</ref>. Besides, Y ( % , % ) and Z ( % , % ) are denoted as the distance of the farthest matching pairs and the closest non-matching pairs, where ( % , % , % ) is a batch of training data. As the above notations, the proposed ROI loss is formulated as Eq. Intuitively, the ROI loss seeks to increase the distinctiveness of descriptors by penalizing the effect caused by any outlier patches. Since in a forward batch of training data, there are usually some patches whose distance (in descriptor space) are far from the average. It is a critical and inevitable issue especially in illumination-related data <ref type="figure">Figure 3</ref>. Proposed IF-Net architecture, L2-Net <ref type="bibr" target="#b30">[31]</ref> is adopted as the backbone. By applying the proposed separation training scheme, two different kinds of training dataset (e.g. PS-dataset <ref type="bibr" target="#b23">[24]</ref> and AMOS-crop dataset) will serve as the inputs to the shared-weight IF-Net and generate a 128-dims descriptor. Both kinds of the descriptors are normalized in order to reduce the impact within different sampled batch in every iteration.</p><p>(Amos-crop). Thus, the proposed ROI loss takes into account the data distribution as the weighting score of the distance value in each of the forward batch. In order for the loss to be minimized, the outlier matching and non-matching pairs will get higher scores, encourage the loss to be minimized and vice-versa -inliers will lead to similar scores, so there is no need to additionally consider the weight within a batch of data. The proposed ROI loss can be served as a plug-in of triplet margin loss by re-formulating as the weighted-average version: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>Training details. The proposed IF-Net is trained for 50 epochs using Adam <ref type="bibr" target="#b15">[16]</ref> with an initial learning rate of 0.1, which was further divided by 10 every 10 epochs after 30 th epoch. The batch size is set to 512, and to extract patches from raw images, TILDE <ref type="bibr" target="#b31">[32]</ref> and SIFT are used as the feature detector. And the generated descriptors are normalized by applying L2 normalization. Training dataset. PhotoSynth (PS) dataset <ref type="bibr" target="#b23">[24]</ref> is widely used for learning local image descriptors, it consists of diversity scenes with viewpoint, scales in comparison to the MVS dataset <ref type="bibr" target="#b2">[3]</ref>. Based on this reason, PS-dataset is chosen to serve as the geometry-related dataset to train IF-Net. As for illumination-related dataset, the proposed AMOS-crop dataset is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Datasets</head><p>Homography dataset. HPatches <ref type="bibr" target="#b0">[1]</ref> is a large-scale patch dataset for evaluating local features. It consists of large viewpoint and small illumination changes. As HPatches provides groundtruth homogrphies and raw images, it can also be used to evaluate image matching performance. For evaluation, we refer to <ref type="bibr" target="#b0">[1]</ref>, consisting of 116 sequences and 580 image pairs. Illumination Matching Dataset. Though HPatches dataset consists data with illumination change, the coverage of the diversity does not enough to represent real-life scenarios. To this end, we propose an Illumination Matching Dataset (IMD), consisted of 54 world attraction scenes selected from Google Image search. Each attraction scene contains an easy-level and hard-level pair, and the level is defined by its illumination and viewpoint diversity. Some examples are shown in <ref type="figure">Figure  4</ref>. It can be clearly seen that IMD contains many significant illumination and viewpoint changes scenarios, which are quite difficult for feature matching. NTU Localization Dataset. To demonstrate the generalization of the proposed IF-Net, we further evaluate the localization ability of our work under severe illumination diversity. We use NTU long-term positioning dataset <ref type="bibr" target="#b3">[4]</ref> for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Protocols</head><p>Patch level. We follow HPatches <ref type="bibr" target="#b0">[1]</ref> evaluation protocols and use mean average precision (mAP) for three sub-tasks, including patch matching, retrieval and verification. Image level. To extract keypoints from IMD raw images, we apply TILDE <ref type="bibr" target="#b31">[32]</ref> as the feature detector on HardNet and IF-Net. Since the proposed IMD contains significant illumination and viewpoint changes, it's hard to obtain a unified and robust ground truth correspondences. So in the following matching accuracy calculation, we manually verify the results on both IF-Net and HardNet. Localization level. The localization is evaluated under different sessions in different lighting conditions (by different weather). 288 images from one session (M1/20 15:00) are used for model reconstruction, which follows the evaluation protocol in <ref type="bibr" target="#b3">[4]</ref>, and each test session includes 108 images with ground truth locations. Since localization accuracy and image registration rate are trade-off relations, thus we list both of them for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Results</head><p>HPatches Evaluation. To demonstrate the efficacy of proposed training schemes of IF-Net, we evaluate several works on HPatches benchmark, including verification, matching and retrieval. While HardNet is the most relevant w ork to ours, we also train HardNet on both PS and AMOScrop datasets for fair comparison. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the proposed training schemes of IF-Net improves the overall performance over the previous best-performing under similar training settings. Furthermore, in both image retrieval and matching tasks, our approach leads significant accuracy improvement against Hard-Net, which demonstrate the robustness of descriptor generated from IF-Net.  <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>, respectively. We report two different kinds matching accuracy: Top-40 and Total. The reason to do report Top-40 is due to some scenarios in IMD contains too much illumination and viewpoint changes, and all methods perform poorly on such cases, which will lead to a non-representative evaluation. As can be seen from <ref type="table" target="#tab_3">Table 3</ref>, IF-Net achieves the best performance regardless of which accuracy scheme is considered. This demonstrate our approach can still generate robust and generic feature descriptors under scenes with large illumination changes. Localization. To justify our approach can tackle a more challenging condition, we evaluate IF-Net on a complex computer vision task: visual localization. The evaluated results are in <ref type="table" target="#tab_2">Table 4</ref>, except for the registration rate, the lower value implies better performance. Since both SIFT and HardNet trained on PS and AMOS have significant lower registration rate than ours (i.e. shows inferior performance on visual localization), we will not consider to compare the localization error between these methods. Instead, as can be seen from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose an Illumination-invariant Feature Network (IF-Net) to tackle image matching problems under large illumination diversity. IF-Net is trained with the proposed ROI loss function and separation training scheme, the former can make deep model be less affected by the noise of training data while the later can balance the distribution between different training data. Furthermore, we construct an AMOS-crop dataset for training and an IMD dataset for matching evaluation. We evaluate IF-Net on both HPatches matching benchmark and NTU visual localization task. The results show that IF-Net achieves state-of-the-art performance, which proves the efficiency and practicality of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGEMENT</head><p>This research was supported in part by the Ministry of Science and Technology of Taiwan (MOST 108-2633-E-002-001, MOST 107-2221-E-009-148-MY2, and MOST 109-2634-F-009-015-), National Taiwan University (NTU-108L104039), and Delta Electronics.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Original AMOS dataset. Each row represents the same scene but during day and night time respectively. (b) Proposed AMOScrop dataset. Each row corresponds to the same 3D point but with different illumination conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison between proposed IF-Net and HardNet [22]. (a) IF-Net. (b) HardNet (PS + AMOS dataset). (c) HardNet (PS dataset). As can be seen clearly, our method outperforms state-of-the-art under the hard-level scene by matching more correct pairs and less incorrect pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of difference dataset scheduling method evaluated on HPatches benchmark</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean average precision on HPatches benchmark<ref type="bibr" target="#b0">[1]</ref>. IF-Net (Ours) performs the best on both matching and retrieval tasks. As for verification task, IF-Net only behind the best results in a very small and acceptable margins (0.02 for Inter and 0.04 for Intra). To show how the proposed IF-Net can handle practical scenarios of image matching, we evaluate IF-Net and HardNet on the proposed IMD. To give a fair comparison, HardNet has two different versions: one is trained on only PS-dataset and the other is trained on both PS and AMOS-crop dataset. The qualitative and quantitative results are shown in</figDesc><table><row><cell cols="2">Training Dataset</cell><cell>None</cell><cell cols="2">PhotoSynth (PS) Dataset</cell><cell></cell><cell cols="2">PS Dataset + AMOS -crop Dataset</cell></row><row><cell></cell><cell></cell><cell>SIFT</cell><cell>DeepDesc [28]</cell><cell>DC-2-Str [33]</cell><cell>Hard-Net</cell><cell>Hard-Net [22]</cell><cell>Ours</cell></row><row><cell>Verification</cell><cell>Inter Intra</cell><cell>82.83 80.35</cell><cell>92.12 89.12</cell><cell>91.49 89.07</cell><cell>93.33 93.61</cell><cell>93.85 93.53</cell><cell>93.83 93.57</cell></row><row><cell>Matching</cell><cell></cell><cell>24.44</cell><cell>25.60</cell><cell>25.50</cell><cell>44.76</cell><cell>41.64</cell><cell>48.65</cell></row><row><cell>Retrieval</cell><cell></cell><cell>41.71</cell><cell>52.71</cell><cell>45.77</cell><cell>63.52</cell><cell>60.85</cell><cell>67.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>, our approach achieves the best results on both registration rate and localization accuracy under almost all lighting conditions compared to HardNet trained on PS dataset. This shows IF-Net can truly generate illuminationinvariance descriptors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Precision rate comparison on proposed IMD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Table 4. Localization error (cm) and registration rate (R-rate) on NTU-</cell></row><row><cell>Total Top-40</cell><cell>Scene level Easy Hard Easy Hard</cell><cell>HardNet (Trained on PS+AMOS) 70.9% 55.3% 86.6% 72.7%</cell><cell>HardNet (Trained on PS) 76.9% 61.1% 92.5% 80.7%</cell><cell>IF-Net 79.7% 62.4% 94.2% 83.2%</cell><cell cols="2">dataset [4]. Test date &amp; time Weather Mean SIFT [18] Stdev. R-rate HardNet [22] Mean</cell><cell>M2/3 10:30 Cloudy 0.853 1.090 72.2% 0.217</cell><cell>M2/5 15:00 Sunny 0.760 1.265 74.0% 0.187</cell><cell>M2/6 12:00 Sunny 0.704 0.622 69.4% 0.243</cell><cell>M2/7 14:00 Cloudy Cloudy M2/10 10:00 0.591 0.776 0.410 0.881 77.7% 66.6% 0.208 0.229</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Trained on</cell><cell>Stdev.</cell><cell>0.191</cell><cell>0.081</cell><cell>0.233</cell><cell>0.116</cell><cell>0.165</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PS+AMOS)</cell><cell>R-rate</cell><cell>87.0%</cell><cell>91.6%</cell><cell>66.6%</cell><cell>75.0%</cell><cell>78.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HardNet (Trained on PS)</cell><cell>Mean Stdev. R-rate</cell><cell>0.326 0.130 100%</cell><cell>0.325 0.135 100%</cell><cell>0.381 0.319 99.0%</cell><cell>0.380 0.337 96.2%</cell><cell>0.324 0.126 100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean</cell><cell>0.310</cell><cell>0.309</cell><cell>0.337</cell><cell>0.327</cell><cell>0.314</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IF-Net</cell><cell>Stdev.</cell><cell>0.129</cell><cell>0.132</cell><cell>0.156</cell><cell>0.140</cell><cell>0.150</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R-rate</cell><cell>100%</cell><cell>100%</cell><cell>98.1%</cell><cell>98.1%</cell><cell>100%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Po-Heng Chen is with the Department of Electronic Engineering, National Chiao Tung University, Hsinchu, 300, Taiwan. The rest of the authors are with the Department of Computer Science, National Chiao Tung University, Hsinchu, 300, Taiwan (email of corresponding author*: kuanwen@cs.nctu.edu.tw)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HPatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5173" to="5182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bold-binary online learned descriptor for efficient image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2367" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="43" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-based positioning for internet-ofvehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="376" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Local image descriptor: modern approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Local multi-grouped binary descriptor with ring-based pooling configuration and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4820" to="4833" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comparative evaluation of binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="759" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The global network of outdoor webcams: properties and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metric and kernel learning using a linear transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="519" to="547" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heavy-tailed distances for gradient based image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="397" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">PCA-SIFT: A more distinctive representation for local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ContextDesc: Local Descriptor Augmentation with Cross-Modality Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2527" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geodesc: Learning local descriptors by integrating geometry constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4826" to="4837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wxbs: Wide baseline stereo generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06603</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A large dataset for improving patch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01466</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From single image query to detailed 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5126" to="5134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1573" to="1585" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">L2-net: Deep learning of discriminative patch descriptor in euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="661" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TILDE: a temporally invariant learned detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5279" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
