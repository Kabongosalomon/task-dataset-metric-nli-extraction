<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wele</forename><surname>Gedara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Bandara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya</forename><surname>Maria</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Valanarasu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
						</author>
						<title level="a" type="main">SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Road extraction is an essential step in building autonomous navigation systems. Detecting road segments is challenging as they are of varying widths, bifurcated throughout the image, and are often occluded by terrain, cloud, or other weather conditions. Using just convolution neural networks (ConvNets) for this problem is not effective as it is inefficient at capturing distant dependencies between road segments in the image which is essential to extract road connectivity. To this end, we propose a Spatial and Interaction Space Graph Reasoning (SPIN) module which when plugged into a ConvNet performs reasoning over graphs constructed on spatial and interaction spaces projected from the feature maps. Reasoning over spatial space extracts dependencies between different spatial regions and other contextual information. Reasoning over a projected interaction space helps in appropriate delineation of roads from other topographies present in the image. Thus, SPIN extracts long-range dependencies between road segments and effectively delineates roads from other semantics. We also introduce a SPIN pyramid which performs SPIN graph reasoning across multiple scales to extract multi-scale features. We propose a network based on stacked hourglass modules and SPIN pyramid for road segmentation which achieves better performance compared to existing methods. Moreover, our method is computationally efficient and significantly boosts the convergence speed during training, making it feasible for applying on largescale high-resolution aerial images. Code available at: https: //github.com/wgcban/SPIN_RoadMapper.git. arXiv:2109.07701v1 [cs.CV] 16 Sep 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Among all the topographic objects found in aerial images, road is one of the essential topographic features with numerous applications ranging from automatic navigation and guidance systems. Extraction of roads from aerial images helps to understand the connectivity between places and thus aid in automating navigation, disaster mitigation, and controlling traffic. Furthermore, road detection helps to determine the drivable areas for autonomous vehicles so that motion planning algorithms can be constrained on drivable roads. In addition, most of the algorithms designed for road boundary extraction and curb detection are based on road segmentation maps as the primary step <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. The extraction of road boundaries and curbs can be used to further improve the safety of autonomous driving <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Classical methods for road segmentation involve geometric-stochastic models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, line network extraction <ref type="bibr" target="#b6">[7]</ref>, and snakes <ref type="bibr" target="#b7">[8]</ref>. There have also been works that consider the problem of road extraction as a problem of graph extraction from images <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Following the popularity of deep learning methods in computer vision <ref type="bibr" target="#b10">[11]</ref>, Authors are with the Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, MD, USA. Emails: {wbandar1, jvalana1, vpatel36}@jhu.edu. <ref type="figure">Fig. 1</ref>. An overview of our proposed method. We build graphs in two spaces: (a) spatial space and (b) a projected latent interaction space from feature maps. Graph reasoning in spatial space extracts connectivity between the road segments, whereas reasoning over interaction space delineates roads from other topographies. Nodes connected with lines in (a) denote how road segments are modeled to understand connectivity in the spatial space. Regions marked with different colors in (b) denote how different semantics are segregated for better road delineation in the interaction space. <ref type="bibr" target="#b11">[12]</ref>, techniques involving ConvNets have been explored for automatic road extraction <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These works pose road extraction as a problem of semantic segmentation where one tries to classify the pixels corresponding to the road from other semantics of the image.</p><p>Segmenting roads from aerial images is not a straightforward segmentation problem because roads appear at different scales in the image due to varying widths and certain road regions are often narrow and get occluded with respect to the terrain. Also, there exists some similarity of the road texture with respect to nearby regions and there are chances of occlusion due to clouds and various weather conditions. One major problem of using ConvNets directly for road segmentation is that they are not good at learning longrange dependencies due to their inherent inductive biases. In aerial images, the road structure is mostly branched throughout the image as road is a connected topography. Also, just using a ConvNet does not constrain the network to learn representations of connected road segments <ref type="bibr" target="#b15">[16]</ref>. These issues make road segmentation from aerial images an open and challenging problem.</p><p>In this work, we focus on improving road segmentation by incorporating a global understanding of the image. Modeling dependencies and relations over regions in the image can help in understanding connectivity between the road segments. We note that transformer-based methods <ref type="bibr" target="#b16">[17]</ref> are currently becoming popular for their property of extracting long-range dependencies. However, it is not feasible for applications on large-scale high-resolution remote sensing datasets as it requires high compute power and significant training time. Thus, we propose using graph reasoning rather than just relying only on stacked convolutions or transformers to model global dependencies. Performing graph reasoning is light-weight and does not add on much to computation cost like transformers.</p><p>A graph convolution <ref type="bibr" target="#b17">[18]</ref> can extract dependencies over distant regions making it more meaningful for using it to understand road information on a global scale in aerial images. Graph convolutions have been explored for video recognition <ref type="bibr" target="#b18">[19]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20]</ref> and semi-supervised classification <ref type="bibr" target="#b17">[18]</ref>. Unlike these works, we propose performing graph reasoning in two domains -spatial and interaction space. In graph reasoning over spatial space, we build a graph over the feature space to extract dependencies between different spatial regions in the input. As we operate on the original coordinate space, performing reasoning over the graph would help to extract rich contextual information for road segmentation. For graph reasoning over interaction space, we construct a new interaction space where we model semantics with similar information together. This causes different semantic objects of the aerial image like roads, buildings, clouds, trees, and other topographic features to be modeled into different spaces. Performing graph reasoning over this interaction space would help in appropriate delineation of roads from other topographies in the image. Combining both, we propose a stand alone Spatial and Interaction space (SPIN) graph reasoning module which performs reasoning in the spatial and interaction space projected from the feature maps. <ref type="figure">Fig 1 illustrates</ref> how SPIN module helps to make road segmentation better.</p><p>SPIN extracts long range dependencies between road segments and is effective at delineating roads from other semantics present in the image. When added to a base network, we show that it improves the segmentation performance by a reasonable amount. It has numerous other advantages as well. SPIN can be plugged easily into a ConvNet architecture after a convolutional block. As SPIN learns highly contextual information, it increases the convergence rate of the network by half saving a lot of training time. This property is highly useful for training ConvNets on large-scale high-resolution images like aerial images. Adding SPIN to a ConvNet is also computationally effective as it adds on only 0.03M parameters. Our proposed network consists of a feature extractor using residual blocks, stacked hourglass modules with skip connections for deep feature extraction and SPIN pyramid for graph reasoning. We analyze the effectiveness of our proposed method by conducting experiments on two large-scale road segmentation datasets -DeepGlobe <ref type="bibr" target="#b20">[21]</ref> and Massachusetts Road <ref type="bibr" target="#b21">[22]</ref> where we achieve a better performance than existing methods in the literature.</p><p>In summary, this paper makes the following contributions:</p><p>? We propose a new module -Spatial and Interaction Space Graph Reasoning (SPIN), which when plugged into a ConvNet performs reasoning over graphs constructed on spatial and interaction space projected from the feature maps.</p><p>? We propose a new network built using stacked hourglass modules and SPIN pyramid for road segmentation from aerial images. ? We conduct extensive experiments on large-scale road segmentation datasets where we achieve better performance than existing methods both qualitatively and quantitatively. ? Our SPIN module is highly computationally efficient and helps in fast network convergence which makes training ConvNets on high-resolution aerial images quick and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Road segmentation: Road segmentation is a well-studied problem in which we classify each pixel in a given aerial image as "road" or "no road" <ref type="bibr" target="#b14">[15]</ref>. Early research on road segmentation primarily relied on probabilistic models to enhance connectivity by combining contextual prior conditions such as road geometry <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and color intensity <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, geometric probability models were used to represent road images, and then maximum likelihood estimation (MLE) was used to predict road pixels. In <ref type="bibr" target="#b25">[26]</ref>, a model based on high-order conditional random fields (CRF) was used to incorporate prior knowledge of roads. However, these traditional probabilistic methods require hand-designed features and complex optimization techniques <ref type="bibr" target="#b22">[23]</ref>.</p><p>One of the earliest attempts to automatically learn features for detecting roads in aerial images using expert labeled data was proposed in <ref type="bibr" target="#b0">[1]</ref>. In this study, unsupervised learning methods such as Principal Component Analysis (PCA) was used to initialize the feature detectors. Later, with the introduction of ConvNets in deep learning, researchers have investigated various ConvNet architectures to efficiently extract roads from aerial images <ref type="bibr" target="#b26">[27]</ref>. Among those, encoderdecoder based architectures are widely used due to its ability to capture relatively large spatial context <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Examples of these include U-Net <ref type="bibr" target="#b27">[28]</ref>, LinkNet <ref type="bibr" target="#b28">[29]</ref>, ResNet18 <ref type="bibr" target="#b11">[12]</ref> and multi-branch ConvNets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. In addition to the architectural changes, researchers also investigated different types of loss functions to replace well-known binary crossentropy loss (BCE) to further improve the quality of road proposals and to incorporate topological constraints. In <ref type="bibr" target="#b1">[2]</ref>, a differentiable IoU loss function was proposed and most of the later work on road segmentation then used it instead of the BCE loss or combined them together for obtaining improved performance. Instead of just formulating the road extraction as a binary segmentation task, <ref type="bibr" target="#b29">[30]</ref> introduced a multi-task learning <ref type="bibr" target="#b31">[32]</ref> approach where both segmentation and orientation of road line segments are used to improve the connectivity of the predicted road networks. Graph convolutions: The main limitation of Fully Convolutional Networks (FCNs) is its limited receptive field <ref type="bibr" target="#b32">[33]</ref>. To improve the receptive field of FCNs, researchers have proposed different solutions, such as adding pooling layers <ref type="bibr" target="#b32">[33]</ref>, dilated convolutions <ref type="bibr" target="#b33">[34]</ref>, depth-wise convolutions <ref type="bibr" target="#b34">[35]</ref>, etc. However, these methods generally learn relations implicitly and are computationally expensive <ref type="bibr" target="#b35">[36]</ref>. Instead, graph convolutions have the potential advantage of performing global reasoning on feature maps with explicit semantic meaning embedded in the graph structure. Due to this reason, many researchers have used graph convolutions in various computer vision tasks such as visual recognition <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref> and semi-supervised classification <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b35">[36]</ref>, a graph reasoning module was proposed to capture multiple long-range contextual patterns of the original feature map through a data-dependent similarity matrix. In contrast to <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b19">[20]</ref> first transformed the original feature space into another latent coordinate space called interaction space and performed relational reasoning via graph convolution in the interaction space. In our proposed SPIN module, we combine the reasoning power of both spatial and interaction space graph reasoning by concatenating the individual outcomes. Further, we perform SPIN graph reasoning on different scales of the feature maps to learn multi-scale contextual relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. SPIN graph reasoning</head><p>The proposed SPIN module is shown in <ref type="figure" target="#fig_0">Figure 2</ref>-(a). We build graphs in two spaces: spatial space and a projected latent interaction space from input feature maps. Then, graph reasoning is performed in spatial space to improve the connectivity between road segments and interaction space to delineate roads from other topographies. Assuming that spatial and interaction space graph reasoning provide different feature representations, we concatenate the output feature maps of both graph reasoning modules, as shown in <ref type="figure" target="#fig_0">Figure  2</ref>-(a), to extract rich global contextual information of road segments. In order to capture multi-scale context of input feature maps, we build SPIN pyramid by performing SPIN graph reasoning on different scales and then aggregate them as shown in <ref type="figure" target="#fig_0">Figure 2</ref></p><formula xml:id="formula_0">-(b).</formula><p>In what follows, we elaborate on each block of the SPIN module in detail. Before that, we briefly review graph reasoning. Graph reasoning: A graph G = (V, E, A) is defined by its nodes V, edges E and similarity matrix A that describes the similarity between each and every pixel (node) in the graph. Let X ? R L?C denote the input feature map where C is the number of channels and L = W ? H. Here, W and H correspond to the width and height of X. Standard 2D convolutions only share information among the positions in a small neighborhood defined by the filter size. In order to achieve a large receptive field and to capture long-range dependencies among the pixels, ConvNet architectures stack multiple convolution layers which is highly inefficient. In contrast, a single graph convolution layer can extract longrange dependencies of input feature map very efficiently and effectively. Formally, the graph convolution is defined as <ref type="bibr" target="#b17">[18]</ref></p><formula xml:id="formula_1">,X = ? (AXW ) ,<label>(1)</label></formula><p>where W is the learnable weight matrix (usually modeled as a convolutional layer), ?(?) is the non-linear activation function (e.g. ReLU) and, A and X are the same as defined above. Note that the only difference between graph convolution and conventional convolution is that in graph convolution, we left-multiply the original feature map X by the similarity matrix A before doing the convolution operation.</p><p>With this background of graph reasoning, we now describe the two main building blocks of our proposed SPIN module: (1) Spatial space graph reasoning and, (2) Interaction space graph reasoning.</p><p>1) Spatial space graph reasoning: The overall procedure of spatial space graph reasoning is depicted in the red box in <ref type="figure" target="#fig_0">Figure 2</ref>-(a). As described in the previous section, the main intuition behind spatial space graph reasoning is to improve the connectivity between the predicted road segments. We first build a fully-connected graph in the spatial domain S using the spatial similarity matrix A S and then perform spatial graph reasoning. We now describe the computation procedure of spatial graph reasoning in detail. Computation of spatial similarity matrix A S : The first step of spatial graph reasoning is to compute the spatial similarity matrix A S ? R L?L . There are different similarity metrics that have been proposed in the literature to calculate the similarity between two given pixels. The most popular are the Euclidean distance and the dot product. In our implementation, we use the dot product to compute the similarity matrix A S .</p><p>The similarity matrix A S for an input feature map X can be represented as a multiplication of three transformations as follows:</p><formula xml:id="formula_2">A S = Softmax(? S (X)?(X)? S (X) T ),<label>(2)</label></formula><p>where ? S (X) ? R L?M is a linear transformation followed by ReLU non-linearity and ?(X) ? R M ?M is the diagonal matrix. Note that M is the dimension of the intermediate feature map.</p><p>In this implementation, the linear transformation ? S (X) is modeled using a 1 ? 1 convolution layer that reduces input feature map dimension from C to M . The transformation ?(X) is represented by a global average pooling followed by a 1 ? 1 convolution. Then we reshape the outputs ? S (X) and ?(X) appropriately to perform matrix multiplication as shown in <ref type="figure" target="#fig_0">Figure 2</ref>-(a) to obtain the similarity matrix A S ? R L?L . Graph reasoning in spatial space: Once we have the similarity matrix A S , we can perform the spatial graph reasoning on input data according to the Eq. (1). First, we reshape the input data appropriately and then we perform the matrix multiplication to obtain A S X. Next, we multiply it by the trainable weight matrix W S that is modeled as a 1 ? 1 convolution layer as shown in <ref type="figure" target="#fig_0">Figure 2-(a)</ref>. Finally, we apply ReLU to obtain the spatial graph reasoned feature matrix X S .</p><p>2) Interaction space graph reasoning: The overall procedure of interaction space graph reasoning is shown in the green box in <ref type="figure" target="#fig_0">Figure 2</ref>-(a). As we described earlier, the spatial space graph reasoning can improve the connectivity between predicted road segments. We now consider projecting the input feature space into another latent space, called the interaction space I, where we try to delineate roads from other objects such as buildings, trees, vehicles, etc. Next, we build a graph that connects these features in the interaction space and performs a relational reasoning on the graph. After reasoning, the updated information is projected back to the original coordinate space. In what follows, we discuss these operations in detail. Projection to interaction space: The first step is to project the original feature map X to the interaction space I. This is done by the projection function f (?) such that the features V I ? R N ?S in the interaction space are more friendly for global reasoning over disjoint and distant regions, where N is the number of nodes and S is the number of states.</p><p>In practice, we first reduce the dimension of the input feature X with the transformation ? I (X) ? R L?N and formulate the projection function ? I (X) ? R L?S as a linear combination of input X such that the new features can aggregate information from multiple regions. Concretely, the input feature X is projected as V I in the interaction space I through the projection function ? I (X) as follows:</p><p>V I = ? I (X) T ? I (X).</p><p>(3) We implement both functions ? I (?) and ? I (?) as 1 ? 1 convolutional layer as shown in <ref type="figure" target="#fig_0">Figure 2</ref>-(a). Graph reasoning in interaction space: After projecting the input feature space into interaction space, we build a fully-connected graph in the interaction space with the node similarity matrix A I ? R N ?N . The similarity matrix A I is randomly initialized and learned during back propagation in contrast to the similarity matrix we defined for the spatial graph reasoning that is dependent on the input data. In addition, we use skip connection (i.e. identity matrix) that speeds up the optimization. Following Eq. (1), the graph convolution in the interaction space is formulated as:</p><formula xml:id="formula_3">Z I = AXW = ((I ? A I )V I )W I ,<label>(4)</label></formula><p>where W I is the trainable weight matrix. Here both matrices W I and A I are implemented as 1D convolution with kernel size of 1 as shown in <ref type="figure" target="#fig_0">Figure 2-(a)</ref>. Reverse projection to the original coordinate space: After graph reasoning in the interaction space, we project the output features Z I to the original coordinate as:</p><formula xml:id="formula_4">Y I = ? I (X) T Z I ,<label>(5)</label></formula><formula xml:id="formula_5">X I = ? I (Y I ).<label>(6)</label></formula><p>We use the same projection matrix ?(X) to transform features to Y I ? R L?S . Then we perform linear projection ? I (?) using a 1 ? 1 convolution layer to transform Y I into the original coordinate space. As a result we have the features X I with feature dimension C at the original coordinate space.</p><p>Once we have the spatial and interaction space graph reasoning outputs, we combine them with the original input feature map and then apply ReLU non-linearity to get the final graph reasoned feature map X IS . Mathematically, we can denote this as, X IS = ReLU(X S + X + X I ).</p><p>(7) 3) SPIN pyramid: We perform our SPIN graph reasoning at multiple scales to further increase the overall receptive field of the network and to improve long-range contextual information present in the intermediate feature maps. Concretely, we perform SPIN graph reasoning at different scales (1, 1/2, and 1/4) of the original feature map as shown in <ref type="figure" target="#fig_0">Figure 2-(b)</ref>. In the results and discussion section, we conduct an ablation study to demonstrate the effect of spatial, interaction, and SPIN graph reasoning on the segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network architecture</head><p>Feature extractor block: Operating the network at highresolution (i.e. 256 ? 256) requires a large GPU memory and computational power. Therefore, we employ a 7 ? 7 convolutional layer with stride 2, followed by a residual block and a max pooling layer to bring it down to the size of 64 ? 64. We then add two subsequent residual modules before sending it to the hourglass module. Bottleneck: Our proposed road segmentation network uses stack of two hourglass modules <ref type="bibr" target="#b30">[31]</ref> at the bottleneck. The hourglass module captures information at different scales by cascading series of residual modules and max pooling layers. When the network reaches the lowest resolution, it performs bilinear upsampling and combines features across the same scales using skip connections. We feed forward the output of the bottleneck to the segmentation branch. Segmentation branch: In the segmentation branch, we use a combination of convolution and transpose convolution layers to upsample the feature maps to the original scale. We then feed forward these feature maps to our proposed SPIN pyramid. To get the output segmentation map, we feed forward these graph reasoned feature maps from the SPIN pyramid to the final classification layer. Orientation learning: For orientation learning, we adopt the same orientation learning technique described in Batra et. al. <ref type="bibr" target="#b29">[30]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, our road segmentation network is divided into two branches after the two hourglass modules to support for both segmentation and orientation learning. The orientation learning task is formulated as a multi-class classification problem where, the orientation of each road-pixel is quantized into bins resulting in a total of 37 orientation classes. Please check the supplementary document for more details. Loss function: The proposed road segmentation network utilizes predictions from intermediate feature maps to compute the loss at multiple scales: ((H/4, W/4), (H/2, W/2) and (H, W )) instead of computing it only at the original scale. This improves network's ability to correctly predict road segments at multiple scales and helps to convergence faster. In this implementation, we make use of two loss functions: (1) segmentation loss, and (2) orientation loss. We use differentiable SoftIoU loss to compute the segmentation loss instead of using the BCE loss. The segmentation loss is computed at multiple scales. The road segmentation loss L seg is defined as follows,</p><formula xml:id="formula_6">L seg = s 1 ? SoftIoU(Y s pred , Y s gt ) ,<label>(8)</label></formula><p>where s denotes the scale having values {(H, W ), (H/2, W/2), (H/4, W/4)}, Y s pred and Y s gt are the predicted and ground-truth segmentation maps at scale s, respectively. Similarly, we calculate the orientation loss at multiple scales. The orientation loss L orient is defined as follows,</p><formula xml:id="formula_7">L orient = s 1 ? Nbins b=0 O s b log(? s b ) ,<label>(9)</label></formula><p>where N bins is the number of bins in the quantized orientation, O s b and? s b are the predicted and ground-truth orientation maps of orientation bin b and scale s, respectively. Finally, the overall loss function L is defined as follows,</p><formula xml:id="formula_8">L final = L seg + L orient .<label>(10)</label></formula><p>IV. EXPERIMENTAL SETTINGS A. Datasets Massachusetts road dataset: The Massachusetts Roads dataset <ref type="bibr" target="#b21">[22]</ref> consists of train, validation and test sets with 1108, 14 and 49 images, respectively, each with a size of 1, 500 ? 1, 500 pixels. Following <ref type="bibr" target="#b39">[40]</ref>, we fill the training images into size of 1536 ? 1536 and then we crop each image into 512 ? 512 patches with overlapping window of 256 pixels to make the training set. We observed that some parts of the images in the Massachusetts Road dataset are partially occluded and these images severely affect the performance of models. Hence, we removed these occluded images from the training set. Similarly, we crop each image in validation and test sets into 512 ? 512 patches without any overlapping window. After these series of operations, the processed Massachusetts Road dataset contains 21782, 124, and 433 images with size of 512 ? 512, corresponding to the train, validation and test set, respectively. DeepGlobe dataset: For the DeepGlobe dataset <ref type="bibr" target="#b20">[21]</ref>, we follow the same experimental and data preparation protocols mentioned in <ref type="bibr" target="#b29">[30]</ref>. The DeepGlobe dataset consists of 6226 images with resolution of 1024 ? 1024. Following <ref type="bibr" target="#b29">[30]</ref>, we create splits of 4696 images for training and 1530 images for testing. Then, we create the patches with 512 ? 512 resolution with an overlapping window of 256 pixels and this results in total of 42264 images for training. Similarly, for the testing dataset also we create patches with resolution of 512 ? 512 without any overlapping pixels and this results in total of 6116 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>We use random crops of resolution 256 ? 256 to train the network for the Massachusetts and DeepGlobe datasets. We use extensive data augmentation techniques such as image rotation, flipping, and mirroring. We use SGD optimizer with a batch size of 32, a momentum of 0.9 and a weight decaying of 0.0005. We use a step-learning rate scheduler with an initial learning rate of 0.01 where steps are scheduled at 50, 90, and 110. We reduce the learning rate by a factor of 0.1 at each step. We train the network for a total of 120 epochs. We implemented our model in PyTorch and used an NVIDIA Quadro RTX 8000 GPU for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance metrics</head><p>For the DeepGlobe dataset accurate road segmentation masks are available and hence, we evaluate the quality of our road predictions using accurate road Intersection over Union (IoU a ) and F 1 score. However, the groundtruth segmentation masks of Massachusetts road dataset have constant width and this will adversely affect the pure pixel based metrics. So, as proposed in <ref type="bibr" target="#b41">[42]</ref> we also use relaxed IoU (IoU r ) with buffer size of 4 in our evaluations. Furthermore, we use Average Path Length Similarity (APLS) metric to measure the difference between ground truth and proposal graphs <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we compare the road segmentation performance of our SPIN Road Mapper with existing methods, quantitatively and qualitatively. In particular, we compare the performance of our method with that of Seg-Net <ref type="bibr" target="#b40">[41]</ref>, U-Net <ref type="bibr" target="#b27">[28]</ref>, LinkNet34 <ref type="bibr" target="#b28">[29]</ref>, HourGlass <ref type="bibr" target="#b30">[31]</ref>, Stack-HourGlass <ref type="bibr" target="#b30">[31]</ref>, and Batra et al. <ref type="bibr" target="#b29">[30]</ref>. Quantitative results: The quantitative results are summarized in <ref type="table" target="#tab_0">Table I</ref>. As can be seen from <ref type="table" target="#tab_0">Table I</ref>, the proposed SPIN Road Mapper achieves the state-of-the-art (SOTA) results in terms of all the performance measures for the Massachusetts dataset. When considering the DeepGlobe dataset, our method achieves the SOTA results in terms of F1, IoU r , and APLS. Furthermore, the improvement in terms of APLS metric is significant (+1.15% and +1.02% for Massachusetts and DeepGlobe datasets, respectively) and which confirms that the proposed SPIN module improves the connectivity of road segments by specially bringing gaps for occluded areas (see <ref type="figure" target="#fig_2">Fig. 5</ref>). These qualitative results confirms that the proposed SPIN module effectively captures the longrange dependencies of feature maps, and thereby helps final classifier to identify road pixels substantially well compared to the available ConvNet architectures for road segmentation. Qualitative results: For the qualitative analysis, we visualize the predicted road maps from SegNet <ref type="bibr" target="#b40">[41]</ref>, LinkNet <ref type="bibr" target="#b28">[29]</ref>, Stack-HourGlass <ref type="bibr" target="#b20">[21]</ref>, Batra et. al <ref type="bibr" target="#b29">[30]</ref>, and our SPIN Road Mapper on the Massachusetts Road dataset in <ref type="figure">Figure 4</ref>. The red boxes in <ref type="figure">Figure 4</ref> highlight the regions where our method performs better than the baseline methods. For example, consider the last row of <ref type="figure">Figure 4</ref>. Roads in the region highlighted by the red box are mostly covered by trees and buildings (as can be seen from the input aerial image), making it difficult for the baseline segmentation networks to correctly identify the presence of roads. In contrast, our method is able to predict most of the road segments due to its ability to capture long-range dependencies between road pixels through spatial graph reasoning, as well as its ability to delineate roads from surrounding structures through interaction space graph reasoning. Ablation study: We conduct an ablation study to demonstrate the effect of spatial, interaction and SPIN graph reasoning on road segmentation. It can be seen from <ref type="table" target="#tab_0">Table II</ref> that integrating spatial and interaction space graph reasoning to the ConvNet-based network results in increase road segmentation accuracy. Combining the spatial and interaction space graph reasoning together in SPIN pyramid results in further improvement over the individual components. In addition to the quantitative comparison, we also present a qualitative comparison in <ref type="figure" target="#fig_2">Figure 5</ref> which clearly demonstrates how each graph reasoning technique improves the quality of road predictions. These experiments show that out proposed SPIN pyramid helps the network learn features with more global contextual information resulting in an improved performance. Convergence: <ref type="figure" target="#fig_3">Figure 6</ref> shows the training convergence plot of the proposed network with and without the SPIN module.   We can observe that adding the SPIN module helps achieve faster convergence. This leads to reduction in training time which is crucial for training ConvNets on large-scale highresolution remote sensing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented a Spatial and Interaction Space Graph Reasoning (SPIN) module that can be plugged into ConvNets to learn distant relationships between road segments in the feature space. Learning global dependencies are essential while extracting complex road topology from aerial images where most of the road segments are partially or completely occluded by trees, buildings, or clouds. The graph reasoning over the spatial space helps the network to extract more dependencies between different spatial regions and other contextual information whereas graph reasoning over a projected interaction space helps to delineate roads from surrounding objects. We conduct extensive experiments and compare the predicted road maps qualitatively and quantitatively with existing methods. We observe that our SPIN module helps convolutional networks to extract long-range dependencies and thereby improve the segmentation quality. SPIN is computationally light and also helps in faster convergence which are crucial while training ConvNets on large-scale high-resolutions datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our proposed method. (a) We perform graph reasoning in both spatial and interaction space. (b) The proposed SPIN pyramid module which performs SPIN graph reasoning at multiple scales (1, 1/2, and 1/4) of original feature map to extract multi-scale long-range contextual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed network for road segmentation from aerial images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>A qualitative comparison for the ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The convergence characteristic for with and without the SPIN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I A</head><label>I</label><figDesc>QUANTITATIVE COMPARISON OF OUR SPIN ROAD MAPPER WITH THE SOTA BASELINES IN TERMS OF F1 SCORE, IoU r AND IoU a . IoU a APLS Precision Recall F1 IoU r IoU a APLS Seg-Net [41] 77.34 79.84 78.57 64.71 58.59 57.76 69.48 72.97 71.19 55.26 49.20 58.55 U-Net [28] 82.46 84.34 83.39 71.51 60.97 61.33 73.55 74.98 74.26 59.06 55.02 61.23 LinkNet [29] 83.25 84.63 83.93 72.32 63.12 66.62 78.34 78.85 78.59 64.73 62.75 67.41</figDesc><table><row><cell>Method</cell><cell></cell><cell>Massachusetts Road Dataset [22]</cell><cell></cell><cell>DeepGlobe Dataset [21]</cell></row><row><cell cols="3">Precision Recall F1 IoU r HourGlass [31] 81.26 81.86 81.56 68.86 61.37 65.37</cell><cell>79.43</cell><cell>80.14 79.78 66.34 60.71 65.33</cell></row><row><cell>Stack-HourGlass [31]</cell><cell>80.12</cell><cell>83.87 81.96 69.43 62.21 67.89</cell><cell>79.33</cell><cell>79.99 79.66 66.19 62.06 69.02</cell></row><row><cell>Batra et al. [30]</cell><cell>83.34</cell><cell>84.61 83.97 72.37 64.44 71.34</cell><cell>83.79</cell><cell>84.14 83.97 72.37 67.21 73.12</cell></row><row><cell>SPIN Road Mapper (ours)</cell><cell>83.90</cell><cell>85.06 84.47 73.12 65.24 72.49</cell><cell>84.14</cell><cell>84.50 84.32 72.89 67.02 74.14</cell></row><row><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Fig. 4. A qualitative comparison between our SPIN Road Mapper and the</cell><cell></cell><cell></cell></row><row><cell>SOTA methods.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>RESULTS OF ABLATION STUDY.</figDesc><table><row><cell>Method</cell><cell>IoU a</cell><cell>F1</cell><cell>APLS</cell></row><row><cell>ConvNet Only</cell><cell>83.97</cell><cell>66.58</cell><cell>73.01</cell></row><row><cell>ConvNet + Spatial GR</cell><cell>84.13</cell><cell>66.82</cell><cell>73.59</cell></row><row><cell>ConvNet + Interaction GR</cell><cell>84.12</cell><cell>66.76</cell><cell>73.52</cell></row><row><cell>ConvNet + SPIN GR</cell><cell>84.32</cell><cell>67.02</cell><cell>74.14</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to detect roads in highresolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeproadmapper: Extracting road topology from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>M?ttyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">icurb: Imitation learning-based detection of road curbs using aerial images for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1097" to="1104" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:2103.17119</idno>
		<title level="m">Topo-boundary: A benchmark dataset on topological roadboundary detection using aerial images for autonomous driving</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic finding of main roads in aerial images by using geometric-stochastic models and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barzohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="707" to="721" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A probabilistic distribution approach for the classification of urban roads in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Victorino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recovering line-networks in images by junction-point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Forstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lafarge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1894" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic extraction of roads from aerial images based on scale space and snakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Road network extraction and intersection detection from aerial images by tracking road footprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Femiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4144" to="4157" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic extraction of urban road networks from multi-view aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting fully convolutional neural networks for fast road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C T</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fr?mont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3174" to="3179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Aerial image geolocalization from recognition and matching of roads and intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08323</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Roadtracer: Automatic extraction of road networks from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4720" to="4728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond the pixel-wise loss for topology-aware delineation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozi?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepglobe 2018: A challenge to parse the earth through satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Machine learning for aerial image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto (Canada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Road network detection using probabilistic and graph theoretical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Unsalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sirmacek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4441" to="4453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A gibbs point process for road extraction from remotely sensed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of road extraction from remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eklund</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2095756416301076" />
	</analytic>
	<monogr>
		<title level="j">Journal of Traffic and Transportation Engineering (English Edition)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="282" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A higherorder crf model for road network extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1698" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning approaches applied to remote sensing datasets for road extraction: A state-of-the-art review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1444</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linknet: Exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved road connectivity by joint learning of orientation and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Computing receptive fields of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<ptr target="https://distill.pub/2019/computing-receptive-fields" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial pyramid based graph reasoning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8950" to="8959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9245" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1858" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Global aggregation then local distribution in fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07229</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiscale road extraction in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wulamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International conference on machine learning</title>
		<meeting>the 29th International conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Spacenet road detection and routing challenge-part i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Etten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
