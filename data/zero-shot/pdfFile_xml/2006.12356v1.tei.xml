<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Sparse Detection Networks for 3D Single-shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
							<email>jgwak@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Sparse Detection Networks for 3D Single-shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Single shot detection</term>
					<term>3D object detection</term>
					<term>generative sparse network</term>
					<term>point cloud</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D object detection has been widely studied due to its potential applicability to many promising areas such as robotics and augmented reality. Yet, the sparse nature of the 3D data poses unique challenges to this task. Most notably, the observable surface of the 3D point clouds is disjoint from the center of the instance to ground the bounding box prediction on. To this end, we propose Generative Sparse Detection Network (GSDN), a fully-convolutional single-shot sparse detection network that efficiently generates the support for object proposals. The key component of our model is a generative sparse tensor decoder, which uses a series of transposed convolutions and pruning layers to expand the support of sparse tensors while discarding unlikely object centers to maintain minimal runtime and memory footprint. GSDN can process unprecedentedly large-scale inputs with a single fully-convolutional feed-forward pass, thus does not require the heuristic post-processing stage that stitches results from sliding windows as other previous methods have. We validate our approach on three 3D indoor datasets including the large-scale 3D indoor reconstruction dataset <ref type="bibr" target="#b4">[5]</ref> where our method outperforms the state-of-the-art methods by a relative improvement of 7.14% while being 3.78 times faster than the best prior work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D reconstructions have become more commonplace as a complete reconstruction pipeline become built into consumer devices, such as mobile phones or headmounted displays, for applications in robotics and augmented reality. Among these applications, perceptions on 3D reconstructions is the first step allowing users to interact with a virtual world in 3D. For example, indoor navigation applications can aid a user to localize objects, and mixed reality applications need to track objects to give users information relevant to the current status of their surroundings. Many of these virtual-reality and mixed-reality applications require identifying and detecting 3D objects in real-time.</p><p>However, unlike 2D images where the input is in a densely packed array, 3D data is scanned or reconstructed as a set of points or a triangular mesh. These arXiv: <ref type="bibr">2006</ref>  <ref type="figure">Fig. 1</ref>: The top-down view of the cross-section of our simplified 3D sparse anchor generation pipeline: a 3D scanner samples the surface of an object which we convert to a sparse tensor. Then, an encoder extracts hierarchical sparse tensor features with a series of convolutions. During the decoder stage, we apply a transposed convolution to upsample and expand the support of the sparse tensor.</p><p>Finally, we prune out unnecessary supports that do not contain anchors and make bounding box anchor predictions.</p><p>data occupy a small portion of the 3D space and pose unique challenges for 3D object detection. First, the space of interest is three dimensional which requires cubic complexity to save or process data. Second, the data of interest is very sparse, and all information is sampled from the surface of objects. Many previous 3D object detectors proposed various methods to process cubically growing sparse 3D data, and can be categorized into one of two branches: 3D object detection by converting sparse 3D data into a dense representation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref> or by directly feeding a set of points into multi-layer perceptrons <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. First, dense 3D representation for indoor object detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13]</ref> uses volumetric features which have memory and computational complexity of O(N 3 ) where N is the resolution of the space. This representation requires large memory, which prevents the utilization of deep networks and requires cropping the scenes and stitching the results to process large or high-resolution scenes. Second, multi-layer perceptrons that process a scene as a set of points limit the number of points a network can process. Thus, as the size of the point cloud increases, the method suffers from either low-resolution input which makes it difficult to scale the method up for larger scenes (see Section 5.2) or apply sliding-window style cropping and stitching which prevents the network to see a larger context <ref type="bibr" target="#b35">[36]</ref>.</p><p>We instead propose to resolve the cubic complexity with our hierarchical sparse tensor encoder, adopting a sparse tensor network to efficiently process a large scene fully-convolutionally. As we use a sparse representation, our network is fast and memory-efficient compared with a single-shot method that uses dense tensors <ref type="bibr" target="#b12">[13]</ref>. It allows our network to adopt extremely deep architectures while requiring a fraction of the memory and computation. Also, compared with multilayer perceptrons, our method scales to large scenes without sacrificing point density or the receptive field size of a network by cropping a scene into smaller windows <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Another key challenge of a 3D object detector is that the support of the input 3D scans and the support of the object bounding box anchors are disjoint. In other words, we have samples of 3D points on the surface of the objects, but Detection results on the entire S3DIS building 5: Our proposed method can process 78M points, 13984m 3 , 53 room building as a whole in a single fullyconvolutional feed-forward pass, only using 5G of GPU memory. Left: bird-eye-view of the entire building 5, Right: partial view of the same building. not on the center of the object where a bounding box anchor is located. This is due to the fact that many objects are convex and we cannot directly observe the object center. For this, we propose a generative sparse tensor decoder that repeatedly upsamples the support of input to expand and cover the support of anchors while discarding unlikely object centers to maintain minimal runtime and memory footprint ( <ref type="figure">Fig. 1)</ref>.</p><p>To sum, we propose Generative Sparse Detector Network (GSDN), a deep fully-convolutional single-shot 3D object detection algorithm with a sparse tensor network. Our single-shot 3D object detection network consists of two components: an hierarchical sparse tensor encoder which efficiently extracts deep hierarchical features, and a generative sparse tensor decoder which expands the support of the sparse input to ground object proposals on. Experimentally, GSDN outperforms the state-of-the-art methods on two large-scale indoor datasets while being faster than the best prior work. We also analyze the speed and memory footprint of the model and demonstrate the extreme scalability of our method on orders of magnitudes larger 3D scenes ( <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review a few branches that are related to our work: 3D indoor object detection, 3D generative networks, and sparse tensor networks. 3D Indoor Object Detection. In a 3D indoor setting or 3D indoor datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>, the distribution of object placement creates unique challenges: objects such as lamps and ceiling lights can be placed on a wall or a ceiling, or objects can be placed on top of another object such as a desk or a bed. However, such challenging setup does not exist in outdoor datasets and most 3D outdoor object detectors simply project the 3D problem into a 2D ground plane <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Thus, in this section, we cover 3D indoor object detection specifically. The indoor 3D object detection using neural networks can be classified into one of the following categories: sliding-window with classification, clustering-based methods, bounding-box proposal, or combinations of the above methods. First, the sliding window with classification extracts a 3D patch for object classification which is used as a simple object detector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Second, clustering-based methods learn features or vectors in a metric space where clustering results in instance segmentation. Lahoud et al. <ref type="bibr" target="#b13">[14]</ref> uses metric learning to train the feature space. Liu et al. <ref type="bibr" target="#b16">[17]</ref>, Yi et al. <ref type="bibr" target="#b36">[37]</ref>, Wang et al. <ref type="bibr" target="#b32">[33]</ref>, and Qi et al. <ref type="bibr" target="#b23">[24]</ref> predict object centers per 3D point and cluster the center votes.</p><p>Third, the bounding box proposal methods adopt 2D rectangular bounding box proposal methods to 3D. Wang et al. <ref type="bibr" target="#b31">[32]</ref> proposed Vote3D, which predicts 3D bounding boxes on a sparse grid for object detection. Yang et al. <ref type="bibr" target="#b35">[36]</ref> directly predicts bounding boxes from MLP of global point cloud features. Hou et al. <ref type="bibr" target="#b12">[13]</ref> makes a straight-forward 3D extension of region proposal networks on dense voxels. GSDN is a bounding box proposal method with a crucial difference in maintaining the sparsity of the input point cloud and target anchor space, enabling much faster inference on many orders of magnitude larger scene with better performance than state-of-the-art methods. 3D Generative Networks. Generating 3D shapes from a neural network can be classified into two broad categories: continuous 3D point representations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31]</ref> and discrete grid representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>. Specifically, within the discrete representations, some use sparse representations for 3D reconstruction which allow a high-resolution voxel or signed-distance-function (SDF) reconstruction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Unlike previous works that focus on the shapes of objects, we use the generative process to predict the bounding box anchors. Also, compared with some sparse generative processes that subdivide voxels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6]</ref>, our method extends the support with transposed convolutions to cover bounding box anchors which are located behind 3D surface observations. Sparse Tensor Networks. A conventional neural network processes a dense tensor such as temporal data, images, or videos using a series of linear operations and non-linear operations. Most of the linear operations also use dense tensors for parametrization. Recently, using a sparse parametrization to compress a neural network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> has been widely studied for mobile and embedded systems. However, using a sparse tensor as an input has only gained popularity after its success on 3D data processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Note that these networks are different from the compressed models using parameter pruning whose weights are sparse matrices but all feature maps are dense tensors; whereas the sparse tensor networks take spatially sparse tensors as inputs and generate spatially sparse feature maps. We adopt these spatially sparse networks, or sparse tensor networks to scale detection networks to an unprecedented depth and to handle extremely large scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this section, we briefly go over the basic 3D representation, a sparse tensor, and introduce basic operations that are critical for the generative sparse tensor network. Throughout the paper, we will use lowercase letters for variable scalars, t; uppercase letters for constants, N ; lowercase bold letters for vectors, v; uppercase bold letters for matrices, R; Euler scripts for tensors, T ; and calligraphic symbols for sets, C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Tensor</head><p>A tensor is a multi-dimensional array that can represent high-dimensional data. A sparse tensor of order-D, T ? R N1?N2?...?N D , is a D-dimensional array where majority of its elements are 0. Adopting the conventional sparse matrix representation, a sparse matrix can be represented as a set of non-zero coordinates C = supp(T ) where supp is the support operator, and corresponding features F.</p><formula xml:id="formula_0">T [x 1 i , x 2 i , ? ? ? , x D i ] = f i if (x 1 i , x 2 i , ? ? ? , x D i ) ? C 0 otherwise (1)</formula><p>where x i d denotes d-th axis coordinate of the i-th non-zero element and f i is the feature associated to the i-th non-zero element. These non-zero elements contain information that are equivalent to a sparse tensor T ? (C, F). These sets can also be converted to matrices C, F in a COO representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Tensor for 3D Data Representation</head><p>The 3D data of interest in this work uses point clouds or meshes to represent 3D surfaces. We can represent a mesh or a point cloud as a sparse tensor by discretizing the coordinates of vertices or points. This process requires defining the discretization step size (voxel size) which is a hyperparameter that affects the performance of a neural network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Sparse Detection Networks</head><p>In this section, we propose the generative sparse detection networks for 3D object detection. Unlike the 2D object detection networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, we use a sparse tensor as the 3D representation throughout the network including the intermediate features. Thus, all layers such as convolution and batch normalization are well defined for sparse tensors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref>. Throughout the paper, we will implicitly refer to all tensors as sparse tensors and layers as sparse tensor counterparts.</p><p>The network consists mainly of two parts: a hierarchical sparse tensor encoder and a generative sparse tensor decoder. The first part of the network generates sparse tensor feature maps that can sufficiently capture geometry and identity of objects and the second part proposes new supports based on the feature maps.  The second stage upsamples the sparse tensor feature maps using transposed convolution and pruning (Sec. 4.2). Note that all feature maps are sparse tensors and all layers process sparse tensors fully-convolutionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchical Sparse Tensor Encoder</head><p>We use residual networks <ref type="bibr" target="#b11">[12]</ref>, specifically high-dimensional variants proposed in Choy et al. <ref type="bibr" target="#b1">[2]</ref>, as the backbone of our model. Note that the backbone network can be replaced with more modern and recent variants. The network consists of residual blocks and strided convolutions that reduce the resolution of the space and increase the receptive field size exponentially. First, the network takes a highresolution sparse tensor as an input T 0 and generate hierarchical feature maps T l with a series of downsampling and residual blocks f l (?; W l ) for l ? [1, ..., L]. The encoder can be represented succinctly as</p><formula xml:id="formula_1">T l ? f l (T l?1 ; W l ) for l ? [1, ..., L]</formula><p>We cache all of the hierarchical sparse tensor feature maps T l for l ? [1, ..., L] which will be fed into the generative sparse tensor decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generative Sparse Tensor Decoder</head><p>The second half of the network expands the support of the hierarchical sparse tensors feature maps T l to cover the support for bounding box anchors. We approximate this process with transposed convolutions (also known as upconvolution, deconvolution). Given an input sparse tensor T , we create an output sparse tensor T that supp(T ) ? supp(T ). However, not all voxels generated from this process contain object bounding box anchors and can be removed to limit the memory and computation cost. This process is the sparsity pruning and we repeatedly apply a transposed convolution followed by sparsity pruning to increase the resolution of the space while limiting the memory and computation cost of a sparse tensor. During this process, we make skip connections between the hierarchical sparse tensor feature maps and the upsampled sparse tensors to recover the fine details of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Transposed Convolution and Sparsity Pruning</head><p>We use transposed convolutions with the kernel size greater than 2 to not just upsample, but expand the support of a sparse tensor. This process affects the sparsity pattern of a sparse tensor and the support of the output sparse tensor is the stencil or outer-product of the convolution kernel shape on the input sparsity pattern supp(T ) = C ? [?K, ..., K] 3 . Mathematically, a transposed convolution on a 3D sparse tensor T with supp(T ) = C can be defined as follows:</p><formula xml:id="formula_2">T [x, y, z] = i,j,k?N (x,y,z) W[x ? i, y ? j, z ? k]T [i, j, k] for (x, y, z) ? C (2) where C = C?[?K, ..., K] 3 , N (x, y, z) = {(i, j, k)||x?i| ? K, |y?j| ? K, |z?k| &lt; K, (i, j, k) ? C},</formula><p>W is the 3D convolution kernel weights and 2K + 1 is the convolution kernel size. This results in denser sparsity pattern on the output tensor T with supp(T ) = C ? [?K, ..., K] 3 . Note that unlike the subdivision, the transposed convolution expands a sparse point into an arbitrarily large dense region and multiple regions could overlap with each other <ref type="figure" target="#fig_2">(Fig. 4)</ref>. After a transposed convolution, not all the newly created coordinates contain object bounding box anchors. Thus, we remove some of these voxels that have a small probability of containing bounding box anchors. We denote a function that returns the probability given features at each voxel as P s (?) and remove all voxels P s (?) &lt; ? , where ? is the sparsity pruning confidence threshold.</p><formula xml:id="formula_3">p = P s (T ; W P )<label>(3)</label></formula><formula xml:id="formula_4">T = SparsityPruning(T , p &lt; ? ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Skip Connection and Sparse</head><p>Tensor Addition The upsampled sparse tensor feature maps from the generative process have gone through extreme spatial compression that allows neurons to see larger context, but have lost spatial resolution. To recover the fine details of the input, we create the skip connections to the cached feature map from the encoder <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Since both the upsampled feature map and the lower layer feature map are all sparse tensors, we use sparse tensor addition. This process also expands the support to be the union of the supports of both sparse tensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-scale Bounding Box Anchor Prediction</head><p>Every voxel after the sparsity pruning potentially contains bounding box anchors. Therefore, we make a direct prediction of the bounding box parameters for every layer of the pruned sparse tensors. Specifically, for each k anchor box, the network predicts 1 object anchor likelihood score, 6 offsets relative to the anchor box, and c semantic class scores. This results in (c + 7)k outputs per voxel.</p><p>To capture as many shape variations, we use bounding box anchors with different aspect ratios. Specifically, for each anchor ratio seed a r , we use all unique permutations of ? a r , ? a r , 1 ? ar as the aspect ratios of an anchor. In total, we use k = 13 anchors with a r ? {1, 2, 4, 1 2 , 1 4 } including the identity ratio. However, even with these various anchor ratios, it is difficult to capture the extreme scale variation among 3D objects. Thus, we predict anchors at various stages of the decoder to capture the scale variation of 3D objects similar to Liu et al. <ref type="bibr" target="#b17">[18]</ref>. We construct the anchors at each level to double the size of the anchors at the previous level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary of GSDN Feed Forward</head><p>We summarize the feed forward pass of the generative sparse detection networks in Alg. 1. The algorithm generates L levels of hierarchical sparse tensor feature maps from the previous level feature maps on Line 3. Then, during the generative phase, we extract anchors and associated bounding box information (Line 8), predict sparsity and prune out voxels (Line 10), and apply transposed convolution (Line 12). We add the upsampled sparse tensor to the corresponding sparse tensor feature map from the encoder (Line 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Losses</head><p>The generative sparse detection network has to predict four types of outputs: sparsity prediction, anchor prediction, semantic class, and bounding box regression. First, the sparsity and anchor prediction are binary classification problems. However, the majority of the predictions are negative as many voxels does not contain positive anchors. Thus, we propose balanced cross entropy loss:</p><formula xml:id="formula_5">L b (?, y) = ? 1 2|P| i?P log(P (? i )) ? 1 2|N | i?N log(1 ? P (? i ))</formula><p>where P = {i|y i = 1} and N = {i|y i = 0} are the set of indices with positive and negative labels respectively. We define an anchor to be positive if any of the anchors in a voxel overlaps with any ground-truth bounding boxes for 3D IoU &gt; 0.35 and negative if 3D IoU &lt; 0.2. As the sparsity prediction must contain all anchors in subsequent levels, we define a sparsity to be positive if any of the subsequent positive anchor associated to the current voxel is positive. We do not enforce loss on anchors that have 0.2 &lt;3D IoU &lt; 0.35. Finally, for positive anchors, we train semantic class prediction of the highest overlapping ground-truth bounding box class with the standard cross entropy, L class , and bounding box center and size regression parameterized by difference of the center location relative to the size of the anchor and the log difference of the size of the bounding box with the Huber loss <ref type="bibr" target="#b26">[27]</ref>, L reg . The final loss is the weighted sum of all losses:</p><formula xml:id="formula_6">L = ? s L s + ? anc L anc + ? class L class + ? reg L reg</formula><p>where we use ? s = 1, ? anc = 1, ? class = 1, ? reg = 0.1 for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Prediction post-processing</head><p>We train the network to overestimate the number of bounding box anchors as we label all anchors with 3D IoU &gt;0.35 as positives. We filter out overlapping predictions with non-maximum suppression and merge them by computing scoreweighted average of all removed bounding boxes to fine tune the final predictions similar to Redmon et al. <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our method on three 3D indoor datasets and compare with state-ofthe-art object detection methods (5.1). We also make a detailed analysis of the   speed and memory footprint of our method (5.2). Finally, we demonstrate the scalability of our proposed method on extremely large scenes (5.3). Datasets. We evaluate our method on the ScanNet dataset <ref type="bibr" target="#b4">[5]</ref>, annotated 3D reconstructions of 1500 indoor scenes with instance labels of 18 semantic classes. We follow the experiment protocol of Qi et al. <ref type="bibr" target="#b23">[24]</ref> to define axis-aligned bounding boxes that encloses all points of an instance without any margin as the ground truth bounding boxes. The second dataset is the Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset <ref type="bibr" target="#b0">[1]</ref>. It contains 3D scans of 6 buildings with 272 rooms, each with instance and semantic labels of 7 structural elements such as floor and ceiling, and five furniture classes. We train and evaluate our method on the official furniture split and use the most-widely used Area 5 for our test split. We follow the same procedure as above to generate ground-truth bounding boxes from instance labels.</p><p>Finally, we demonstrate the scalability of GSDN on the Gibson environment <ref type="bibr" target="#b34">[35]</ref> as it contains high-quality reconstructions of 575 multi-story buildings. Metrics. We adopt the average precision (AP) and class-wise mean AP (mAP) to evaluate the performance of object detectors following the widely used convention of 2D object detection. We consider a detection as a positive match when a 3D intersection-over-union(IoU) between the prediction and the ground-truth bounding box is above a certain threshold. Training hyper-parameters. We train our models using SGD optimizer with exponential decay of learning rate from 0.1 to 1e-3 for 120k iterations with the batch size 16. As our model can process an entire scene fully-convolutionally, we do not make smaller crops of a scene. We use high-dimensional ResNet34 <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Object detection performance analysis</head><p>We compare the object detection performance of our proposed method with the previous state-of-the-art methods on <ref type="table" target="#tab_2">Table 1 and Table 2</ref>. Our method, despite being a single-shot detector, outperforms all two-stage baselines with 4.2% mAP@0.25 and 1.3% mAP@0.5 performance gain and outperforms the state-of-the-art on the majority of semantic classes. We also report the S3DIS detection results on <ref type="table" target="#tab_7">Table 3</ref>. It is also worth noting that Yang et al. <ref type="bibr" target="#b35">[36]</ref> and some preceding works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> crop a scene into multiple 1m?1m floor areas, and merge them with heuristics <ref type="bibr" target="#b32">[33]</ref>, which not only heavily restricts the receptive field but also require slow pre-processing and post-processing. Our method in contrast takes the whole scene as an input.</p><p>We plot class-wise precision-recall curves of ScanNet validation set on <ref type="figure">Figure 5</ref>. We found that some of the PR curves drop sharply, which indicates that the simple aspect-ratio anchors have a low recall.</p><p>Finally, we visualize qualitative results of our method on <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref>. In general, we found that our method suffers from detecting thin structures such as bookcase and board, which may be resolved by adding more extreme-shaped anchors. Please refer to the supplementary materials for the class-wise breakdown of mAP@0.5 on the ScanNet dataset and class-wise precision-recall curves for the S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.T.</head><p>Hou et al. <ref type="bibr" target="#b12">[13]</ref> Qi et al. <ref type="bibr" target="#b23">[24]</ref> Ours <ref type="figure">Fig. 6</ref>: Qualitative object detection results on the ScanNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Speed and Memory Analysis</head><p>We analyze the memory footprint and runtime in <ref type="figure">Figure 8</ref> and <ref type="figure" target="#fig_4">Figure 9</ref>. For the memory analysis, we compare our method with the dense object detector <ref type="bibr" target="#b12">[13]</ref> and measured the peak memory usage on ScanNetV2 validation set. As expected, our proposed network maintains extremely low memory consumption regardless of the depth of the network while that of the dense counterparts grows noticeably.</p><p>For runtime analysis, we compare the network feed forward and post-processing time of our method with Qi et al. <ref type="bibr" target="#b23">[24]</ref> in <ref type="figure" target="#fig_4">Figure 9</ref>. On average, our method takes 0.12 seconds while Qi et al. <ref type="bibr" target="#b23">[24]</ref> takes 0.45 seconds to process a scene of ScanNetV2 validation set. Moreover, the runtime of our method grows linearly to the number of points and sublinearly to the floor area of the point cloud, due to the sparsity of our point representation. Note that Qi et al.  <ref type="bibr" target="#b23">[24]</ref> changes significantly as the point cloud gets larger. However, our method maintains the constant density as shown in <ref type="figure">Figure 8</ref>, which allows our method to scale to extremely large scenes as shown in Section 5.3. In sum, we achieve 3.78? speed up and 4.2% mAP@0.25 performance gain compared to Qi et al. <ref type="bibr" target="#b23">[24]</ref> while maintaining the same point density from small to large scenes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scalability and generalization of GSDN on extremely large inputs</head><p>We qualitatively demonstrate the scalability and generalization ability of our method on large scenes from the S3DIS dataset <ref type="bibr" target="#b0">[1]</ref> and the Gibson environment <ref type="bibr" target="#b34">[35]</ref>. First, we process the entire building 5 of S3DIS which consists of 78M points, 13984m 3 volume, and 53 rooms. GSDN takes 20 seconds for a single feed-forward of the entire scene including data pre-processing and post-processing. The model uses 5G GPU memory to detect 573 instances of 3D objects, which we visualized on <ref type="figure" target="#fig_0">Figure 2</ref>. Similarly, we train our network on ScanNet dataset <ref type="bibr" target="#b4">[5]</ref> which only contain single-floor 3D scans. However, we tested the network on multi-story buildings. On <ref type="figure" target="#fig_5">Figure 10</ref>, we visualize our detection results on the scene named Uvalda from Gibson, which is a 3-story building with 173m 2 floor area. Note that our fully-convolutional network, which was only trained on single-story 3D scans, generalizes to multi-story buildings without any ad-hoc pre-processing or postprocessing. GSDN takes 2.2 seconds to process the building from the raw point cloud and takes up 1.8G GPU memory to detect 129 instances of 3D objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present the Generative Sparse Detection Network (GSDN) for single-shot fully-convolutional 3D object detection. GSDN maintains sparsity throughout the network by generating object centers using the proposed generative sparse tensor decoder. GSDN can efficiently process large-scale point clouds without cropping the scene into smaller windows to take advantage of the full receptive field. Thus, GSDN outperforms the previous state-of-the-art method by 4.2 mAP@0.25 while being 3.78? faster. In the follow-up work, we will examine and adopt various image detection techniques to boost the accuracy of GSDN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1 Controlled experiments and analysis</head><p>In this section, we perform a detailed analysis of GSDN through various controlled experiments. For all experiments, we use the same network architecture, and train and validate the model on the ScanNet dataset <ref type="bibr" target="#b4">[5]</ref>. We use the same hyperparameters for all experiments except for one control variable and train all networks for 60k iterations. Note that the performance of the networks trained for 60k iterations is lower than that of networks trained for 120k iterations reported on the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1.1 Balanced cross entropy loss</head><p>One of the main challenges we face during training GSDN is the heavy class imbalance of the sparsity and anchor labels. Such class imbalance is prevalent in object detection and we adopt the balanced cross entropy, one of the well-studied techniques that mitigate various problems associated with class imbalance, for sparsity and anchor prediction. In this section, we demonstrate the effectiveness of the balanced cross entropy loss by comparing it with the network trained with the regular cross entropy loss for sparsity and anchor prediction. We present the object detection result on the ScanNet validation set in <ref type="table">Table 4</ref>. The balanced cross entropy loss improves the performance of our network, especially the sparsity prediction. This is due to the nature of our generative sparse tensor decoder which adds cubically growing coordinates from all surface voxels, most of which need to be pruned except for a few points that contain target anchor boxes.  <ref type="table">Table 4</ref>: The effectiveness of balanced cross entropy (BCE) and cross entropy (CE) losses for generative sparse object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1.2 Sparsity pruning confidence</head><p>Our proposed generative sparse detection network predicts more proposals as we lower the sparsity pruning confidence ? and we found that the threshold ? has a significant impact on the performance. We analyze the effect of the pruning confidence on average recall, mAP@0.25, and decoder runtime on the ScanNet dataset in <ref type="figure" target="#fig_6">Figure 11</ref>. In this experiment, we train three models with ? = {0.1, 0.3, 0.5} and test them on ? = {0.1, 0.2, . . . , 0.9}. For the pruning confidences that do not have corresponding networks, we select the model trained with the closest pruning confidence. The general trend of <ref type="figure" target="#fig_6">Figure 11</ref> is that smaller pruning confidences ? perform better. Lower pruning confidences lead to more proposals and higher average recall. Also, note that the average precision follows the similar trend, which indicates that the performance is mostly capped by the recall, as shown in the precision/recall curve in the main paper. Lastly, the decoder runs marginally faster as the pruning confidence increases, since it generates fewer proposals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1.3 Encoder backbone models</head><p>We vary the sparse tensor encoder and analyze its impact on performance in <ref type="table">Table 5</ref>. As expected, GSDN with deeper encoder performs better. Additionally, we plot the detailed runtime break-down of each component of our proposed model with varying backbones in <ref type="figure" target="#fig_0">Figure 12</ref>. Overall, the decoder and post-processing time stay almost constant while the encoder dominates the runtime.  <ref type="table">Table 5</ref>: Analysis of the impact of different backbone models on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.1.4 Anchor ratios</head><p>We examine the impact of anchor ratios on the performance of our model in  marginally as we use more anchors. However, the improvement of mAP@0.5 with more anchors is significant. mAP@0.5 considers a prediction box with intersection-over-union greater than 0.5 with the corresponding ground-truth box to be positives. In other words, it requires approximately 80% overlap between a prediction and the ground-truth box for each of the three axes for the prediction to be positive. Thus, more anchors allow the network to capture various ground truth boxes more accurately and mAP@0.5 improves significantly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2 Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2.1 Experiments on the ScanNet dataset</head><p>In <ref type="table" target="#tab_14">Table 7</ref>, we report class-wise mAP@0.5 result on the ScanNet v2 validation set. Our method outperforms two-state object detector, Qi et al. <ref type="bibr" target="#b23">[24]</ref>, despite being a single-shot object detector. In <ref type="figure" target="#fig_1">Figure 13</ref> and <ref type="figure" target="#fig_2">Figure 14</ref>, we compare qualitative results on the ScanNet V2 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2.2 Stanford Large-Scale 3D Indoor Spaces Dataset</head><p>We visualize the precision/recall curve of our object detection result on the S3DIS dataset in <ref type="figure">Figure 16</ref>. We observe that certain classes with extreme bounding box cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP   ratios such as board and bookcase tend to underperform and have a very low recall. In <ref type="figure">Figure 15</ref>, we visualize additional qualitative results of our method on the S3DIS building 5.  <ref type="figure">Fig. 16</ref>: Per-class object detection precision/recall curves of GSDN on the building 5 of the S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.2.3 Gibson environment</head><p>We demonstrate the scalability and generalization capability of our network by testing a model trained on the ScanNet dataset which consists of 3D scans of single-story rooms to the multi-story multi-room building in the Gibson environment <ref type="bibr" target="#b34">[35]</ref>. Since our network is fully-convolutional and is translation invariant, our model perfectly generalizes to scenes without extra post-processing such as sliding-window-style cropping and stitching results. We further analyze the runtime and GPU memory usage of our method on the entire 572 Gibson V2 environments. As shown in <ref type="figure" target="#fig_9">Figure 17</ref>, the runtime and GPU memory usage of our method grows linearly to the number of input points and sublinearly to the volume of the point cloud. This indicates that our method is relatively invariant to the curse of dimensionality. In <ref type="figure" target="#fig_10">Figure 18</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Detection results on the entire S3DIS building 5: Our proposed method can process 78M points, 13984m 3 , 53 room building as a whole in a single fullyconvolutional feed-forward pass, only using 5G of GPU memory. Left: bird-eye-view of the entire building 5, Right: partial view of the same building.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Network overview: generative sparse detection networks process a sparse tensor input first with a series of strided convolutions followed by a few residual network blocks to generates hierarchical sparse tensor feature maps (Sec. 4.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Expansion and pruning: transposed convolution upsamples a lowresolution sparse tensor into a highresolution sparse tensor. Then, we prune out some of the upsampled coordinates with sparsity predictions P s (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Qualitative object detection results on the S3DIS dataset. Left: Memory usage comparison on ScanNet dataset evaluation: Our proposed sparse encoder and decoder maintains low memory usage compared to the dense counterparts. Right: Point cloud density on ScanNet dataset. Our model maintains constant input point cloud density compared to Qi et al. [24], which samples constant number of points regardless of the size of the input. constant number of points from input point clouds regardless of the size of the input point clouds. Thus, the point density of Qi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Runtime comparison on ScanNet v2 validation set: Qi et al.<ref type="bibr" target="#b23">[24]</ref> samples a constant number of points from a scene and their post-processing is inversely proportional to the density, whereas our method scales linearly to the number of points, and sublinearly to the floor area while being significantly faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Detection results on a Gibson environment scene Uvalda<ref type="bibr" target="#b34">[35]</ref>: GSDN can process a 17-room building with 1.4M points in a single fully-convolutional feed-forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>Analysis of the impact of sparsity pruning confidence ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 :</head><label>12</label><figDesc>Runtime breakdown of our model with varying backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 :Fig. 15 :</head><label>1315</label><figDesc>Qualitative object detection results on the ScanNet dataset<ref type="bibr" target="#b4">[5]</ref>. Qualitative object detection results on the S3DIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 17 :</head><label>17</label><figDesc>, we visualize additional qualitative results of our method on the Gibson environment. Runtime and peak memory usage analysis on 572 Gibson V2 environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 18 :</head><label>18</label><figDesc>Qualitative object detection results on the Gibson V2 environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.12356v1 [cs.CV] 22 Jun 2020</figDesc><table><row><cell>Input Surface</cell><cell>Voxelized Input</cell><cell>Conv Output</cell><cell>Conv Output</cell><cell>ConvTr Output</cell><cell>Pruned Output</cell><cell>BBox Preds</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Generative Sparse Detection NetworksInput: T , f l (?; W l ), f Tr l (?; W Tr l ), g b l (?; W b l ), Ps(?; G s l ) for l ? [1, ..., L], ?s Output: {B l } l for l ? [1, ..., L] 1 T0 ? T /* Hierarchical Sparse Tensor Encoder ? 4.1 */ 2 for l ? 1, ...L do TL 5 for l ? L, ..., 1 do</figDesc><table><row><cell>3</cell><cell cols="2">T l ? f l (T l?1 )</cell><cell cols="2">// Hierarchical feature tensors</cell></row><row><cell></cell><cell cols="3">/* Generative Sparse Tensor Decoder  ? 4.2</cell><cell>*/</cell></row><row><cell cols="3">4 T Tr L ? 6 if l &lt; L then</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>T Tr l</cell><cell>? T Tr l</cell><cell>+ T l</cell><cell>// Skip connection  ?4.2.2</cell></row><row><cell>8</cell><cell cols="2">B l ? g b l (T Tr l )</cell><cell></cell><cell>// Anchor predictions  ?4.3</cell></row><row><cell>9</cell><cell cols="2">p l ? P s l (T Tr l )</cell><cell></cell><cell>// Sparsity predictions</cell></row><row><cell>10</cell><cell>T Tr</cell><cell></cell><cell></cell><cell></cell></row></table><note>l ? SparsityPruning(T Tr l , p l &lt; ? ) // Pruning ?4.2.111 if l &gt; 1 then12 T Tr l+1 ? f Tr l (T Tr l ) // Transposed convolution ?4.2.113 return {B l } l</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Object detection mAP on the ScanNet v2 validation set. DSS, MRCNN 2D-3D, FPointNet are from<ref type="bibr" target="#b12">[13]</ref>. GSPN from<ref type="bibr" target="#b23">[24]</ref>. Our method, despite being single-shot, outperforms all previous state-of-the-art methods.cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP Hou et al. [13] 12.75 63.14 65.98 46.33 26.91 7.95 2.79 2.30 0.00 6.92 33.34 2.47 10.42 12.17 74.51 22.87 58.66 7.05 25.36 Hou et al. [13] + 5 views 19.76 69.71 66.15 71.81 36.06 30.64 10.88 27.34 0.00 10.00 46.93 14.06 53.76 35.96 87.60 42.98 84.30 16.20 40.23 Qi et al. [24] 36.27 87.92 88.71 89.62 58.77 47.32 38.10 44.62 7.83 56.13 71.69 47.23 45.37 57.13 94.94 54.70 92.11 37.20 58.65 GSDN (Ours) 41.58 82.50 92.14 86.95 61.05 42.41 40.66 51.14 10.23 64.18 71.06 54.92 40.00 70.54 99.97 75.50 93.23 53.07 62.84</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Class-wise mAP@0.25 object detection result on the ScanNet v2 valida- tion set. Our method outperforms previous state-of-the-art on majority of the semantic classes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Per-class precision/recall curve of ScanNetV2 validation object detection.</figDesc><table><row><cell cols="3">35FXUYHRI6FDQ1HWYYDO#,R8</cell><cell cols="3">35FXUYHRI6FDQ1HWYYDO#,R8</cell></row><row><cell>SUHFLVLRQ</cell><cell></cell><cell>SUHFLVLRQ</cell><cell></cell><cell></cell><cell>FDELQHW EHG FKDLU VRID WDEOH GRRU ZLQGRZ ERRNVKHOI SLFWXUH FRXQWHU GHVN FXUWDLQ UHIULJHUDWRU VKRZHUFXUWDLQ WRLOHW VLQN EDWKWXE RWKHUIXUQLWXUH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UHFDOO</cell></row><row><cell cols="2">Fig. 5: IoU Thres. Metric</cell><cell>Method</cell><cell cols="3">table chair sofa bookcase board avg</cell></row><row><cell>0.25</cell><cell>AP</cell><cell cols="4">Yang et al. [36]* 27.33 53.41 9.09 GSDN (ours) 73.69 98.11 20.78 33.38 12.91 47.77 14.76 29.17 26.75</cell></row><row><cell></cell><cell>Recall</cell><cell cols="4">Yang et al. [36]* 40.91 68.22 9.09 GSDN (ours) 85.71 98.84 36.36 61.57 26.19 61.74 29.03 50.00 39.45</cell></row><row><cell>0.5</cell><cell>AP</cell><cell cols="2">Yang et al. [36]* 4.02 17.36 0.0 GSDN (ours) 36.57 75.29 6.06</cell><cell>2.60 6.46</cell><cell>13.57 7.51 1.19 25.11</cell></row><row><cell></cell><cell>Recall</cell><cell cols="4">Yang et al. [36]* 16.23 38.37 0.0 GSDN (ours) 50.00 82.56 18.18 18.52 12.44 33.33 20.08 2.38 34.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2,12]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Object detection result on furniture subclass of S3DIS dataset building 5. *: Converted the instance segmentation results to bounding boxes for reference for the encoder. For all experiments, we use voxel size of 5cm, transpose kernel size of 3, with L = 4 scale hierarchy, sparsity pruning confidence ? = 0.3, and 3D NMS threshold 0.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Overall, mAP@0.25 (mAP with IoU threshold of 0.25) improves</figDesc><table><row><cell cols="3">5XQWLPHEUHDNGRZQSHUEDFNERQHPRGHO</cell></row><row><cell>UXPWLPHDYHUDJHV</cell><cell></cell><cell>SRVWSURFHVVLQJ GHFRGHU</cell></row><row><cell></cell><cell></cell><cell>HQFRGHU</cell></row><row><cell>5HV1HW</cell><cell>5HV1HW %DFNERQHPRGHO</cell><cell>5HV1HW</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Analysis of the impact of different anchor ratios on performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Hou et al.<ref type="bibr" target="#b12">[13]</ref> 5.06 42.<ref type="bibr" target="#b18">19</ref> 50.11 31.75 15.12 1.38 0.00 1.44 0.00 0.00 13.66 0.00 2.63 3.00 56.75 8.68 28.52 2.55 14.60 Hou et al. [13] + 5 views 5.73 50.28 52.59 55.43 21.96 10.88 0.00 13.18 0.00 0.00 23.62 2.61 24.54 0.82 71.79 8.94 56.40 6.87 22.53 Qi et al. [24] 8.07 76.06 67.23 68.82 42.36 15.34 6.43 28.00 1.25 9.52 37.52 11.55 27.80 9.96 86.53 16.76 78.87 11.69 33.54 Ours 13.18 74.91 75.77 60.29 39.51 8.51 11.55 27.61 1.47 3.19 37.53 14.10 25.89 1.43 86.97 37.47 76.88 30.53 34.82</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Class-wise mAP@0.5 object detection result on the ScanNet v2 validation.</figDesc><table><row><cell>G.T.</cell><cell>Hou et al. [13]</cell><cell>Qi et al. [24]</cell><cell>Ours</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sg-nn: Sparse generative neural networks for self-supervised scene completion of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00036</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9256" to="9266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<title level="m">Vehicle detection from 3d lidar using fully convolutional network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Masc: multi-scale affinity with sparse convolution for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04478</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05119</idno>
		<title level="m">Exploring sparsity in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Sliding Shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gnu parallel-the command-line power tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The USENIX Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="47" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2017/TDB17b" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15607" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gibson env: realworld perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6737" to="6746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Qualitative object detection results on the ScanNet dataset</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
