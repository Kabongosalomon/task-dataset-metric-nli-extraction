<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Bipartite Graph Learning for Video Domain Adaptation Source Domain Target Domain Bipartite Graph Domain Adaptation Adversarial Domain Adaptation Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
							<email>lyadanluol@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
							<email>zijian.wang@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Bio-Computing Research Center</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
							<email>m.baktashmotlagh@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Bipartite Graph Learning for Video Domain Adaptation Source Domain Target Domain Bipartite Graph Domain Adaptation Adversarial Domain Adaptation Adaptation</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413897</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Transfer learning</term>
					<term>Activity recognition and understanding KEYWORDS Video Action Recognition</term>
					<term>Domain Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation techniques, which focus on adapting models between distributionally different domains, are rarely explored in the video recognition area due to the significant spatial and temporal shifts across the source (i.e. training) and target (i.e. test) domains. As such, recent works on visual domain adaptation which leverage adversarial learning to unify the source and target video representations and strengthen the feature transferability are not highly effective on the videos. To overcome this limitation, in this paper, we learn a domain-agnostic video classifier instead of learning domain-invariant representations, and propose an Adversarial Bipartite Graph (ABG) learning framework which directly models the source-target interactions with a network topology of the bipartite graph. Specifically, the source and target frames are sampled as heterogeneous vertexes while the edges connecting two types of nodes measure the affinity among them. Through message-passing, each vertex aggregates the features from its heterogeneous neighbors, forcing the features coming from the same class to be mixed evenly. Explicitly exposing the video classifier to such cross-domain representations at the training and test stages makes our model less biased to the labeled source data, which in-turn results in achieving a better generalization on the target domain. The proposed framework is agnostic to the choices of frame aggregation, and therefore, four different aggregation functions are investigated for capturing appearance and temporal dynamics. To further enhance the model capacity and testify the robustness of the proposed architecture on difficult transfer tasks, we extend our model to work in a semi-supervised setting using an additional video-level bipartite graph. Extensive experiments conducted on four benchmark datasets evidence the effectiveness of the proposed approach over the state-of-the-art methods on the task of video recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An illustration of the video domain adaptation task. With the unaligned inputs from different domains, the video classifier learned in existing adversarial DA approaches (shown as grey doted lines) can easily overfit the labeled source data. By contrast, the classifier learned using our proposed ABG framework (shown as orange doted lines) is domain-agnostic, which performs equally well at the training and test stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of multimedia streaming <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> and gaming data, automatically recognizing and understanding human actions and events in videos have become increasingly important, especially for practical tasks such as video retrieval <ref type="bibr" target="#b16">[17]</ref>, surveillance <ref type="bibr" target="#b28">[28]</ref>, and recommendation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>. Over the past decades, great efforts have been made to boost the recognition performance with deep learning for different purposes including appearances and short-term motions learning <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref>, temporal structure modeling <ref type="bibr" target="#b39">[39]</ref>, and human skeleton and pose embedding <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b45">45]</ref>. While effective, deep learning enables machine recognition at a great cost of labeling large-scale data. To relieve the burden of tedious and expensive labeling, one alternative is to transfer knowledge from the existing annotated training data (i.e. source domain) to the unlabeled or partially labeled test data (i.e. target domain). However, the source and target sets are commonly constructed under varying conditions such as illuminations, camera poses and backgrounds, leading to a huge domain shift. For instance, the Gameplay-Kinetics <ref type="bibr" target="#b1">[2]</ref> dataset is built under the challenging "Synthetic-to-Real" protocol, where the training videos are synthesized by game engines and the test samples are collected from real scenes. In this case, the domain discrepancy between the source and target domains inevitably leads to a severe degradation of the model generalization performance.</p><p>To combat the above dilemmas, domain adaptation (DA) approaches have been investigated to mitigate the domain gap by aligning the distributions across the domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b46">46]</ref> or learning domain-invariant representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">48]</ref>. While the notion of domain adaptation has been widely exploited in the past, the resulting techniques are mostly designed to cope with still images rather than the videos. These image-level DA methods could hardly achieve a good performance on the video recognition tasks as they don't take into account the temporal dependency of the frames when minimizing the discrepancy between the domains.</p><p>Lately, video domain adaptation techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">29]</ref> have emerged to address the domain shift in videos using adversarial learning. By segmenting the source and target videos into a set of fixed-length action clips, DAAA <ref type="bibr" target="#b10">[11]</ref> directly matches the segment representations from different domains with the 3D-CNN <ref type="bibr" target="#b36">[36]</ref> feature extractor. TA 3 N [2] weights the source and target segments with a proposed temporal attention mechanism, forcing the model to attend the temporal features of low domain discrepancy. Different from the prior work that mainly concentrates on intra-domain interactions, TcoN <ref type="bibr" target="#b29">[29]</ref> proposes a cross-domain co-attention module to measure the affinity of the segment-pairs from source and target domains and further highlight the key segments shared by both domains.</p><p>Nevertheless, existing adversarial video domain adaptation methodologies are limited in three aspects. First, when data distributions embody complex structures like videos, there is no guarantee for the two distributions to become sufficiently similar when the discriminator is fully confused, as illustrated in <ref type="figure">Figure 1</ref>. Second, existing algorithms perform asymmetrically at the training and test stages. For instance, TcoN takes as input the source and target pairs and calculates the cross-domain attention scores at training stage, but inferences are done only based on the target data at the test time. This discrepancy unavoidably causes the exposure bias and deteriorates the model performance. Third, utilizing a general domain classifier for adversarial learning is only able to match marginal distributions <ref type="bibr" target="#b6">[7]</ref>, and so does not align the class-conditional distributions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">48]</ref>. The video recognition models trained in this manner are hereby less likely to achieve the class-wise alignment.</p><p>To address the above-mentioned issues, in this paper, we take a more feasible strategy, i.e., to construct a domain-agnostic video classifier instead of pursuing with domain-invariant feature learning. In the proposed Adversarial Bipartite Graph (ABG) framework as illustrated in <ref type="figure">Figure 2</ref>, the video classifier is explicitly exposed to the mixed cross-domain representations, which preserves the temporal correlations across the domains modeled with a network topology of the bipartite graph. In particular, the source and target frames are sampled as heterogeneous vertexes of the bipartite graph, and the edges connecting the two types of nodes measure their similarity. Through message-passing, each vertex aggregates the features of its heterogeneous neighbors, making those from the similar source and target frames to be evenly mixed in the shared subspace. The proposed strategy performs symmetrically during the training and test phases, which successfully addresses the exposure bias issue.</p><p>Moreover, as the proposed framework is agnostic to the choices of frame aggregation, four different aggregation mechanisms are investigated, followed by a conditional adversarial module to preserve the class-specific consistency across the domains. The source labels and the target predictions are embedded as vectors which provide semantic cues for the domain classifier. To cope with large domain discrepancy, we additionally apply a video-level bipartite graph on the original model, called Hierarchical ABG. To testify the robustness of the proposed model, we further extend it to a semisupervised domain adaptation setting (Semi-ABG), by adding the partial edge supervision. Extensive experiments conducted on four benchmark datasets evidence the superiority of the proposed adversarial bipartite framework over the state-of-the-art approaches. Overall, our contributions can be briefly summarized as follows:</p><p>? We introduce a new Adversarial Bipartite Graph (ABG) framework for unsupervised video domain adaptation, which focuses on recognizing domain-agnostic concepts rather than learning domain-invariant representations. It is further generalized to its hierarchical variant for challenging transfer tasks. ? To address the exposure bias issue, the proposed model is trained and tested symmetrically. ? The proposed ABG framework is seamlessly equipped with a conditional domain adversarial module which globally aligns the class-conditional distributions from different domains. ? We have demonstrated the effectiveness of the proposed strategy through extensive experiments on four large-scale video domain adaptation datasets and released the source code for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Video Action Recognition</head><p>Activity recognition has been one of the core topics in computer vision areas, with a wide range of real-world applications including video surveillance <ref type="bibr" target="#b28">[28]</ref>, environment monitoring and video captioning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b47">47]</ref>. A typical pipeline is leveraging a twostream convolutional neural network to classify actions based on the individual video frames or local motion vectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">33]</ref>. To better capture the action dynamics and gesture changes, later work models the long-term temporal information with recurrent neural networks <ref type="bibr" target="#b3">[4]</ref>, 3D convolutions <ref type="bibr" target="#b36">[36]</ref>, and multi-scale temporal relation networks (TRN) <ref type="bibr" target="#b49">[49]</ref>. Another line of work augments the extracted RGB and optical flow features with multi-modal pose representations <ref type="bibr" target="#b45">[45]</ref>, complex object interactions <ref type="bibr" target="#b24">[25]</ref>, and 3D human skeleton <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31]</ref>, which relieve the view dependency and the noises from different lighting conditions. However, the all abovementioned work requires expensive annotations and could barely generalize to an unseen circumstance, which greatly hinders the feasibility in practice.  <ref type="figure">Figure 2</ref>: An overview of the proposed Adversarial Bipartite Graph (ABG) architecture, its hierarchical variant HABG (shown in blue), and the Semi-supervised ABG (shown in green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain Adaptation</head><p>Unsupervised Domain Adaptation (UDA) tackles such a limitation by trying to transfer knowledge from a labeled source domain to an unlabeled target domain. The discrepancy between the two domains refers to the domain shift <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, which is addressed by minimizing a distribution distance such as Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b7">[8]</ref> with its variants <ref type="bibr" target="#b46">[46]</ref> and/or learning domaininvariant representations with adversarial learning <ref type="bibr" target="#b5">[6]</ref> recently. Alternatively, an emerging line of work incorporates graph neural networks (GNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">37]</ref> to bridge the domain gap at a manifold level, learning the intra-domain correlations in a transductive way. Very recently, GCAN <ref type="bibr" target="#b25">[26]</ref> constructs a densely-connected instance graph for the source and target nodes, and assigns pseudo labels for the target samples for aligning class centroids from different domains. While effective, existing graph-based work fails to model the inter-domain interactions, which makes it far from optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video Domain Adaptation</head><p>Despite of the fact that domain adaptation has made great progress in a broader set of image recognition tasks, it is barely investigated for transferring knowledge across the videos. Early efforts <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b44">44]</ref> on video domain adaptation utilize a shallow model, that employs collective matrix factorization or PCA to learn a common latent semantic space for the source and target domains. Of late, the focus has shifted to the deep models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">29]</ref>. Jamal et al. projected the pre-extracted C3D <ref type="bibr" target="#b36">[36]</ref> representations of the source and target videos to a Grassmann manifold, and performed domain adaptation with adaptive kernels and adversarial learning. To extend the idea of modeling actions on the latent subspace, an end-to-end Deep Adversarial Action Adaptation (DAAA) <ref type="bibr" target="#b10">[11]</ref> is derived to learn the source and target video clips in the same temporal order. Zhang et al. transferred from the trimmed video domain to the untrimmed video domain with Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b7">[8]</ref> for action localization, yet without tasking into consideration any frame-level feature alignment. Chen et al. proposed a temporal attentive adversarial adaptation network (TA 3 N), which leverages the entropy of the domain predictor to attend the local temporal features of low domain discrepancy. Pan et al. <ref type="bibr" target="#b29">[29]</ref> designed a co-attention module to minimize the domain discrepancy, which concentrates on the key segments shared by the both domains. Nevertheless, prior work is vulnerable and unreliable due to the overfitting and exposure bias issues. Instead, the proposed adversarial bipartite graph model is capable of learning domain-agnostic concepts and aligning class-conditional distributions locally and globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first formulate the task of unsupervised video domain adaptation, and then elaborate the details of the derived Adversarial Bipartite Graph (ABG) framework and its hierarchical variant (HABG). To testify the robustness of the proposed model, it is further generalized to a semi-supervised setting (Semi-ABG) in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Give a labeled source video collection D s = {(X s i , y i )} N s i=1 and an unlabeled target video set D t = {X t j } N t j=1 containing N s and N t videos respectively, the aim is to design a transfer network for predicting the labels D t of the unlabeled target videos. The source and target domains are of different distributions yet they share the same label space Y ? R C , where C is the number of classes. Each source video X s i or target video X t j consists of K frames, i.e.,</p><formula xml:id="formula_0">X s i = {x k i } K k =1 and X t j = {x k j } K k =1 , where x k i , x k j ? R D indicate</formula><p>the features of the k-th frame, and D is the feature dimension of the vectors. For constructing each mini-batch, we forward B s source features and B t target features to update the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Bipartite Graph Learning (ABG)</head><p>To model the data affinity across two domains, it is natural to formulate the problem with a bipartite graph, whose vertices can be divided into two disjoint and independent sets, with the edges connecting the vertices from different sets. As discussed in Section 3.2, the general pipeline of ABG consists of (1) mixing the similar source and target features with the frame bipartite graph; (2) aggregating frame features into the global video representations; (3) aligning class-conditional distributions with adversarial learning; and (4) classifying the obtained source and target representations. To enhance the model capacity for difficult transfer tasks, we design a hierarchical structure HABG, incorporated in the video bipartite graph, as detailed in Section 3.2. Frame-level Bipartite Graph. Let the frame-level directed bipar-</p><formula xml:id="formula_1">tite graph be G f = (V s f , V t f , E st f ), where the cross-domain edge feature map E st f ? R K ?B s ?K ?B t</formula><p>represents the node affinity between the pairs of source and target frames. The source vertex</p><formula xml:id="formula_2">set V s f = {v s ik | K k =1 } B s i=1 ? R K ?B s ?D v and target vertex set V t f = {v t jk | K k =1 } B t j=1 ? R K ?B t ?D v , with D v</formula><p>the vertex feature dimension, are expected to dynamically aggregate information across the domains based on the learned edge features, thus closing the domain gap gradually. The propagation rules for the cross-domain edge update and node update are elaborated as follows.</p><p>Frame Edge Update. To calculate the similarity between the source and target frames, the normalized edge matrix is defined as,</p><formula xml:id="formula_3">A f = ? F f e (|V s f ? V t f |; ? f e ) , A f = A f i ? ?A f i ? ? 1 , E st f =? f ?j ?? f ?j ? 1 ,<label>(1)</label></formula><p>with ? the sigmoid function, and ??? 1 the</p><formula xml:id="formula_4">L 1 norm. V s f ? R K ?B s ?K ?B t ?D v and V t f ? R K ?B s ?K ?B t ?D v</formula><p>are the augmented tensors of the source and target vertexes, with the dimensions being expanded by repeating. F f e (?; ? f e ) is the frame-level metric network parameterized by ? f e , which computes the similarity scores between the source and target frames. To ease the impact of the number of cross-domain neighbors, the row normalization and column normalization are adopted on the edge feature map E st f . Frame Node Update. The generic rule to update node features can be formulated as follows,</p><formula xml:id="formula_5">V s f = E st f V t f ,? t f = (E st f ) T V s f , V s f ? F f v ([V s f ;? s f ]; ? f v ), V t f ? F f v ([V t f ;? t f ]; ? f v ),<label>(2)</label></formula><p>where [?; ?] is the concatenation operation, and F f v (?; ? f v ) is a node update network for both source and target nodes. The node embedding is initialized with the extracted representation from the backbone embedding model,</p><formula xml:id="formula_6">i.e., V s f = {(x 1 i , x 2 i , . . . , x K i )} B s i=1 ? R K ?B s ?D , V t f = {(x 1 j , x 2 j , . . . , x K j )} B t j=1 ? R K ?B t ?D . Frame Aggregation.</formula><p>To group the sampled frames into a unified video representation and capture appearance and temporal dynamics, the frame aggregation is applied on the learned source and target node embeddings. As the proposed framework is agnostic to the choices of frame aggregation, we examine multiple aggregation functions, including a symmetric average pooling function which is invariant to the order of frames, two memory based modules to capture the temporal information among frames, and a temporal relation network to explore the multi-scale temporal dynamics.</p><p>Mean Average Pooling. By viewing the video as a collection of key frames, the video representation can be obtained by averaging the frame features temporally. Hence, each source video representation</p><formula xml:id="formula_7">H s i ? R D v and target video representation H t i ? R D v are computed</formula><p>as,</p><formula xml:id="formula_8">H s i = 1 K K k=1 v s ik , H t j = 1 K K k =1 v t jk .<label>(3)</label></formula><p>Memory Based Aggregators. Considering the temporal characteristics in human actions and events, two memory based aggregators, i.e., LSTM and GRU, are tested to construct the i-th source representation</p><formula xml:id="formula_9">H s i = H s iK ? R D v , and j-th target representation H t j = H t jK ? R D v as: H s ik = LST M(H s ik ?1 , v s ik ), H t jk = LST M(H t jk?1 , v t jk ), H s ik = GRU (H s ik ?1 , v s ik ), H t jk = GRU (H t jk?1 , v t jk ),<label>(4)</label></formula><p>with H the output hidden states, and K the last step. Temporal Relation Network (TRN). Inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">49]</ref>, we further build up on a fine-grained relationship among the multi-scale video segments. In particular, the temporal relation network (TRN) <ref type="bibr" target="#b49">[49]</ref> is able to preserve the short-term (e.g., 2-frame relation), and longterm (e.g., 5-frame relation) action dynamics, which potentially expands the temporal information that the learned video features could convey. The multi-scale temporal relations for the source data H s ? R B s ?D v and the target data H t ? R B t ?D v are defined as the composite functions below,</p><formula xml:id="formula_10">T 2 (V s f ) = G k 1 &lt;k 2 F t (v s ?k 1 , v s ?k 2 ) ,T 2 (V t f ) = G k 1 &lt;k 2 F t (v t ?k 1 , v t ?k 2</formula><p>) ,</p><formula xml:id="formula_11">H s = K k =2 T k (V s f ), H t = K k=2 T k (V t f ),<label>(5)</label></formula><p>with the T 2 (?) indicates the 2-frame local relation function. Note that the multi-scale function is the sum of the local relation scores from 2-frame to K-frame. The F t (?; ? t ) and G(?; ? G ) are fully connected layers, fusing the features of different ordered frames. Video-level Bipartite Graph. For difficult transfer tasks, we additionally apply a video-level bipartite graph on top of the frame aggregation network, fusing the source and target data hierarchically. It allows video features to be grouped into tighter clusters which improves classification performance. Similarly, we construct the video-level directed bipartite graph </p><formula xml:id="formula_12">G v = (N s v , N t v , E st v ), where the E st v ? R B s ?B</formula><formula xml:id="formula_13">v = {n s i } B s i=1 ? R B s ?D n and target node set N t v = {n t j } B t j=1 ? R B t ?D n ,</formula><p>with D n the feature dimension of video nodes, are learned through message passing as defined below.</p><p>Video Edge Update. To calculate the similarity between the source and target frames, the normalized edge matrix is defined as,</p><formula xml:id="formula_14">A v = ? F ve (|N s v ? N t v |; ? ve ) , A v = A v i ? ?A v i ? ? 1 , E st v =? v ?j ?? v ?j ? 1 ,<label>(6)</label></formula><p>with ? the sigmoid function, ? ? ? 1 the L 1 norm. N s v ? R B s ?B t ?D n and N t v ? R B s ?B t ?D n are the augmented tensors of the source and target vertices, with the dimensions being expanded by repeating. F ve (?; ? ve ) is the video-level metric network parameterized by ? ve , computing the correlations among the source-target video pairs.</p><p>Video Node Update. The generic rule to update the node features can be formulated as follows,</p><formula xml:id="formula_15">N s v = E st v N t v ,? t v = (E st v ) T N s v , N s v ? F vn ([N s v ;? s v ]; ? vn ), N t v ? F vn ([N t v ;? t v ]; ? vn ),<label>(7)</label></formula><p>where [?; ?] is the concatenation operation and F vn (?; ? vn ) is a node update network for the both source and target nodes. The node embeddings are initialized with the aggregated features i.e., N s</p><formula xml:id="formula_16">v = {H s i } B s i=1 ? R B s ?D v , N t v = {H t j } B t j=1 ? R B t ?D v . Video Classification.</formula><p>To predict the labels for the source and target samples, we construct a video classier F y (?; ? y ) based on the aggregated video features for the ABG structure and the video vertex features for the HABG, respectively. Since the source data is labeled, the classifier is trained to minimize the negative log likelihood loss for each mini-batch,</p><formula xml:id="formula_17">L s y = ? 1 B s B s i=1 y i log(F y (n s i )).<label>(8)</label></formula><p>Instead of the supervised loss, for the unlabeled target data, a soft entropy based loss is adopted to alleviate the uncertainty of the predictions:</p><formula xml:id="formula_18">L t y = ? 1 B t B t j=1 F y (n t j ) log(F y (n t j )).<label>(9)</label></formula><p>Conditional Adversarial Learning. Besides leveraging bipartite graph neural networks to fuse the source and target neighbors, a conditional adversarial module is applied to align the class-conditional distributions. To achieve this, the module is composed of a label embedding function F l (?; ? l ) and a domain classifier D(?; ? d ). The label embedding function projects the i-th source video label y i and the jth target frame predictions F y (n t j ) into the latent vectors? s i ? R D n and? t j ? R D n , providing the domain-invariant semantic cues for the domain classifier. The domain classifier is then conditioned on the classes for which the samples may belong to, and trained to discriminate between the features coming from the source or target data. The bipartite graphs are viewed as the feature generator to fool the discriminator. The adversarial objective function for the conditional adversarial module is formulated as:</p><formula xml:id="formula_19">L d = E n s i ?N s v log[D(n s i +? s i )] + E n t j ?N t v log[1 ? D(n t j +? t j )], y s i = F l (y i ; ? l ),? t j = F l (F y (n t j ); ? l ).<label>(10)</label></formula><p>Consequently, the learned features will be more discriminative and aligned when the two-player mini-max game reaches an equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-supervised ABG and HABG</head><p>To verify the robustness of the proposed ABG and HABG structure, we further extend them to a semi-supervised setting. In this circumstance, part of the target labels y t ? ? R |? |?C in a mini-batch are available for training. Here, we denote ? and ?? as the indices of the labeled and unlabeled target data, respectively. To fully take advantage of the partial target supervision, the classification objective functions are modified accordingly,</p><formula xml:id="formula_20">L s y = ? 1 B s B s i=1 y i log(F y (n s i )) ? 1 |?| j ?? y t j log(F y (n t j )), L t y = ? 1 |??| j ??? F y (n t j ) log(F y (n t j )),<label>(11)</label></formula><p>with |??| = B t ? |?|. Moreover, the edge maps learned from either the frame-level or video-level bipartite graphs are able to be partially supervised. The newly added edge supervision is a binary cross entropy loss, which can be formulated as,</p><formula xml:id="formula_21">L f e = B s i=1, j ?? K k =1 E st f (i + k, j + k)? (y i = y t j ), L v e = B s i=1, j ?? E st v (i, j)? (y i = y t j ),<label>(12)</label></formula><p>where ? (?) is the Kronecker delta function that is equal to one when y i = y t j , and zero otherwise. E st f (i + k, j + k) indicates the element from the (i + k)-th row and (j + k)-th column of the frame-level edge map, and E st v (i, j) represents the element from the i-th row and j-th column of the video-level edge map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>Our ultimate goal is to learn the optimal parameters for the proposed model,</p><formula xml:id="formula_22">(? * f e , ? * f v , ? * a , ? * ve , ? * vn , ? * l , ? * y ) = arg min L ? ? L d , ? * d = arg min L + ? L d , L = L s y + ? L t y + ?(L v e + ? L f e ),<label>(13)</label></formula><p>with ? a being the learnable parameters of the frame aggregation module, and ?, ? , ? and ? being the loss coefficients respectively. Notably, for the UDA setting, we have, L f e = 0, and L v e = 0. The overall algorithm is provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets</head><p>We compare and contrast our proposed approach with the existing domain adaptation approaches on four benchmark datasets, i.e., the UCF-HMDB small , UCF-HMDB f ull , UCF-Olympic and Kinetics-Gameplay. For fair comparison, we follow the dataset partition and feature extraction strategies from <ref type="bibr" target="#b1">[2]</ref>, that utilizes the ResNet101 model pre-trained on ImageNet as the frame-level feature extractor. The statistics of the four datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The UCF-HMDB small and UCF-HMDB f ull are the overlapped subsets of two large-scale action recognition datasets, i.e., the UCF101 <ref type="bibr" target="#b34">[34]</ref> and HMDB51 <ref type="bibr" target="#b15">[16]</ref>, covering 5 and 12 highly relevant categories respectively. The UCF-Olympic selects the shared 6 classes from the UCF101 and Olympic Sports Datasets <ref type="bibr" target="#b27">[27]</ref>, including Basketball, Clearn and Jerk, Diving, Pole Vault, Tennis and Discus Throw. The Kinetics-Gameplay is the most challenging cross-domain dataset, with a large domain gap between the synthetic videos and real-world videos. The dataset is build by selecting 30 shared categories between Gameplay <ref type="bibr" target="#b1">[2]</ref> and one of the largest  <ref type="bibr" target="#b12">[13]</ref>. Each category may also correspond to multiple categories in both dataset, which poses another challenge of class imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our approach with several state-of-the-art video domain adaptation methods, image domain adaptation approaches, single-domain action recognition models, and a basic ResNet-101 classification model pre-trained on the ImageNet dataset. Singledomain action recognition models include the 3D ConvNets (C3D) <ref type="bibr" target="#b36">[36]</ref> and Temporal Segment Networks (TSN) <ref type="bibr" target="#b39">[39]</ref>, which are pretrained on the source domain and tested on the target domain. Four classical image-level domain adaptation methods, i.e., Domain-Adversarial Neural Network (DANN) <ref type="bibr" target="#b5">[6]</ref>, Joint Adaptation Network (JAN) <ref type="bibr" target="#b20">[21]</ref>, Adaptive Batch Normalization (AdaBN) <ref type="bibr" target="#b17">[18]</ref> and Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b32">[32]</ref> are adjusted to align the distributions of video features with the frame aggregation module. As for non-deep video domain adaptation, we compare the proposed HABG method with Many-to-One <ref type="bibr" target="#b44">[44]</ref> Encoder, two variants of Action Modeling on Latent Subspace (AMLS) <ref type="bibr" target="#b10">[11]</ref>, i.e., the Subspace Alignment (AMLS-SA) and Geodesic Flow Kernel (AMLS-GFK). For deep video domain adaptation methods, we adopt the Deep Adversarial Action Adaptation (DAAA) <ref type="bibr" target="#b10">[11]</ref>, Temporal Adversarial Adaptation Network (TA 2 N) <ref type="bibr" target="#b1">[2]</ref>, Temporal Attentive Adversarial Adaptation Network (TA 3 N) <ref type="bibr" target="#b1">[2]</ref> and Temporal Co-attention Network (TCoN) <ref type="bibr" target="#b29">[29]</ref> for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Our source code is based on PyTorch <ref type="bibr" target="#b30">[30]</ref>, which is available in a Githu repository 1 for reference. All experiments are conducted on two servers with two GeForce GTX 2080 Ti GPUs. 4.3.1 Video Pre-processing. Following the standard protocol used in <ref type="bibr" target="#b1">[2]</ref>, we sample a fixed-number K of frames with an equal spacing from each video for training, and encode each frame with the Resnet-101 <ref type="bibr" target="#b9">[10]</ref> pre-trained on ImageNet into a 2048-D vector, i.e., D = 2048. For fair comparison, we set K to 5 in our experiments.  Kinetics-Gameplay dataset, and 30 for the rest of datasets. The batch size B s , B t for the source data and target data are set to 128. The stochastic gradient optimizer (SGD) is used as the optimizer with a momentum of 0.9 and weight decay of 1?10 ?4 . The learning rate ? is initiated as 4 ? 10 ?2 then decayed as the number of epoch increases, which follows the rule used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. The loss coefficients ? and ? are empirically fixed at 0.1 and 1 for semi-supervised experiments. The dropout rate is set to 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons with State-of-The-Art</head><p>Under the unsupervised domain adaptation protocol, we compare the proposed ABG method with multiple baseline approaches on UCF-HMDB small , UCF-Olympic and UCF-HMDB f ull datasets.</p><p>With different backbone networks, the comparison results achieved from the relatively small datasets are reported in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="table" target="#tab_2">Table 3</ref> presents the results on the full UCF-HMDB dataset using various frame aggregation strategies. It is observed that the proposed ABG framework is superior to all the compared image-and video-level domain adaptation methods in most cases, especially achieving a significant performance boost on the large-scale testbed. Notably, Source Only indicates the backbone model pretrained on the source domain and tested on the target domain. Target Only  denotes the backbone model trained and tested on the target domain. From <ref type="table" target="#tab_1">Table 2</ref>, it is demonstrated that the deep video DA methods (line 6-10) generally outperforms the non-deep video DA approaches (line 3-5) and the classification models without DA (line 1-2). Among the deep TCoN and TA 3 N leverage the attention mechanism and then suppress the variance caused by the outlier frames, improving the recognition accuracy by up to 6.3% and 11.1% over DAAA on the UCF-Olympic dataset. With the same backbone of TA 3 N, our ABG model performs comparably without relying on the frame attention or complex frame aggregation strategies. This phenomenon is also observed in the large UCF-HMDB f ull dataset, in which the proposed ABG with the average pooling boosts the performance by up to 10.1% and 11.5% over the state-of-the-art TA 3 N on the UCF?HMDB and HMDB?UCF tasks, respectively. As shown in <ref type="table" target="#tab_2">Table 3</ref>, average pooling (AvgPool) and LSTM suits the proposed model better among other frame aggregation functions. We infer the reasons behind is the frame bipartite graph has already fused similar frames regardless the order, which weakens the power of multi-scaled TRN aggregation. Notably, it is observed that AdaBN surpasses the most of image domain adaptation methods. As it separates the batch normalization layer for source and target data, AdaBN minimizes the risk of being overfitting to the source domain, which provides a strong support to our statement discussed in Section 1. To further investigate the detailed performance of the proposed ABG with respect to specific classes, four confusion matrices are provided in <ref type="figure">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Semi-supervised Learning</head><p>To study the robustness of the proposed algorithm, we extend the unsupervised domain adaptation to a semi-supervised setting, where a part of target labels are available for training. Extensive experiments are conducted on the most challenging "Syntheticto-Real" testbed, i.e., the Kinetics-Gameplay dataset, on which the results with varying ratios of target labels are reported in <ref type="table" target="#tab_3">Table 4</ref>. The seen ratio of target labels ranges from 0.3 to 0.9. Similarly, Source Only / Target only represents the backbone model trained with source / target data only. All image-level domain adaptation methods (line 2-5), basic classification model (line 1,10) and our models (line 8-9) utilize AvgPool as the frame aggregation function. TA 2 N and TA 3 N use the TRN aggregator, since TRN is the major part of their works. It can bee seen that the overall performance is lower on the Gameplay?Kinetics compared to Kinetics?Gameplay, due to the insufficient samples in the source domain. The proposed Semi-ABG and its hierarchical variant Semi-HABG achieve higher recognition accuracy on both two transfer tasks, since the integrated graphs help to propagate the label information and take a full advantage of the supervision. The respective recognition accuracies of the proposed HABG on two transfer takss are improved by up to 8.0% and 39.6% over the stateof-the-art TA 3 N with only 30% of target labels available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>To investigate the validity of the derived modules and objective functions, we compare the four variants of ABG model on the full UCF-HMDB dataset. The comparison results are summarized in HABG is the hierarchical variant of the plain ABG model, performing better on the challenging UCF?HMDB transfer task, which is consistent with the findings in semi-supervised experiments. The ABG model is more versatile and suitable for small datasets and easier transfer tasks such as HMDB?UCF.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Parameter Sensitivity</head><p>To study the effect of the loss coefficients, we conduct the experiments on the UCF-HMDB f ull dataset with the varying values of ? and ? . The ? and ? are utilized to reconcile the adversarial loss and the entropy loss, respectively. We compare the proposed ABG model and its variant HABG integrated with the identical AvgPool frame aggregation function. As plotted in <ref type="figure" target="#fig_6">Figure 5</ref>, the average accuracies for both ABG and HABG models become quite stable when reaching sufficiently large loss coefficients. This indicates that our framework is robust with respect to loss coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Visualization</head><p>To shed a qualitative light on evaluating the proposed model with various aggregation functions, we conduct the experiments on the HMDB?UCF task, and visualize the features with t-SNE in <ref type="figure" target="#fig_5">Figure  4</ref>. The features are extracted from the last layer of ABG and the baseline models, including DANN, JAN, MCD and TA 3 N. Different colors indicate different classes. Circles represent the source videos and triangles represent the target videos. It is clearly shown that the features from ABG achieve the tighter clusters compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we propose a bipartite graph learning framework for unsupervised and semi-supervised video domain adaptation tasks. Different the existing approaches which learn domain-invariant features, we construct a domain-agnostic classifier by leveraging the bipartite graphs to combine the similar source and target features at the training and test time, which helps with reducing the exposure bias. Experiments evidence effectiveness of our proposed approach over the state-of-the-art methods, improving their performance by up to 39.6% in a semi-supervised setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t indicates the node affinity among the source and target videos. The source node set N s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 3 . 2</head><label>32</label><figDesc>Module Architecture. The edge update networks F f e and F ve project the affinity map from D and D v dimensions to 1-dim scores, which are composed of the two convolutional layers, batch normalization, LeakyReLU and edge dropout. The node update networks F f v and F vn map the 2D-dim and 2D v -dim concatenation of node features and the neighbors' features to the D v -dim and D ndim vectors, respectively. F f v and F vn include two convolutional layers, batch normalization, LeakyReLU and node dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 3 . 3 Figure 3 :</head><label>333</label><figDesc>Parameter Settings. The hidden size, the feature dimension of frame nodes (i.e. D v ) and the feature dimension of video nodes (i.e. D n ) are fixed to 512. The total number of training epochs M is 60 for 1 https://github.com/Luoyadan/MM2020_ABG The confusion matrices of the proposed ABG method performed on three benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The t-SNE visualization of the learned source and target video representations on the HMDB?UCF task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparisons of the proposed ABG and HABG with respect to the varying loss coefficients on the UCF?HMDB (shown in the upper row) and HMDB?UCF (shown in the bottom row) tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The general statistics of the four datasets used in our experiments.</figDesc><table><row><cell>Property</cell><cell>UCF-HMDB small</cell><cell>UCF-HMDB f ull</cell><cell>UCF-Olympic</cell><cell>Kinetics-Gameplay</cell></row><row><cell>Video Length</cell><cell>?21 Seconds</cell><cell>?33 Seconds</cell><cell>?39 Seconds</cell><cell>? 10 Seconds</cell></row><row><cell>Classes</cell><cell>5</cell><cell>12</cell><cell>6</cell><cell>30</cell></row><row><cell>Training Videos</cell><cell cols="4">UCF: 482 / HMDB: 350 UCF:1,438 / HMDB: 840 UCF: 601 / Olympic: 250 Kinetics: 43,378 / Gameplay: 2,625</cell></row><row><cell cols="3">Validation Videos UCF: 189 / HMDB: 571 UCF: 360 / HMDB: 350</cell><cell>UCF: 240 / Olympic: 54</cell><cell>Kinetics: 3,246 / Gameplay: 749</cell></row><row><cell cols="2">public video datasets Kinetics-600</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recognition accuracies (%) on the UCF-HMDB small and UCF-Olympic datasets. U: UCF, H: HMDB, O: Olympic.MethodBackbone U?H small H?U small U?O O?U</figDesc><table><row><cell>Source Only</cell><cell>TSN C3D</cell><cell>--</cell><cell>82.10 -</cell><cell>80.00 82.13</cell><cell>76.67 83.16</cell></row><row><cell cols="2">Many-to-One [44] Action Bank</cell><cell>82.00</cell><cell>82.00</cell><cell>87.00</cell><cell>75.00</cell></row><row><cell>AMLS-SA [11]</cell><cell>C3D</cell><cell>90.25</cell><cell>94.40</cell><cell>83.92</cell><cell>86.07</cell></row><row><cell>AMLS-GFK [11]</cell><cell>C3D</cell><cell>89.53</cell><cell>95.36</cell><cell>84.65</cell><cell>86.44</cell></row><row><cell>DAAA [11]</cell><cell>TSN</cell><cell>-</cell><cell>88.36</cell><cell>88.37</cell><cell>86.25</cell></row><row><cell>DAAA [11]</cell><cell>C3D</cell><cell>-</cell><cell>-</cell><cell>91.60</cell><cell>89.96</cell></row><row><cell>TCoN [29]</cell><cell>TSN</cell><cell>-</cell><cell>93.01</cell><cell>93.91</cell><cell>91.65</cell></row><row><cell>TA 3 N [2]</cell><cell>ResNet-101</cell><cell>99.33</cell><cell>99.47</cell><cell cols="2">98.15 92.92</cell></row><row><cell>ABG-AvgPool</cell><cell>ResNet-101</cell><cell>99.33</cell><cell>98.41</cell><cell cols="2">98.15 92.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="9">: Recognition accuracies (%) of the domain adapta-</cell></row><row><cell cols="9">tion methods and the proposed ABG model with respect to</cell></row><row><cell cols="9">various frame aggregation strategies on the full UCF-HMDB</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">UCF?HMDB</cell><cell></cell><cell></cell><cell cols="2">HMDB?UCF</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">AvgPool LSTM GRU</cell><cell cols="4">TRN AvgPool LSTM GRU</cell><cell>TRN</cell></row><row><cell>Source Only</cell><cell>70.28</cell><cell cols="3">69.17 70.83 71.67</cell><cell>74.96</cell><cell cols="3">70.05 76.36 73.91</cell></row><row><cell>DANN [6]</cell><cell>71.11</cell><cell cols="3">70.00 70.83 75.28</cell><cell>75.13</cell><cell cols="3">75.83 75.13 76.36</cell></row><row><cell>JAN [21]</cell><cell>71.39</cell><cell cols="3">70.56 72.50 74.72</cell><cell>77.58</cell><cell cols="3">77.58 77.75 79.36</cell></row><row><cell>AdaBN [18]</cell><cell>75.56</cell><cell cols="3">74.17 74.72 72.22</cell><cell>76.36</cell><cell cols="3">77.41 74.96 77.41</cell></row><row><cell>MCD [32]</cell><cell>71.67</cell><cell cols="3">70.00 74.44 73.89</cell><cell>76.18</cell><cell cols="3">68.30 78.81 79.34</cell></row><row><cell>TA 2 N [2]</cell><cell>71.11</cell><cell cols="3">70.00 70.83 77.22</cell><cell>76.36</cell><cell cols="3">70.75 76.89 80.56</cell></row><row><cell>TA 3 N [2]</cell><cell>71.94</cell><cell cols="3">70.00 69.72 78.33</cell><cell>76.36</cell><cell cols="3">70.75 77.23 81.79</cell></row><row><cell>ABG</cell><cell>79.17</cell><cell cols="3">75.56 75.56 76.67</cell><cell>85.11</cell><cell cols="3">84.24 83.36 81.79</cell></row><row><cell>Target Only</cell><cell>80.56</cell><cell>-</cell><cell>-</cell><cell>82.78</cell><cell>92.12</cell><cell>-</cell><cell>-</cell><cell>94.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Experimental results under the semi-supervised setting on the Kinetics-Gameplay dataset. 16.29 16.29 16.29 14.82 14.82 14.82 14.82 DANN [6] 49.67 56.74 60.48 65.02 33.24 40.05 40.44 43.38 JAN [21] 47.40 53.94 60.35 61.28 32.69 22.95 41.31 32.75 AdaBN [18] 55.54 59.95 64.62 67.56 44.82 47.67 47.73 48.00 MCD [32] 47.53 52.60 57.68 59.95 36.29 40.08 41.13 42.05 TA 2 N [2] 57.14 60.08 64.09 65.02 42.39 44.82 44.39 45.66 TA 3 N [2] 56.61 62.35 63.02 63.95 43.25 44.27 44.02 42.05 Semi-ABG 57.28 64.35 65.42 68.36 59.80 62.19 62.46 63.67 Semi-HABG 61.15 65.29 67.29 70.36 60.39 62.54 63.36 63.98</figDesc><table><row><cell></cell><cell cols="4">Kinetics?Gameplay</cell><cell cols="4">Gameplay?Kinetics</cell></row><row><cell>Method</cell><cell>30%</cell><cell>50%</cell><cell>70%</cell><cell>90%</cell><cell>30%</cell><cell>50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell cols="9">Source Only 16.29 Target Only 54.21 58.88 61.55 66.62 39.96 42.54 44.58 44.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Removing the bipartite graphs, the ABG w/o Graph suffers a drop dramatically compared with the full model. The ABG w/o L d is the variant without the conditional adversarial learning, decreasing the recognition accuracy by 8.4% and 2.95% on average for the UCF?HMDB and HMDB?UCF tasks, respectively. The ABG w/o L t y refers to the variant without the entropy loss for target data, which triggers a slight decrease on the model performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The ablation performance of the proposed ABG and HABG models on the full UCF-HMDB dataset.UCF?HMDB f ull HMDB?UCF f ull</figDesc><table><row><cell>Method</cell><cell>AvgPool</cell><cell>TRN</cell><cell>AvgPool</cell><cell>TRN</cell></row><row><cell>ABG w/o Graph</cell><cell>71.39</cell><cell>73.89</cell><cell>74.96</cell><cell>74.61</cell></row><row><cell>ABG w/o L d ABG w/o L t y</cell><cell>72.78 78.33</cell><cell>70.00 75.83</cell><cell>82.14 82.67</cell><cell>79.86 81.79</cell></row><row><cell>HABG</cell><cell>80.00</cell><cell>76.94</cell><cell>82.49</cell><cell>80.21</cell></row><row><cell>ABG</cell><cell>79.17</cell><cell>76.67</cell><cell>85.11</cell><cell>81.79</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by ARC DP 190102353.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain Adaptation on the Statistical Manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tafazzoli</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2481" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekwon</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12743</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CANZSL: Cycle-Consistent Adversarial Networks for Zero-Shot Learning from Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Yangyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly Supervised Dense Event Captioning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3063" to="3073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Machine Learning (ICML)</title>
		<meeting>Int. Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Domain Adaptation in Action Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshad</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">264</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-Scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Kinetics Human Action Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Learning Representations</title>
		<meeting>Int. Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense-Captioning Events in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Computer Vision (ICCV</title>
		<meeting>Int. Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Computer Vision (ICCV</title>
		<meeting>Int. Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive Story Maker: Tagged Video Retrieval System for Video Re-creation Service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Uk</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minho</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun-Joong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeong-June</forename><surname>Hahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1270" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive Batch Normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global Context-Aware Attention LSTM Networks for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3671" to="3680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning with Joint Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Machine Learning (ICML)</title>
		<meeting>Int. Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from the Past: Continual Meta-Learning with Bayesian Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5021" to="5028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2341" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Progressive Graph Learning for Open-Set Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning, ICML</title>
		<meeting>International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attend and Interact: Higher-Order Object Interactions for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6790" to="6800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GCAN: Graph Convolutional Adversarial Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="8266" to="8276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV</title>
		<meeting>European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video-based Person Re-identification via Self-Paced Learning and Deep Reinforcement Learning Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1562" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial Cross-Domain Action Recognition with Co-Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10405</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Action Recognition Model from Depth and Skeleton Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Computer Vision (ICCV)</title>
		<meeting>Int. Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5833" to="5842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-domain action recognition via collective matrix factorization with graph Laplacian regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoubiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="119" to="126" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Computer Vision (ICCV</title>
		<meeting>Int. Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Learning Representations (ICLR</title>
		<meeting>Int. Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Memory Modelling for Video Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV</title>
		<meeting>European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human Consensus-Oriented Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="659" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Collaborative Discrete Hashing with Semantic-Invariant Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR. 905???908</title>
		<meeting>International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR. 905???908</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Personalized Hashtag Recommendation for Micro-videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuzheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1446" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Richang Hong, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dual many-toone-encoder-based transfer learning for cross-dataset human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PA3D: Pose-Action 3D Machine for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7922" to="7931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="945" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Catching the Temporal Regionsof-Interest for Video Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia (MM)</title>
		<meeting>ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On Learning Invariant Representations for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Machine Learning (ICML)</title>
		<meeting>Int. Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal Relational Reasoning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="831" to="846" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
