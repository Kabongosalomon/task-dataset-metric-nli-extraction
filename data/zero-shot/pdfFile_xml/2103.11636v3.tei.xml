<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimization for Arbitrary-Oriented Object Detection via Representation Invariance Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimization for Arbitrary-Oriented Object Detection via Representation Invariance Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Arbitrary-oriented objects exist widely in natural scenes, and thus the oriented object detection has received extensive attention in recent years. The mainstream rotation detectors use oriented bounding boxes (OBB) or quadrilateral bounding boxes (QBB) to represent the rotating objects. However, these methods suffer from the representation ambiguity for oriented object definition, which leads to suboptimal regression optimization and the inconsistency between the loss metric and the localization accuracy of the predictions. In this paper, we propose a Representation Invariance Loss (RIL) to optimize the bounding box regression for the rotating objects. Specifically, RIL treats multiple representations of an oriented object as multiple equivalent local minima, and hence transforms bounding box regression into an adaptive matching process with these local minima. Then, the Hungarian matching algorithm is adopted to obtain the optimal regression strategy. We also propose a normalized rotation loss to alleviate the weak correlation between different variables and their unbalanced loss contribution in OBB representation. Extensive experiments on remote sensing datasets and scene text datasets show that our method achieves consistent and substantial improvement. The source code and trained models are available at https://github.com/ming71/RIDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Arbitrary-oriented object detection has a wide range of application scenarios, such as scene text detection <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b5">6]</ref>, face detection <ref type="bibr" target="#b28">[28]</ref>, object detection in remote sensing images <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b39">39]</ref>, and 3D object detection <ref type="bibr" target="#b46">[46]</ref>. In recent years, with the breakthroughs made by convolutional neural networks (CNNs) in the field of object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25]</ref>, a series of CNN-based rotation detectors have been proposed to achieve high-performance * Corresponding author oriented object detection <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>Unlike generic object detection that uses the horizontal bounding box to represent the objects, rotation detectors usually adopt the oriented bounding box (OBB) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b19">19]</ref> or the quadrilateral bounding box (QBB) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b16">16]</ref> to describe the rotating objects, which induces the representation ambiguity. Whether using OBB or QBB, representation ambiguity indicates that an object can be represented in many different forms. These ambiguous representations constitute the representation space of a ground-truth (GT) object g, denoted as ?(g) = {g 0 , g 1 , g 2 , ...}. Ideally, all representations in ?(g) are equivalent local optimal solutions in the regression optimization process. However, the loss function of the current rotation detectors can only converge to the given GT representation g 0 , and the rest g k ? ?(g) will cause a sharp increase in regression loss. The incorrect loss metric cannot truly reflect the localozation quality of the predictions, which makes the regression process hard to converge and further degrades the detection performance. We next separately analyze the influence of representation ambiguity on the two forms of bounding box definitions.</p><p>QBB directly uses four vertices of the quadrilateral to represent the oriented objects. The unordered vertices have P 4 4 = 24 permutations, so theoretically there are 24 quadrilateral representations for one box in the representation space ?. These diverse representations provide more easily searchable local minima for regression optimization theoretically, but the existing methods do not consider all feasible instances, only to learn a given fixed-sequence quadrilateral representation. In this case, ambiguous representations are not effectively utilized but causes additional learning costs. As shown in the left side of <ref type="figure">Fig. 1</ref>, the predicted quadrilateral is close to the ground-truth box, and the ideal regression strategy is {(a ? D), (b ? A), (c ? B), (d ? C)}. But most current detectors will follow the established sequence for regression: {(a ? A), (b ? B), (c ? C), (d ? D)}. The suboptimal regression strategy would lead to a sharp loss boost, and thereby increasing the difficulty of network convergence. Due to boundary problems <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b37">37]</ref> of angle definition of the oriented rectangle box, we suggest that the representation ambiguity still exists in OBB. On the one hand, the periodicity of angle brings a lot of equivalent representations. On the other hand, the interchangeability between width and height also derives many representations whose angles exceed the boundary. The current detectors cannot handle these ambiguous representations with out-ofbounds angles well. For example, the GT box in the right of <ref type="figure">Fig. 1</ref> is given as g 0 : (0, 0, 100, 600, ?30 ? ). It can also be represented as g 1 : (0, 0, 600, 100, ?120 ? ) if without restriction of the angle boundary. For the preset anchor a : (0, 0, 600, 100, ?90 ? ), the ideal optimization target is g 1 , which only requires a small counterclockwise angle offset. However, due to angle limitation in OBB, it can be only regressed to the suboptimal representation g 0 , which would make the deviation of width, height, and angle very large, and the regression loss hard to converge. Besides, the contribution of different variables to the regression accuracy is unequal in OBB, and thus careful weighting is required to balance the loss terms of different variables.</p><p>Multiple representations due to representation ambiguity are essentially not bad for the regression task, because they would produce multiple equivalent local minima, and falling into any one of them would reach equivalently good convergence status, which would make the optimization process more flexible, and alleviate the problem of sharp boost and discontinuity in regression loss. Current work, unfortunately, has not fully realized the above benefits and the rotation detectors usually suffer from suboptimal regression strategies that affect the network convergence and degrade detection performance.</p><p>In this paper, we propose a Representation Invariance Loss (RIL) that treats the bounding box regression as an adaptive matching process with the local minima of multiple representations, and then searches for the optimal regression strategy dynamically through the Hungarian algorithm during training. Specifically, for QBB, RIL transforms the point regression problem into a dynamic sequential-free vertices matching process, and then Hungarian loss is used to improve convergence and achieve superior performance. For OBB, the RIL allows the predictions with the out-of-bounds angles and adaptively searches for the optimal regression strategy from all ambiguous representations. Besides, a normalized rotation loss is adopted to alleviate the inconsistency between different variables.</p><p>Rotation detectors trained with RIL treat the ambiguous representations as the equivalent local minima of the regression loss. These local minima provide better convergence than the unique minima defined by the GT representation. Therefore, during the training process, the regressor can dynamically select the optimal local minima in the representation space as the regression target according to the loss function. Based on the proposed RIL, we further build two Representation Invariant Detectors (RIDet) : RIDet-O and RIDet-Q, to prove the superiority of the proposed method. In summary, the main contribution is as follows:</p><p>1) We point out representation ambiguity in arbitraryoriented object detection and analyzed the problems caused by multiple representations from the perspective of regression optimization.</p><p>2) The novel representation invariance loss (RIL) is proposed to transforms the bounding box regression task into an optimal matching process. RIL not only solves the inconsistency between the loss and localization quality caused by ambiguous representations, but also utilizes these local minima to improve network convergence and achieve better performance.</p><p>3) RIL can be easily applied to existing methods without any additional overhead. Extensive experiments on multiple rotation detectors and datasets prove the superiority of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Arbitrary-Oriented Object Detection</head><p>Arbitrary-oriented objects appear widely in natural scenes, so oriented object detection has gradually become a hot topic in computer vision. In recent years, many advanced frameworks have been proposed to achieve accurate rotating object detection <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>Unlike generic object detection that uses the horizontal bounding box to represent objects, OBB and QBB are usually adopted for oriented object detection. For example, RoI Transformer <ref type="bibr" target="#b1">[2]</ref> learns a rotated region of the horizontal region of interest to improve detection performance. RRPN <ref type="bibr" target="#b18">[18]</ref> generates inclined proposals with text orienta-  tion information to fit into the text region more accurately. R 3 Det [34], S 2 A-Net <ref type="bibr" target="#b4">[5]</ref>, and CFC-Net <ref type="bibr" target="#b19">[19]</ref> obtain oriented anchors from horizontal anchors to achieve better spatial alignment with oriented objects. TextBoxes++ <ref type="bibr" target="#b11">[11]</ref> predicts the vertex offsets from predefined anchor boxes to detect arbitrary-oriented text. Xu et al. <ref type="bibr" target="#b32">[32]</ref> proposed a novel framework to regress four ratios on each corresponding side to detect multi-orientation objects.</p><p>There are also some other arbitrary-oriented object representation methods, such as polar coordinates <ref type="bibr" target="#b47">[47]</ref>, middle lines <ref type="bibr" target="#b30">[30]</ref>, ellipse representation <ref type="bibr" target="#b14">[14]</ref>, and box boundaryaware vectors <ref type="bibr" target="#b42">[42]</ref>. Nevertheless, these methods are relatively complex and not general, so the mainstream methods are still OBB and QBB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Representation Problems of Oriented Objects</head><p>Inappropriate representation of rotating objects will give rise to many problems. Some previous work summarized the issues as the boundary problem <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b37">37]</ref> in OBB representation, and suggested that this will cause a discontinuous regression loss and hinder the convergence of network training. To solve the issues, Circular Smooth Label <ref type="bibr" target="#b37">[37]</ref> and Densely Coded Labels <ref type="bibr" target="#b33">[33]</ref> transform the angle regression task into a classification task to avoid the discontinuity of regression loss. SCRDet <ref type="bibr" target="#b40">[40]</ref> adopts IoU-Smooth-L 1 loss to alleviate the sharp increase of loss caused by angle boundary. RSDet <ref type="bibr" target="#b24">[24]</ref> utilizes a modulation loss function to solve the rotation-sensitive error of out-of-bounds angles.</p><p>There are also some methods adopt different oriented object representations to avoid the confusion issue of sequential label points under QBB representation <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b2">3]</ref>. Specifically, SBD <ref type="bibr" target="#b16">[16]</ref> predicts key edges of the vertices, and uses an extra combination learning to determine the quadrilateral. Gliding Vertex <ref type="bibr" target="#b32">[32]</ref> and TOSO <ref type="bibr" target="#b2">[3]</ref> predict the horizontal bounding box of the object and the relative gliding offset to represent the quadrilateral bounding box.</p><p>However, these methods simply attemp to eliminate the problems caused by the nonunique definitions of oriented objects, but ignoring that these ambiguous representations are essentially equivalent local optima for regression opti-  mization. Unlike the previous work, our method utilizes diverse representations to search for optimal regression strategy. In this case, the representation ambiguity not only no longer causes the various representation problems mentioned above, but also helps to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Representation Invariant Detector</head><p>In this section, we first build a baseline and then design Representation Invariance Loss (RIL) for OBB methods and QBB methods, respectively. Then, we apply RIL to the baseline detector to construct Representation Invariant Detector (RIDet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cas-RetinaNet for Orineted Object Detection</head><p>We build the Cascaded RetinaNet (Cas-RetinaNet) as the baseline to achieve high-efficiency oriented object detection. Cas-RetinaNet uses horizontal preset anchors, and the Oriented Anchor Refinement Module (O-ARM) is adopted to obtain high-quality training samples. The overall structure of our model is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The Cas-RetinaNet is built based on RetinaNet <ref type="bibr" target="#b13">[13]</ref>, in which FPN is applied to extract multi-scale features. The detection head performs classification and regression based on the high-quality anchors generated by O-ARM.</p><p>The training loss of Cas-RetinaNet consists of three parts: classification loss, anchor refining loss, and bounding box regression loss. The loss function is as follows:</p><formula xml:id="formula_0">L = 1 N i F L (pi, p * i ) + 1 Np 1 i L ref (t i , t * i ) + 1 Np 2 i LRI (b i , t * i ) ,<label>(1)</label></formula><p>in which F L denotes focal loss <ref type="bibr" target="#b13">[13]</ref> for classification task, and L ref is smooth-L 1 loss for oriented anchor refinement. p * and t * are ground-truth labels for classification, anchor  refinement and box regression, while p, t, b are corresponding predictions, respectively. N is the number of training samples, and N p1 , N p2 denote the number of positives in the O-RAM stage and the detection stage, respectively. L RI is the representation invariance loss, which will be elaborated in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Quadrilateral Regression as Point Assignment</head><p>Quadrilateral bounding boxes for most arbitrary-oriented objects are convex polygons, such as scene text <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b8">9]</ref>, remote sensing objects <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b48">48]</ref>, human faces <ref type="bibr" target="#b28">[28]</ref>, and retail products <ref type="bibr" target="#b22">[22]</ref>. Therefore, the sequence of vertices is redundant to define a convex quadrilateral. Even so, the current detectors still learn the one-to-one matching between the given GT representation and the predictions as illustrated in the upper part of <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>Inspired by the Hungarian method for solving the assignment problem <ref type="bibr" target="#b10">[10]</ref>, we treat the quadrilateral regression as a dynamic sequential-free points assignment problem. As shown in <ref type="figure" target="#fig_6">Fig. 4</ref>, we denote the GT quadrilateral as an unordered point set as g = {A, B, C, D} and the predicted quadrilateral as p = {a, b, c, d}. The bounding box regression process can be transformed to the distance optimization between two point sets. We further use Hungarian loss to measure the distance between two sets, and thus the representation invariance loss (RIL) for QBB is as follows:</p><formula xml:id="formula_1">L RI (p, g) = min ??? v?p, v * ?g? L reg (v, v * ),<label>(2)</label></formula><p>in which ? is permutations of {1, 2, 3, 4}, it represents all feasible sequence of the quadrilateral. ? is a permutation in ?, and g ? represents a certain kind of quadrilateral. L reg denotes smooth-L 1 loss here.</p><p>With the RIL defined above, the network tends to search for the permutation with the minimal loss value among all ambiguous representations for bounding box optimization. For example, the vertices of the prediction in <ref type="figure" target="#fig_6">Fig. 4</ref> are very close to the GT quadrilateral, but the unmatched point sequence leads to a large loss for the common rotation detectors. Our RIL can perform sequential-free auto-matching to achieve the optimal regression process and accurately predict the position of the vertices. Note that the training pro- cess is constantly changing, so the optimal regression strategy is not fixed, and it may switch between different sequences in ? during training. RIL can adaptively select the most suitable representation according to the current optimization situation, and thus can achieve better convergence and performance.</p><p>The Hungarian method, which is representation invariant, no longer focuses on the sequence of points, and adaptively searches for the nearby local minimum in all possible permutations to achieve better convergence quality. Moreover, each four-point set determines at most one circumscribed convex quadrilateral. Therefore, in the inference stage, the network allows multiple predictions corresponding to ambiguous representations of the GT objects, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Out-of-bounds Matching for OBB</head><p>For the OBB, the boundary of the angle definition constrains the search space of regression loss, and thus the ambiguous representations cannot be effectively utilized to optimize the regression process. Based on the above observations, a Hungarian method based out-of-bounds matching strategy is proposed to solve the boundary constrains. Specifically, we transform the oriented bounding box regression into the adaptive optimal matching between the predictions and the representation space of GT.</p><p>Given an object g denoted as (cx, cy, w, h, ?), its representation space ?(g) is as follows:</p><formula xml:id="formula_2">?(g) = {g 0 , g 1 , g 2 , ..., g i },<label>(3)</label></formula><p>in which the instances are defined as:</p><formula xml:id="formula_3">g i = (cx, cy, w, h, ? + i? 2 ), i ? {2k|k ? Z} (cx, cy, h, w, ? + i? 2 ), i ? {2k + 1|k ? Z}<label>(4)</label></formula><p>Further the RIL for OBB is as follows:</p><formula xml:id="formula_4">L RI (p, g) = min gi??(g) L nrl (p, g i ),<label>(5)</label></formula><p>where p is the prediction, and L nrl denotes the normalized rotation loss for regression which will be elaborated next. RIL guides the prediction to match the optimal instance in ?(g) to search for the nearby local optimum. Noted that the periodicity of the angle leads to an infinite number of possible representations, it is impossible to exhaust all of them for the best matching. Therefore, we further propose a normalized loss to reduce the dimensionality of ?(g).</p><p>Given the anchor (x 1 , y 1 , w 1 , h 1 , ? 1 ) and GT box (x 1 , y 1 , w 2 , h 2 , ? 2 ) with overlapping center points as shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, the orientations of the two bounding boxes form an acute angle ??. This angle can be approximated by the projection of its area and the direction of the reference axis. Taking the orientation of the GT as the reference line, l 1 and l 2 are the projections of the overlapping parallelograms, and the intersection area S 0 and projected area S 1 are as follows:</p><formula xml:id="formula_5">l 1 = h 1 sin ? , l 2 = h 2 tan ? , S 0 = l 1 ? h 2 , S 1 = (l 1 + l 2 ) ? h 2 .<label>(6)</label></formula><p>Then the IoU mapping of the angle is as:</p><formula xml:id="formula_6">L ? = S 0 S 1 ? 0.5 = 1 1 + ? ? cos ? ? 0.5,<label>(7)</label></formula><p>in which ? = min( h1 h2 , h2 h1 ), and L ? ? [0, 0.5]. Only when the heights of the anchor and the GT box are equal, and the angle deviation is equal to 0, L ? reaches the minimum value of 0. With this metric, the representation space of g can be simplified into ?(g) = {g 0 , g 1 }. In addition, as shown in the right of <ref type="figure" target="#fig_7">Fig. 5</ref>, L ? is not sensitive to the aspect ratio of the bounding box itself, and it can converge steadily to objects of different shapes.</p><p>OBB representation requires five variables to define the oriented rectangular box. The deviation of IoU caused by the offset of a specic variable varies with the scale and aspect ratio of the GT box, as illustrated in the right of the <ref type="figure" target="#fig_8">Fig. 6(a)</ref>. Besides, the IoU deviations caused by the same offset of different variables are also different, as shown in <ref type="figure" target="#fig_8">Fig. 6(b)</ref>. As a result, there will be inconsistencies between the loss function and the detection metric, which has also been discussed in some previous work <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b38">38]</ref>. Therefore, it is necessary to normalize the loss of variable offsets, just like L ? , to obtain a consistent and invariant loss metric.</p><p>We further constrain center coordinates and shape of the oriented bounding box to balance the loss contribution between different variables. The center loss L c is as follows:</p><formula xml:id="formula_7">L c = c g ? c a 2 0.5 ? d 2<label>(8)</label></formula><p>in which c g and c a are the center coordinates of GT and anchor respectively, d = (w, h) denotes the shape of the GT box. During the label assignment phase, only the anchors whose center points fall within the GT box may be selected as positive samples for regression <ref type="bibr" target="#b44">[44]</ref>. In this way, L c is normalized in [0, 1]. For example, small objects are more susceptible to inaccurate center point prediction with smooth-L 1 loss. Our normalized center loss is no longer sensitive to the shape of GT box, and thus it is conducive to the detection performance of small objects. The same scale-sensitive problem also exists in shape regression. As discussed by Redmon et al. <ref type="bibr" target="#b25">[25]</ref>, small deviations in large boxes matter less than in small boxes. Therefore, we utilize the IoU of the horizontal box as the scaleindependent shape loss, which is defined as follows:</p><formula xml:id="formula_8">L s = min gi??(g) hIoU (p, g i ),<label>(9)</label></formula><p>in which hIoU calculates the IoU with two overlapping centers without angle consideration, it is defined as follows:</p><formula xml:id="formula_9">hIoU (b 1 , b 2 ) = min(w 1 , w 2 ) ? min(h 1 , h 2 ) max(w 1 , w 2 ) ? max(h 1 , h 2 ) .<label>(10)</label></formula><p>Obviously, the range of the shape metric is also in [0, 1].</p><p>The shape loss only evaluates the similarity of the shape and does not specify the width and height, which can speed up the convergence. The prediction of width and height is embedded in the angle loss to achieve an accurate description of the oriented rectangle box jointly. Then the normalized rotation loss is as follows:</p><formula xml:id="formula_10">L nrl = L ? + L c + L s ,<label>(11)</label></formula><p>This novel RIL effectively uses entire representation space to optimize the regression process. Simultaneously, the normalized rotation loss can better alleviate the inconsistency of the contribution of different variables to the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct extensive experiments on multiple datasets to prove the effectiveness of the proposed method, including three remote sensing datasets: HRSC2016 <ref type="bibr" target="#b17">[17]</ref>, UCAS-AOD <ref type="bibr" target="#b48">[48]</ref>, DOTA <ref type="bibr" target="#b31">[31]</ref>, and two scene text datasets: IC-DAR2015 <ref type="bibr" target="#b8">[9]</ref>, MSRA-TD500 <ref type="bibr" target="#b41">[41]</ref>. HRSC2016 <ref type="bibr" target="#b17">[17]</ref> is a challenging ship detection dataset, which uses the oriented bounding box to annotate remote sensing ships. It contains 1,061 images with more than 20 categories of various ships. The dataset is divided into training, validation and test set, which contain 436, 181, 444 images respectively. UCAS-AOD <ref type="bibr" target="#b48">[48]</ref> is an aerial image dataset for oriented aircraft and car detection. It contains 1,510 images, including 1,000 airplane images and 510 car images. Since no official dataset division strategy is provided, we randomly divide it into the training set, the validation set and the test set with a ratio of 5:2:3. DOTA <ref type="bibr" target="#b31">[31]</ref> is the largest public dataset with oriented bounding box annotations for object detection in remote sensing imagery. It contains totally 2,806 large size images (e.g. 4000?4000 pixels). There are 15 categories in total. Half of the images are the training set, 1/6 as the validation set, and 1/3 as the test set. Since that images in DOTA are too large, we crop images into 800?800 patches with the stride set to 200.</p><p>ICDAR2015 <ref type="bibr" target="#b8">[9]</ref> is the scene text dataset for the incidental scene text challenge in ICDAR 2015 Robust Reading Competition. It contains 1,500 images, of which 1,000 for training and 500 for testing. MSRA-TD500 <ref type="bibr" target="#b41">[41]</ref> dataset consists of 300 training images and 200 testing images. It contains text in both English and Chinese. The text regions are annotated with oriented bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Deteails</head><p>We use the Cas-RetinaNet as the baseline model, and RIL is applied to the baseline to build a Representation Invariant Detector (RIDet). It is further divided into RIDet-O for OBB and RIDet-Q for QBB. We use Pytorch [23] to implement the above methods.</p><p>All images are resized to 416?416 or 800?800 for training and testing. Horizontal anchors are preset with aspect ratios of {0.5, 1, 2}, and scales of {2 0 , 2 1/3 , 2 2/3 }. The IoU thresholds used to select training samples are 0.4 in O-RAM and 0.5 in the detection stage. We use Adam optimizer for training with the initial learning rate set to 1e-4. We train the models in 100 epochs for HRSC2016, 24 epochs for DOTA, and 40 epochs for UCAS-AOD, ICDAR2015 and MSRA-TD500. All models are trained on RTX 2080 Ti GPUs with batch size set to 8. Random flip, rotation, and HSV colour  <ref type="table">Table 2</ref>. Performance improvement of RIL on different methods on HRSC2016 dataset. space transformation are adopted for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation of normalized rotation loss for OBB</head><p>Experiments conducted on the two scene texts datasets confirmed the effectiveness of RIL under OBB representation. The experimental results are shown in Tab. 1. The baseline model is the Cas-RetinaNet detailed above. The regression head outputs the oriented rectangular bounding box in the experiments in this section. Note that we do not use complex data augmentation in this part.</p><p>RIDet-O achieves the F-measure of 77.6% and 78.2% on the ICDAR2015 and MSRA-TD500, which are improved by 1.7% and 2.1% compared to the baseline model, respectively. The optimal matching strategy allows the out-ofbounds predictions to search for the optimal local minima for regression, which is conducive to network convergence, and thus better performance can be achieved. It improves the F-measure by 0.6% and 0.5%. The normalized rotation loss further anchieves the increase of 0.7% and 0.9% on two scene text datasets, which balances the contribution of different variables and alleviates the inconsistency between loss and detection metric. Finally, selecting the anchors with the center point inside the GT box for training improves by 0.4% and 0.7%. These high-quality positives not only provide better priori about the position of the center points but also help constrain the range of center loss, which has also been demonstrated in ATSS <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Evaluation on different models</head><p>We further conduct experiments on different detectors to verify the generalization of RIL. The experimental results on HRSC2016 <ref type="bibr" target="#b17">[17]</ref> and DOTA <ref type="bibr" target="#b31">[31]</ref>   <ref type="table">Table 3</ref>. Comparison with some related methods on HRSC2016.</p><p>into the optimal matching process between predction and the representation space of GT can be beneficial in different representation methods. The performance in Tab. 5 further confirms this point. RIDet-Q and RIDet-O achieved 2.26% and 1.81% improvement on the corresponding baseline detector, respectively. S 2 A-Net <ref type="bibr" target="#b4">[5]</ref> is an advanced rotation detector that achieves 76.38% mAP on the DOTA . Integrating RIL to S 2 A-Net improves performance by 1.24% to achieve the mAP of 77.62%, which further proves the generalization and superiority of our method. DAL <ref type="bibr" target="#b21">[21]</ref> and CFC-Net <ref type="bibr" target="#b19">[19]</ref> are also currently advanced detectors that achieve accurate oriented object detection. But these detectors will still be confused by ambiguous representations of GT. After training with RIL, the detection performance is further improved by 1.9% and 1.4% on DAL <ref type="bibr" target="#b21">[21]</ref> and CFC-Net <ref type="bibr" target="#b19">[19]</ref>, respectively. It indicates that as long as there is representation ambiguity, our method can obtain consistent performance gains.</p><p>Extensive experiments on different models and representations prove that our method can effectively optimize the representation learning method for the rotation detectors and achieve stable performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Comparison with related methods</head><p>We have conducted comparisons of the performance of different models in related work on the HRSC2016 dataset , and the results are shown in Tab. 3.</p><p>RIDet-O achieves the highest mAP of 89.63% among the compared methods. IoU-Smooth <ref type="bibr" target="#b40">[40]</ref> and MR Loss <ref type="bibr" target="#b24">[24]</ref> solve the loss discontinuity caused by multiple representations and achieve the mAP of 82.59% and 83.60%, respectively. However, the weight of skew IoU in IoU-Smooth Loss <ref type="bibr" target="#b40">[40]</ref> is not differentiable, and MR Loss <ref type="bibr" target="#b24">[24]</ref> only considers limited redundant representations. DCL <ref type="bibr" target="#b33">[33]</ref> converts the angle regression into fine-grained angle classification to eliminate the problems caused by the angle boundary, and thus achieving the mAP of 89.46%. But it brings additional computational overhead and cannot be directly used to other detectors. Compared with these works with OBB, our method can be directly applied to mainstream rotation detectors to achieve the optimal regression and balanced training without any inference overhead.</p><p>Among the compared quadrilateral detectors, RIDet-Q achieves the highest mAP of 89.10%, which proves the effectiveness of our RIL. MR Loss <ref type="bibr" target="#b24">[24]</ref> achieves the mAP  The visualization results in <ref type="figure" target="#fig_9">Fig. 7(a)</ref> show that the convergence of the angle regression is slow in the baseline model which only learns a specific representation. RIDet treats all feasible representations as equivalent local optimums. Therefore the regression loss converges faster, and better performance can be achieved under the same number of iterations, as shown in <ref type="figure" target="#fig_9">Fig. 7(b</ref>     can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Results on UCAS-AOD</head><p>The UCAS-AOD dataset contains a large number of densely arranged cars and airplanes. The experimental results on the UCAS-AOD dataset are shown in Tab. 6. RIDet-Q and RIDet-O achieve the mAP of 89.23% and 89.62%, respectively. RIDet outperforms other advanced rotation detectors such as CFC-Net <ref type="bibr" target="#b19">[19]</ref> and RoI Transformer <ref type="bibr" target="#b1">[2]</ref> in our experiments, which proves the superiority of our method. The visualization results of vehicle detection of RIDet-Q are shown in <ref type="figure" target="#fig_9">Fig. 7</ref>. RIL treats the quadrilateral regression as the point matching problem, which signi cantly improves detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we discuss the suboptimal regression problem caused by the ambiguous representations in oriented object detection. The Representation Invariance Loss (RIL) is proposed to solve the representation ambiguity and improve network convergence to improve detection perfor- mance. RIL treats multiple representations as equivalent local minima, and then transforms the bounding box regression task into the optimal matching process between predictions and these local minima. Extensive experiments on multiple datasets proved the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The Mapping between Object and Representation</head><p>We denote an object as a set O with one element, and its all possible representations as a set R. Ideally, f : R ? O is a bijective function that will not cause ambiguity. However, due to the disorder of the quadrilateral under the QBB and the interchangeability between the width and height under the OBB, this conclusion does not hold. The existence of multiple representations makes f : R ? O a surjection. And specific representation can specify the unique object. Therefore, we can use multiple representations as the local minimum for optimization. This conclusion is obviously established in the OBB representation. For QBB representation, this conclusion is only valid when the ground-truth box is a convex quadrilateral. Fortunately, almost all oriented objects that use quadrilateral labels are convex polygons, such as arbitrary-oriented scene text, remote sensing objects, human faces, and retail products. Therefore, our RIL can effectively use disorder vertices to optimize regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Eliminating Representation Ambiguity via Optimal Matching for OBB</head><p>The matching-based method can effectively eliminate the problems caused by ambiguous representation, which has been introduced in the main body of the paper. Here is a supplementary explanation for the elimination of ambiguity under the OBB representation.</p><p>As shown in the left of <ref type="figure" target="#fig_10">Fig. 8</ref>, RIL searches for the nearest representation when optimizing the regression loss and  fits this minimum value with boundary-free predictions. In this way, the loss function will not increase suddenly but will fit the GT box in the fastest way.</p><p>The middle of <ref type="figure" target="#fig_10">Fig. 8</ref> shows the solution of RIL in the square-like problem <ref type="bibr" target="#b33">[33]</ref>. In this case, due to the extreme similarity of width and height, there are two local optima. However, the constraints of definition boundary of the angle hinder free regression of angle, and current detectors cannot  The illustration on the right in <ref type="figure" target="#fig_10">Fig. 8</ref> is similar to the one on the left, so it won't be elaborated here. The root cause of the ambiguity in the 180 ? representation is the boundedness of the angle definition, while the 90 ? representation also suffers from the interchangeability between the edges and the angle. It is worth noting that the normalized rotation mapping loss uses the angular deviation between the two boxes to optimize the orientation and constrain it into [0, 90 ? ]. Therefore, it can independently solve the representation ambiguity caused by the out-of-bounds definition of angle, such as the situation on the right side of <ref type="figure" target="#fig_10">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Visualization of Detection Results</head><p>We visualized some detection results on different datasets, including three remote sensing data sets: HRSC2016 (shown in <ref type="figure" target="#fig_11">Fig. 9</ref>), UCAS-AOD (shown in <ref type="figure" target="#fig_2">Fig. 12</ref>), DOTA (shown in <ref type="figure" target="#fig_4">Fig. 13</ref>), and two scene text detection datasets: ICDAR2015 (shown in <ref type="figure" target="#fig_12">Fig. 10</ref>), MSRA-TD500 (shown in <ref type="figure" target="#fig_13">Fig. 11</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>30 Figure 1 .</head><label>301</label><figDesc>Examples of suboptimal regression process caused by the representation ambiguity under QBB (left) and OBB (right). g0 is the given GT representation, and g1 is one of the ambiguous representations of GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The overall structure of Cas-RetinaNet. C represents the number of classes, and A denotes the number of predefined anchors. N is determined by the parameters of the representation, N = 5 for OBB, while N = 8 for QBB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of regression results of RIL and smooth-L1 loss under QBB. RIL treats ambiguity representations as equivalent local minima, allowing the detector to regress and output sequential-free predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of matching between GT quadrilateral box and predicted quadrilateral box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of normalized angle mapping loss. (a) shows the definition of normalized angle mapping loss. (b) reveals its superiority for measuring angular deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of inconsistency between loss contribution and IoU variation. (a) shows the same offsets cause the inconsistent IoU changes due to the different aspect ratios of the GT box. (b) reveals that the same contribution of different variables to the loss cause different IoU changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of detection results between detectors with smoth-L1 loss (a) and RIL (b) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>The bounding box regression process of oriented object under QBB (top) and OBB (bottom) characterization. The red box is the GT box, and the blue box is the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of detection results on HRSC2016 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of detection results on ICDAR2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of detection results on MSRA-TD500 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Visualization of detection results on UCAS-AOD dataset be optimized efficiently. RIL can effectively use all redundant representations for optimal matching and resolve this problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are shown in Tab. 2 and Tab. 5, respectively.Experimental results in Tab. 2 show that RIL improves the mAP by 2.3% on RetinaNet with OBB, and 2.0% on Cas-RetinaNet with QBB on HRSC2016 . It proves that the strategy of transforming the bounding box regression task</figDesc><table><row><cell>BBox</cell><cell>Models</cell><cell cols="2">mAP BBox</cell><cell>Model</cell><cell>mAP</cell></row><row><cell></cell><cell cols="2">IoU-Smooth [40] 82.59</cell><cell></cell><cell>MR Loss [24]</cell><cell>86.50</cell></row><row><cell>OBB</cell><cell>MR Loss [24] DCL [33]</cell><cell>83.60 89.46</cell><cell>QBB</cell><cell cols="2">Gliding Vertex [32] 88.20 BBAVectors [42] 88.60</cell></row><row><cell></cell><cell>RIDet-O</cell><cell>89.63</cell><cell></cell><cell>RIDet-Q</cell><cell>89.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons with advance methods on HRSC2016.</figDesc><table><row><cell>of 86.5%, which also consider the order of the vertices for</cell></row><row><cell>QBB. The main difference is that MR loss traverses the four</cell></row><row><cell>ordered point sequences to find the suitable representation,</cell></row><row><cell>which make it limited to the ordered quadrilateral represen-</cell></row><row><cell>tation, and the traversal of the possible permutations is both</cell></row><row><cell>time-consuming and insufficient. The offset prediction in</cell></row><row><cell>Gliding Vertex [32] depends on the accurate prediction of</cell></row><row><cell>the corresponding horizontal box, which further leads to op-</cell></row><row><cell>timization difficulties. In contrast, RIDet-Q is a completely</cell></row><row><cell>sequence-free quadrilateral detector. It solves the problem</cell></row><row><cell>of representation ambiguity via dynamically searching for</cell></row><row><cell>optimal equivalent representations, and thus improves the</cell></row><row><cell>detection performance.</cell></row><row><cell>4.4. Comparison with the State-of-the-Art</cell></row><row><cell>4.4.1 Results on HRSC2016</cell></row><row><cell>The HRSC2016 dataset [17] contains lots of oriented ships.</cell></row><row><cell>As shown in Tab. 4, RIDet outperforms other single-stage</cell></row><row><cell>detectors and even some recent advanced two-stage detec-</cell></row><row><cell>tors such as DCL [33], RoI Transformer[2]. RIDet-O and</cell></row><row><cell>RIDet-O achieve the mAP of 89.10%, and 89.63%, respec-</cell></row><row><cell>tively. Especially when the images are resized to 416?416</cell></row><row><cell>with lightweight ResNet-18 as the backbone, RIDet-O still</cell></row><row><cell>achieve competitive performance of 89.25%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 ICN [1] R-101 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16 RoI Trans. [2] R-101 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 CAD-Net [43] R-101 87.80 82.40 49.40 73.50 71.10 63.50 76.70 90.90 79.20 73.30 48.40 60.90 62.00 67.00 62.20 69.90 O 80.77 54.07 76.38 79.81 81.99 89.13 90.72 83.58 87.22 64.42 67.56 78.08 79.17 62.07 77.62</figDesc><table><row><cell>Methods</cell><cell cols="2">Backbone PL</cell><cell>BD</cell><cell>BR GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC mAP</cell></row><row><cell>Two-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">RRPN [18] 88.52 2 -DNet [30] R-101 H-104 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04</cell></row><row><cell>SCRDet [40]</cell><cell>R-101</cell><cell cols="14">89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61</cell></row><row><cell>Single-stage:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DRN [22]</cell><cell>H-104</cell><cell cols="14">88.91 80.22 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 70.70</cell></row><row><cell>DAL [21]</cell><cell>R-101</cell><cell cols="14">88.61 79.69 46.27 70.37 65.89 76.10 78.53 90.84 79.98 78.41 58.71 62.02 69.23 71.32 60.65 71.78</cell></row><row><cell>RSDet [24]</cell><cell>R-101</cell><cell cols="14">89.80 82.90 48.60 65.20 69.50 70.10 70.20 90.50 85.60 83.40 62.50 63.90 65.6 0 67.20 68.00 72.20</cell></row><row><cell>BBAVector [42]</cell><cell>R-101</cell><cell cols="14">88.35 79.96 50.69 62.18 78.43 78.98 87.94 90.85 83.58 84.35 54.13 60.24 65.22 64.28 55.70 72.32</cell></row><row><cell>CFC-Net [19]</cell><cell>R-50</cell><cell cols="14">89.08 80.41 52.41 70.02 76.28 78.11 87.21 90.89 84.47 85.64 60.51 61.52 67.82 68.02 50.09 73.50</cell></row><row><cell>R 3 Det [34]</cell><cell>R-152</cell><cell cols="14">89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17 73.74</cell></row><row><cell>S 2 A-Net [5]</cell><cell>R-50</cell><cell cols="14">89.07 82.22 53.63 69.88 80.94 82.12 88.72 90.73 83.77 86.92 63.78 67.86 76.51 73.03 56.60 76.38</cell></row><row><cell>Baseline-Q</cell><cell>R-101</cell><cell cols="14">88.65 75.53 43.68 66.48 62.92 78.59 79.42 90.84 79.97 74.86 62.95 58.77 68.29 72.06 54.83 70.52</cell></row><row><cell>RIDet-Q</cell><cell>R-101</cell><cell cols="14">87.38 75.64 44.75 70.32 77.87 79.43 87.43 90.72 81.16 82.52 59.36 63.63 68.11 71.94 51.42 72.78</cell></row><row><cell>Baseline-O</cell><cell>R-101</cell><cell cols="14">88.36 75.31 46.33 66.59 76.88 77.88 87.23 90.51 82.73 83.84 60.89 60.73 67.29 72.01 56.91 72.89</cell></row><row><cell>RIDet-O</cell><cell>R-101</cell><cell cols="14">88.94 78.45 46.87 72.63 77.63 80.68 88.18 90.55 81.33 83.61 64.85 63.72 73.09 73.13 56.87 74.70</cell></row><row><cell>S 2 A-Net + RIL</cell><cell>R-50</cell><cell>89.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Performance evaluation of OBB task on DOTA dataset. H-104 is Hourglass-104. R-50 and R-101 denote ResNet-50 and ResNet-101, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>4.4.2 Results on DOTAWe compared the proposed method with other state-of-theart detectors on the DOTA dataset. The ground-truth labels of the test set are not publicly available, and thus the results in Tab. 5 are obtained by submitting the detection results to the official DOTA evaluation server.As shown in Tab. 5, RIDet-Q and RIDet-O achieve mAP of 72.78% and 74.70%, respectively. Note that the baseline model we used is the naive one-stage detector Cas-RetinaNet. After using RIL, the performance of RIDet can be comparable to many state-of-the-art methods, such as R 3 Det<ref type="bibr" target="#b34">[34]</ref> and CFC-Net<ref type="bibr" target="#b19">[19]</ref> that also adopt anchor refinement module. The S 2 A-Net<ref type="bibr" target="#b4">[5]</ref> trained with RIL achieves a mAP of 77.62%, surpassing the other compared methods. More visualization of detection results on the DOTA dataset</figDesc><table><row><cell>Methods</cell><cell>car</cell><cell>airplane</cell><cell>mAP</cell></row><row><cell>YOLOv3 [26]</cell><cell>74.63</cell><cell>89.52</cell><cell>82.08</cell></row><row><cell>RetinaNet [13]</cell><cell>84.64</cell><cell>90.51</cell><cell>87.57</cell></row><row><cell>FR-O [31]</cell><cell>86.87</cell><cell>89.86</cell><cell>88.36</cell></row><row><cell>RoI Transformer [2]</cell><cell>87.99</cell><cell>89.90</cell><cell>88.95</cell></row><row><cell>RIDet-Q</cell><cell>88.50</cell><cell>89.96</cell><cell>89.23</cell></row><row><cell>RIDet-O</cell><cell>88.88</cell><cell>90.35</cell><cell>89.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Detection performance on UCAS-AOD dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toso: Student&apos;st distribution aided one-stage orientation target detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengming</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4057" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align deep features for oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A densely connected endto-end neural network for multiscale and multiscene sar ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20881" to="20892" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Figure 13. Visualization of detection results on DOTA dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efn: Field-based object detection for aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">3630</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cfc-net: A critical feature capturing network for arbitrary-oriented object detection in remote-sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse label assignment for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<idno>2664, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic anchor learning for arbitraryoriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjuan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2355" to="2363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="11207" to="11216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time rotation-invariant face detection with progressive calibration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning point-guided localization for detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lurui</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dense label encoding for boundary discontinuity free rotation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09670,2020.3</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50839" to="50849" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking rotated object detection with gaussian wasserstein distance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11952</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenglong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13316</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Oriented object detection in aerial images with box boundary-aware vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2150" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rotation-robust intersection over union for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="464" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haigang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
