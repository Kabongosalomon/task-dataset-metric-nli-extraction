<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 26-28, 2018,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">M</forename><surname>Abien</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarap</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 26-28, 2018,</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3195106.3195117</idno>
					<note>ACM ISBN 978-1-4503-6353-2/18/02. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Supervised learning by clas- sification</term>
					<term>Support vector machines</term>
					<term>Neural networks</term>
					<term>? Se- curity and privacy ? Intrusion detection systems</term>
					<term>KEYWORDS artificial intelligence</term>
					<term>artificial neural networks</term>
					<term>gated recurrent units</term>
					<term>intrusion detection</term>
					<term>machine learning</term>
					<term>recurrent neural net- works</term>
					<term>support vector machine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gated Recurrent Unit (GRU) is a recently-developed variation of the long short-term memory (LSTM) unit, both of which are variants of recurrent neural network (RNN). Through empirical evidence, both models have been proven to be effective in a wide variety of machine learning tasks such as natural language processing[23], speech recognition <ref type="bibr" target="#b3">[4]</ref>, and text classification <ref type="bibr" target="#b24">[24]</ref>. Conventionally, like most neural networks, both of the aforementioned RNN variants employ the Softmax function as its final output layer for its prediction, and the cross-entropy function for computing its loss. In this paper, we present an amendment to this norm by introducing linear support vector machine (SVM) as the replacement for Softmax in the final output layer of a GRU model. Furthermore, the cross-entropy function shall be replaced with a margin-based function. While there have been similar studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">22]</ref>, this proposal is primarily intended for binary classification on intrusion detection using the 2013 network traffic data from the honeypot systems of Kyoto University. Results show that the GRU-SVM model performs relatively higher than the conventional GRU-Softmax model. The proposed model reached a training accuracy of ?81.54% and a testing accuracy of ?84.15%, while the latter was able to reach a training accuracy of ?63.07% and a testing accuracy of ?70.75%. In addition, the juxtaposition of these two final output layers indicate that the SVM would outperform Softmax in prediction time -a theoretical implication which was supported by the actual training and testing time in the study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>By 2019, the cost to the global economy due to cybercrime is projected to reach $2 trillion <ref type="bibr" target="#b9">[10]</ref>. Among the contributory felonies to cybercrime is intrusions, which is defined as illegal or unauthorized use of a network or a system by attackers <ref type="bibr" target="#b6">[7]</ref>. An intrusion detection system (IDS) is used to identify the said malicious activity <ref type="bibr" target="#b6">[7]</ref>. The most common method used for uncovering intrusions is the analysis of user activities <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17]</ref>. However, the aforementioned method is laborious when done manually, since the data of user activities is massive in nature <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">14]</ref>. To simplify the problem, automation through machine learning must be done.</p><p>A study by <ref type="bibr" target="#b17">Mukkamala, Janoski, &amp; Sung (2002)</ref> <ref type="bibr" target="#b17">[17]</ref> shows how support vector machine (SVM) and artificial neural network (ANN) can be used to accomplish the said task. In machine learning, SVM separates two classes of data points using a hyperplane <ref type="bibr" target="#b4">[5]</ref>. On the other hand, an ANN is a computational model that represents the human brain, and shows information is passed from a neuron to another <ref type="bibr" target="#b18">[18]</ref>.</p><p>An approach combining ANN and SVM was proposed by Alalshekmubarak &amp; Smith <ref type="bibr" target="#b1">[2]</ref>, for time-series classification. Specifically, they combined echo state network (ESN, a variant of recurrent neural network or RNN) and SVM. This research presents a modified version of the aforementioned proposal, and use it for intrusion detection. The proposed model will use recurrent neural network (RNNs) with gated recurrent units (GRUs) in place of ESN. RNNs are used for analyzing and/or predicting sequential data, making it a viable candidate for intrusion detection <ref type="bibr" target="#b18">[18]</ref>, since network traffic data is sequential in nature. might be pivotal in a more effective investigation on intrusion detection. Only 22 dataset features were used in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Preprocessing</head><p>For the experiment, only 25% of the whole 16.2 GB network traffic dataset was used, i.e. ?4.1 GB of data (from January 1, 2013 to <ref type="bibr">June 1, 2013)</ref>. Before using the dataset for the experiment, it was normalized first -standardization (for continuous data, see Eq. 1) and indexing (for categorical data), then it was binned (discretized).</p><formula xml:id="formula_0">z = X ? ? ? (1)</formula><p>where X is the feature value to be standardized, ? is the mean value of the given feature, and ? is its standard deviation. But for efficiency, the StandardScaler().fit_transform() function of Scikit-learn <ref type="bibr" target="#b19">[19]</ref> was used for the data standardization in this study.</p><p>For indexing, the categories were mapped to [0, n ? 1] using the LabelEncoder().fit_transform() function of Scikit-learn <ref type="bibr" target="#b19">[19]</ref>.</p><p>After dataset normalization, the continuous features were binned (decile binning, a discretization/quantization technique). This was done by getting the 10 t h , 20 t h , ..., 90 t h , and 100 t h quantile of the features, and their indices served as their bin number. This process was done using the qcut() function of pandas <ref type="bibr" target="#b16">[16]</ref>. Binning reduces the required computational cost, and improves the classification performance on the dataset <ref type="bibr" target="#b15">[15]</ref>. Lastly, the features were one-hot encoded, making it ready for use by the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The GRU-SVM Neural Network Architecture</head><p>Similar to the work of Alalshekmubarak &amp; Smith (2013) <ref type="bibr" target="#b1">[2]</ref> and Tang (2013) <ref type="bibr" target="#b22">[22]</ref>, the present paper proposes to use SVM as the classifier in a neural network architecture. Specifically, a Gated Recurrent Unit (GRU) RNN (see <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>The proposed GRU-SVM architecture model, with n?1 GRU unit inputs, and SVM as its classifier.</p><p>For this study, there were 21 features used as the model input. Then, the parameters are learned through the gating mechanism of GRU <ref type="bibr" target="#b2">[3]</ref> (Equations (2) to <ref type="bibr" target="#b4">(5)</ref>).</p><formula xml:id="formula_1">z = ? (W z ? [h t ?1 , x t ]) (2) r = ? (W r ? [h t ?1 , x t ])<label>(3)</label></formula><formula xml:id="formula_2">h t = tanh(W ? [r t * h t ?1 , x t ])<label>(4)</label></formula><formula xml:id="formula_3">h t = (1 ? z t ) * h t ?1 + z t * h t<label>(5)</label></formula><p>But with the introduction of SVM as its final layer, the parameters are also learned by optimizing the objective function of SVM (see Eq. 6). Then, instead of measuring the network loss using crossentropy function, the GRU-SVM model will use the loss function of SVM (Eq. 6).</p><formula xml:id="formula_4">min 1 2 ?w? 2 1 + C n i=1 max(0, 1 ? y ? i (w T x i + b i ))<label>(6)</label></formula><p>Eq. 6 is known as the unconstrained optimization problem of L1-SVM. However, it is not differentiable. On the contrary, its variation, known as the L2-SVM is differentiable and is more stable <ref type="bibr" target="#b22">[22]</ref> than the L1-SVM:</p><formula xml:id="formula_5">min 1 2 ?w? 2 2 + C n i=1 max(0, 1 ? y ? i (w T x i + b i )) 2<label>(7)</label></formula><p>The L2-SVM was used for the proposed GRU-SVM architecture. As for the prediction, the decision function f (x) = si?n(wx + b) produces a score vector for each classes. So, to get the predicted class label y of a data x, the ar?max function is used:</p><formula xml:id="formula_6">predicted_class = ar?max(si?n(wx + b))</formula><p>The ar?max function will return the index of the highest score across the vector of the predicted classes.</p><p>The proposed GRU-SVM model may be summarized as follows:</p><p>(1) Input the dataset features {x i | x i ? R m } to the GRU model.</p><p>(2) Initialize the learning parameters weights and biases with arbitrary values (they will be adjusted through training). (6) An optimization algorithm is used for loss minimization (for this study, the Adam <ref type="bibr" target="#b12">[12]</ref> optimizer was used). Optimization adjusts the weights and biases based on the computed loss. <ref type="bibr" target="#b6">(7)</ref> This process is repeated until the neural network reaches the desired accuracy or the highest accuracy possible. Afterwards, the trained model can be used for binary classification on a given data.</p><p>The program implementation of the proposed GRU-SVM model is available at https://github.com/AFAgarap/gru-svm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data Analysis</head><p>The effectiveness of the proposed GRU-SVM model was measured through the two phases of the experiment: (1) training phase, and (2) test phase. Along with the proposed model, the conventional GRU-Softmax was also trained and tested on the same dataset. The first phase of the experiment utilized 80% of total data points (?3.2 GB, or 14, 856, 316 lines of network traffic log) from the 25% of the dataset. After normalization and binning, it was revealed through a high-level inspection that a duplication occurred. Using the DataFrame.drop_duplicates() of pandas <ref type="bibr" target="#b16">[16]</ref>, the 14, 856, 316-line data dropped down to 1, 898, 322 lines (?40MB).</p><p>The second phase of the experiment was the evaluation of the two trained models using 20% of total data points from the 25% of the dataset. The testing dataset also experienced a drastic shrinkage in size -from 3, 714, 078 lines to 420, 759 lines (?9 MB).</p><p>The parameters for the experiments are the following: (1) Accuracy, <ref type="formula">(2)</ref>   <ref type="bibr" target="#b17">[17]</ref> in their study where they compared SVM and a feed-forward neural network for intrusion detection. Lastly, the statistical measures for binary classification were measured (true positive rate, true negative rate, false positive rate, and false negative rate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>All experiments in this study were conducted on a laptop computer with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4, 16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU. The hyperparameters used for both models were assigned by hand, and not through hyper-parameter optimization/tuning (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>Both models were trained on 1,898,240 lines of network traffic data for 5 epochs. Afterwards, the trained models were tested to classify 420,608 lines of network traffic data for 5 epochs. Only the specified number of lines of network traffic data were used for the experiments as those are the values that are divisble by the batch size of 256. The class distribution of both training and testing dataset is specified in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The experiment results are summarized in <ref type="table" target="#tab_2">Table 3</ref>. Although the loss for both models were recorded, it will not be a topic of further discussion as they are not comparable since they are in different scales. Meanwhile, <ref type="table" target="#tab_3">Tables 4 &amp; 5</ref> show the statistical measures for binary classification by the models during training and testing. <ref type="figure" target="#fig_2">Figure 2</ref> shows that for 5 epochs on the 1,898,240-line network traffic data (a total exposure of 9,491,200 to the training dataset), the     <ref type="figure" target="#fig_0">Figure 3</ref> shows that for 5 epochs on the 420,608-line network traffic data (a total test prediction of 2,103,040), the GRU-SVM model was able to finish its testing in 1 minute and 22 seconds. On the other hand, the GRU-Softmax model finished its testing in 1 minute and 40 seconds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>The empirical evidence presented in this paper suggests that SVM outperforms Softmax function in terms of prediction accuracy, when used as the final output layer in a neural network. This finding corroborates the claims by Alalshekmubarak &amp; Smith (2013) <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b22">Tang (2013)</ref> <ref type="bibr" target="#b22">[22]</ref>, and supports the claim that SVM is a more practical approach than Softmax for binary classification. Not only did the GRU-SVM model outperform the GRU-Softmax in terms of prediction accuracy, but it also outperformed the conventional model in terms of training time and testing time. Thus, supporting the theoretical implication as per the respective algorithm complexities of each classifier.</p><p>The reported training accuracy of ?81.54% and testing accuracy of ?84.15% posits that the GRU-SVM model has a relatively stronger predictive performance than the GRU-Softmax model (with training accuracy of ?63.07% and testing accuracy of ?70.75%). Hence, we propose a theory to explain the relatively lower performance of Softmax compared to SVM in this particular scenario. First, SVM was designed primarily for binary classification <ref type="bibr" target="#b4">[5]</ref>, while Softmax is best-fit for multinomial classification <ref type="bibr" target="#b10">[11]</ref>. Building on the premise, SVM does not care about the individual scores of the classes it predicts, it only requires its margins to be satisfied <ref type="bibr" target="#b10">[11]</ref>. On the contrary, the Softmax function will always find a way to improve its predicted probability distribution by ensuring that the correct class has the higher/highest probability, and the incorrect classes have the lower probability. This behavior of the Softmax function is exemplary, but excessive for a problem like binary classification. Given that the sigmoid ? function is a special case of Softmax (see Eq. 8-9), we can refer to its graph as to how it classifies a network output. </p><p>It can be inferred from the graph of sigmoid ? function (see <ref type="figure" target="#fig_4">Figure 4</ref>) that y values tend to respond less to changes in x. In other words, the gradients would be small, which gives rise to the "vanishing gradients" problem. Indeed, one of the problems being solved by LSTM, and consequently, by its variants such as GRU <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. This behavior defeats the purpose of GRU and LSTM solving the problems of a traditional RNN. We posit that this is the cause of misclassifications by the GRU-Softmax model.</p><p>The said erroneous manner of the GRU-Softmax model reflects as a favor for the GRU-SVM model. But the comparison of the exhibited predictive accuracies of both models is not the only reason for the practicality in choosing SVM over Softmax in this case. The amount of training time and testing time were also considered. As their computational complexities suggest, SVM has the upper hand over Softmax. This is because the algorithm complexity of the predictor function in SVM is only O(1). On the other hand, the predictor function of Softmax has an algorithm complexity of O(n). As results have shown, the GRU-SVM model also outperformed the GRU-Softmax model in both training time and testing time. Thus, it corroborates the respective algorithm complexities of the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND RECOMMENDATION</head><p>We proposed an amendment to the architecture of GRU RNN by using SVM as its final output layer in a binary/non-probabilistic classification task. This amendment was seen as viable for the fast prediction time of SVM compared to Softmax. To test the model, we conducted an experiment comparing it with the established GRU-Softmax model. Consequently, the empirical data attests to the effectiveness of the proposed GRU-SVM model over its comparator in terms of predictive accuracy, and training and testing time.</p><p>Further work must be done to validate the effectiveness of the proposed GRU-SVM model in other binary classification tasks. Extended study on the proposed model for a faster multinomial classification would prove to be prolific as well. Lastly, the theory presented to explain the relatively low performance of the Softmax function as a binary classifier might be a pre-cursor to further studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENT</head><p>An appreciation to the open source community (Cross Validated, GitHub, Stack Overflow) for the virtually infinite source of information and knowledge; to the Kyoto University for their intrusion detection dataset from their honeypot system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )</head><label>3</label><figDesc>The cell states of GRU are computed based on the input features x i , and its learning parameters values. (4) At the last time step, the prediction of the model is computed using the decision function of SVM: f (x) = si?n(wx + b). (5) The loss of the neural network is computed using Eq. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Epochs, (3) Loss, (4) Run time, (5) Number of data points, (6) Number of false positives, (7) Number of false negatives. These parameters are based on the ones considered by Mukkamala, Janoski, &amp; Sung (2002)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Training accuracy of the proposed GRU-SVM model, and the conventional GRU-Softmax model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Testing accuracy of the proposed GRU-SVM model, and the conventional GRU-Softmax model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Image from<ref type="bibr" target="#b8">[9]</ref>. Graph of a sigmoid ? function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e y 0 + e y 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameters used in both neural networks.</figDesc><table><row><cell cols="3">Hyper-parameters GRU-SVM GRU-Softmax</cell></row><row><cell>Batch Size</cell><cell>256</cell><cell>256</cell></row><row><cell>Cell Size</cell><cell>256</cell><cell>256</cell></row><row><cell>Dropout Rate</cell><cell>0.85</cell><cell>0.8</cell></row><row><cell>Epochs</cell><cell>5</cell><cell>5</cell></row><row><cell>Learning Rate</cell><cell>1e-5</cell><cell>1e-6</cell></row><row><cell>SVM C</cell><cell>0.5</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Class distribution of training and testing dataset.</figDesc><table><row><cell>Class</cell><cell cols="2">Training data Testing data</cell></row><row><cell>Normal</cell><cell>794,512</cell><cell>157,914</cell></row><row><cell>Intrusion detected</cell><cell>1,103,728</cell><cell>262,694</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of experiment results on both GRU-SVM and GRU-Softmax models.</figDesc><table><row><cell>Parameter</cell><cell cols="2">GRU-SVM GRU-Softmax</cell></row><row><cell>No. of data points -Training</cell><cell>1,898,240</cell><cell>1,898,240</cell></row><row><cell>No. of data points -Testing</cell><cell>420,608</cell><cell>420,608</cell></row><row><cell>Epochs</cell><cell>5</cell><cell>5</cell></row><row><cell>Accuracy -Training</cell><cell>?81.54%</cell><cell>?63.07%</cell></row><row><cell>Accuracy -Testing</cell><cell>?84.15%</cell><cell>?70.75%</cell></row><row><cell>Loss -Training</cell><cell>?131.21</cell><cell>?0.62142</cell></row><row><cell>Loss -Testing</cell><cell>?129.62</cell><cell>?0.62518</cell></row><row><cell>Run time -Training</cell><cell>?16.72mins</cell><cell>?17.18mins</cell></row><row><cell>Run time -Testing</cell><cell>?1.37mins</cell><cell>?1.67mins</cell></row><row><cell>No. of false positives -Training</cell><cell>889,327</cell><cell>3,017,548</cell></row><row><cell>No. of false positives -Testing</cell><cell>192,635</cell><cell>32,255</cell></row><row><cell>No. of false negatives -Training</cell><cell>862,419</cell><cell>487,175</cell></row><row><cell>No. of false negatives -Testing</cell><cell>140,535</cell><cell>582,105</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Statistical measures on binary classification: Training performance of the GRU-SVM and GRU-Softmax models.</figDesc><table><row><cell>Parameter</cell><cell cols="2">GRU-SVM GRU-Softmax</cell></row><row><cell cols="2">True positive rate ?84.3726%</cell><cell>?91.1721%</cell></row><row><cell cols="2">True negative rate ?77.6132%</cell><cell>?24.0402%</cell></row><row><cell cols="2">False positive rate ?22.3867%</cell><cell>?75.9597%</cell></row><row><cell cols="2">False negative rate ?15.6273%</cell><cell>?8.82781%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Statistical measures on binary classification: Testing performance of the GRU-SVM and GRU-Softmax models.</figDesc><table><row><cell>Parameter</cell><cell cols="2">GRU-SVM GRU-Softmax</cell></row><row><cell cols="2">True positive rate ?89.3005%</cell><cell>?55.6819%</cell></row><row><cell cols="2">True negative rate ?75.6025%</cell><cell>?95.9149%</cell></row><row><cell cols="2">False positive rate ?10.6995%</cell><cell>?4.08513%</cell></row><row><cell cols="2">False negative rate ?24.3975%</cell><cell>?44.3181%</cell></row><row><cell cols="3">GRU-SVM model was able to finish its training in 16 minutes and</cell></row><row><cell cols="3">43 seconds. On the other hand, the GRU-Softmax model finished</cell></row><row><cell cols="2">its training in 17 minutes and 11 seconds.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/Softwareavailablefromtensorflow.org" />
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<meeting><address><addrLine>Dan Man?, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Novel Approach Combining Recurrent Neural Network and Support Vector Machines for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alalshekmubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Information Technology (IIT), 2013 9th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00994018</idno>
		<ptr target="https://doi.org/10.1007/BF00994018" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial intelligence and intrusion detection: Current and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th national computer security conference</title>
		<meeting>the 17th national computer security conference<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Program Behavior Profiles for Intrusion Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schwartzbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Intrusion Detection and Network Monitoring</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">51462</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Grember</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/questions/45793856" />
		<title level="m">Binary classification with Softmax. Stack Overflow</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cybercrime will cost Businesses over $2 Trillion by</title>
		<ptr target="https://www.juniperresearch.com/press/press-releases/cybercrime-cost-businesses-over-2trillion." />
		<imprint>
			<date type="published" when="2015-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anrej</forename><surname>Karpathy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="http://cs231n.github.io/." />
		<title level="m">CS231n Convolutional Neural Networks for Visual Recognition</title>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An application of pattern matching in intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">H</forename><surname>Spafford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">DARPA Intrusion Detection Evaluation Data Set</title>
		<ptr target="https://www.ll.mit.edu/ideval/data/1999data.html." />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>MIT Lincoln Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving classification performance with discretization on biomedical datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanathi</forename><surname>Jonathan L Lustgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA annual symposium proceedings</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data Structures for Statistical Computing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wes</forename><surname>Mckinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<editor>St?fan van der Walt and Jarrod Millman</editor>
		<meeting>the 9th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intrusion detection: support vector machines and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Mukkamala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guadalupe</forename><surname>Janoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Joint Conference on Neural Networks (ANNIE)</title>
		<meeting>the IEEE International Joint Conference on Neural Networks (ANNIE)<address><addrLine>St. Louis, MO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1702" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Guide to Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negnevitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Pearson Education Ltd</publisher>
			<pubPlace>Essex, England</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Description of kyoto university benchmark data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungsuk</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Takakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Okabe</surname></persName>
		</author>
		<ptr target="http://www.takakura.com/Kyoto_data/BenchmarkData-Description-v5.pdf" />
		<imprint>
			<date type="published" when="2006-03-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cost-based modeling and evaluation for data mining with application to fraud and intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Prodromidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip K</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Results from the JAM Project by Salvatore</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01745</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
