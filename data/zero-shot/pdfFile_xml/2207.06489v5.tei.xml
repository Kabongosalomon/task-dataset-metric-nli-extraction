<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tandon School of Engineering New York University New York</orgName>
								<address>
									<postCode>11202</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cirrone</surname></persName>
							<email>cirrone@courant.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science New York University and Colton Center for Autoimmunity NYU Grossman School of Medicine New York</orgName>
								<address>
									<postCode>10011</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Computer Vision</term>
					<term>Medical Image Analysis</term>
					<term>Autoimmune diseases</term>
					<term>Histopathology Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. In autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level, and how they interact with each other. While these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding great promise for novel insights. In this paper, we empirically develop deep learning approaches that use dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. Our approach improves classification performance by 26% and segmentation performance by 5%. We also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our understanding of diseases and their classification has improved multifold with developments in medical science. However, our understanding of autoimmune diseases continues to be incomplete, missing vital information. No mechanism is in place to systematically collect data about the prevalence and incidence of autoimmune diseases (as it exists for infectious diseases and cancers). This deficiency is because we lack a comprehensive and universally acceptable list of arXiv:2207.06489v5 [eess.IV] <ref type="bibr" target="#b21">22</ref> Oct 2022 autoimmune diseases. The most cited study in the epidemiology of autoimmune diseases <ref type="bibr" target="#b11">[12]</ref>, estimates that autoimmune diseases, combined, affects about 3% of the US population or 9.9 million US citizens. A number comparable to the 13.6 million (US citizens) affected by cancer, which affects almost 4% of the population. Nevertheless, cancer has been widely studied in medical science and at the intersection of medical science and artificial intelligence. The study of autoimmune diseases is not only critical because they affect a sizeable portion of the population, but because we don't have a complete understating of their etiology and treatment. Other compelling reasons to study these diseases is their increasing prevalence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> and their further increase with the recent COVID-19 pandemic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Major outstanding research questions exist for autoimmune diseases regarding the presence of different cell types and their role in inflammation at the tissue level. In addition to studying preexisting patterns for different cell interactions, the identification of new cell occurrence and interaction patterns will help us better understand the diseases. While these patterns and interactions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification tasks provide a much more efficient and quicker way to understand these architectures of inflammation in autoimmune disease and hold great promise for novel insights. The application of artificial intelligence for medical image analysis has also seen a rapid increase, propelled by the increase in performance and efficiency of such architectures. However, even with these developments mentioned previously, the application of artificial intelligence in autoimmune biopsy analysis has not received the same attention as others. Firstly, autoimmune diseases are highly underrepresented because of significantly fewer data available for aforementioned reasons. Secondly, even within the few existing studies on the application of artificial intelligence for autoimmune disease analysis, dermatomyositis has received significantly less attention. Most research has focused on psoriasis, rheumatoid arthritis, lupus, scleroderma, vitiligo, inflammatory bowel diseases, thyroid eye sisease, multiple sclerosis sisease, and alopecia <ref type="bibr" target="#b21">[22]</ref>. We also observe that most of these approaches are based on older techniques and architectures that do not have open-source code to allow more researchers to expand their investigations into this area.</p><p>To help bridge this gap, we aim to draw more attention in this paper to autoimmune diseases, specifically to dermatomyositis. With this paper: (i) we improve upon the existing method for classification and segmentation of autoimmune disease images <ref type="bibr" target="#b22">[23]</ref> with 26% improvement for classification and 5% for segmentation, (ii) we propose an Autoencoder for Post-Processing (APP) and using image reconstruction loss improve segmentation performance by a further 3% as compared to (i), and (iii) based on these experimentation, we make recommendations for future researchers/practitioners to improve the performance of architectures and understanding of autoimmune diseases. All the aforementioned contributions have been implemented in PyTorch and are publicly available at https://github.com/pranavsinghps1/DEDL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Application of Artificial intelligence for autoimmune diseases</head><p>Researchers in <ref type="bibr" target="#b21">[22]</ref> conducted an in-depth study of the application of computer vision and deep learning in autoimmune disease diagnosis. Based on their study, we found a common trend among the datasets within autoimmune medical imaging -most of the datasets used are extremely small, with a median size of 126 samples. This also correlated with the findings of <ref type="bibr" target="#b19">[20]</ref>, wherein they also mentioned the median dataset size available for autoimmune diseases ranged between 99-540 samples. Medical imaging datasets tend to be smaller than natural image datasets, and even within medical datasets, autoimmune datasets are comparably smaller than the datasets of diseases with similar prevalence. For example, the prevalence of cancer is around 4% compared to the prevalence of autoimmune diseases at around 3%. However, cancer datasets of sizes ranging from a few thousand samples are readily available as opposed to that of autoimmune, where the median dataset size is between 99-540 samples. Another difference is that most of the autoimmune disease datasets are institutionally restricted. In addition, there are few studies on the application of artificial intelligence in autoimmune diseases. In 2020, <ref type="bibr" target="#b19">[20]</ref> conducted a systematic review of the literature and relevant papers at the intersection of artificial intelligence and autoimmune diseases with the following exclusion criteria: studies not written in English, no actual human patient data included, publication prior to 2001, studies that were not peer-reviewed, non-autoimmune disease comorbidity research and review papers. Only 169 studies met the criteria for inclusion. On further analyzing these 169 studies, only a small proportion of studies 7.7% (13/169) combined different data types. Cross-validation, combined with independent testing set for a more robust model evaluation, occurred only in 8.3% (14/169) of papers. In 2022, <ref type="bibr" target="#b21">[22]</ref> studied the usage of computer vision in autoimmune diseases, its limitations and the opportunities that technology offers for future research. Out of the more than 100 classified autoimmune diseases, research work has mostly focused ten diseases (psoriasis, rheumatoid arthritis, lupus, scleroderma, vitiligo, inflammatory bowel diseases, thyroid eye disease, multiple sclerosis disease and alopecia). <ref type="bibr" target="#b22">[23]</ref> is the first, to the best of our knowledge, to apply and study artificial intelligence for medical image analysis of dermatomyositis -an autoimmune disease that has not been studied in much detail. We used the same dataset as used by <ref type="bibr" target="#b22">[23]</ref> and propose innovative techniques and architectures to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmentation</head><p>Medical image segmentation, i.e., automated delineation of anatomical structures and other regions of interest (ROIs) paradigms, is an important step in computer-aided diagnosis; for example it is used to extract key quantitative measurements and localize a diseased area from the rest of the slide image. Good segmentation requires the object to see fine picture and intricate details at the same time. The same encoder-decoder architectures have been favored that often use different techniques (e.g., feature pyramid network, dilated networks and atrous networks) to help increase the receptive field of the architecture. When it comes to medical image segmentation, UNet <ref type="bibr" target="#b18">[19]</ref> has been the most cited and widely used network architecture. It uses an encoder and decoder architecture with skip connections to learn the segmentation masks. An updated version of UNet was introduced by <ref type="bibr" target="#b26">[27]</ref> called UNet++ that is essentially a nested version of UNet. The encoder is a feature extractor that down-samples the input, the decoder then consecutively up-samples to learn the segmentation mask for an input image. We added channel level attention with the help of squeeze and excitation blocks as proposed in <ref type="bibr" target="#b9">[10]</ref>. A squeeze and excitation is basically a building block that can be easily incorporated with CNN architecture. Comprised of a bypass that emerges after normal convolution, this is where the squeeze operation is performed. Squeezing basically means compressing each two-dimensional feature map until it becomes a real number. This is followed by an excitation operation that generates a weight for each feature channel to model relevance. Applying these weights to each original feature channel, the importance of different channels can be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>Classification is another important task for medical image analysis. CNNs have been the de facto standard for classification task. The adoption of Transformers for vision tasks from language models have been immensely beneficial. This gain in performance could be the result of Transformer's global receptive field as opposed to limited receptive field on CNNs. Although CNNs have inductive priors that make them more suited for vision tasks, Transformers learn them over the training period. Recently, there has been increased interest in combining the abilities of Transformers and CNNs. Certain CNNs have been trained the way Transformers have been trained as in <ref type="bibr" target="#b15">[16]</ref>, similarly the introduction of CNN type convolution has been incorporated in Transformers <ref type="bibr" target="#b14">[15]</ref>.Within medical image analysis, Transformers with their global receptive field could be extremely beneficial as this can help us learn features that CNNs with their limited receptive field could have missed. We start with the gold-standard of CNNs -ResNet-50 and the ResNet family <ref type="bibr" target="#b8">[9]</ref> of architectures. A ResNet-18 model can be scaled up to make ResNet-200. In most cases, this yields a better performance. But this scaling is very random as some models are scaled depth-wise and some are scaled width-wise. This problem was addressed with <ref type="bibr" target="#b20">[21]</ref>, in which the concept of compound scaling was used. They proposed a family of models with balanced scaling that also improved overall model performance. In ResNet-like architectures, batch normalization is often applied at the residual branch. This stabilizes the gradient and enables training of significantly deeper networks. Yet computing batch-level statistics is an expensive operation. To address these issues a set of NFnets (Normalizer-Free Networks) were published <ref type="bibr" target="#b1">[2]</ref>. Instead of using batch normalization, NFnets use other techniques to create batch-normalization like effect such as modified residual branches and convolutions with scaled weight standardization and adaptive gradient clipping. Transformers have a global receptive field, as opposed to CNN. <ref type="bibr" target="#b4">[5]</ref> introduced Vision Transformers (ViT) adapted from NLP, and since then they have improved upon many benchmarks for vision tasks. Hence, we study the effect of using Transformers as classifiers on the autoimmune dataset. Within Transformers, we start by looking at vision transformers introduced in <ref type="bibr" target="#b4">[5]</ref>. Unlike CNNs, Transformer first split each image into patches by a patch module. These patches are then used as "tokens". We then examine the Swin transformer family <ref type="bibr" target="#b14">[15]</ref>. Swin transformers use a hierarchical approach with shifted windows. The shifted windows scheme brings greater efficiency by limiting self-attention computation to non-overlapping windows and allows cross-window connection. It also first splits an input RGB image into non-overlapping image patches by a patch splitting module, like ViT <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmentation</head><p>For segmentation, our contribution is twofold. Firstly, we improved upon existing approaches by reducing blank image tiling, adding channel level attention in decoder, by using squeeze and excitation blocks to better map the importance of different channels and by using a pixel normalized cross-entropy loss. This improved performance by 5% over existing approach on the same dataset by <ref type="bibr" target="#b22">[23]</ref>, secondly, we introduce a novel post-processing Autoencoder which further improved the performance of the segmentation architecture as compared to the first step by 3%.</p><p>We used the same dermatomyositis dataset as used in <ref type="bibr" target="#b22">[23]</ref>. We started our experimentation by using the same metrics, image tiling and splits as their work, to make comparison easier. Once we surpassed their benchmark, we changed a few things as described in Section 5.1.</p><p>For segmentation benchmark as mentioned already, we based our study around one of the most widely-cited networks for biomedical image segmentation -UNet and a nested version of UNet -UNet++. These architectures are widely used not only in general segmentation tasks but also in biomedical segmentation. UNet has also been used in previous autoimmune segmentation tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Intuition for the Autoencoder Post-Processing architecture. Traditionally for training segmentation architectures, the output from the decoder is compared against ground truth with a loss function, in our case, we used a crossentropy loss function. Increasing the model size should help the model with more extracted features from the input but based on our experimentation, this is not the case <ref type="table" target="#tab_1">Table 2</ref>. So to provide the segmentation architecture with meaningful insight to improve learning, we provide additional feedback on "How easy is it to reconstruct the ground truth mask with the predicted mask?". To do so, we used simple encoder-decoder architecture with cross entropy loss. This post-processing autoencoder takes the output of the segmentation architecture as input and then computes this reconstruction loss, which the leading segmentation architecture then uses to improve learning. We expanded more on the experiments and study the results of adding these autoencoders in Section 5.1.  <ref type="figure" target="#fig_0">Figure 1</ref> shows the post-processing autoencoder architecture in conjunction with segmentation architectures during training. The feedback from the autoencoder is only used during training and then the trained model is saved for inference. This incurs minimal computational costs during training (as shown in Section 5.1); the saved weights are no bigger in size than the saved weights without the autoencoder post-processing architecture and with no change in inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification</head><p>We used their phenotype as markers to classify the different cell types. Wherein, the presence of a specific phenotype directly correlated to the presence of a cell type -T cell, B Cell, TFH-217, TFH-like cells and other cells. These cells' presence or absence aided us in diagnosing dermatomyositis. As previously mentioned, to develop a better understanding of autoimmune diseases, the study of nonconforming cells to the mapped phenotype-cell classification is extremely important. We classified them as 'others'. This would be potentially helpful in diagnosis and understanding of novel cell patterns present in biopsies, which in-turn could help us understand autoimmune diseases to a better extent by categorizing novel phenotype-cell relationships. We used the auto-fluorescence images of size 352 by 469 and RGB. Since there could be multiple cells present per sample, this would be a multi-label classification. To address this class imbalance, we use the Focal loss <ref type="bibr" target="#b13">[14]</ref> function that penalizes the dominant and the underrepresented class in a dataset. We used it to label distribution normalized class weights instead of vanilla cross-entropy, which fails to address this class imbalance. We also used a sixfold approach to train and reported our results on the test set to address any biases that might occur during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the same dataset as used in <ref type="bibr" target="#b22">[23]</ref>. This dataset contains 198 TIFF image samples, each containing slides of different protein-stained images -DAPI, CXCR3,CD19, CXCR5, PD1, CD4, CD27 and autofluorescence. Binary thresholds were set for each channel (1-DAPI, 2-CXCR3, 3-CD19, 4-CXCR5, 5-PD1, 6-CD4, 7-CD27, 8-Autofluorescence) to show presence/absence of each representative phenotype. These phenotypes were then classified into B cells and T cells using channels 2-7. The autofluorescence slides are an overlap of all the channels used for classifying cell types.</p><p>We use the DAPI stained image for semantic segmentation and the autofluorescence slide images for classification. This approach is a shift from previous work where researchers used DAPI channel images for both tasks as well as using UNet for segmentation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Segmentation</head><p>We use qubvel's implementation of Unet and Unet++ segmentation architectures https://github.com/qubvel/segmentation_models.pytorch. To start, we first convert the TIFF image file into NumPy and then into a PIL Image. We then apply Random Rotation, Random vertical and horizontal flip, and channel normalization before finally converting to tensors. We use the same splits for training, testing and validation as <ref type="bibr" target="#b22">[23]</ref> to keep our results comparable. We use cross-entropy loss with component normalized weights, Adam optimizer with 1e-05 decay and cosine learning rate with minimum learning rate 3.4e-04. We also use <ref type="bibr" target="#b9">[10]</ref> squeeze and excitation units in the decoder to add channel level attention.</p><p>For the Autoencoder processing architecture (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>), we use six layers, out of which five are downsampling layers followed by five upsampling layers. The encoder part contains 6 layers with first layer upscaling the input from 480 to 15360. From there on consecutive layers downsample from 15360 to 256, 128, 64, 32 to 16. This is then fed to the decoder which then systematically up scales it from 16, 32, 64, 128, 256 and 480. We use GELU (Gaussian Error Linear Unit) activation in all the layers. This post-processing architecture is only used during training as an added feedback mechanism; this helps the segmentation model improve learning. Times for saved model after training and inference remain same with or without the auto-encoder post-processing. We use Adam optimizer with a constant learning rate of 1e-3. For loss, we use the Cross Entropy (CE) loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification</head><p>To address the class imbalance problem, we use focal loss <ref type="bibr" target="#b13">[14]</ref>, which is essentially an oscillating cross entropy and this modulation fluctuates with easy and complex examples in the dataset.</p><p>We perform an 80/20 split for training to testing and use sixfold crossvalidation. We then report the average F1-score across all the folds and different initialization in Section 5. We use Adam optimizer without weight decay with a cosine learning schedule and a learning rate of 1e-6 for 16 iterations. For a more generalized result and better optimization, we use Stochastic Weight Averaging <ref type="bibr" target="#b10">[11]</ref>. We use timm's implementation <ref type="bibr" target="#b23">[24]</ref> of CNN and Transformers for benchmarking.</p><p>We conducted all our experiments on a single NVIDIA RTX-8000 GPU with 45GB of use-able video memory, 64GB RAM and two cores of CPU. We also used early-stopping with a patience of 5 epochs for both our classification and segmentation training pipeline. We compute average results over 5 runs with different seed values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Segmentation</head><p>Improvement over existing work. Since the images are 1408 by 1876, we tiled them in 256 2 and then used blank padding at the edges to make them fit in size 256 2 .</p><p>For segmentation, we started by using metrics, architecture and parameters as suggested in <ref type="bibr" target="#b22">[23]</ref>. We begin with ImageNet weights as they are readily available for various backbones instead of the brain-MRI segmentation weights used in <ref type="bibr" target="#b22">[23]</ref>. By changing the existing learning rate schedule from step to cosine and by using normalized weights of blank pixels and pixels that aren't blank (i.e., hold some information) in the cross-entropy loss, adding channel level attention with squeeze and excitation block to better feature map level channel relationships. We observed that the overall performance improved from 0.933 overall accuracy for <ref type="bibr" target="#b22">[23]</ref> to 0.9834 for Unetplusplus with ResNet34 backbone -an improvement of 5%.</p><p>We observed that: (i) by using image tiling of size 256 2 , margin padding is primarily empty, and some of the tiles do not have any part of the cell stained on them. This might give false perception of performance as the model achieves higher metric performance from not learning anything. To mitigate this, we tiled the images into 480 2 . This is depicted in <ref type="figure" target="#fig_2">Fig.3;</ref> (ii) because accuracy is not the most appropriate metric for segmentation performance, we instead used IoU or Jaccrad Index as our metric. IoU is a better representation if the model has learned meaningful features.</p><p>Fine-tuning. For further refining, we use one of the most cited and used architectures for biomedical segmentation, UNet <ref type="bibr" target="#b18">[19]</ref>, and a nested version of UNet called the UNetplusplus <ref type="bibr" target="#b26">[27]</ref>. These two architectures contain an encoder-decoder structure wherein the encoder is a feature extractor, and the decoder learns the segmentation mask on the extracted features. For the choice of the encoder, we used the ResNet family <ref type="bibr" target="#b8">[9]</ref> and the newer, more efficient family <ref type="bibr" target="#b20">[21]</ref>. These architectures do not have an attention mechanism built-in. So we artificially included channel level attention mechanism using Squeeze and Excitation blocks <ref type="bibr" target="#b9">[10]</ref>. In addition to these changes we propose adding a post-processing autoencoder architecture as described in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Adding Autoencoder. As mentioned previously, we also experimented by adding autoencoder after the tuned segmentation architecture. We call this Autoencoder Post Processing or APP. We start by using the GELU (Gaussian Error Linear Unit) as our activation function for the autoencoder architecture (AE) as described in Section 3.1 and <ref type="figure" target="#fig_0">Figure 1</ref>. We report the IoU score on test set for Unet and Unet++ in <ref type="table" target="#tab_1">Table 2</ref>. The autoencoder adds additional feedback to the segmentation architecture by computing reconstruction loss between the segmentation architecture's output and ground truth. This adds some extra time during training but the space of saved model and inference time remain constant when compared to networks without the autoencoder post-processing architecture.</p><p>Ablation Study. In the original implementation, we used GELU activation function with a constant learning rate and Adam optimizer in the autoencoder. In this section, we experimented with the hyperparameters of only the autoencoders to determine its effect on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Computational cost: For a training set with 1452 images and validation set</head><p>with 720 images of size 480 2 over 50 epochs, we report the following testing time with and without using the GELU autoencoder post processing for UNet with ResNet backbones. We observed that there is an average increase of 1.875% over the ResNet family of encoders. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Using ReLU (Rectified Linear Unit) instead of GELU (Gaussian Error Linear</head><p>Units) for activation function. <ref type="table" target="#tab_1">Table 2</ref> shows the difference in performance for when we changed the activation of the autoencoders layers from ReLU to GELU. And for the same we observed that GELU performed better than ReLU activated autoencoder by around 3% for UNet and UNet++. We observed in most cases for UNet and UNet++, the addition of the autoencoder post processing is highly beneficial. With the addition of autoencoder, GELU activation performed much better than ReLU activation. We further expand upon these results in Appendix 7.1. 3. Using Adam optimizer with cosine learning schedule. For this we kept rest of the setup same. We only added a cosine learning schedule to the autoencoder architecture and ReLU activated layers. From <ref type="table" target="#tab_2">Table 3</ref> we observed that except for two cases in UNet++, autoencoder with constant learning rate perform much better than the one with Adman optimizer and cosine learning rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification</head><p>For classification our objective is to classify the different phenotypes in a given image and based on the presence of different cells. Since multiple labels could be assigned to the same image, this is a multi-class classification problem. From <ref type="figure" target="#fig_1">Fig. 2</ref>, we observed that the classes are highly imbalanced. <ref type="bibr" target="#b22">[23]</ref> also reported an F1 score of 0.63 for the classification on this dataset. We improved upon their score with an F1 score of 0.891. This improvement could be attributed to the following, (i) we use focal loss <ref type="bibr" target="#b13">[14]</ref> instead of cross entropy as the previous work. Focal loss applies a modulating term to the cross-entropy loss in order to focus learning on hard misclassified examples. It is a dynamically scaled crossentropy loss, where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. (ii) We used newer and more efficient architectures. We benchmarked pure CNNs, Transformers and newer generation of CNNs trained with Transformer-like techniques Convnext <ref type="bibr" target="#b15">[16]</ref>. As mentioned in Section 3, in this section we present our results and discuss the effect of different architectures.</p><p>Effect of Architecture. Recently, the emergence of Transformers for vision tasks has taken the field of computer vision by storm. They have been able to improve upon many benchmarks set by CNNs. They are particularly of interest in the medical imaging because of their global fidelity as opposed to the centered fidelity of CNNs.</p><p>More recently, some of the Transformer techniques have also been applied to train CNNs. This has given rise to new architectures like the ConvNeXts family of pure ConvNet models <ref type="bibr" target="#b15">[16]</ref>. We studied the effect of using different architectures on performance and hence have found the most suitable architecture for the multiclass classification task at hand. We resized each image to be 384 2 .</p><p>Amongst CNNs and Transformers, the peak performance is closely matched with Swin Transformer Base (Patch 4 Window 12) achieving a new state of the art performance for the autoimmune dataset with an F1 Score of 0.891, an improvement of 26.1% over previous work by <ref type="bibr" target="#b22">[23]</ref>; compared to nfnet-f3 that provides peak performance for CNNs with an F1 score of 0.8898.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our framework can be adapted to other tissues and diseases datasets. It provides an efficient approach for clinicians to identify and detect cells within histopathology images in order to better comprehend the architecture of inflammation (i.e., which cell types are involved in inflammation at the tissue level, and how cells interact with one other). Based on our experimentation, we observe that for segmentation of biopsies affected by dermatomyositis, it is better to use Imagenet initialization with a normalized cross-entropy loss. Further performance can be increased by using our proposed autoencoder post-processing architecture (APP). APP gains 3% consistently over architectures without any post-processing segmentation architecture. This addition comes at minimal extra training cost and at no extra space and time to develop inference models. For classification, using stochastic weight averaging improves generalization and class normalized weights with focal loss and helps to counter the class imbalance problem. In comparing architectures, the performance is relatively similar for CNNs and transformers, but transformers perform slightly better than CNNs. These changes helped us register an improved performance of 8% in segmentation and of 26% in classification performance on our dermatomyositis dataset. <ref type="table">Table 4</ref>. We report the F1 score for with ImageNet initialization for latest set of CNNs and Transformers. We observed the usual trend of increasing ImageNet performance with increasing size of the model is not followed. Overall the best performance is achieved by nfnet-f3 for CNNs. Overall, out of all the tested architectures Swin Transformer Base with Patch 4 and Window 12 performed the best</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test F1-Score  <ref type="bibr" target="#b22">[23]</ref> 0.63 7 Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Expansion of Results</head><p>In <ref type="table">Tables 6 and 7</ref> we show complete results with mean and standard deviation. These are an expansion of <ref type="table" target="#tab_1">Table 2</ref> in Section 5.1 of the main paper. Tables were compressed to save space and only focus on the main results. To provide a complete picture, we added extended results in this section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Autoencoder with efficientnet encoder for segmentation</head><p>In <ref type="table">Table 8</ref> and 9 we compared the time taken to train and the performance of the respective trained architecture for segmentation using EfficientNet encoders. We observed that with the addition of autoencoder post-processing, training time increased by an average of 3m 7.3s over 50 epochs (averaged over the entire efficientnet family). This is an increase of 2.93% in training time over the eight encoders. In other words, an average increase of 0.36% increase in time per encoder over 50 epochs. Performance wise architecture with autoencoder post-processing consistently outperformed segmentation architectures without them by 2.75%. Similarly, we compared computational and performance for UNet++ with and without the autoencoder post-processing in <ref type="table" target="#tab_0">Tables 10 and 11</ref> respectively.</p><p>In this case, we observed that the gain in performance with autoencoder postprocessing is 5% averaged over the efficientnet family of encoders. This also corresponds to a 3m 7s increase in training time which is an increase of 2.6%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Metrics Description</head><p>For measuring segmentation performance, we use IoU or intersection over union metric. It helps us understand how similar sample sets are. IoU = area of overlap area of union = Here the comparison is made between the output mask by segmentation pipeline against the ground truth mask.</p><p>For measuring classification performance, we use the F1 score. Computed as F1 = 2*Precision*Recall Precision+Recall = 2*TP 2*TP+FP+FN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Effect of different weights</head><p>ImageNet initialization has been the defacto norm for most transfer learning tasks. Although in some cases, as in <ref type="bibr" target="#b0">[1]</ref> it was observed that noisy student weights performed better than ImageNet initialization. To study the effect in our case, we used advprop and noisy student initialization. ImageNet weights for initialization work for medical data not because of feature reuse but because of better weight scaling and faster convergence <ref type="bibr" target="#b17">[18]</ref>. Noisy student training <ref type="bibr" target="#b25">[26]</ref> extends the idea of self-training and distillation with the use of equal-or-larger student models, and noise such as dropout, stochastic depth, and data augmentation via RandAugment is added to the student during learning so that the student generalizes better than the teacher. First, an EfficientNet model is trained on labelled images and is used as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labelled and pseudo-labelled images. This helps reduce the error rate, increases robustness and improves performance over the existing state-of-the-art on ImageNet.</p><p>(ii)AdvProp training, which banks on Adversarial examples, which are commonly viewed as a threat to ConvNets. In <ref type="bibr" target="#b24">[25]</ref> they present an opposite perspective: adversarial examples can be used to improve image recognition models. They treat adversarial examples as additional examples to prevent overfitting. It performs better when the models are bigger. This improves upon performance for various ImageNet and its' subset benchmarks.</p><p>Since initially all these were developed for the EfficientNet family of the encoders, we used them for benchmarking. We present their results in <ref type="table" target="#tab_0">Table 12</ref>.</p><p>Similarly, we conduct similar experiments for classification with different initialization. We reported these results in <ref type="table" target="#tab_0">Table 13  Table 12</ref>. Using different initialization, we saw that the performance of different encoders of the EfficientNet family on UNet. We report the IoU over the test set in the following table. We observe that while performance gains for smaller models, ImageNet initialisation works better for larger models. Also, the fact that advprop and noisy are not readily available for all models, hence the choice of ImageNet still dominates.  <ref type="table" target="#tab_0">Table 13</ref>. We report the F1 score for different initializations for the EfficientNet family of encoders. We reported the average of 6-fold runs on the test set with five different seed values. We observed that 0.8463 is the peak with ImageNet, 0.843 with advprop and 0.8457 with noisy student initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder ImageNet</head><p>Advprop Noisy Student B0 0.8372? 0.0007 0.8416?0.0008 0.839?0.0008 B1 0.8346?0.0026 0.8367?0.0007 0.8448?0.0014 B2 0.828?0.00074 0.843?0.00138 0.8430?0.0012 B3 0.8369?0.0094 0.84138?0.00135 0.8457?0.007 B4 0.8418?0.0009 0.82135?0.00058 0.8377?0.00032 B5 0.8463?0.00036 0.8133?0.002 0.8326?0.00066 B6 0.8263?0.00147 0.8237?0.004 0.8233?0.0065 B7 0.8129?0.001 0.8132?0.004 0.8257?0.0008</p><p>As we can see for segmentation, ImageNet initialization performed better in most cases. Similarly, in classification, it not only performed better in most cases but also provided the best overall result-these inferences, combined with the fact that advprop and noisy student requires additional computational resources. Hence we decide to stick with ImageNet initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Expansion on Experimental Details</head><p>Segmentation We used PyTorch lightning's <ref type="bibr" target="#b6">[7]</ref> seed everything functionality to seed all the generator values uniformly. For setting the seed value, we randomly generated a set of numbers in the range of 1 and 1000. We did not perform an extensive search of space to optimise performance with seed value as suggested in <ref type="bibr" target="#b16">[17]</ref>. We used seed values 26, 77, 334, 517 and 994. For augmentation, we used conversion to PIL Image to apply random rotation (degrees=3), random vertical and horizontal flip, then conversion to tensor and finally channel normalisation. We could have used a resize function to reshape the 1408 by 1876 Whole Slide Images (WSI), but we instead tilled them in 480 square tile images. We then split them into a batch size of 16 before finally passing through the segmentation architecture (UNet/UNet++). We used channel attention only decoder, with ImageNet initialisation and a decoder depth of 3 (256, 128, 64).</p><p>We used cross-entropy loss with dark/light pixel normalization, Adam optimizer with LR set to 3.6e-04 and weight decay of 1e-05. We used a cosine scheduling rate with a minimum value set to 3.4e-04.</p><p>APP Segmentation When using APP we used GELU activation by default with adam optimizer and lr set to 1e-3.</p><p>Classification For Classification, we used the same seed values with PyTorch lightning's <ref type="bibr" target="#b6">[7]</ref> seed everything functionality, as described for segmentation above. For augmentation, we resized the images to 384 square images, followed by randomly applying colour jitter (0.2, 0.2, 0.2) or random perspective (distortion scale=0.2) with probability 0.3, colour jittering (0.2, 0.2, 0.2) or random affine (degrees=10) with probability 0.3, random vertical flip and random horizontal flip with probability 0.3 and finally channel normalization.</p><p>We used Stochastic weigh averaging with adam optimizer. We used a cosine learning rate starting at 1e-3 and a minimum set to 1e-6. We used focal loss with normalized class weight as our loss function. We used 6-fold validation with each fold of 20 epochs and batch size of 16. We used same parameters for both CNN and Transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Autoencoder Post-Processing (APP): we introduced the autoencoder labeled with yellow and red colour, after getting the segmentation mask from the segmentation architecture(in our case UNet and UNet++).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Distribution of different cell types within the dataset. Observing that there is an imbalance in the distribution, we classified cells on the basis of the presence or absence of certain cell phenotype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Changing the image tiling reduces the number of blank tiles which are mostly concentrated towards the edges and some of the intermediate tiles. Top images shows 256 2 tiling while bottom one shows 480 2 tiling. As we can see for the top image tilled in 256 by 256 image size, the right bottom tiles are almost all empty. Including some of the tiles in the center portion. While for the bottom image tilled in 480 by 480 image size, no tiles are empty. The tiles highlighted in red boxes are empty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>This table shows training timefor UNet with and without APP for ResNet family of encoders for 50 epochs. We observed that for smaller encoders like ResNet-18 and 34 the training time increase is greater as opposed to larger encoders.</figDesc><table><row><cell cols="2">Encoder Without APP With APP</cell></row><row><cell>Resnet-18 1h 24m 44s</cell><cell>1h 27m 40s</cell></row><row><cell>Resnet-34 1h 27m 41s</cell><cell>1h 30m 39s</cell></row><row><cell>Resnet-50 1h 45m 45s</cell><cell>1h 46m 24s</cell></row><row><cell>Resnet-101 1h 45m 24s</cell><cell>1h 46m 25s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>This table showsthe IoU score on test set for UNet and UNet++ architectures with and without using cosine learning rate for the autoencoder. Except for ResNet-18 and 101 with UNet++, autoencoder always provide an improvement without cosine learning rate.</figDesc><table><row><cell>Encoder</cell><cell>UNet</cell><cell></cell><cell cols="2">UNet++</cell><cell></cell></row><row><cell>Without AE</cell><cell>With ReLU AE</cell><cell>With GELU AE</cell><cell>Without AE</cell><cell>With ReLU AE</cell><cell>With GELU AE</cell></row><row><cell>Resnet 18 0.4347</cell><cell cols="5">0.4608 0.4788 0.5274 0.4177 0.4707</cell></row><row><cell>Resnet 34 0.4774</cell><cell cols="3">0.4467 0.4983 0.3745</cell><cell cols="2">0.4535 0.4678</cell></row><row><cell>Resnet 50 0.3798</cell><cell cols="3">0.4187 0.3827 0.4236</cell><cell cols="2">0.4685 0.4422</cell></row><row><cell>Resnet 101 0.3718</cell><cell cols="3">0.4074 0.4402 0.4311</cell><cell cols="2">0.4265 0.4467</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>This table showsthe IoU score on test set for UNet and UNet++ architectures. We compared results without, with autoencoder for both ReLU and GELU activations. Except for ResNet-18 with UNet++, autoencoder always provide an improvement. Within autoencoders we see that using GELU activation is much better than ReLU activation.</figDesc><table><row><cell>Encoder</cell><cell cols="2">UNet</cell><cell cols="2">UNet++</cell></row><row><cell></cell><cell>Without Cosine LR</cell><cell>With Cosine LR</cell><cell>Without Cosine LR</cell><cell>With Cosine LR</cell></row><row><cell cols="3">Resnet 18 0.4608 0.4106</cell><cell>0.4177</cell><cell>0.4717</cell></row><row><cell cols="3">Resnet 34 0.4467 0.3665</cell><cell cols="2">0.4535 0.4345</cell></row><row><cell cols="3">Resnet 50 0.4187 0.3965</cell><cell cols="2">0.4685 0.4268</cell></row><row><cell cols="3">Resnet 101 0.4074 0.3846</cell><cell>0.4265</cell><cell>0.4518</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>We compare the best results provided by our algorithm (in bold) as compared to previous benchmark on the same dataset.</figDesc><table><row><cell>Resnet-18</cell><cell>0.8635?0.0087</cell></row><row><cell>Resnet-34</cell><cell>0.82765?0.0073</cell></row><row><cell>Resnet-50</cell><cell>0.8499?0.007</cell></row><row><cell>Resnet-101</cell><cell>0.871?0.009</cell></row><row><cell>Efficient-net B0</cell><cell>0.8372?0.0007</cell></row><row><cell>Efficient-net B1</cell><cell>0.8346?0.0026</cell></row><row><cell>Efficient-net B2</cell><cell>0.828?0.00074</cell></row><row><cell>Efficient-net B3</cell><cell>0.8369?0.0094</cell></row><row><cell>Efficient-net B4</cell><cell>0.8418?0.0009</cell></row><row><cell>Efficient-net B5</cell><cell>0.8463?0.00036</cell></row><row><cell>Efficient-net B6</cell><cell>0.8263?0.00147</cell></row><row><cell>Efficient-net B7</cell><cell>0.8129?0.001</cell></row><row><cell>nfnet-f0</cell><cell>0.82035?0.007</cell></row><row><cell>nfnet-f1</cell><cell>0.834?0.007</cell></row><row><cell>nfnet-f2</cell><cell>0.8652?0.0089</cell></row><row><cell>nfnet-f3</cell><cell>0.8898?0.0011</cell></row><row><cell>nfnet-f4</cell><cell>0.8848?0.0109</cell></row><row><cell>nfnet-f5</cell><cell>0.8161?0.0074</cell></row><row><cell>nfnet-f6</cell><cell>0.8378?0.007</cell></row><row><cell>ConvNext-tiny</cell><cell>0.81355?0.0032</cell></row><row><cell>ConvNext-small</cell><cell>0.84795?0.00246</cell></row><row><cell>ConvNext-base</cell><cell>0.80675?0.002</cell></row><row><cell>ConvNext-large</cell><cell>0.8452?0.000545</cell></row><row><cell>Swin Transformer large</cell><cell></cell></row><row><cell cols="2">(Patch 4 Window 12) 0.8839?0.001</cell></row><row><cell>Swin Transformer Base</cell><cell></cell></row><row><cell cols="2">(Patch 4 Window 12) 0.891?0.0007</cell></row><row><cell>Vit-Base/16</cell><cell>0.8426?0.007</cell></row><row><cell>Vit-Base/32</cell><cell>0.8507?0.0079</cell></row><row><cell>Vit-large/16</cell><cell>0.80495?0.0077</cell></row><row><cell>Vit-large/32</cell><cell>0.845?0.0077</cell></row><row><cell>Model</cell><cell>Test F1-Score</cell></row><row><cell>Swin Transformer Large (Patch 4 Window 12)</cell><cell>0.891?0.0007</cell></row><row><cell>nfnet-f3</cell><cell>0.8898?0.0109</cell></row><row><cell>Vanburen et all</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>This table showsthe IoU score on the test set for UNet. We compared results without and with autoencoder for both ReLU and GELU activations for UNet Architecture. These results are averaged over five runs with different seed values. We observed that in all cases addition of APP improved performance. GELU activated APP seems out perform the ReLU activated APP in all cases except for ResNet-50. This table shows the IoU score on the test set for UNet++. These results are averaged over five runs with different seed values. We compare results without and with autoencoder for both ReLU and GELU activations for UNet++ Architecture. We observed that in most cases, APP improves performance except for UNet++ with Resnet-18, where APP segmentation techniques lag by around 5%. However, as a counter for ResNet-34 APP-based segmentation techniques are almost 10% better than UNet++ without APP.</figDesc><table><row><cell>Encoder</cell><cell>UNet</cell><cell></cell></row><row><cell>Without AE</cell><cell>With ReLU AE</cell><cell>With GELU AE</cell></row><row><cell cols="2">ResNet 18 0.4347?0.0006 0.4608?0.0001</cell><cell>0.4788?0.0004</cell></row><row><cell cols="2">ResNet 34 0.4774?0.0004 0.4467?0.0012</cell><cell>0.4983?0.0008</cell></row><row><cell cols="3">ResNet 50 0.3798?0.00072 0.4187?0.0006 0.3827?0.0003</cell></row><row><cell cols="2">ResNet 101 0.3718?0.0001 0.4074?0.0012</cell><cell>0.4402?0.00018</cell></row><row><cell>Encoder</cell><cell>UNet++</cell><cell></cell></row><row><cell>Without AE</cell><cell>With ReLU AE</cell><cell>With GELU AE</cell></row><row><cell cols="3">ResNet 18 0.5274?0.0004 0.4177?0.0005 0.4707?0.00067</cell></row><row><cell cols="3">ResNet 34 0.3745?0.0006 0.4535?0.0008 0.4678?0.0004</cell></row><row><cell cols="3">ResNet 50 0.4236?0.0004 0.4685?0.0002 0.4422?0.0007</cell></row><row><cell cols="3">ResNet 101 0.4311?0.0003 0.4265?0.0002 0.4467?0.0003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>In this table we report the running time averaged over 5 runs with different seeds, for efficient-net encoder family with UNet.The variation is almost negligible(&lt; 6s). In this table we report the IoU averaged over 5 runs with different seeds, for efficient-net encoder family with UNet architecture.</figDesc><table><row><cell cols="3">Encoder Without APP With APP</cell></row><row><cell>B0</cell><cell>1h 26m 27s</cell><cell>1h 29m 04s</cell></row><row><cell>B1</cell><cell>1h 31m 16s</cell><cell>1h 33m 42s</cell></row><row><cell>B2</cell><cell>1h 32m 12s</cell><cell>1h 34m 27s</cell></row><row><cell>B3</cell><cell>1h 38m</cell><cell>1h 40m 33s</cell></row><row><cell>B4</cell><cell>1h 44m 20s</cell><cell>1h 50m 02s</cell></row><row><cell>B5</cell><cell>1h 55m 46s</cell><cell>1h 58m 40s</cell></row><row><cell>B6</cell><cell>2h 06m 55s</cell><cell>2h 10m 08s</cell></row><row><cell>B7</cell><cell>2h 16m 40s</cell><cell>2h 19m 59s</cell></row><row><cell cols="3">Encoder Without APP With APP</cell></row><row><cell>B0</cell><cell cols="2">0.3785?0.00061 0.4282?0.0008</cell></row><row><cell>B1</cell><cell cols="2">0.3301?0.0002 0.4237?0.0006</cell></row><row><cell>B2</cell><cell cols="2">0.2235?0.0007 0.3735?0.0009</cell></row><row><cell>B3</cell><cell cols="2">0.3982?0.0007 0.2411?0.0004</cell></row><row><cell>B4</cell><cell cols="2">0.3826?0.0004 0.3829?0.0006</cell></row><row><cell>B5</cell><cell cols="2">0.4056?0.0008 0.4336?0.0008</cell></row><row><cell>B6</cell><cell cols="2">0.4001?0.0001 0.4311?0.0006</cell></row><row><cell>B7</cell><cell cols="2">0.3631?0.0002 0.3937?0.0004</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>In this table we report the running time averaged over 5 runs with different seeds, for efficient-net encoder family with UNet++.</figDesc><table><row><cell cols="3">Encoder Without APP With APP</cell></row><row><cell>B0</cell><cell>1h 32m 50s</cell><cell>1h 35m 31s</cell></row><row><cell>B1</cell><cell>1h 37m 40s</cell><cell>1h 40m 51s</cell></row><row><cell>B2</cell><cell>1h 38m 30s</cell><cell>1h 40m 41s</cell></row><row><cell>B3</cell><cell>1h 46m 30s</cell><cell>1h 49m 34s</cell></row><row><cell>B4</cell><cell>1h 54m 01s</cell><cell>1h 57m 41s</cell></row><row><cell>B5</cell><cell>2h 07m 54s</cell><cell>2h 11m 39s</cell></row><row><cell>B6</cell><cell>2h 20m 23s</cell><cell>2h 23m 41s</cell></row><row><cell>B7</cell><cell>2h 29m 01s</cell><cell>2h 32m 04s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 .</head><label>11</label><figDesc>In this table we report the IoU averaged over 5 runs with different seeds, for efficient-net encoder family with UNet++ architecture.</figDesc><table><row><cell cols="2">Encoder Without APP With APP</cell></row><row><cell>B0</cell><cell>0.3584?0.0002 0.3751?0.0007</cell></row><row><cell>B1</cell><cell>0.4260?0.0005 0.4269?0.0003</cell></row><row><cell>B2</cell><cell>0.3778?0.0007 0.3942?0.0009</cell></row><row><cell>B3</cell><cell>0.3928?0.0006 0.4174?0.0003</cell></row><row><cell>B4</cell><cell>0.4138?0.0003 0.4273?0.0002</cell></row><row><cell>B5</cell><cell>0.3884?0.0001 0.3875?0.0005</cell></row><row><cell>B6</cell><cell>0.4090?0.0008 0.4214?0.0007</cell></row><row><cell>B7</cell><cell>0.3784?0.0009 0.4002?0.0005</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">P.Singh &amp; J.Cirrone</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We would like to thank NYU HPC team for assisting us with our computational needs. We would also like to thank Prof. Elena Sizikova (Moore Sloan Faculty Fellow, Center for Data Science (CDS), New York University (NYU)) for her valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification of melanoma using efficient nets with multiple ensembles and metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhalani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dixit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Intelligence</title>
		<meeting>International Conference on Computational Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="101" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pslsnet: Automated psoriasis skin lesion segmentation using modified u-net-based fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Londhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Semwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sonawane</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2019.04.002</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1746809419300990" />
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="226" to="237" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Increasing prevalence of antinuclear antibodies in the united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dinse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Co</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Zeldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arthritis &amp; Rheumatology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1026" to="1035" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehrenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tincani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andreoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cattalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alijotas-Reig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zinserling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amital</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Covid-19 and autoimmunity</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">102597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning3" />
		<title level="m">Pytorch lightning. GitHub</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoimmune and inflammatory diseases following covid-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galeotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Rheumatology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="413" to="414" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learningfor image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ComputerScience</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Epidemiology and estimated population burden of selected autoimmune diseases in the united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical immunology and immunopathology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The world incidence and prevalence of autoimmune diseases is increasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeremias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthias</surname></persName>
		</author>
		<idno type="DOI">10.12691/ijcd-3-4-8</idno>
		<ptr target="http://pubs.sciepub.com/ijcd/3/4/8" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Celiac Disease</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="151" to="155" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Torch.manual seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno>abs/2109.08203</idno>
		<ptr target="https://arxiv.org/abs/2109.08203" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kellermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mossotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Macarthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ennis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computer vision in autoimmune diseases diagnosis-current status and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Tsakalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Papakostas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Vision and Bio-Inspired Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="571" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Artificial intelligence and deep learning to map immune cell types in inflamed human tissue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Buren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Puranik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Loomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Niewold</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jim.2022.113233</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0022175922000205" />
	</analytic>
	<monogr>
		<title level="j">Journal of Immunological Methods</title>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<biblScope unit="page">113233</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In: Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
