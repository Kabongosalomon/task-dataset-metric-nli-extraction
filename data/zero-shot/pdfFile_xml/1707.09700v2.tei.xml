<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Graph Generation from Objects, Phrases and Region Captions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scene Graph Generation from Objects, Phrases and Region Captions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection, scene graph generation and region captioning, which are three scene understanding tasks at different semantic levels, are tied together: scene graphs are generated on top of objects detected in an image with their pairwise relationship predicted, while region captioning gives a language description of the objects, their attributes, relations and other context information. In this work, to leverage the mutual connections across semantic levels, we propose a novel neural network model, termed as Multi-level Scene Description Network (denoted as MSDN), to solve the three vision tasks jointly in an end-to-end manner. Object, phrase, and caption regions are first aligned with a dynamic graph based on their spatial and semantic connections. Then a feature refining structure is used to pass messages across the three levels of semantic tasks through the graph. We benchmark the learned model on three tasks, and show the joint learning across three tasks with our proposed method can bring mutual improvements over previous models. Particularly, on the scene graph generation task, our proposed method outperforms the stateof-art method with more than 3% margin. Code has been made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding visual scenes is one of the primal goals of computer vision. Visual scene understanding includes numerous vision tasks at several semantic levels, including detecting and recognizing objects, estimating the pairwise visual relations of the detected objects, and describing image regions with free-form sentences. In recent years, great progress has been made to build intelligent visual recognition systems for the three vision tasks, object detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>, scene graph generation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref>, and image/region captioning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The three vision tasks target on different semantic levels of scene understanding. Take the image in <ref type="figure" target="#fig_0">Fig.1</ref> as an example. Object detection focuses on detecting in- dividual objects such as woman, toothbrush, and child. Scene graph generation recognizes not only the objects but also their relationships. Such relationships can be represented by directed edges, which connect two objects as a subject-predicate-object phrase, like woman-usetoothbrush . Region captioning generates a free-form sentence involving uncertain number of the objects, their attributes, and their interactions, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. We can see that though there are connections among the three tasks, the weak alignment across different tasks makes it difficult to learn a model jointly. Our work explores the possibility in understanding the image from these three levels together through a single neural network model.</p><p>The key to connect these three tasks is to leverage the spatial and semantic correlations of their visual features. For example in <ref type="figure" target="#fig_0">Fig. 1</ref>, the phrase woman-watch-child provides the constraint that two persons are interacting with each other. This constraint validates the existence of the woman and child. In addition, the region caption 'mom and her cute babies are brushing their teeth' provides constraints on the existence of the objects (woman, child, and toothbrush), their attributes (cute), and their relationships (the woman is watching the child and they are using toothbrush) within this area. Therefore, the features for these three tasks are highly correlated and can be the complementary information of each other. Based on this observation, we propose to jointly refine the features of different semantic levels by introducing a novel framework to align the three tasks and a message passing structure to leverage the complementary effects for mutual improvements.</p><p>In this work, we propose an end-to-end Multi-level Scene Description Network (MSDN) to simultaneously detect objects, recognize their relationships and predict captions at salient image regions. This model effectively leverages the rich annotations at three semantic levels and their connections for image understanding.</p><p>The contributions of this paper are summarized as follows: 1) We propose a novel model to learn features of different semantic levels by simultaneously solving three vision tasks, object detection, scene graph generation and region captioning. 2) In the model, given an image, a graph is built to align the object, phrase, and caption regions within an image. Since images have different objects, phrases and captions, constructed graphs could be different for different images. We provide a dynamic graph construction layer in the CNN to construct such a graph. 3) A feature refining structure is used to pass message from different semantic levels through the graph. In this way, the three tasks are integrated into one single model, and the features of three semantic levels are jointly optimized. On the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref>, our proposed model outperforms the state-ofart methods on scene graph generation by 3.63%?4.31%. The mutual improvement effects are also shown on the object detection and region captioning tasks. Code has been made publicly available to facilitate further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection: Object detection is the foundation of image understanding. Objects serve as bricks to build up the house of the scene graph. Since CNNs were firstly introduced to the object detection by Girshick et al. in R-CNN <ref type="bibr" target="#b14">[15]</ref>, many region-based object detection algorithms, such as Fast R-CNN <ref type="bibr" target="#b13">[14]</ref>, SPP-Net <ref type="bibr" target="#b17">[18]</ref>, Faster R-CNN <ref type="bibr" target="#b33">[34]</ref>, were proposed to improve the accuracy and speed. Although YOLO <ref type="bibr" target="#b32">[33]</ref> and SSD <ref type="bibr" target="#b29">[30]</ref> further sped up the detection process by sharing more layers between region proposal and region recognition, Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> is still a popular choice for object detection because of its excellent performance. Therefore, we will adopt the pipeline of Faster R-CNN as the basis of our proposed model. Visual Relationship Detection: Visual Relationship detection is not a new concept. It has been investigated by numerous studies in the last decade. In the early days, most works targeted specific types of phrases <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> or used visual phrases to improve other tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, researchers pay more attention to general visual relationship detection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref> . Lu et al. utilized the language prior in detecting visual phrases and their components in <ref type="bibr" target="#b30">[31]</ref>. Li et al. used the message passing structure among subject, object and predicate branches to model their dependencies <ref type="bibr" target="#b27">[28]</ref>. Xu et al. built up a fully-connected graph to iteratively pass messages along the scene graph <ref type="bibr" target="#b39">[40]</ref>. Liang et al. applied the reinforcement learning method to the relationship and attribute detection <ref type="bibr" target="#b28">[29]</ref>. However, connections between phrases and captions are not built up in existing works. In this paper, we will view the objects, phrases and region captions as different semantic levels and build up their connections based on their spatial and semantic relationships.</p><p>Image Captioning: Recently, increasingly more researchers put their attentions on interactions bwtween vision and language <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b6">7]</ref>, of which, image captioning is a fantastic research topic that connects the two areas. It has been investigated for years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, CNN plus RNN has been adopted as the standard pipeline for image captioning task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41]</ref>. Captioning was based on the whole image until the work of Johnson et al. <ref type="bibr" target="#b19">[20]</ref> introduced the dense captioning task which focuses on the regions. Existing works on image/region captioning, however, do not explicitly leverage the scene graph. Our proposed model integrates the highlystructured scene graph into our model to learn better feature for region captioning. And in return, the captioning task can also provide additional information for scene graph generation.</p><p>Multi-task Learning: Multi-task learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47]</ref> has been used to model the relationships among correlated tasks. In <ref type="bibr" target="#b45">[46]</ref>, a convex formulation was derived for multi-task learning. A group of related tasks was identified using statistical models in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>. Multi-task deep learning is used for learning facial key point detection aided by face attributes <ref type="bibr" target="#b47">[48]</ref>. Group sparsity is used in <ref type="bibr" target="#b20">[21]</ref> to determine a group of tasks that will share feature representations. Our work propose a novel way to leverage the complementary effects from three tasks of different semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-level Scene Description Network</head><p>An overview of our proposed MSDN is shown in <ref type="figure">Figure 2</ref>. It adopts the region-based detection pipeline in <ref type="bibr" target="#b33">[34]</ref>. The model contains three parallel branches for three different vision tasks. MSDN is based on the convolutional layers of VGG-16 <ref type="bibr" target="#b36">[37]</ref>, which is shared by the region proposal network (RPN) and recognition network.</p><p>The entire process can be summarized as below: 1) Region proposal. To generate ROIs for objects, phases and, region captions. 2) Feature specialization. Given ROIs, to obtain specialized features that will be used for different semantic tasks. 3) Dynamic graph construction. Dynamically construct a graph to model the connections among feature nodes of different branches based on the semantic and spatial relationships of corresponding ROIs. 4) Feature refining. To jointly refine the features for different tasks by passing messages of different semantic levels along the graph. 5) Final prediction. Using the refined features to classify objects, predicates and generate captions. The scene graph  <ref type="figure">Figure 2</ref>. Overview of MSDN. The two RPNs <ref type="bibr" target="#b33">[34]</ref> for object and caption regions are omitted for simplicity, which share the convolutional layers with other parts. Phrase regions are generated by grouping object regions into pairs. With the region proposals for objects, phrases, and captions, ROI-pooling is used for obtaining their features. These features go through two fully connected layers and then pass messages to each other. After message passing, features for objects are used for object detection, similarly for phrase detection and region captioning. Message passing is guided by the dynamic graph constructed from the object and caption region proposals. Features, bounding boxes and predicted labels for object (red), phrase (green) and region (yellow) are assigned with different colors.</p><p>is generated from detected objects and their recognized relationships (predicate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Region Proposal</head><p>Three sets of proposals are generated:</p><p>? object region proposals: directly generated using Region Proposal Network (RPN) proposed in <ref type="bibr" target="#b33">[34]</ref>;</p><p>? phrase region proposals: grouping the N object proposals to N (N ? 1) object pairs (two identical proposals will not be grouped) which fully connects object proposals with directed edges;</p><p>? caption region proposals: directly generated by another RPN trained with ground truth region bounding boxes.</p><p>RPNs for object and caption region proposals share the base convolutional layers of VGG-16 <ref type="bibr" target="#b37">[38]</ref>. The anchors of two RPNs are generated by clustering the logarithmic widths and heights of ground truth boxes the training set using k-means clustering <ref type="bibr" target="#b16">[17]</ref>. To reduce the size of ROI sets, non-maximum suppression is used for object and caption ROIs separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Specialization</head><p>Different branches correspond to different vision tasks. To make different branches learn their own features, we first feed the three sets of ROIs to ROI-pooling and then use different FC layer sets for different branches. In our implementation, we use two 1024-dim FC layers for each branch. After feature specialization, each branch has its own features for its specific task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Graph Construction</head><p>For different input images, the topology structures of the connections are different. Thus, the connection graph is dynamically built up based on the semantic and spatial relationships among the ROIs.</p><p>Connections between phrases and objects are naturally built during constructing phrase proposals. Each phrase proposal will be connected to two object proposals as a subject-predicate-object triplet with two directed edges. The connection between phrase and caption proposals is obtained based on their spatial relationship. When a caption proposal, denoted by b (r) , covers enough fraction (the threshold 0.7 is used in our experiment) of a phrase proposal, denoted by b (p) , there is an undirected edge between b (r) and b <ref type="bibr">(p)</ref> . We ignore the direct connection between captions and objects for simplicity as they can be connected indirectly through the phrase level.</p><p>From the steps above, a graph is constructed to model the connections among objects, phrases and caption proposals. <ref type="figure" target="#fig_2">Fig. 3</ref> shows an example of this graph.</p><p>The graph G, contains a node set V and an edge set  E. For V , each node in V corresponds to the specialized features of an ROI. The edge set E contains a set of the undirected edges between caption and phrase, E p,r , and two directed edge set, E s,p and E o,p , where s and o denotes the subject and object in the phrase. In the following sections, we will use the denotations for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Refining</head><p>After determining the connections between different levels of nodes, message is passed among features through the edges of the graph. We divide the feature refining procedure into three parallel steps, object refining, phrase refining and caption refining ( <ref type="figure" target="#fig_4">Figure 4</ref>). In addition, the refining procedure can be applied iteratively.</p><p>We will analyze the message passing from phrase nodes to object nodes as an example. And it can be extended to message passing between other types of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Refining Features of Objects</head><p>For each object node, there will be two kinds of connections, subject-predicate and predicate-object. We merge the phrase features into two sets according to the connection type and then refine the object feature with the merged phrase features.</p><p>Phrase feature merge. Since the features from different phrases have different importance factors for refining objects, we use a gate function to determine weights. The features from multiple phrases are averaged by the gate as follows (we use subject-predicate as an example):</p><formula xml:id="formula_0">x (p?s) i = 1 E i,p (i,j)?Es,p ? o,p x (o) i , x (p) j x (p) j<label>(1)</label></formula><p>wherex (p?s) i denotes the average of gated features from the phrase that connects the object by the subject-predicate connections with the i-th object node. E s,p is the set of subject-predicate connections and E i,p denotes the number of phrases connected with the i-th object as the subject ? predicate pairs. ? o,p denotes the gate function for the object-phrase connections which is controlled by the source and target features:</p><formula xml:id="formula_1">? o,p x (o) i , x (p) j = G g=1 sigmoid w (g) o,p ? x (o) i , x (p) j ,<label>(2)</label></formula><p>where G denotes the number of the gate templates for the input features, and we use 128 in our experiment. Each g of w (g) corresponds to a template. When the input feature matches the template, the value after sigmoid will be 1, and the gate will open. The weights w . Then refine the i-th object feature as follows:</p><formula xml:id="formula_2">x (o) i,t+1 = x (o) i,t + F (p?s) x (p?s) i + F (p?o) x (p?o) i (3)</formula><p>where t denotes the refining step since the feature refining can be done iteratively. F (?) = W ? ReLU (?), which is implemented by a ReLU followed by an FC layer because all the features in Eq. 3 are pre-ReLU ones. Since the merged featuresx</p><formula xml:id="formula_3">(p?s) i andx (p?o) i</formula><p>are in the domain of phrase features, we use additional FC layers, F (p?s) and F (p?o) , for modality transformation. In addition, the two FC layers do not share parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Refining Features of Visual Phrase and Caption</head><p>Each phrase node is connected to two object nodes, which are subject and object in the subject ? predicate ? object triplet. And each caption node connects several phrase nodes. Similar to the procedure in refining features of objects, the refinement for phrase and caption also adopt the Merge-and-Refine paradigm:</p><formula xml:id="formula_4">x (p) j,t+1 = x (p) j,t + F (s?p) x (s?p) j + F (o?p) x (o?p) j + F (r?p) x (r?p) j , x (r) k,t+1 = x (r) k,t + F (p?r) x (p?r) k ,<label>(4)</label></formula><p>where x With this feature refining structure, messages are passed through the graph to update the features of objects, phrases, and captions by absorbing supplementary information from the connected nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Scene Graph Generation</head><p>Since the feature refining step has pass message between object and phrase nodes, object and corresponding pairwise relationship categories are predicted directly based on the features of objects and phrases.</p><p>We use a matrix to represent the scene graph, where the element (i, i) at diagonal position is the ith object and the element at the (i, j) position for i = j is the phrase representing the relationship between the ith and jth object. For the ith object, it is predicted as an object class or background from its refined object features. Similarly, the (i, j)th phrase is predicted as a pre-defined predicate class or irrelavant for subject i and object j from phrase features. Then the scene graph is generated based on the matrix. If the object i and j are not classified as background and the predicate (i, j) is not irrelavant , then the two objects are connected through the predicate (i, j). In this way, we will get a scene graph based on the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Region Caption Generation</head><p>Different from the object and phrase nodes, the region features contains a wide range of information, such as objects, their interactions and attributes, scene-related information, etc. Therefore, we feed them into an LSTM-based language model to generate natural sentences to describe the region. We adopt the vanilla language model widely used for image captioning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The language model takes the image vector as input and outputs a free-form sentence to describe the content in the region. The model consists of four parts: 1) an image encoder, which is used to transform the image feature to the same domain of word features; 2) a word encoder, to transform the one-hot vector to a word embedding; 3) a twolayer LSTM model, which is to encode the image information and the temporal dependencies within the sequence; 4) a word decoder, which is used to decode the output feature of LSTM to a distribution over words.</p><p>At the first time step, image vectors are transformed to the same domain of word vectors by image encoder. Then coded image feature will be fed into a two-layer LSTM. At the second step, the start token will be fed into the model to indicate the start of the sentence. Then the predicted word at time t will be fed into the model as input until the end or the maximum length is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>Scene graph generation can be viewed as an intermediate task connecting the object detection and region captioning, which aims at capturing the structural information of an image with a set of pair-wise relationships. Compared to object detection, the scene graph generation measures the feature learning from more aspects. And different from the region captioning, the performance of the scene graph generation model is easier to measure quantitatively and it excludes the influence brought by the different language model implementations. Therefore, the experiment part mainly focuses on this task.</p><p>Some explanatory experiments are also done on the object detection and region captioning tasks to show mutual improvements brought by the joint inference across semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>All the experiments are done on the Visual Genome <ref type="bibr" target="#b22">[23]</ref> dataset. The objects and relationships are from the Relationship subset, and the region caption annotations are based on the Region Description subset. The two subsets share the image but target on different tasks.</p><p>First, we do some preprocessing on the relationship annotations. We normalize the words in different tenses and then select the top-150 frequent object categories and top-50 predicate categories. Moreover, the object boxes whose shorter edges are smaller than 16 pixels are removed. After preprocessing, there are 95998 images left.</p><p>For the remaining 95998 images, we further pre-process the region caption annotations. All the words are changed to lower case. Top-10000 frequent words (including punctuations) are used to build up the dictionary and all the other words are changed to unknown token. In addition, all the small regions with shorter edges smaller than 32 are removed. NLTK <ref type="bibr" target="#b3">[4]</ref> is used to tokenize the sentence.</p><p>After the two preprocessing steps above, a cleansed dataset containing the annotations of localized objects, phrases and region descriptions are built for our experiments. From the 95998 images in the dataset, 25000 images are sampled as the testing set and the remaining 70998 images are used as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Model training details Our model is initialized on the ImageNet pretrained VGG-16 network <ref type="bibr" target="#b37">[38]</ref>. To reduce the number of parameters, we only use 1024 neurons of the fully-connected layers from the original 4096 ones and then scale up the weights accordingly as initialization. The newly introduced parameters are randomly initialized. We first train RPNs and then jointly train the entire model from the base learning rate 0.01 using SGD with gradients clipping. The parameters of VGG convolutional layers are fixed at first, and then trained with 0.1 times the learning rates of other layers after the first decay of the base learning rate. In addition, there is no weight decay for the language model and the parameters are updated using Adam.  <ref type="table">Table 1</ref>. Ablation studies of the proposed model. PredCls denotes predicate recognition task. PhrCls denotes phrase recognition task. SGGen denotes the scene graph generation task. Message passing denotes whether to add feature refining structure to pass message. Cap. branch denotes whether to use the caption branch as an extra connection source. Cap. Supervision indicates whether to use region caption annotation as the supervision to guide the learning of the caption branch. FR-iters denotes the number of feature refining iterations.</p><p>Loss Functions For the object branch, we use the crossentropy loss for the object classification and the smooth L1 loss for the box regression. For the phrase branch, the crossentropy loss is used for predicting the labels of predicates. For the caption branch, the cross-entropy loss is used for generating the every word of free-form sentences and the smooth L1 loss is used for regressing the corresponding proposals. Three losses are summed up equally. Since every step at feature refining parts is differentiable, BP can be applied for the feature refining part.</p><p>Mini-batch preparation for training A mini-batch contains one image. After generating proposals with RPN layers, we use 0.7 and 0.75 as the NMS threshold for object proposals and caption proposals respectively and keep at most 2000 boxes after NMS. Then we sample 256 object proposals and 128 caption proposals from each image. As the number of phrase proposals is too large and the positive samples are sparse, we sample 512 with 25% positive instances. In addition, we assign irrelavant to the negative phrase samples, background to the negative objects, and the end to the negative caption proposals.</p><p>Details for inference. In testing, we set the NMS threshold to 0.35 and 0.45 for object and caption region proposals. After the graph for the image is constructed, features from all the sampled proposals are used for refining their features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Scene Graph Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experiment settings</head><p>Performance Metric. Following <ref type="bibr" target="#b30">[31]</ref>, the Top-K recall (denoted as Rec@K) is used as the main performance metric, which is the fraction of the ground truth instances hit in the top-K predictions. The reason of using recall instead of mean average precision(mAP) is that the annotations of the relationships are incomplete. mAP will falsely penalize the positive but unlabeled predictions. In our experiment, Rec@50 and Rec@100 will be reported.</p><p>Task Settings. Since scene graph generation involves the classification of the subject-predicate-object triplet and localization of objects. We evaluate our proposed model on three sub-tasks of scene graph generationz proposed in <ref type="bibr" target="#b39">[40]</ref>:</p><p>? Predicate Recognition (PredCls): To recognize the relationship between the objects given the ground truth location of object bounding boxes. This task is aimed at examining the model performance on the classification of the predicates alone.</p><p>? Phrase Recognition (PhrCls): To predict the predicate categories as well as the object categories given the ground-truth location of objects. This task evaluates the model performance on the recognition of both predicates and objects.</p><p>? Scene Graph Generation (SGGen): To detect objects and recognize their pair-wise relationships. The object is correctly detected if it is correctly classified and its overlap with the ground truth bounding box is larger than 0.5. A relationship is correctly detected if both the subject and object are correctly detected and the predicate is correctly predicted. The location of objects is not provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with existing works</head><p>We compare our proposed MSDN with the following methods under the three task settings: (1) The model using Language Prior (LP) <ref type="bibr" target="#b30">[31]</ref>, which detects objects first and then estimate the categories of predicate using visual features and word embeddings. (2) Iterative Scene Graph Generation (ISGG) <ref type="bibr" target="#b39">[40]</ref>, which uses the iterative message passing along the scene graph with a GRU-based feature refining scheme. We have reimplemented their model. The model is trained and tested on the cleansed dataset mentioned in Section 4.1. All the methods are based on the VGG-16 model. From the results in <ref type="table">Table 2</ref>, we can see that our proposed model performs better than the existing works. Compared to the ISGG model <ref type="bibr" target="#b39">[40]</ref>, our model introduces the caption branch to provide more context information for phrase  <ref type="table">Table 2</ref>. Evaluation on the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref>. We compare our proposed model with existing works on the three tasks illustrated in Sec. 4.3.1. The result of LP is reported in <ref type="bibr" target="#b39">[40]</ref>. ISGG is reimplemented by ourselves and evaluated on our cleansed dataset.</p><p>recognition. In addition, our model passes message as residual, which makes the model easier to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Component Analysis</head><p>There are many components that influence the performance of MSDN. <ref type="table">Table 1</ref> shows our investigation on the performance of different settings of MSDN on the Visual Genome dataset <ref type="bibr" target="#b22">[23]</ref>.</p><p>Message passing. Model 1 in <ref type="table">Table 1</ref> is the baseline that does not use message passing to refine features and does not have the branch for caption. Model 2 is based on Model 1 and passes message between related object and phrase nodes. By comparing Model 1 and 2 in <ref type="table">Table 1</ref>, we can see that passing message with the feature refining structure proposed in Sec. 3.4 can help to leverage the connection between the objects and phrases, which significantly improves the model performance by 5.34% ? 6.69% on SGGen task.</p><p>Caption region branch. Based on Model 2, Model 3 only has an extra caption branch without the caption supervision. We remove the LSTM language model in <ref type="figure">Fig. 2</ref> and only use the caption branch as extra context information source. Model 3 has 0.47% ? 0.84% gain when compared with Model 2. This improvement is more likely to come from the more parameters introduced by the caption branch.</p><p>Region Caption Supervision. Model 4 further uses additional supervision of region caption sentences for the region caption branch. It outperforms Model 3 by 2.03% ? 2.64%. The improvement mainly comes from the complementary features learned with additional information. Supervision helps the caption branch learn it own specialized features, which can provided extra information for other branches. Compared to the object and predicate categories, region captions provide another way to understand the image.</p><p>The number of feature refining iterations. Model 4?6 are different in the number of iterations in message passing. By comparing Model 4?6, the results show that two iterations may be the optimal settings for the scene graph generation. Compared to Model 4 with one iteration, Model 5 with two iterations constructs the connection between captions and objects indirectly, which brings 0.33% ? 0.49% gain. However, more iterations make the model harder to train. Therefore, when we refine the features for three iterations, the training issue suppress the gain brought by the better feature refining. Therefore, the performance of Model 6 will deteriorate by 0.21% ? 0.27%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Object Detection</head><p>We further evaluate our proposed MSDN on object detection task.</p><p>Setup. We directly use the objects within the dataset prepared in 4.3.1. All the objects have at least one relationship with other objects. We adopt the mean Average Precision (mAP) metric as one evaluation metric. In addition, as most of the objects are small, poor localization of the objects highly influences the mAP metrics, we also report the accuracy of the object classification with the ground truth bounding boxes given.</p><p>We compare our proposed MSDN with Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> (FRCNN) trained on the same dataset. In addition, to check whether the additional supervision can benefit the feature learning of convolutional layers, we also show the results for the model with the feature refining structure removed (Baseline-3-bran.) and use the object branch for object detection (like the model 1 in 1).</p><p>Results. Since the Visual Genome Dataset has many object classes that are small and hard to detect, the mAP is small for all approaches. Nevertheless, our model outperforms Faster R-CNN and baseline model with three separated branches on the Visual Genome Dataset, because our model introduces more context information from phrases and captions ( when trained with more than two iterations) to the objects as complementary source, which serves as visual cues to help recognize objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on Region Captioning</head><p>We further evaluate our model on the region caption task. Setup. We adopt the evaluation metric proposed by Johnson et al. in <ref type="bibr" target="#b19">[20]</ref> for region captioning. It measures the mean Average Precision across a range of thresholds for both localization and language accuracy. The Meteor scores <ref type="bibr" target="#b1">[2]</ref> are used as the language metric, because it is highly correlated with human judgments. During the evaluation, the ground truth bounding regions are merged as one region with several reference annotations if they are heavily overlapped with each other (based on IOU with threshold of 0.7).</p><p>To make the model comparable, we re-implement the main part of Densecap <ref type="bibr" target="#b19">[20]</ref> using Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> pipeline based on VGG-Net (Baseline) and use the same language model as our proposed model. Our implementation performs comparably with the original Densecap under same settings (4.41% vs 4.62% evaluated on our cleansed dataset). In addition, similar to 4.4, we also include another baseline model with three separated branch without message passing (Baseline-3-bran.). All the models are evaluated on our cleansed dataset.</p><p>Quantitative Results. From <ref type="table">Table.</ref> 3, we can see that, our proposed model outperforms the other two baseline models. Because we have excluded the influence brought by the number of parameters and utilized the same language model for them, the gain is obtained by the extra information introduced through the message passing. And the messages passed to the region come from the scene graph composed by the objects and their relationships. Such structural information can help the region branch infer the content of the region. In addition, by comparing the two baseline models, simply introducing extra supervision will not improve the accuracy.</p><p>Qualitative Results. Region captioning results with the highest score are shown in <ref type="figure" target="#fig_7">Figure 5</ref>. We also show the objects and their relationships that are connected to the captions through the dynamic graph. We can see that the region captioning result is highly correlated to the scene graph. We also observe failure case (bottom right in <ref type="figure" target="#fig_7">Figure 5</ref>, where the misclassification of objects and relationships would mislead the caption branch to recognize the region as a large pile of luggage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper targets on scene understanding by jointly modeling three vision tasks, i.e. object detection, visual relationship detection and region captioning, with a single deep neural network in an end-to-end manner. The three tasks at different semantic levels are tightly connected. A Multi-level Scene Description Network (MSDN) model is proposed to leverage such connection for better understanding image. In MSDN, given an input image, a graph is dynamically constructed to establish the links among regions with different semantic meaning. The graph provides a novel way to align features from different tasks. Experimental results show that this joint inference process brings improvement in all the three tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 .</head><label>1</label><figDesc>https://github.com/yikang-li/MSDN Mom and her cute baby are Image with annotations of different semantic levels: objects, phrases and region captions. Scene graph is generated using all objects and their relationships in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Dynamical graph construction. (a) the input image. (b) object(bottom), phrase(middle) and caption region(top) proposals. (c) The graph modeling connections between proposals. Some of the phrase boxes are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Feature refining for object nodes (a), phrase nodes (b) and caption nodes (c). The arrow means passing direction. The two kinds of lines connected to the object nodes are used to distinguish the subject-predicate and predicate-object connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are learned. Similar to the procedure in (1), we can obtain the merged features x (p?o) i for the predicate-object connections. Object feature refining. For the i-th object, there are two merged features,x (p?s) i andx (p?o) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>are respectively the refined phrase features and caption features at time step t + 1.x (s?p) j and x (o?p) j denote the features merged from its subject and object respectively in the subject-predicate-object phrase for j-th phrase node, andx(r?p) j denotes the feature merged from its connected caption nodes.x (p?r) k are the merged feature for the k-th caption node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for region captioning. The most salient regions with captions are shown (yellow boxes). We also show several relationships that are connected to the captions. The connection is built by our proposed dynamic graph generation in Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ID Message Passing Cap. branch Cap. Supervision FR-iters</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PredCls</cell><cell cols="2">PhrCls</cell><cell cols="2">SGGen</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Rec@50 Rec@100 Rec@50 Rec@100 Rec@50 Rec@100</cell></row><row><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0</cell><cell>49.28</cell><cell>52.69</cell><cell>7.31</cell><cell>10.48</cell><cell>2.39</cell><cell>3.82</cell></row><row><cell>2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>63.12</cell><cell>66.41</cell><cell>19.30</cell><cell>21.82</cell><cell>7.73</cell><cell>10.51</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>-</cell><cell>1</cell><cell>63.82</cell><cell>67.23</cell><cell>20.91</cell><cell>23.09</cell><cell>8.20</cell><cell>11.35</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>66.70</cell><cell>71.02</cell><cell>23.42</cell><cell>25.68</cell><cell>10.23</cell><cell>13.89</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>67.03</cell><cell>71.01</cell><cell>24.22</cell><cell>26.50</cell><cell>10.72</cell><cell>14.22</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>66.23</cell><cell>70.43</cell><cell>23.16</cell><cell>25.28</cell><cell>10.01</cell><cell>13.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Object detection and region captioning results evaluated on Visual Genome dataset<ref type="bibr" target="#b22">[23]</ref>. Baseline-3-bran. has 3 separate branches without message passing.</figDesc><table><row><cell>Object Det.</cell><cell cols="3">FRCNN [34] Baseline-3-bran. Ours</cell></row><row><cell>mean AP(%)</cell><cell>6.72</cell><cell>6.70</cell><cell>7.43</cell></row><row><cell>Acc. Top-1(%)</cell><cell>53.57</cell><cell>53.14</cell><cell>61.12</cell></row><row><cell>Acc. Top-5(%)</cell><cell>83.50</cell><cell>83.25</cell><cell>89.86</cell></row><row><cell>Region Caption</cell><cell>Baseline</cell><cell cols="2">Baseline-3-bran. Ours</cell></row><row><cell>AP [20](%)</cell><cell>4.41</cell><cell>4.28</cell><cell>5.39</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06029</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A kmeans clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning cross-modality similarity for multinomial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07571</idno>
		<title level="m">Densecap: Fully convolutional localization networks for dense captioning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficiently selecting regions for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalizing image captions for image-text parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Vip-cnn: Visual phrase guided convolutional neural network. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Phrase localization and visual relationship detection with comprehensive linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06641</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02426</idno>
		<title level="m">Scene graph generation by iterative message passing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multitask learning for classification with dirichlet process priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ppr-fcn: Weakly supervised visual relation detection via parallel pairwise rfcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.3536</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Probabilistic multi-task feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<title level="m">Simple baseline for visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards contextaware interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<title level="m">Care about you: towards large-scale human-centric visual relationship detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
