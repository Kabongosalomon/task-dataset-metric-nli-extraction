<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefeng</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Changxing</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyin</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person re-identification</term>
					<term>part-based models</term>
					<term>image-text retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image person re-identification (ReID) aims to search for images containing a person of interest using textual descriptions. However, due to the significant modality gap and the large intra-class variance in textual descriptions, text-toimage ReID remains a challenging problem. Accordingly, in this paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the above problems. First, we propose a novel method that automatically extracts part-level textual features for its corresponding visual regions. Second, we design a multiview non-local network that captures the relationships between body parts, thereby establishing better correspondences between body parts and noun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use of textual descriptions for other images of the same identity to provide extra supervision, thereby effectively reducing the intra-class variance in textual features. Finally, to expedite future research in text-to-image ReID, we build a new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN outperforms state-of-theart approaches by significant margins. Both the new ICFG-PEDES database and the SSAN code will be released at https: //github.com/zifyloo/SSAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T EXT-to-image person re-identification (ReID) refers to searching for images containing a person of interest (e.g. a missing child) based on natural language descriptions. It is a vital and powerful video surveillance tool when there are no probe images of the target person and only textual descriptions are available. Compared with ReID works using pre-defined attributes <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, textual descriptions contain significantly more information and therefore describe more diverse and more fine-grained visual patterns. Unfortunately, the majority of the existing ReID literature focuses on image-based ReID, with text-to-image ReID still in its infancy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Text-to-image ReID is much more challenging than imagebased ReID. One of the main reasons for this is that textual descriptions are free-form, which creates two main problems. First, as illustrated in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, textual descriptions of the same image may vary dramatically, leading to large intra-class variance in textual features. Second, body parts are usually Zefeng Ding and Zhiyin Shao are with the School of Electronic and Information Engineering, South China University of Technology, 381 Wushan Road, Tianhe District, Guangzhou 510000, China (e-mail: eeze-fengding@mail.scut.edu.cn; eezyshao@mail.scut.edu.cn).</p><p>Changxing Ding is with the School of Electronic and Information Engineering, South China University of Technology, 381 Wushan Road, Tianhe District, Guangzhou 510000, China. He is also with the Pazhou Lab, Guangzhou 510330, China (e-mail: chxding@scut.edu.cn).</p><p>Dacheng Tao is with the JD Explore Academy at JD.com, China (e-mail: taodacheng@jd.com). The man has short dark hair and wears white sneakers, tan pants and a short sleeved maroon shirt. well aligned after pedestrian detection; however, as illustrated in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, body parts may be described in an arbitrary order with various number of words, thereby introducing difficulty in extracting semantically aligned part-level features from both modalities.</p><p>Accordingly, cross-modal alignment is crucial for text-toimage ReID. One popular cross-modal alignment strategy involves adopting attention models to acquire correspondences between body parts and words <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>. However, this strategy depends on cross-modal operations for each image-text pair, which are computationally expensive <ref type="bibr" target="#b8">[9]</ref>. Another intuitive strategy involves splitting one textual description into several groups of noun phrases by using external tools, e.g. the Natural Language ToolKit <ref type="bibr" target="#b9">[10]</ref>. Each group of noun phrases corresponds to one specific body part <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. The downside of this approach is that the quality of textual feature is sensitive to the reliability of the external tools. Furthermore, the splitting operation destroys correlations between noun phrases, degrading the quality of textual feature.</p><p>In this paper, we propose a novel model, named Semantically Self-Aligned Network (SSAN), which efficiently extracts semantically aligned visual and textual part features. SSAN does not split the textual description or perform cross-modal operations; instead, it explores the relatively well aligned body parts in images as supervision and utilizes the contextual cues in language descriptions to achieve this goal. More specifically, we first extract part-level visual features by splitting the feature maps from visual backbone into non-overlapping regions following <ref type="bibr" target="#b13">[14]</ref>. Then, we process each textual description with a bidirectional long short-term memory network (Bi-LSTM) <ref type="bibr" target="#b14">[15]</ref> to capture the relationships among the words. With the aid of contextual cues, we infer the word-part correspondences using a Word Attention Module (WAM) based on the representation of each word. Accordingly, we can obtain the raw part-level textural features with reference to the word-part correspondences. The part-level visual and textual features are further refined by one shared 1 ? 1 convolutional (Conv) layer between the two modalities. Finally, by constraining the partlevel features from both modalities to be similar, WAM is forced to make reasonable predictions and the semantic gap between the two modalities is reduced.</p><p>However, the above model ignores correlations between body parts. In textual descriptions, one noun phrase often covers several body parts (e.g. "long dress"). Moreover, textual descriptions may specify certain spatial relationships between image regions (e.g. "holding a bag"). Accordingly, we propose a multi-view non-local network (MV-NLN) based on the nonlocal attention mechanism <ref type="bibr" target="#b15">[16]</ref> to capture the relationships between body parts. Briefly, we first compute the similarity between the k-th part feature and each of the other part features in a shared embedding space via multi-view projection. Next, the similarity scores specify the strength of the interactions between the k-th part and the other parts. After interaction, the receptive field of each part feature extends to become more consistent with the noun phrases.</p><p>Furthermore, to overcome the intra-class variance in the descriptions, we propose a Compound Ranking (CR) loss. In traditional ranking loss <ref type="bibr" target="#b16">[17]</ref>, positive pairs are composed of exactly matched image-text pairs. Motivated by the observation that one textual description can roughly annotate other images of the same identity, the CR loss adopts them to compose weakly supervised terms to optimize the network. However, the descriptive power of this rough annotation varies dramatically, depending on both the quality of the text and the difference in appearance between two images. Accordingly, we propose a strategy to adaptively adjust the margin for the new loss terms. The CR loss can be considered as a novel data augmentation method.</p><p>Finally, as there is only one large-scale database available (i.e., CUHK-PEDES <ref type="bibr" target="#b5">[6]</ref>), we build a new database named Identity-Centric and Fine-Grained Person Description Dataset (ICFG-PEDES) for text-to-image ReID. Compared with CUHK-PEDES, our new database has three key advantages. First, its textual descriptions are identity-centric and fine-grained; in comparison, the textual descriptions contained in CUHK-PEDES are relatively short and may contain identity-irrelevant details (such as actions and backgrounds). More specifically, ICFG-PEDES has 58% more words per caption on average than CUHK-PEDES. Second, the images included in ICFG-PEDES are more challenging, containing more appearance variability due to the presence of complex backgrounds and variable illumination <ref type="bibr" target="#b17">[18]</ref>. Third, the scale of ICFG-PEDES is larger: it contains 36% more images than CUHK-PEDES. We have made the ICFG-PEDES database publicly available, which is expected to further expedite research in text-to-image ReID.</p><p>We conduct extensive experiments on both the ICFG-PEDES and CUHK-PEDES databases. The results show that SSAN outperforms existing approaches by large margins. Moreover, SSAN has further advantages in terms of efficiency and ease of use.</p><p>The remainder of this paper is organized as follows. Related works are briefly reviewed in Section II. The SSAN model structure is introduced in Section III. Training and inference schemes are described in Section IV. Detailed experiments and their analysis are presented in Section V. Finally, we conclude this paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review the literature on (i) image-text retrieval, and (ii) part feature learning for image-based ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image-text Retrieval</head><p>Image-text retrieval is an important vision and language task. To obtain aligned features from the two modalities, early works typically projected holistic images and textual descriptions into a shared feature space <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>. For example, Frome et al. <ref type="bibr" target="#b18">[19]</ref> proposed to learn linear transformations for image features and skip-gram word features, with a ranking loss employed to aid optimization. More recent approaches have further established accurate correspondences between image regions and words <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b30">[31]</ref>. For example, Zhang et al. <ref type="bibr" target="#b27">[28]</ref> considered both the interactions between the two modalities and semantic relationships in a single modality. Briefly, they adopted a region-to-word attention model to align cross-modal features, while also utilizing a second-order attention model to explore intra-modal correlations.</p><p>Due to its fine-grained nature, text-to-image ReID is one of the most challenging image-text retrieval tasks. Depending on the strategy used to align cross-modal features, existing works can be divided into two categories as follows:</p><p>Novel Model Structures. These works usually design various attention-based models to establish region-word [6]- <ref type="bibr" target="#b7">[8]</ref> or region-phrase <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> correspondences. For example, as a region-word-based method, Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed a model that computed the affinity between each word and image region pair. They then selected the besting matching image region for each word. The final image-text affinity score was obtained via a word attention sub-network. Among the regionphrase-based methods, Niu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a cross-modal attention model to align features from the two modalities at the global-to-global, global-to-local, and local-to-local levels in order to extract multi-granular features. However, these works require cross-modal operations for each image-text pair, which introduces a high computational cost <ref type="bibr" target="#b8">[9]</ref>. Recently, Wang et al. <ref type="bibr" target="#b12">[13]</ref> proposed an approach that is free from cross-modal operations. They aligned body parts with noun phrases with the help of an auxiliary segmentation layer and an external toolkit for sentence parsing. However, the textual features' quality relies on the reliability of the external tools.</p><p>New Optimization Strategies. These works utilize various objective functions to optimize text-to-image ReID models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b35">[36]</ref>. For example, Faghri et al. <ref type="bibr" target="#b16">[17]</ref> proposed a ranking loss to minimize the intra-class distance and maximize the inter-class distance. Sarafianos et al. <ref type="bibr" target="#b31">[32]</ref> adopted an adversarial loss to drive textual features to be close enough to visual features such that a modality discriminator could not distinguish the two. Zheng et al. <ref type="bibr" target="#b33">[34]</ref> proposed an instance loss that achieved better inter-modal alignment by sharing classifiers for the two modalities. However, these methods do not explicitly solve the significant intra-class variance problem in the textual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Part Feature Learning for Image-based ReID</head><p>Part-based methods are popular for image-based ReID, as part-level representations include fine-grained features and thus effectively alleviate the overfitting risk associated with deep models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Early methods extract part-level representations from uniformly partitioned feature maps produced by a CNN backbone <ref type="bibr" target="#b13">[14]</ref>, as body parts were roughly aligned after pedestrian detection. In comparison, recent works promote the quality of part features by accounting for slight changes in part locations.</p><p>One intuitive strategy involves detecting body parts before feature extraction. Methods in this category detect body parts by utilizing the outside tools <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b42">[43]</ref> or the attention mechanism [44]- <ref type="bibr" target="#b47">[48]</ref>. For example, Guo et al. <ref type="bibr" target="#b42">[43]</ref> detected body parts using a human parsing tool <ref type="bibr" target="#b48">[49]</ref> and important accessories with a self-attention scheme. A few recent methods have attempted to bypass part detection during the testing stage <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>; this has been implemented via more complicated training strategies, e.g. the teacher-student training scheme <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> or multi-task learning <ref type="bibr" target="#b49">[50]</ref>. For example, Zhang et al. <ref type="bibr" target="#b50">[51]</ref> leveraged a 3D person model to construct a set of semantically aligned part images for each training image. These part images enable a teacher model to learn semantically aligned part features. Through the use of this teacher-student training scheme, the teacher model transfers the body part concepts to a student model, which enables the student model to independently extract part-aware features in the testing phase.</p><p>While the methods discussed above are powerful, they also tend to utilize complex model architectures or training strategies to obtain high-quality part-level visual features. In this paper, our research emphasizes the efficient extraction of high-quality part-level textual features. Therefore, we adopt the simple uniform partitioning strategy for part-level visual feature extraction <ref type="bibr" target="#b13">[14]</ref>. It is worth noting that if the quality of part-level visual features is improved, we can accordingly obtain better part-level textual features by the proposed SSAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SEMANTICALLY SELF-ALIGNED NETWORK</head><p>The overall architecture of SSAN is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. In what follows, we first introduce the backbone for representation extraction in Section III-A and then describe the branches for global and part feature learning in Sections III-B and III-C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Backbone</head><p>Visual Representation Extraction: We adopt the popular ResNet-50 <ref type="bibr" target="#b52">[53]</ref> model as the visual feature extraction backbone. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we first extract the feature maps F ? R H?W ?C , where H, W , and C represent the height, width, and channel number of the feature maps, respectively. For the global branch, we directly utilize F to learn the global visual feature. For the part branches, we first uniformly partition F into K non-overlapping parts F k ? R C?H ?W (1 ? k ? K), following <ref type="bibr" target="#b13">[14]</ref>. We then extract the part-level visual features from F k . Textual Representation Extraction: We build a matrix W e ? R V ?U containing word embeddings of all unique words in the training set. Here, V and U denote the word embedding dimension and the number of unique words, respectively. Given a description D of length n, we obtain the word embedding</p><formula xml:id="formula_0">x i ? R V for its i-th word from W e .</formula><p>To capture the relationships among the words, we adopt a bidirectional long short-term memory network (Bi-LSTM) <ref type="bibr" target="#b14">[15]</ref> as our textual backbone. Bi-LSTM processes word embeddings from both x 1 to x n and x n to x 1 as follows:</p><formula xml:id="formula_1">? ? h i = ????? LST M (x i , ? ? h i?1 ),<label>(1)</label></formula><formula xml:id="formula_2">? ? h i = ????? LST M (x i , ? ? h i+1 ),<label>(2)</label></formula><p>where ? ? h i , ? ? h i ? R C and represent the forward and backward hidden states of the i-th word, respectively. Next, the representation for the i-th word is defined as follows:</p><formula xml:id="formula_3">e i = ? ? h i + ? ? h i 2 .<label>(3)</label></formula><p>Finally, we stack e i (1 ? i ? n) to represent the textual description D, as follows:</p><formula xml:id="formula_4">E = [e 1 , e 2 , ..., e n ],<label>(4)</label></formula><p>where E ? R C?n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Global Feature Extraction</head><p>Following the recent works <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, SSAN aslo projects holistic visual and textual features into a common space. To obtain the global features, we first perform Global Max Pooling (GMP) on F and Row-wise Max Pooling (RMP) on E. We then project the obtained features into a common feature space via a shared 1 ? 1 Conv layer W g :</p><formula xml:id="formula_5">v g = W g GM P (F),<label>(5)</label></formula><formula xml:id="formula_6">t g = W g RM P (E),<label>(6)</label></formula><p>where W g ? R M ?C . v g , t g ? R M and represent the global visual and textual features. Compared to previous methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, the weight-sharing strategy on W g encourages F and E to be more tightly aligned in terms of semantics. In this way, our global branch outperforms that presented in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, as demonstrated in experiment below. Eventually, the similarity between global features of one image-text pair is denoted as follows:</p><formula xml:id="formula_7">S g = v T g t g v g t g .<label>(7)</label></formula><p>Visual Feature Maps ( )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Feature Vectors ( )</head><p>Her hair is in a ponytail with a white headband, she has on a white polo shirt, denim Capri's, white sneakers and is carrying a brown cross body bag.  The other K-1 part branches have the same structure as the k-th one. For the global branch, we adopt a weight sharing strategy on the last 1 ? 1 Conv layer, which aligns the features from the two modalities more tightly in terms of semantics. Each part branch includes one Part-specific Feature Learning (PFL) module and one Part Relation Learning (PRL) module. The former module enables SSAN to automatically extract part-level features from both modalities, without using any external tools or cross-modal operations; the latter enables SSAN to capture the relationships between body parts so as to establish better semantic correspondences with noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-50</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Part-level Feature Extraction</head><p>Part-level representations are essential for ReID <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. However, as explained in <ref type="figure" target="#fig_1">Fig. 1</ref>, extracting part-level textual features is challenging. Therefore, we introduce the part branches in SSAN that efficiently extract semantically aligned part-level visual and textual features. Each part branch includes one Part-specific Feature Learning (PFL) module and one Part Relation Learning (PRL) module.</p><p>1) Part-specific Feature Learning: To obtain part-level textual features, existing approaches <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> usually first detect noun phrases using external tools and then extract textual features for each part. However, this strategy breaks textual context. For example, as illustrated in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, the noun phrase "white line" may relate to different items of clothing or accessories; without textual context, the correspondence between "white line" and body parts cannot be inferred.</p><p>It is therefore highly desirable to extract part-level textual features directly from the original textual description without external tools. As shown in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, the body parts in images are usually well-aligned. Therefore, we utilize the well-aligned human body parts as supervision to facilitate achieving this goal. Furthermore, we argue that after the LSTM processing is complete, e i will have obtained contextual cues that can be used to infer which part the i-th word corresponds to. Accordingly, we propose the following approach for the efficient extraction of semantically aligned part-level visual and textual features.</p><p>First, we introduce the Word Attention Module (WAM) to infer the word-part correspondences. As illustrated in <ref type="figure" target="#fig_3">Fig. 3(b</ref>  The WAM model structure. We take the k-th part branch as an example. In this figure, k is equal to 3.</p><p>we predict the probability that the i-th word belongs to the k-th part as follows:</p><formula xml:id="formula_8">s k i = ?(W k p e i ),<label>(8)</label></formula><p>where s k i denotes the probability and ? stands for the sigmoid function. W k p ? R 1?C . We modify E to represent the textual <ref type="figure">Fig. 4</ref>: (a) One noun phrase may cover two or more equally divided body parts, e.g. "long dress". (b) Textural descriptions may include relationships between parts or image regions, e.g. "holding a bag". (c) The MV-NLN model structure. We take the k-th visual part feature v k l as an example. In this figure, k is equal to 3.</p><formula xml:id="formula_9">? + ? | =1, ? 1?1 Conv (K-1) 1?1 Conv 1?1 Conv 1?1 Conv (c) (b) (a) ?</formula><p>description for the k-th part as follows:</p><formula xml:id="formula_10">E k = [s k 1 e 1 , s k 2 e 2 , ..., s k n e n ].<label>(9)</label></formula><p>Second, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, we obtain the visual features for the k-th part by feeding F k into one GMP layer and one 1 ? 1 Conv layer. Similarly, we generate the k-th part-level textual features by performing RMP on E k and feed it to the same 1 ? 1 Conv layer as F k . Formally,</p><formula xml:id="formula_11">v k l = W k l GM P (F k ),<label>(10)</label></formula><formula xml:id="formula_12">t k l = W k l RM P (E k ),<label>(11)</label></formula><p>where W k l ? R M ?C and denotes the parameters of the shared 1 ? 1 Conv layer. v k l , t k l ? R M and represent the visual and textual features for the k-th part, respectively.</p><p>By constraining v k l and t k l to be both discriminative and similar, WAM is forced to make reasonable predictions about word-part correspondences. It is worth noting that one word may correspond to several parts, as explained in <ref type="figure">Fig. 4(a)</ref>. Moreover, the shared 1 ? 1 Conv layer is forced to select elements relevant to the k-th part from F k and E k . In this way, we can obtain semantically aligned part-level visual and textual features without any external tools.</p><p>Finally, the similarity between the part-level features of one image-text pair is denoted as follows:</p><formula xml:id="formula_13">S l = v T l t l v l t l ,<label>(12)</label></formula><p>where v l and t l ? R KM . They are obtained by concatenating the K part-level visual and textual features, respectively.</p><p>2) Part Relation Learning: The equal partition strategy on F is effective for image-based ReID <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b36">[37]</ref>. However, it maybe suboptimal for text-to-image ReID, as one phrase may cover two or more equally divided parts (e.g. "long dress" in <ref type="figure">Fig. 4(a)</ref>). Moreover, textual descriptions may specify relationships between parts (e.g. "holding a bag" and "a bag cross chest" in <ref type="figure">Fig. 4(b)</ref>). In this case, the correlation between parts is vital for differentiating the two phrases.</p><p>Here, we introduce the multi-view non-local network (MV-NLN) to address these problems. In the following, we take the k-th visual part feature as an example. As illustrated in <ref type="figure">Fig. 4(c)</ref>, we first compute the similarity between v k l and v i l (i = k) in a shared embedding space via multi-view projections:</p><formula xml:id="formula_14">S ki = ? k (v k l ) T ? i (v i l ) ? k (v k l ) ? i (v i l ) ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_15">? k (v k l ) = W k ? v k l , ? i (v i l ) = W i ? v i l . W k ? , W i ? ? R M ?M .</formula><p>Then, the interaction strength between the k-th visual part feature and the other K-1 part features can thus be denoted as follows:</p><formula xml:id="formula_16">? ki = exp(S ki ) K i=1,i =k exp(S ki ) ,<label>(14)</label></formula><p>and ? ki is utilized to aggregate the K-1 part features:</p><formula xml:id="formula_17">v k lin = W k ? ( K i=1,i =k ? ki ? i (v i l )).<label>(15)</label></formula><p>Finally, the part-level visual features produced by MV-NLN can be denoted as follows:</p><formula xml:id="formula_18">v k n = W k n (v k l + v k lin ),<label>(16)</label></formula><p>where W k n ? R N ?M and W k ? ? R M ?M . Similar to the visual features, we also process the partlevel textual features by using MV-NLN to capture their correlations. Note that the parameters of MV-NLN are shared between the two modalities. Similar to S g and S l , we adopt the cosine metric to evaluate the similarity between the features produced by MV-NLN for one image-text pair:</p><formula xml:id="formula_19">S n = v T n t n v n t n ,<label>(17)</label></formula><p>where v n and t n are obtained by concatenating the K partlevel visual and textual features produced by MV-NLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIMIZATION</head><p>The popular ranking loss <ref type="bibr" target="#b16">[17]</ref> applies a constraint such that the intra-class similarity score must be larger than the interclass similarity by a margin of ? as follows:</p><formula xml:id="formula_20">L r = max(? ? S(I p , D p ) + S(I p , D n ), 0) + max(? ? S(I p , D p ) + S(I n , D p ), 0),<label>(18)</label></formula><p>where I p and D p are drawn from a matching image-text pair. D n and I n denote the hardest negative text for I p and the hardest negative image for D p in a mini-batch, respectively. However, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, textual descriptions are flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Matching:</head><p>The woman has a pony tail and wears a white shirt with a dark picture and has a green shorts with grey flat shoes and wears a bag on her left shoulder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Weakly matching:</head><p>The lady has long black hair. She is wearing a sleeveless white dress that hangs to just above her knees. She carries a stool in her hands. The above ranking loss utilizes only the matching image-text pairs to compose positive pairs, which may result in a risk of overfitting. As <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates, textual descriptions can approximately annotate other images of the same identity; in other words, each textual description can be considered as a coarse caption for the other images of the same identity. Inspired by this observation, we propose a Compound Ranking (CR) loss that includes both strong and weak supervision terms. The positive pairs in strong supervision terms are drawn from image-text pairs that exactly match. By contrast, the positive pairs in weak supervision terms are composed of one image and the textual description for another image of the same identity. In this way, the CR loss exploits more diverse textual descriptions for each training image, which acts as a data augmentation strategy. Formally, the CR loss is defined as follows:</p><formula xml:id="formula_21">L cr = max(? 1 ? S(I p , D p ) + S(I p , D n ), 0) + max(? 1 ? S(I p , D p ) + S(I n , D p ), 0) + ? ? max(? 2 ? S(I p , D p ) + S(I p , D n ), 0) + ? ? max(? 2 ? S(I p , D p ) + S(I n , D p ), 0),<label>(19)</label></formula><p>where D p refers to the textual description for another image of the same identity as I p . ? 1 and ? 2 indicate the margins. ? denotes the weight for the weak supervision terms.</p><p>However, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>, the descriptive power of D p for I p varies due to the rich intra-class variance in image appearances, indicating that a fixed margin in the two weak supervision terms may be suboptimal. To overcome this problem, we propose the following strategy to adaptively adjust the value of ? 2 :</p><formula xml:id="formula_22">? 2 = (? + 1) ? 1 2 ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_23">? = min( S(I p , D p ) S(I p , D p ) , 1).<label>(21)</label></formula><p>(a) The man is wearing high top sneakers. He has on track ants and a grey tshirt. He is moving his right foot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) A woman in white Capri's stands in an Asian market, appearing to look at something on the shelves.</head><p>(c) The man wears a pink and white shirt blue pants with black. He walks towards a bike and grey railing fence.</p><p>(d) A lean woman with short dark hair is carrying a tote bag of blue color. She is wearing a black knee-length winter blazer over a knee-length grey dress and black ankle leather boots.</p><p>(e) A tall, thin man in his late 30's with short black hair is wearing a white shirt, blue tie, and a black colored formal suit along with formal black shoes.</p><p>(f) A young woman is wearing a red woolen sweater over a white shirt and light blue skin-fitted jeans. She is also wearing brown boots with dark brown fur and wearing a red checkered scarf around her neck. She has grey gloves and earphones plugged in her ears. She has long black hair. The CR loss and the popular ID loss <ref type="bibr" target="#b33">[34]</ref> are employed together to optimize the global features, the part features produced by PFL, and the part features produced by PRL, respectively. Note that the ID loss is imposed on each of the K part features, while the CR loss is imposed on the concatenated K part features. The weights of all loss terms for the three types of features are set to 1, 0.5, and 0.5.</p><formula xml:id="formula_24">(a) (b) (c) (d) (e) (f)</formula><p>In the testing stage, the overall similarity score between one image-text pair is the sum of S g , S l , and S n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we conduct experiments on the CUHK-PEDES database <ref type="bibr" target="#b5">[6]</ref>, as well as our newly constructed ICFG-PEDES database. Following <ref type="bibr" target="#b5">[6]</ref>, we adopt Rank-1, Rank-5, and Rank-10 accuracies to evaluate performance on both databases.</p><p>CUHK-PEDES contains 40,206 images and 80,412 textual descriptions for 13,003 identities with two captions per image. There are on average 23.5 words in each textual description. In line with the official evaluation protocol, the training set includes 34,054 images and 68,108 textual descriptions for 11,003 persons. The validation and test sets include data for 1,000 persons, respectively. The images and textual descriptions of the testing data make up the gallery set and probe set, respectively.</p><p>As there is only one large-scale database available for textto-image ReID. It is hard for existing works to reliably verify the effectiveness of text-to-image ReID methods. Moreover, as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>, in CUHK-PEDES, textual descriptions may contain identity-irrelevant details (e.g. actions and backgrounds). Therefore, we construct a new database named ICFG-PEDES. The new database contains more identitycentric and fine-grained textual descriptions than CUHK-PEDES. It contains a total of 54,522 pedestrian images of 4,102 identities. All images are gathered from the MSMT17 database <ref type="bibr" target="#b17">[18]</ref>. There is one caption per image and there are on average 37.2 words for each description. The database contains a total of 5,554 unique words. Similar to the protocol in the original MSMT17 database, we divide ICFG-PEDES into a training set and a testing set: the former comprises 34,674 image-text pairs of 3,102 persons, while the latter contains 19,848 image-text pairs for the remaining 1,000 persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Following a recent text-to-image ReID work <ref type="bibr" target="#b12">[13]</ref>, we resize all images to 384 ? 128 pixels and adopt random horizontal flipping for data augmentation. To facilitate fair comparison with existing approaches, we adopt VGG-16 <ref type="bibr" target="#b53">[54]</ref> and ResNet-50 <ref type="bibr" target="#b52">[53]</ref> as the visual backbone, respectively; both backbones are pre-trained on the ImageNet database <ref type="bibr" target="#b54">[55]</ref>. For textual descriptions, we count the unique words to build a vocabulary in the training set. Following previous methods <ref type="bibr" target="#b12">[13]</ref>, K, V , M , N , and ? 1 are set to 6, 512, 1024, 512, and 0.2, respectively. M and ? are empirically set to 512 and 0.1, respectively. During training, we adopt Adam <ref type="bibr" target="#b55">[56]</ref> as the optimizer. We set the batch size and number of epochs to 64 and 60, respectively. The learning rate is initially set as 0.001.</p><p>The baseline model in this section refers to the model in which we remove all part branches and the CR loss from SSAN; accordingly, the baseline model only extracts global features, as described in Section III-B. It adopts the crossentropy loss and ordinary ranking loss <ref type="bibr" target="#b16">[17]</ref> for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>In the following, we conduct ablation study to analyze the effectiveness of each key component of SSAN, i.e., Part Feature Learning module (PFL), Part Relation Learning module (PRL), and the CR loss. Experimental results on both databases are summarized in <ref type="table" target="#tab_2">Table I.</ref> 1) Effectiveness of PFL: In this experiment, we demonstrate the effectiveness of Part Feature Learning module. <ref type="table" target="#tab_2">Table I</ref> presents the result of equipping the "baseline" with K part branches adopting the PFL module alone. As shown in <ref type="table" target="#tab_2">Table I</ref>, PFL promotes the Rank-1 accuracy of the baseline model by 4.58% and 3.56% on CUHK-PEDES and ICFG-PEDES, respectively. The above comparisons demonstrate the effectiveness of PFL for automatically learning semantically aligned part-level features.</p><p>To further demonstrate the effectiveness of PFL in automatically extracting semantically aligned part-level textual features, we compare its performance with two representative approaches, i.e., SCAN <ref type="bibr" target="#b25">[26]</ref> and ViTAA <ref type="bibr" target="#b12">[13]</ref>, in <ref type="table" target="#tab_2">Table II</ref> words, and subsequently extract semantically aligned partlevel textual features. ViTAA manually determines the correspondence between noun phrases and body parts. We directly report the performance of ViTAA in its original paper <ref type="bibr" target="#b12">[13]</ref>. Moreover, as our proposed WAM is able to predict the correspondence between body parts and words, we embed it into the framework of ViTAA to replace the manual annotations. This model is denoted as ViTAA * in <ref type="table" target="#tab_2">Table II</ref>. From the comparisons between the above models, we can make the following observations. First, PFL consistently outperforms both SCAN and ViTAA by considerable margins. Second, with our WAM, ViTAA * significantly outperforms ViTAA significantly in terms of Rank-1 accuracy. This means that WAM more accurately predicts the correspondence between words and body parts. One reason for this is that ViTAA <ref type="bibr" target="#b12">[13]</ref> only annotates the word-part correspondence for nouns. In comparison, by utilizing the contextual cues, WAM is able to predict the correspondence for all words, e.g., nouns and adjectives. The above comparisons further demonstrate the advantages of the proposed PFL.</p><p>2) Effectiveness of PRL: We next equip the baseline with both PFL and MV-NLN. As shown in <ref type="table" target="#tab_2">Table I</ref>, PRL further improves ReID performance by 1.33% and 0.95% in terms of Rank-1 accuracy on CUHK-PEDES and ICFG-PEDES respectively.</p><p>To ensure that SSAN benefits from the use of correlations between parts, we remove all layers except for the last 1 ? 1 Conv of MV-NLN in PRL; the other experimental settings remain the same. As shown in <ref type="table" target="#tab_2">Table I</ref>, this model yields limited performance improvements on both databases, which indicate that the improvements achieved by PRL are due to its part-relation modeling ability.</p><p>3) Effectiveness of CR loss: We next explore the effectiveness of the CR loss. As shown in <ref type="table" target="#tab_2">Table I</ref>, the CR loss promotes the Rank-1 accuracy of the baseline model by 1.62% and 1.21% on CUHK-PEDES and ICFG-PEDES, respectively.</p><p>Furthermore, we equip the baseline model with PFL, MV-NLN, and the CR loss together, referred to as SSAN in <ref type="table" target="#tab_2">Table I</ref>. Compared with the model that uses PFL and MV-NLN alone, the CR loss promotes the Rank-1 accuracy by 0.78% and 0.7% on the two databases, respectively.</p><p>Finally, we provide comparisons between adaptive margins and fixed margins for the weak supervision terms in   <ref type="figure" target="#fig_6">Fig. 7</ref>. It is shown that SSAN with adaptive margins consistently outperforms SSAN with fixed margins on both databases. Compared with fixed margins, adaptive margins are more flexible to the dramatic variation in the descriptive power of the rough annotations. The above comparisons demonstrate the effectiveness of the proposed CR loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Performance Comparisons on CUHK-PEDES:</head><p>We compare the performance of SSAN with state-of-the-art methods on the CUHK-PEDES database in <ref type="table" target="#tab_2">Table III</ref>. Most approaches in this table are also part-based <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Depending on which backbone is adopted, we divide these approaches into two groups: VGG-16-based methods and ResNet-50based methods. We can make the following observations from <ref type="table" target="#tab_2">Table III.</ref> First, thanks to the weight-sharing strategy, our baseline model performs better than that in ViTAA <ref type="bibr" target="#b12">[13]</ref>. Without the weight-sharing strategy, our baseline achieves 53.12% Rank-1 accuracy with the ResNet-50 backbone, which is approximately the same as that in ViTAA.</p><p>Second, SSAN significantly outperforms all existing approaches in terms of all metrics. More specifically, SSAN outperforms the most recent method ViTAA <ref type="bibr" target="#b12">[13]</ref> by 5.4% in terms of Rank-1 accuracy. It is worth noting that both ViTAA and SSAN adopt the same input image size and backbones. Of the VGG-16-based methods, SSAN outperforms TDE <ref type="bibr" target="#b56">[57]</ref> by 3.15% in terms of Rank-1 accuracy. Another advantage of SSAN is its ability to automatically extract semantically aligned part features from both modalities. By comparison, the other methods typically rely on external tools to extract partlevel textual features <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. These comparisons demonstrate the effectiveness of SSAN.</p><p>2) Performance Comparisons on ICFG-PEDES: We evaluate the performance of existing methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> with open-source code on the new ICFG-PEDES database. Furthermore, as shown in <ref type="table" target="#tab_2">Table II</ref>, ViTAA * both outperforms ViTAA and enjoys the advantage of independence on manual   <ref type="bibr" target="#b60">[61]</ref>. The CUHK-PEDES database is divided into five independent subsets as five domains according to the source of the images, i.e., CUHK03 <ref type="bibr" target="#b61">[62]</ref>, Market-1501 <ref type="bibr" target="#b62">[63]</ref>, SSM <ref type="bibr" target="#b63">[64]</ref>, VIPER <ref type="bibr" target="#b64">[65]</ref>, and CUHK01 <ref type="bibr" target="#b65">[66]</ref>. SSM (S) is chosen as the source domain and there are four transfer tasks, namely, S?C03 (CUHK03), S?M (Market-1501), S?V (VIPER), and S?C01 (CUHK01). For each task, there are two different settings: (i) Source Only (SO). In this setting, one model is  <ref type="table" target="#tab_6">Table V</ref>, we directly cite their performance reported in <ref type="bibr" target="#b60">[61]</ref>. From these results, we can make the following observations. First, in the SO setting, SSAN outperforms all other methods in <ref type="table" target="#tab_6">Table V</ref> by considerable margins for all four transfer tasks. More specifically, SSAN outperforms SCAN [26] by 4.3%, 6.9%, 11.2%, and 12.3% in terms of Rank-1 accuracy on the four transfer tasks, respectively. This is because SCAN extracts part-level textual features by performing cross-modal operations; accordingly the quality of the extracted part-level textual features is vulnerable to the change in visual domain. By contrast, SSAN extracts the part-level textual features with the help of WAM, which is independent from the visual modality during the inference stage. Therefore, SSAN is more robust than SCAN in these four transfer tasks.</p><p>Second, SSAN significantly outperforms all existing approaches in the ST setting in terms of all metrics. Specifically, SSAN outperforms the most recent method MAN [61] by 6.0%, 6.0%, 2.1%, and 12.3% in terms of Rank-1 accuracy on the four transfer tasks, respectively. The superior performance of SSAN is not only due to the extraction of part-level features, but also due to its robustness to the change of visual domain during the inference stage. These experimental results justify the robustness of SSAN for text-to-image ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head><p>As mentioned in Section III-C, WAM utilizes the relatively well-aligned visual body parts as supervision and the contextual cues in textual description to infer which part one word corresponds to. Therefore, WAM not only makes correct predictions for words that correspond to fixed body parts, but also makes reasonable predictions for words that correspond to objects with flexible positions. Here we provide the qualitative results about these two abilities in <ref type="figure" target="#fig_7">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>, respectively. Each score denotes the probability that one word belongs to a certain body part. From the figure, we can make the following observations. First, supervised by the relatively well-aligned body parts in images, WAM makes correct predictions for words that vest shirt pants shoes hat Captions?This man is wearing a green vest over a blue short sleeved shirt, black pants, and black shoes. He is also wearing a red hat. <ref type="bibr">Part</ref>  Captions?The woman has a shoulder length hair with a dark long to the floor ruffled dress that is sleeveless and zips up back. correspond to fixed body parts. For example, as illustrated in <ref type="figure" target="#fig_7">Fig. 8</ref>(a)-(b), the word "shirt" has high scores for the second and third parts. By contrast, both of the word "jumpsuit" in <ref type="figure" target="#fig_7">Fig. 8</ref>(c) and the word "dress" in <ref type="figure" target="#fig_7">Fig. 8(d)</ref> have high scores on the second to the fifth parts. The above qualitative results are in line with our common sense, which verify the reliability of WAM's predictive ability for the words that correspond to fixed body parts.</p><p>Second, by utilizing the contextual cues in textual description, WAM also makes reasonable predictions for words that correspond to objects with flexible positions. For example, the word "bag" in the phrase "a bag over his left shoulder" has high scores on the second and third parts in <ref type="figure">Fig. 9(a)</ref>. By contrast, the word "bag" in the phrase "a bag in his right hand" has high scores on the fourth and fifth parts in <ref type="figure">Fig. 9(b)</ref>. The above qualitative results demonstrate the effectiveness of WAM's predictive ability for the words that correspond to objects with flexible positions. man shirt shorts hat bag Captions?A man is wearing a blue shirt, a pair of white and black shorts, a red hat and a bag over his left shoulder. <ref type="bibr">Part</ref>   <ref type="figure">Fig. 9</ref>: Illustration of WAM's prediction for one word that corresponds to object with flexible positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a novel model, named SSAN, to automatically extract semantically aligned features from the visual and textual modalities for text-to-image ReID. Specifically, we introduce a word attention module that reliably attends to part-relevant words. This enables SSAN to automatically extract part-level features for the two modalities by shared 1 ? 1 Conv layers. We further propose a multi-view non-local network to capture the relationships between body parts. Moreover, to overcome the large intraclass variance problem in textual descriptions, we propose a CR loss including both strong and weak supervision terms. Finally, to expedite research in text-to-image ReID, we build a new database that features identity-centric and fine-grained textual descriptions. Extensive experiments on two databases demonstrate the effectiveness of SSAN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Textual descriptions are free-form, which introduces unique challenges to text-to-image ReID. (a) Textual descriptions for the same image may vary dramatically. (b) Body parts may be described in an arbitrary order. The numbers in this figure indicate the order in which the body parts are described.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>The SSAN model architecture. Based on the ResNet-50 and Bi-LSTM backbones, SSAN extracts global and part features respectively from both modalities. For simplicity, only the global branch and the k-th part branch are shown. k is equal to 3 in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) The same noun phrase (e.g., "white line") can be associated with different items of clothing or accessories. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Textual descriptions can roughly annotate the other images of the same identity. However, their descriptive power varies dramatically. (a) The textual description for the left image is also nearly perfect for the right one. (b) The textual description for the left image is only weakly relevant to the right image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Comparisons between textual descriptions in the CUHK-PEDES and ICFG-PEDES databases. (a)-(c) Descriptions from CUHK-PEDES. The colored words are identityirrelevant. (d)-(f) Descriptions from ICFG-PEDES show that the descriptions are more fine-grained in this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>. In our experiments, SCAN adopts the same model architecture as SSAN to extract global features and part-level visual features. It employs cross-modal operations to align body parts and Comparisons between adaptive margins and fixed margins for the weak supervision terms in the CR loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Illustration of WAM's prediction for one word that corresponds to fixed body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Ablation Study on Each Component of SSAN</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Components</cell><cell></cell><cell></cell><cell>CUHK-PEDES</cell><cell></cell><cell></cell><cell>ICFG-PEDES</cell><cell></cell></row><row><cell>Metric</cell><cell cols="3">Global PFL MV-NLN CR loss</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.68</cell><cell>75.42</cell><cell>82.73</cell><cell>49.02</cell><cell>69.05</cell><cell>76.55</cell></row><row><cell>+ PFL</cell><cell></cell><cell>-</cell><cell>-</cell><cell>59.26</cell><cell>78.56</cell><cell>85.32</cell><cell>52.58</cell><cell>71.70</cell><cell>78.75</cell></row><row><cell>+ PFL + PRL</cell><cell></cell><cell></cell><cell>-</cell><cell>60.59</cell><cell>79.37</cell><cell>86.01</cell><cell>53.53</cell><cell>72.38</cell><cell>79.23</cell></row><row><cell>+ PFL+1?1 Conv</cell><cell></cell><cell>-</cell><cell>-</cell><cell>59.36</cell><cell>79.06</cell><cell>85.56</cell><cell>52.63</cell><cell>72.05</cell><cell>78.90</cell></row><row><cell>+ CR loss</cell><cell>-</cell><cell>-</cell><cell></cell><cell>56.30</cell><cell>77.21</cell><cell>84.03</cell><cell>50.23</cell><cell>69.56</cell><cell>77.13</cell></row><row><cell>SSAN</cell><cell></cell><cell></cell><cell></cell><cell>61.37</cell><cell>80.15</cell><cell>86.73</cell><cell>54.23</cell><cell>72.63</cell><cell>79.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">: Performance comparisons of different alignment</cell></row><row><cell cols="3">methods on CUHK-PEDES. Rank-1 accuracy is adopted as</cell></row><row><cell>the evaluation metric.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Global+Part</cell><cell>Part</cell></row><row><cell>SCAN [26]</cell><cell>55.86</cell><cell>54.24</cell></row><row><cell>ViTAA [13]</cell><cell>55.97</cell><cell>39.26</cell></row><row><cell>ViTAA  *</cell><cell>57.58</cell><cell>56.73</cell></row><row><cell>PFL</cell><cell>59.26</cell><cell>58.02</cell></row></table><note>the CR loss. Experimental results are illustrated in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance Comparisons on CUHK-PEDES</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell></row><row><cell></cell><cell>GNA-RNN [6]</cell><cell>19.05</cell><cell>-</cell><cell>53.64</cell></row><row><cell>VGG-16</cell><cell>IATVM [7] PWM-ATH [8] PMA [11] MIA [12]</cell><cell>25.94 27.14 47.02 48.00</cell><cell>-49.45 68.54 70.70</cell><cell>60.48 61.02 78.06 79.30</cell></row><row><cell></cell><cell>TDE [57]</cell><cell>52.37</cell><cell>75.26</cell><cell>83.31</cell></row><row><cell></cell><cell>SSAN</cell><cell>55.52</cell><cell>76.17</cell><cell>83.45</cell></row><row><cell></cell><cell>Dual Path [34]</cell><cell>44.40</cell><cell>66.26</cell><cell>75.07</cell></row><row><cell></cell><cell>CMPM + CMPC [33]</cell><cell>49.37</cell><cell>-</cell><cell>79.27</cell></row><row><cell></cell><cell>MIA [12]</cell><cell>53.10</cell><cell>75.00</cell><cell>82.90</cell></row><row><cell>ResNet-50</cell><cell>PMA [11] TDE [57] SCAN [26] ViTAA [13]</cell><cell>53.81 55.25 55.86 55.97</cell><cell>73.54 77.46 75.97 75.84</cell><cell>81.23 84.56 83.69 83.52</cell></row><row><cell></cell><cell>Baseline in [13]</cell><cell>52.27</cell><cell>73.33</cell><cell>81.61</cell></row><row><cell></cell><cell>Baseline in SSAN</cell><cell>54.68</cell><cell>75.42</cell><cell>82.73</cell></row><row><cell></cell><cell>SSAN</cell><cell>61.37</cell><cell>80.15</cell><cell>86.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Performance Comparisons on ICFG-PEDES We therefore report the performance of ViTAA * on ICFG-PEDES. Comparison results on the ICFG-PEDES database are summarized in Table IV. All methods in this table adopt the same backbone model, i.e., ResNet-50. It is shown that SSAN outperforms all other methods by significant margins. For example, SSAN beats the SCAN model [26] by 4.18% in terms of Rank-1 accuracy. Moreover, when compared with ViTAA * , SSAN still achieves a large performance gain by as much as 3.25% in terms of Rank-1 accuracy. These experimental results demonstrate the effectiveness of SSAN for text-to-image ReID. 3) Performance Comparisons on CUHK-PEDES with Cross-domain Settings: To further validate the generalization ability of SSAN, we carry out experiments on CUHK-PEDES with the cross-domain settings proposed in MAN</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="3">Rank-1 Rank-5 Rank-10</cell></row><row><cell></cell><cell>Dual Path [34]</cell><cell>38.99</cell><cell>59.44</cell><cell>68.41</cell></row><row><cell>ResNet-50</cell><cell>CMPM + CMPC [33] MIA [12] SCAN [26] ViTAA  *  [13]</cell><cell>43.51 46.49 50.05 50.98</cell><cell>65.44 67.14 69.65 68.79</cell><cell>74.26 75.18 77.21 75.78</cell></row><row><cell></cell><cell>SSAN</cell><cell>54.23</cell><cell>72.63</cell><cell>79.53</cell></row><row><cell cols="2">annotations.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Performance Comparisons on CUHK-PEDES in the Cross-domain Settings</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">S?C03</cell><cell>S?M</cell><cell></cell><cell>S?V</cell><cell></cell><cell cols="2">S?C01</cell></row><row><cell></cell><cell>Metric</cell><cell cols="2">Rank-1 Rank-5</cell><cell cols="2">Rank-1 Rank-5</cell><cell cols="2">Rank-1 Rank-5</cell><cell cols="2">Rank-1 Rank-5</cell></row><row><cell></cell><cell>CMPM+CMPC [33]</cell><cell>42.3</cell><cell>69.2</cell><cell>63.4</cell><cell>85.1</cell><cell>57.8</cell><cell>84.7</cell><cell>44.8</cell><cell>70.9</cell></row><row><cell>SO</cell><cell>MIA [12] SCAN [26]</cell><cell>49.0 50.2</cell><cell>76.7 75.9</cell><cell>66.2 64.2</cell><cell>86.2 86.2</cell><cell>55.1 55.1</cell><cell>84.7 81.1</cell><cell>50.2 48.2</cell><cell>75.9 76.8</cell></row><row><cell></cell><cell>SSAN</cell><cell>54.5</cell><cell>78.5</cell><cell>71.1</cell><cell>88.6</cell><cell>66.3</cell><cell>89.3</cell><cell>60.5</cell><cell>81.3</cell></row><row><cell></cell><cell>SPGAN [58]</cell><cell>44.7</cell><cell>72.5</cell><cell>63.3</cell><cell>85.3</cell><cell>60.7</cell><cell>85.7</cell><cell>45.3</cell><cell>71.2</cell></row><row><cell>ST</cell><cell>ADDA [59] ECN [60]</cell><cell>45.1 45.8</cell><cell>72.8 73.2</cell><cell>63.9 64.3</cell><cell>85.7 86.1</cell><cell>61.4 62.5</cell><cell>86.0 86.4</cell><cell>45.7 46.6</cell><cell>71.6 72.1</cell></row><row><cell></cell><cell>MAN [61]</cell><cell>48.5</cell><cell>74.8</cell><cell>65.1</cell><cell>87.4</cell><cell>64.2</cell><cell>87.2</cell><cell>48.2</cell><cell>73.2</cell></row><row><cell cols="5">trained with only labeled source data. (ii) Source and Target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(ST). In this setting, one model is trained with the labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">source data and unlabeled target data. Comparison results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">in the two settings are summarized in Table V. SCAN is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">implemented according to the description in Section V-B. MIA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">is implemented based on its open source code. They extract</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">both global and part features; therefore, the comparisons</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">among SCAN, MIA, and SSAN are fair. For all other methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Captions?The young man wears a black ball cap backwards and a pair of white shoes. His shirt and shorts are black, too.Captions?The woman is wearing a blue jumpsuit. She has on a white scarf on her head and a pair of brown shoes.</figDesc><table><row><cell></cell><cell>1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>man</cell><cell>Part 1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell>cap</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shoes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shirt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shorts</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>woman</cell><cell>Part 1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell>jumpsuit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>scarf</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shoes</cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>woman</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dress</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sleeveless</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>zips</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Captions?The man is wearing a black shirt and black shorts. He is holding a bag in his right hand.Captions?The man is wearing a white shirt sleeve shirt and blue jeans. He is carrying a trumpet with both hands in front of him.Captions? A male is wearing a black and white shirt, light brown pants and sneakers. He is carrying a black shoulder bag.</figDesc><table><row><cell></cell><cell>1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>man</cell><cell>Part 1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell>shirt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shorts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bag</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>man</cell><cell>Part 1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell>shirt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jeans</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trumpet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>male</cell><cell>Part 1</cell><cell>Part 2</cell><cell>Part 3</cell><cell>Part 4</cell><cell>Part 5</cell><cell>Part 6</cell></row><row><cell>shirt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sneakers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bag</cell><cell></cell><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical feature embedding for attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="55" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sequence-based person attribute recognition with joint ctc-attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08115</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beyond intra-modality: A survey of heterogeneous person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10048</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of open-world person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1092" to="1108" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity-aware textualvisual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comout</title>
		<meeting>IEEE Winter Conf. Appl. Comout</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1879" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware multiview summarization network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2020</title>
		<meeting>ACM Int. Conf. Multimedia, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno>cs/0205028</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pose-guided multi-granularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving descriptionbased person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5542" to="5556" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vitaa: Visual-textual attributes alignment in person search by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Inf. Process. Syst</title>
		<meeting>Advances Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep correlation for matching images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3441" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="655" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical multimodal lstm for dense visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1881" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context-aware attention network for image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3536" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-modality cross attention network for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">950</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph structured network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">930</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Camp: Cross-modal adaptive message passing for text-image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5764" to="5773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial representation learning for text-to-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5814" to="5824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Trans. Multi. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep adversarial graph attention convolution network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Visual-textual association with hardest and semi-hard negative pairs mining for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03083</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pose-invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4814" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-task learning with coarse priors for robust part-aware person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Factorized distillation: Training holistic person reidentification model by distilling an ensemble of partial reid models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08073</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Textual dependency embedding for person search by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2020</title>
		<meeting>ACM Int. Conf. Multimedia, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="4032" to="4040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cross-modal cross-domain moment alignment network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Person re-identification meets image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02171</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Perform</title>
		<meeting>Int. Workshop Perform</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
