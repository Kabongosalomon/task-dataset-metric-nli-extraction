<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Out-of-Distribution Detection Without Class Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Abutbul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Out-of-Distribution Detection Without Class Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Out-of-distribution detection seeks to identify novelties, samples that deviate from the norm. The task has been found to be quite challenging, particularly in the case where the normal data distribution consists of multiple semantic classes (e.g., multiple object categories). To overcome this challenge, current approaches require manual labeling of the normal images provided during training. In this work, we tackle multi-class novelty detection without class labels. Our simple but effective solution consists of two stages: we first discover "pseudo-class" labels using unsupervised clustering. Then using these pseudo-class labels, we are able to use standard supervised out-of-distribution detection methods. We verify the performance of our method by a favorable comparison to the state-of-the-art, and provide extensive analysis and ablations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detecting novelties, images that are semantically different from normal ones, is a key ability required by many intelligent systems. Some applications include: detecting unknown, interesting scientific phenomena (e.g. new star categories such as supernovae), or detecting safety-critical events (e.g. alerting an autonomous car when an unexpected object is encountered). In this paper, we assume that a training set entirely consisting of normal images is provided. A trained model is used at test time to classify new samples as normal or anomalous, i.e. similar or different from previously seen training samples. The normal data may consist of one or more semantic classes (e.g."dog", "cat"), where the case of a single semantic class is of particular interest. Most recent works that address out-of-distribution detection (OOD) have relied on supervised class labels for each normal sample. On the other hand, methods that do not assume such labels have mostly focused on single-class data novelty detection. It is common to assume a normal-only dataset during training, so methods that do not assume class labels for this set are referred to as unlabelled methods. This setting of out-of-distribution detection with a normal only train set, but without class labels for the normal data, or multiclass anomaly detection (AD), received little attention. .</p><p>Here, we propose a simple approach for anomaly detection on unlabeled, multi-class normal only data. It is based on the following principle: unlabeled multi-class AD can be approximated using unsupervised clustering followed by supervised multi-class AD. In practice, we perform unsupervised image clustering using SCAN <ref type="bibr" target="#b38">[39]</ref>, a recent state-of-the-art method. After obtaining the approximate labels for every image in the dataset, we can use them for adapting ImageNet-pre-trained features for the AD task. The adapted features are then used as input to generic AD methods e.g. k nearest neighbor (kNN) or the Mahalanobis distance.</p><p>Extensive experiments and ablations are performed to evaluate our method. We find that our method outperforms state-of-the-art self-supervised <ref type="bibr" target="#b37">[38]</ref> and pre-trained <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref> anomaly detection methods. As an additional result, we show that the features learned by the unsupervised clustering method are already competitive with the best self-supervised anomaly detection methods.</p><p>Our main contributions are:</p><p>1. Presenting a framework that uses unsupervised clustering to obtain pseudolabels which are then used to adapt feature representations for multi-class anomaly detection. 2. State-of-the-art results for unlabeled, multi-class anomaly detection on popular datasets. 3. Demonstrating that deep self-supervised image clustering methods are excellent feature learners for self-supervised multi-class anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>Out-of-Distribution Detection (OOD). In some settings, the normal data are provided together with their semantic class ground truth labels. Using these labels for supervised training highly increases the model's ability to detect OOD samples <ref type="bibr" target="#b12">[13]</ref>. A model trained to classify labelled semantic classes, may have less confidence in predicting the class label of a sample coming from a new distribution. Moreover, training a model to distinguish between semantic classes of the normal data enhances the sensitivity of the learnt features for the desired attribute. For example, a model trained to distinguish between animal species may learn a representation sensitive to their attributes (such as skin color and texture, head shape, etc.) and less sensitive to nuisance attributes (angle of view, lighting conditions, etc.). Using such representations, one can detect anomalous samples based on a large Mahalanobis distance in representation space, improving out-of-distribution detection capabilities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>One Class Classification. In the absence of positive anomaly examples, anomaly detection methods design inductive biases allowing models to detect anomalies without seeing any during training. The common thread behind these methods is learning a strong representation of the data with which separating between normal and anomalous data is easily done. Learning representations cannot be performed by using actual labels of the normal training data. Therefore, anomaly detection algorithms, as other self-supervised method (e.g. image clustering <ref type="bibr" target="#b38">[39]</ref> or disentanglement <ref type="bibr" target="#b15">[16]</ref>), use a variety of techniques to design the inductive bias of the model toward having desired properties. Such techniques include selfsupervised training to increase the model sensitivity to the properties essential for the task at hand <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>, simulating samples which may resemble expected anomalies <ref type="bibr" target="#b19">[20]</ref>, and data augmentation used to guide the model to ignore nuisance variation modes <ref type="bibr" target="#b37">[38]</ref>.</p><p>Adaptation of Pre-trained Representations. When pre-trained representations are available, they can be used to significantly improve anomaly detection accuracy. Early attempts suggested a compactness loss to map the normal training data closer together, relying on auto-encoder pretraining <ref type="bibr" target="#b34">[35]</ref>. Similar techniques were adopted to ImageNet pre-trained features <ref type="bibr" target="#b27">[28]</ref>, and later improved with self-supervised feature adaptation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. Pre-trained representations were lately adapted for the OE setting as well <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Clustering-based Anomaly Detection. Most anomaly detection methods perform some form of density estimation of the normal data. When the normal training data is multi-class i.e. consists of multiple clusters, it is natural that the density model would include this inductive bias. Indeed, multiple clusteringbased approaches have been used for anomaly detection including: K-means <ref type="bibr" target="#b23">[24]</ref> and GMMs <ref type="bibr" target="#b20">[21]</ref>. As clustering the raw data directly is unlikely to achieve strong results for images, deep learning methods have been applied to learn better representations, notably DAGMM <ref type="bibr" target="#b43">[44]</ref>. In the past few years, clustering methods have not dominated anomaly detection benchmarks, even for multi-class data.</p><p>In this paper, we revisit clustering-based anomaly detection and show that it achieves very strong performance for multi-class anomaly detection with and without pre-trained features. We note that SSD, a recently published work, uses K-means clustering for anomaly scoring in the setting of OOD without labels <ref type="bibr" target="#b35">[36]</ref>. We show that K-means clustering is less successful than our method on this task.</p><p>Self-Supervised Image Clustering. The task of clustering images without labels had been extensively studied. Most recent methods learn deep features simultaneously with optimizing the images cluster assignments <ref type="bibr" target="#b2">[3]</ref>. Methods free to optimize their own features are prone to cluster according to nuisance properties, which the self-learnt feature might pick up. A large variety of techniques have been used to mitigate this problem, including augmentations <ref type="bibr" target="#b14">[15]</ref> and contrastive feature learning <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section we present the necessary background for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Image Clustering</head><p>Recent years have seen significant progress in deep image clustering. This is mostly thanks to improvements in self-supervised representation learning. In this paper we take advantage of SCAN <ref type="bibr" target="#b38">[39]</ref>, a state-of-the-art unsupervised image clustering method. SCAN operates in three steps: (i) Representation learning. SimCLR <ref type="bibr" target="#b3">[4]</ref>, a contrastive self-supervised representation learning method, is used to learn a strong feature extractor ? SimCLR .</p><p>(ii) Classifier training. The feature extractor ? SimCLR is used to compute the nearest neighbors for every training image. A classifier C is trained (initialized with the features of ? SimCLR ) to classify each training image x into one of K clusters. The classifier C is trained under two constraints: (i) an equal number of images are assigned to each cluster. Formally, the entropy of E x?Xtrain C(x) is maximized (ii) each image should be confidently assigned to a similar cluster as its nearest neighbours. Formally, we minimize x?kN N (x) log(C(x) ? C(x)). By the end of training the classifier produces a clustering of the training set.</p><p>(iii) Self-training. In the final stage, the clustering is improved using a selftraining approach [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-Adaptation for Labeled Multi-Class Anomaly Detection</head><p>In the labeled multi-class anomaly detection task (also known as OOD), each normal training image x is labeled by its class label y. Standard OOD methods train a classifier C to predict the class probability vector C(x) given x. The standard confidence-based score, measures for a test image x test the maximum prediction probability (probability of the most confident class):</p><formula xml:id="formula_0">Conf idence = max C(x test )<label>(1)</label></formula><p>Images with low maximal confidence, are denoted as anomalous. Many techniques have been proposed to improve the obtained OOD detection <ref type="bibr" target="#b39">[40]</ref>. Here, we extend the work of Hendrycks et al. <ref type="bibr" target="#b13">[14]</ref> and Fort et al. <ref type="bibr" target="#b10">[11]</ref> which suggested initializing the classifier C with weights that are pre-trained on a large-scale datasets. This significantly increases the OOD detection accuracy. We propose a clustering-based technique for adapting these methods to unlabeled datasets. <ref type="figure">Fig. 1</ref>: An illustration of our feature adaptation and scoring procedure. Left -the data original unadpted pre-trained features. Feature adaptation increases the separation between the normal data (blue, green) and the anomaly (red). We illustrate that when labels are noisy w.r.t. the ground truth clusters, Mahalanobis distance scoring may misclassify anomalies (center, anomaly within the ellipses fitted to the normal data), while kNN scoring criterion (right, dashed line shows the anomaly's nearest in-distribution neighbour) may detect anomalies better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Clustering for Multi-Class Anomaly Detection</head><p>In this work, we deal with anomaly detection when the normal training data is composed of many different semantic classes but no labels. We suggest taking advantage of the multi-class nature of the normal data as an inductive bias. The knowledge that the normal data comes from different, relatively distinct classes, rather than a single uni-modal class, or other possible distributions, serves us to learn better representations. As illustrated in <ref type="figure">Fig.1</ref>, adapting features for better inter-cluster separation can provide better discrimination of anomalies, even when the cluster labels are imperfect. We first propose a naive but effective approach for using unsupervised clustering method for anomaly detection. We then propose a more accurate, two-stage approach which combines unsupervised clustering with pre-trained features and supervised multi-class anomaly detection for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Supervised Clustering for Multi-Class Anomaly Detection</head><p>We first address the fully self-supervised case where no pre-trained features are available. We suggest a surprisingly simple but effective approach for this setting. First, we run the SCAN clustering algorithm (see Sec.2) on the normal training images. We than use the features from the penultimate layer of the SCAN-trained classifier to represent each image. Anomalous samples are expected to be found in low-density area of this feature space. We estimate the normal samples density around a target image by its kNN distance to the normal training data. We show in Sec.4.2 that this simple method already achieves competitive results with the state-of-the-art methods when no auxiliary datasets are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Finetuning Pre-trained Features with Pseudo Labels</head><p>Pre-trained Features. Using effective representations is at the core of deep anomaly detection techniques. It is well established by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref> that transferring deep representations learned on auxiliary, large-scale datasets (e.g. ImageNet or CLIP) is effective for anomaly detection. The main reason for their effectiveness is the ability to measure semantic similarity. In such representations, the normal data lies in relatively compact regions and are typically more separated from semantic anomalies. The strong results from the fully self-supervise case above, motivate us to apply similar multi-class priors to pre-trained representation. Feature Adaptation. Pre-trained representations are typically trained on large datasets, which are not necessarily representative of our normal in-distribution samples. Better AD results are obtained when the pre-trained features are adapted using the normal samples provided for training <ref type="bibr" target="#b29">[30]</ref>. A naive approach to adapt pre-trained features to our multi-class normal data is to finetune them using the loss of a clustering method, such as SCAN. However, we found that this form of feature adaptation results in a representation worse than the initial one. We believe this is caused by catastrophic forgetting, where a network extensively trained for a new task loses capabilities and knowledge that it had during pretraining.</p><p>To overcome this limitation, we suggest a two stage approach. In the first stage, we simply train a self-supervised clustering method on the training data X train achieving approximate "pseudo-labels" describing clusters of our normal training data. In the second stage, we transfer the knowledge obtained from the self-supervised clustering into the pre-trained network. To do this, we follow <ref type="bibr" target="#b10">[11]</ref> and finetune our pre-trained network to classify training images into their pseudo-labels y train . We train the classifier C OOD , using standard cross entropy loss L CE :</p><formula xml:id="formula_1">L CE = ? i ( y train ) i ? log C OOD (X train ) i<label>(2)</label></formula><p>Model Averaging. Although our two-stage approach is more stable than adaptation using the clustering objective directly, it can still lead to catastrophic forgetting. During finetuning, the network gains knowledge from the clusters we found, but forgets its pre-trained knowledge, which is crucial for the density estimation-based anomaly scoring we use. Although the model achieves its best performance on one particular epoch, it is non-trivial to select that epoch without relying on having anomalous samples during training. Therefore, we choose to score our anomalies with an average model, taken as the moving average of the weights of the model during different training epochs.</p><p>Anomaly Scoring. To detect anomalies, we use density estimation with the adapted features. Recent methods suggested a per-class Mahalanobis approach <ref type="bibr" target="#b10">[11]</ref> deeming a sample normal if it lies within a small Mahalanobis distance to any of the cluster centers. However, when the provided labels are not accurate this approach might fail. False cluster assignment may distort the Mahalanobis distance which relies on the empirical covariance matrix of each cluster. Instead, we use kNN scoring which makes few assumptions on the distribution of the data. Precisely, we encode all train images and the target images using the feature encoder. We then compute the distance between the features of each target image and all the normal train images. The anomaly score for each target image is set as the minimum of the distances to all nomral train samples. This score is relatively robust and less sensitive to the accuracy of the clustering algorithm ( <ref type="figure">Fig.1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate our method on two commonly used OOD detection datasets. We use a variety of other datasets to simulate anomalies: CIFAR-10 [18]: Contains images from 10 different classes, with a 32x32 resolution. We evaluate it against a variety of dataset supplied in a similar resolution, namely: CIFAR-100 <ref type="bibr" target="#b17">[18]</ref>, SVHN <ref type="bibr" target="#b24">[25]</ref>, LSUN <ref type="bibr" target="#b41">[42]</ref>. The hardest benchmark here is the CIFAR-100 which contain the most similar classes to CIFAR-10. For LSUN we report both a version with some artifacts used by previous works, and a version suggested by CSI <ref type="bibr" target="#b37">[38]</ref> 2 where the downscaling was done more carefully to avoid these artifacts. We do not compare on ImageNet <ref type="bibr" target="#b7">[8]</ref> data here, as this dataset was used for pretraining.</p><p>ImageNet-30: Contain images from 30 classes of high resolution images chosen from the ImageNet <ref type="bibr" target="#b7">[8]</ref> dataset. Accordingly, we evaluate it against of a variety of datasets that have similar resolution, namely: CUB-200 <ref type="bibr" target="#b40">[41]</ref>, Dogs <ref type="bibr" target="#b7">[8]</ref>, Pets <ref type="bibr" target="#b26">[27]</ref>, Flowers <ref type="bibr" target="#b25">[26]</ref>, Food <ref type="bibr" target="#b1">[2]</ref>, Places <ref type="bibr" target="#b42">[43]</ref>, Caltech <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-Class Anomaly Detection With Pre-trained Features</head><p>In this section we provide results for the case where pre-trained features are available.</p><p>Methods: We evaluate anomaly detection methods that utilize pre-trained features, and do not require class labels for the train data. We present a comparison to a naive initialization of unsupervised clustering with pre-trained features in Sec.5.</p><p>Deep Nearest Neighbors (DN2) <ref type="bibr" target="#b29">[30]</ref>: A kNN density estimation method based on pre-trained features. Test samples are scored according to the distance to their nearest normal training images. A larger distance indicates a low density of normal samples, and therefore a high probability of abnormality. We follow MeanShifted [31] by using the cosine similarity distance instead of l 2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Shifted Contrastive Loss (MSCL) [31]:</head><p>A-state-of-the-art method for adapting pre-trained features for anomaly detection using contrastive learning. It suggests to compute the contrastive loss around the mean of the normal training data features to mitigate catastrophic forgetting.</p><p>Ours: The two stage clustering with pseudo-label adaptation method described in Sec.3.1. The hyperparameters and implementation details are described in Sec.4.3.</p><p>Comparison: As can be seen in Tab.2 and Tab.3, feature adaptation has significantly improved our multi-class anomaly detection performance. A substantial part of our improvement comes from using large network architectures. We emphasize that the ability to easily use readily-available pre-trained networks is inherent to our approach. Results were also improved in MSCL, but as MSCL is not explicitly designed to deal with multi-class data, our method significantly outperforms it. Using more powerful pretraining networks (as in <ref type="bibr" target="#b10">[11]</ref>) can also improve the results of our method. We find that ViT <ref type="bibr" target="#b8">[9]</ref> pretrained on ImageNet-21 <ref type="bibr" target="#b16">[17]</ref> achieves better results than ResNet152 pre-trained on ImageNet. As ImageNet pretraining include the exact ImageNet class labels of the ImageNet-30 dataset we wish to avoid in our setting we used a CLIP <ref type="bibr" target="#b28">[29]</ref> pre-trained ViT architecture for all pre-trained methods (including ours) in the ImageNet-30 evaluations.</p><p>When we compare between the kNN image retrievals of the original pretrained features and our adapted features, we observe some differences. As can be seen in <ref type="figure" target="#fig_0">Fig.2</ref> the raw pre-trained features retrieve images from different classes. These images are similar to our target out-of-distribution target sample in various semantic attribute. Adapted features, however, tend to associate our target image with a single normal class, resulting in a lower similarity between the target image and the retrieved normal ones. This is desirable, as we want to distinguish out-of-distribution images from our normal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Class Anomaly Detection Without Pretraining</head><p>Although strong pre-trained features are often available, we cannot always assume their availability. In this case, we find that self-supervised clustering on its own can often learn a good enough representation to outperform previous stateof-the-art on unlabelled multi-class anomaly detection without pretraining. Methods: We compare the following methods: Density based methods: Classical methods use direct density estimation techniques to estimate the likelihood of the data. Different methods suggested modification to this score to account for the dataset statistics (Likelihood Ratio <ref type="bibr" target="#b31">[32]</ref>) or its complexity (Input Complexity <ref type="bibr" target="#b36">[37]</ref>). Rot <ref type="bibr" target="#b13">[14]</ref>: A classification based method utilizing an auxiliary task of rotation prediction for self-supervised detection of anomalies. GOAD <ref type="bibr" target="#b0">[1]</ref>: Another rotation-prediction method, that proposed to learn a feature space where inter-class separation of the normal data is relatively small. CSI <ref type="bibr" target="#b37">[38]</ref>: A contrastive learning method which contrasts against distributionshifted augmentations of the data samples along with other samples. SSD <ref type="bibr" target="#b35">[36]</ref>: A self-supervised method with similar features to CSI <ref type="bibr" target="#b37">[38]</ref>. It scores anomalies using the Mahalanobis distance with respect to K-means clusters. Although this method is somewhat similar to ours, it relies on Mahalanobis distance and K-means clustering, which are not optimal for feature adaptation without labels (see Sec.5). We note that SSD <ref type="bibr" target="#b35">[36]</ref> also reports higher results for the ResNet-34 architecture, which is a non-standard evaluation for AD without pretraining methods. <ref type="bibr" target="#b38">[39]</ref>: Features taken from the last stage of of the SCAN clustering methods, used to score anomalies as in DN2 (see Sec.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCAN Features</head><p>Comparison: As can be seen in Tab.2, simple utilization of SCAN features often performs better or on par with the top competing self-supervised method. We note that the last stage of the SCAN method, namely, self-labelling, is somewhat similar to adapting on pseudo-label performed by our method. Therefore, we do not expect our method to provide further gains over SCAN's final representation.  Although clustering methods such as K-means and GMM have classically been very popular, most recent deep learning methods do not use clustering. The results reported here provide strong motivation for revisiting self-supervised clustering methods for anomaly detection. We conclude that relying on the multiclass distribution prior often allows us to outperform other methods, even in the setting where no pretraining is allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Clustering: We use SCAN's official implementation 3 for all clustering tasks unless mentioned otherwise. We ran all of our clustering algorithms with the same number of clusters K = 10. We use the SCAN algorithm's default parameters for each dataset. For ImageNet-30 dataset we use the configuration originally provided by the authors for the ImageNet-50 dataset. We note that SCAN unsupervised image clustering use a MoCo <ref type="bibr" target="#b4">[5]</ref> pretraining on the entire ImageNet dataset (pre-trained without labels). We therefore do not compare it to self-supervised methods that do not use pretraining <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>. Pretraining: For all models using ResNet152, we use ImageNet <ref type="bibr" target="#b7">[8]</ref> pretraining. For ViT, we used ImageNet-21 <ref type="bibr" target="#b16">[17]</ref> pretraining. To evaluate our method with ImageNet-30, we used the CLIP <ref type="bibr" target="#b28">[29]</ref> ViT visual head rather than pretraining on labelled ImageNet data (Tab.3). Optimization: We ran the adaptation for 5 epochs using an Adam optimizer. We used Cosine Annealing learning rate scheduler with an initial learning rate of 1e ? 5 and final learning rate of 1e ? 6. Model Averaging: We average on the model weights at the end of each of the 5 training epochs. Scoring: We used k = 1 for all our kNN evaluations. Comparison to MSCL: For the MSCL comparison we experimented with 5, 10 and 100 training epochs, and chose the best performing number of epochs. The rest of the parameters were left unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Do we actually need our two-stage approach? We compare between pretrained feature adaptation using two approaches: (i) Our two stage clustering with pseudo-class labels supervised adaptation (ii) A single-stage initialization of the unsupervised clustering method with pre-trained features. In the latter, we initialize the SimCLR features used in SCAN with ResNet152 ImageNet pretrained features. The results are presented in Tab.4. The full faeture adaptation on the CIFAR-10 vs. CIFAR-100 evaluation results in 89.3% ROCAUC which is far worse than the 93.3% results achieved by our two-stage approach. During the clustering stage, the pre-trained features are used find the data clusters, performance but at the same time, the features deteriorate further away from their pre-trained initialization. This justifies our two stage approach, which better preserves the expressivity of the pre-trained features. Is kNN density estimation preferable to other anomaly scoring methods? Sometimes. Previous works used a variety of scoring criterion for multi-class anomaly detection <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref>. Although the Mahalanobis distance was shown to give stronger results in a previous work, it is sensitive to the approximate labels that are typically generated by self-supervised clustering (see <ref type="figure">Fig.1</ref> for an illustration). A comparison between the different scoring methods can be seen in Tab.5. We find that the optimal scoring method may differ between datasets. Following previous works <ref type="bibr" target="#b5">[6]</ref>, all our main results are reported using kNN with k = 1. Can simple K-means clustering be used to finetune our features? To a small extent. We evaluated the labels obtained by K means clustering as an alternative for the more complicated SCAN clustering method. Although the adaptation yielded small improvements, the results underperformed SCAN significantly. For example the multi-class anomaly detection results only improved from 86.5% ROCAUC to 87.3% (CIFAR-10 vs. CIFAR-100). Can using a larger model with ImageNet pretraining assist the SCAN clustering performance? Not significantly. We tried to initialize the SCAN algorithm with a ResNet152 pre-trained on ImageNet, but achieved only a minor improvement in the clustering accuracy. Although adaptation of pretrained features using SCAN results in catastrophic forgetting, it is very likely that image clustering accuracy can be substantially improved with a method designed to take advantage of pre-trained representations. Such future improvements in image clustering are likely to directly enhance the results of our approach too.</p><p>Does model averaging result in a good representation with comparison to individual checkpoints? Yes. Model weight averaging can yield a similar (or better) multi-class anomaly detection accuracy compared to that of the optimal epoch along the training process. For example, on CIFAR-10 vs. CIFAR-100 with pre-trained ResNet152, the initial pre-trained features scored anomalies with 86.5% ROCAUC. The 5 individual training epochs scored 92.9%, 88.9%, 92.9%, 92.6%, 92.3%. The averaged model scored 93.3% ROCAUC. This phenomenon has also been noticed by previous works <ref type="bibr" target="#b21">[22]</ref>.</p><p>What is the sensitivity of our method to random repetitions? We do not provide error bars for each of our runs since our results are fairly consistent. As a typical case, we ran 3 repetitions of our approach for the CIFAR10-CIFAR100 and CIFAR10-LSUN (fix) experiments. The standard deviations were 0.4% and 0.5% respectively.</p><p>What is the sensitivity of our method to the number of clusters K? While changing K may somewhat vary the results, our method can often perform useful adaptation without knowing the exact ground truth number of classes (Tab.6). When available, we advise using the ground-truth number of clusters. Recently published methods have been able to infer the value of K automatically <ref type="bibr" target="#b33">[34]</ref>.</p><p>How does adaptation with the pseudo-labels obtained by clustering compare to using the ground-truth labels? There is still a significant gap between the pseudo-labels provided by our clustering algorithm and the ground truth class labels (on CIFAR-10, SCAN clustering accuracy is 88.3%). Yet, the gap in the multi-class anomaly detection results seems to be smaller. For example, in the CIFAR-10 vs. CIFAR-100 multi-class anomaly detection task, our method achieves 96.7% ROCAUC, while a similar method by Fort et al. <ref type="bibr" target="#b10">[11]</ref> achieves 98.4% ROCAUC using the ground truth labels. Even though this a significant difference, we find it is reasonable given the significant inaccuracy of self-supervised clustering we used. Relation to auxiliary-task based anomaly detection. Our work can be seen as an extension of a line of methods utilizing auxiliary tasks to address a oneclass-classification setting. Such methods were previously suggested for image anomaly detection <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b13">[14]</ref>, and also for other data modalities <ref type="bibr" target="#b0">[1]</ref>. These methods rely on predefined augmentations to create an auxiliary task in order to guide the model learning toward meaningful properties of our data. The prediction of pseudo-labels from clustering algorithms can be viewed as another example of such auxiliary work. Future works may suggest new kinds of data-adaptive auxiliary tasks for similar settings.</p><p>New data modalities. Multi-class anomaly detection may be encountered in other modalities beyond images. Data modalities where transfer learning and self-supervised learning show promising results include natural language, video and audio. Therefore, we believe similar methods may provide comparable improvements in these modalities.</p><p>Guided anomaly detection. Another interesting application of clustering based out-of-distribution detection is using the pseudo labels to guide our anomaly detection algorithm toward the type of samples that we wish to consider as anomalies. For example, if one wishes to define anomalies according to colors, they may use augmentations to guide the clustering procedure toward finding color-based clusters. Similarly, one may wish to ignore an attribute, and guide the clustering algorithm accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Highly unbalanced multi-class datasets. Most state-of-the-art clustering algorithms rely on the assumption of an approximately equal split of the data among the classes. As our method relies on such algorithms, performance may decrease on imbalanced datasets. We expect that future clustering methods will overcome this limitations, consequently also freeing our method from this issue.</p><p>Fine-grained and non-standard classes. Many self-supervised learning algorithms, including self-supervised clustering, heavily rely on augmentations and other sources of inductive bias. The inductive bias in these methods guides the model to be sensitive to a single salient object in the center of the image. In cases when the anomalies or the semantic classes in the normal data are not object-centric, current self-supervised methods may not perform as well. While this is a limitation of our method, it is similarly a limitation of many other anomaly detection techniques reliant on self-supervised learning, including nearly all competing baseline methods. We therefore consider this as a limitation of the field in general, rather than a limitation specific to our method.</p><p>Pre-trained features. The main results of our paper are reliant on pretrained features. As discussed, pre-trained features achieve strong results on most datasets, but may not be a good choice in some settings. One possible solution to this limitation, is combining pre-trained and fully self-supervised methods in scenarios where the preferred method cannot be determined in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We address the problem of multi-class anomaly detection without labels. We propose a conceptually simple but effective method to combine recent improvements in unsupervised clustering with pre-trained features for anomaly detection. Our approach outperforms state-of-the-art feature adaptation methods for multi-class anomaly detection. Future work may utilize future improvements in self-supervised clustering of image datasets to further improve results. We also expect similar improvements in other data modalities such as natural language, video, audio and time-series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>This work was partly supported by the Malvina and Solomon Pollack scholarship and, the Federmann Cyber Security Research Center in conjunction with the Israel National Cyber Directorate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Raw pre-trained features vs. our finetuned features: For each outof-distribution image from the Caltech-256 dataset, the top 5 nearest neighbors from the in-distribution data (ImageNet-30) are shown according to their order. Note how raw pre-trained features (Raw) neighbors are chosen from all classes. In contrast, finetuned pre-trained features with our method (FT) neighbors belong to a single semantic class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different novelty detection settings</figDesc><table><row><cell cols="4">Normal Data One-class (OCC) Multi-class (OOD) Our Setting</cell></row><row><cell>Multi-class</cell><cell>? 1</cell><cell>?</cell><cell>?</cell></row><row><cell>Without Labels</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table><note>arXiv:2112.07662v2 [cs.CV] 22 Sep 2022</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>OOD detection without class labels on CIFAR-10 ROCAUC(%)</figDesc><table><row><cell></cell><cell></cell><cell>Network</cell><cell cols="4">CIFAR-100 SVHN LSUN LSUN (FIX)</cell></row><row><cell></cell><cell>Likelihood</cell><cell>Glow</cell><cell>58.2</cell><cell>8.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Likelihood Ratio [32] PixelCNN++</cell><cell>-</cell><cell>91.2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Input Complexity [37]</cell><cell>Glow</cell><cell>73.6</cell><cell>95.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Rot [14]</cell><cell>ResNet-18</cell><cell>82.3</cell><cell cols="2">97.8 92.8</cell><cell>81.6</cell></row><row><cell></cell><cell>GOAD [1]</cell><cell>ResNet-18</cell><cell>77.2</cell><cell cols="2">96.3 89.3</cell><cell>78.8</cell></row><row><cell></cell><cell>CSI [38]</cell><cell>ResNet-18</cell><cell>89.2</cell><cell cols="2">99.8 97.5</cell><cell>90.3</cell></row><row><cell></cell><cell>SSD [36]</cell><cell>ResNet-18</cell><cell>89.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>SCAN Features</cell><cell>ResNet-18</cell><cell>90.2</cell><cell cols="2">94.3 92.4</cell><cell>92.1</cell></row><row><cell></cell><cell>DN2 [30]</cell><cell>ResNet-18</cell><cell>83.3</cell><cell cols="2">95.0 91.0</cell><cell>88.9</cell></row><row><cell>Pretrained</cell><cell>DN2 [30] MSCL [31] Ours Ours Ours</cell><cell>ResNet-152 ResNet-152 ResNet-18 ResNet-152 ViT</cell><cell>86.5 90.0 90.8 93.3 96.7</cell><cell cols="2">96.2 88.7 98.6 90.6 98.6 98.6 98.8 95.4 99.9 99.3</cell><cell>86.7 92.6 94.3 95.7 99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>OOD detection without class labels on ImageNet-30 ROCAUC(%).</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Network CUB-200 Dogs Pets Flowers-102 Food-101 Places-365 Caltech-256</cell></row><row><cell></cell><cell>Rot</cell><cell>ResNet-18 74.5</cell><cell>77.8 70.0</cell><cell>86.3</cell><cell>71.6</cell><cell>53.1</cell><cell>70.0</cell></row><row><cell></cell><cell cols="2">GOAD ResNet-18 71.5</cell><cell>74.3 65.5</cell><cell>82.8</cell><cell>68.7</cell><cell>51.0</cell><cell>67.4</cell></row><row><cell></cell><cell>CSI</cell><cell>ResNet-18 90.5</cell><cell>97.1 85.2</cell><cell>94.7</cell><cell>89.2</cell><cell>78.3</cell><cell>87.1</cell></row><row><cell>Pretr.</cell><cell cols="2">DN2 CLIP ViT 93.8 Ours CLIP ViT 99.4</cell><cell>94.2 89.7 95.9 94.9</cell><cell>93.2 98.3</cell><cell>95.7 96.4</cell><cell>96.7 96.1</cell><cell>90.3 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison between OOD detection w.o. labels using the representations obtained by different stages of the SCAN clustering method (CIFAR-10 vs. CIFAR-100) ROCAUC (%). The results show that the naive approach of directly using SCAN with pre-trained features falls behind our two-stage approach.</figDesc><table><row><cell cols="4">Ours (ResNet-152) SIMCLR SCAN Self-Labelling</cell></row><row><cell>93.3</cell><cell>87.3</cell><cell>83.4</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between scoring methods ROCAUC (%)</figDesc><table><row><cell>Scoring method</cell><cell cols="3">1NN 2NN 5NN 10NN Mahalanobis Confidence</cell></row><row><cell cols="2">CIFAR10-CIFAR100 96.7 96.8 96.9 96.8</cell><cell>96.6</cell><cell>92.9</cell></row><row><cell cols="2">CIFAR10-SVHN 99.9 99.9 99.9 99.9</cell><cell>92.4</cell><cell>97.1</cell></row><row><cell cols="2">ImageNet30-CUB200 99.3 99.4 99.4 99.4</cell><cell>99.6</cell><cell>99.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different numbers of clusters K (ImageNet-30 vs.</figDesc><table><row><cell>CUB200) ROCAUC (%)</cell><cell></cell></row><row><cell cols="2">K 10 20 30 No Adaptation</cell></row><row><cell>99.1 98.3 98.9</cell><cell>93.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A few novelty detection methods do evaluate on multi-class data without labels, and we compare to them in this work</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/alinlab/CSI</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/wvangansbeke/Unsupervised-Classification</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer-based semantic anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2546" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03004</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10917</idno>
		<title level="m">Deep anomaly detection using geometric transformations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12340</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
	<note>Part V 16</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Learning multiple layers of features from tiny images</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting outof-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anomaly detection via a gaussian mixture model for flight operation and safety monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hansman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Palacios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09832</idno>
		<title level="m">Merging models with fisher-weighted averaging</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">350</biblScope>
			<biblScope unit="page" from="365" to="369" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Traffic anomaly detection using k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>M?nz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GI/ITG Workshop MMBnet</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Reading digits in natural images with unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panda: Adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03844</idno>
		<title level="m">Mean-shifted contrastive loss for anomaly detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02845</idno>
		<title level="m">Likelihood ratios for out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transfer learning gaussian anomaly detection by fine-tuning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04116</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepdpm: Deep clustering with an unknown number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Finder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9861" to="9870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ssd: A unified framework for self-supervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12051</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Input complexity and out-of-distribution detection with likelihood-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11480</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09310</idno>
		<title level="m">Mitigating neural network overconfidence with logit normalization</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
