<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Geo-localization</term>
					<term>Image Retrieval</term>
					<term>Agriculture</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-view geo-localization is to spot images of the same geographic target from different platforms, e.g., droneview cameras and satellites. It is challenging in the large visual appearance changes caused by extreme viewpoint variations. Existing methods usually concentrate on mining the fine-grained feature of the geographic target in the image center, but underestimate the contextual information in neighbor areas. In this work, we argue that neighbor areas can be leveraged as auxiliary information, enriching discriminative clues for geolocalization. Specifically, we introduce a simple and effective deep neural network, called Local Pattern Network (LPN), to take advantage of contextual information in an end-to-end manner. Without using extra part estimators, LPN adopts a square-ring feature partition strategy, which provides the attention according to the distance to the image center. It eases the part matching and enables the part-wise representation learning. Owing to the square-ring partition design, the proposed LPN has good scalability to rotation variations and achieves competitive results on three prevailing benchmarks, i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN can be easily embedded into other frameworks to further boost performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Ours Inputs Satellite Drone <ref type="figure">Fig. 1</ref>. Difference of the activation maps generated by the baseline method <ref type="bibr" target="#b1">[2]</ref> and our method. The first column shows two input images from different platforms, i.e., satellite and drone, with the same geo-tag. We observe that the contextual information, such as the neighbor building in the yellow box, can be used as an auxiliary clue to facilitate the cross-view image-based geographic localization. In the second column, we visualize the activation map of the baseline model <ref type="bibr" target="#b1">[2]</ref>. We could observe that the baseline method <ref type="bibr" target="#b1">[2]</ref> activates only the patterns at the center geographic target, while our method activates more contextual information around the center geographic target (see the third column). ? : The baseline method is a three-branch network with ResNet-50 <ref type="bibr" target="#b4">[5]</ref> as the backbone, and the model is optimized by the instance loss <ref type="bibr" target="#b5">[6]</ref>.</p><p>In recent years, cross-view geo-localization has obtained a significant development due to the advance in deep learning. Most works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> explore the deep neural network with the metric learning to learn the discriminative feature. Specifically, the network is to learn one feature space that brings matched image pairs closer and pushes non-matched pairs far apart <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The attention mechanism and orientation information are also widely used in the network design <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, most existing methods only consider the global information via pooling functions, ignoring the contextual information (see <ref type="figure">Figure 1)</ref>.</p><p>Generally, the aerial-view platform, e.g., drone or satellite, captures the scene image with a wide angle. When the platform acquires a geographic target, the contextual information around the target is also captured as a by-product. When existing works usually ignore such information, we argue that the contextual information provides a key clue for crossview geo-localization. For instance, when there is no apparent difference between two geographic targets, such as two straight roads, the human visual system is challenging to identify the true-match target. However, the task is much easier with the help of contextual information, e.g., neighbor houses. Mining and utilizing the contextual information in the image can improve the accuracy of the cross-view geo-localization.</p><p>Our work is inspired by the procedure that the human visual system interprets and matches the same scene of different viewpoints <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. When recognizing a geography scene of two different platforms, the human visual system generally adopts a hierarchical processing manner to improve the accuracy of judgement. Specifically, the human visual system first pays attention to whether the same geographic target is contained in different viewpoint scenes. Then, the human visual system will check the contextual information around the geographic target to verify the correctness of the match. When there is no remarkable landmark, people usually resort to the map to find discriminative neighbor areas. Imitating the process mentioned above, we design a Local Pattern Network (LPN), which is an effective way to explicitly explore the contextual information in an end-toend learning manner. Specifically, we divide the high-level feature into several parts in a square-ring partition, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Because the geographic target is generally located in the center of the image with the contextual information surrounded. Our partition method can obtain not only the geographic target information (the region of A) but also several contextual-information parts (the region of B and C) with different distances from the geographic target. Therefore, we can explicitly exploit contextual information to optimize LPN. We also observe that our partition strategy is robust to the image rotation in nature. For instance, when rotating the left image in <ref type="figure" target="#fig_0">Figure 2</ref> as the right image, the three regions (A, B, and C) still contain the same semantic information as corresponding regions of the left image. Therefore, the network designed according to the square-ring manner has good scalability to image rotation. To verify the effectiveness of the proposed method, we conduct experiments on three public datasets, i.e., University-1652 <ref type="bibr" target="#b1">[2]</ref>, CVUSA <ref type="bibr" target="#b16">[17]</ref> and CVACT <ref type="bibr" target="#b3">[4]</ref>. LPN achieves the Recall@1 accuracy of 75.93% for the task of drone-view target localization (Drone ? Satellite) and Recall@1 accuracy of 86.45% for the task of drone navigation (Satellite ? Drone), which is higher than the baseline work [2] by 17.44% and 15.27% respectively. Similar results are also observed on CVUSA and CVACT. Compared with the baseline model <ref type="bibr" target="#b1">[2]</ref>, the Recall@1 accuracy increases from 43.91% to 79.69% (+35.78%) on CVUSA and 31.20% to 73.85% (+42.65%) on CVACT. Besides, the proposed method is complementary to most previous works. The proposed method could be easily fused with the state-ofart methods, i.e., SAFA <ref type="bibr" target="#b0">[1]</ref>, and boost the performance from 89.84% Recall@1 accuracy to 92.83% (+2.99%) Recall@1 accuracy on CVUSA and 81.03% Recall@1 accuracy to 83.66% (+2.63%) Recall@1 accuracy on CVACT.</p><p>In summary, the main contributions of this paper are as follows:</p><p>? We propose a simple and effective model, called Local Pattern Network (LPN). Different from existing works, LPN explicitly takes contextual patterns into consideration and leverages the surrounding environment around the target building. Specifically, the model deploys the square-ring partition strategy and learns contextual information in an end-to-end manner.</p><p>? We demonstrate the effectiveness of our method on three prevailing cross-view geo-localization datasets, i.e., University-1652 <ref type="bibr" target="#b1">[2]</ref>, CVUSA <ref type="bibr" target="#b16">[17]</ref> and CVACT <ref type="bibr" target="#b3">[4]</ref>. Our method outperforms the strong baseline on both benchmarks by a large margin. Furthermore, we show that the proposed method is complementary to existing works, and can be fused with the state-the-art approaches to further boost the performance. The simplified diagram of our partition strategy, which is invariant to the rotation. The region of part A represents the geographic target in the center. According to the distance from the geographic target, the region of part B can be viewed as the first hierarchical contextual information, and the region of part C is the second hierarchical contextual information.</p><p>We organize the rest of this paper as follows. In Section II, we briefly introduce some of the relevant works. Section III presents our designed LPN in detail. Experimental results are presented in Section IV and followed by the conclusion in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review related previous works, including deep cross-view geo-localization and part-based representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Cross-view Geo-localization</head><p>Cross-view geo-localization has been attracting more attention in recent years due to a large number of potential applications. Some pioneering approaches <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> focus on extracting hand-crafted features. Inspired by the great success of the deep convolutional neural networks (CNNs) on ImageNet, researchers resort to the deeply-learned feature in recent years. Workman et al. <ref type="bibr" target="#b2">[3]</ref> are among the first attempts to utilize a pre-trained CNN to extract features for the crossview localization task. They demonstrate that features from the high-level layer of CNN contain semantic information about the geographic location. To take one step further, Workman et al. <ref type="bibr" target="#b21">[22]</ref> fine-tune the pre-trained network by reducing the feature distance between pairs of ground-level images and aerial images, yielding better performance. Inspired by the face verification approaches, Lin et al. <ref type="bibr" target="#b22">[23]</ref> adopt a modified Siamese Network <ref type="bibr" target="#b23">[24]</ref>, which optimizes network parameters by the contrastive loss <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Zhai et al. <ref type="bibr" target="#b8">[9]</ref> plug the NetVLAD <ref type="bibr" target="#b24">[25]</ref> into a Siamese-like architecture, making image descriptors robust against large viewpoint changes. Liu et al. <ref type="bibr" target="#b3">[4]</ref> stress the importance of orientation information and encode corresponding coordinate information into the network to boost the discrimination of the feature. In a recent work, Shi et al. <ref type="bibr" target="#b6">[7]</ref> use the spatial layout information to make up the shortcoming of the global aggregation step in feature extraction. Furthermore, Shi et al. <ref type="bibr" target="#b0">[1]</ref> improve the performance of cross-view geo-localization through domain alignment and spatial attention mechanism. Besides, DSM <ref type="bibr" target="#b7">[8]</ref> considers a limited Field of View setting and adopts a dynamic similarity matching module to align the orientation of cross-view images. Another line of works considers the metric learning and designs different training objectives to learn the discriminative representation. Vo et al. <ref type="bibr" target="#b25">[26]</ref> design an orientation regression loss, yielding the performance improvement. Hu et al. <ref type="bibr" target="#b8">[9]</ref> employ a weighted soft margin ranking loss, which not only speeds up the training convergence but also improves the retrieval accuracy. Different from adopting metric learning loss (i.e., contrastive loss <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and triplet loss <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>), Zheng et al. <ref type="bibr" target="#b1">[2]</ref> regard the cross-view image retrieval as a classification task. They apply the instance loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b5">[6]</ref> to optimize the network and has achieved a competitive result. However, these methods usually concentrate on exploring the global information but ignore the contextual information as shown in <ref type="figure">Figure 1</ref>. Different from existing works, the proposed method intends to take advantage of the neighbor areas. We deploy the feature-level partition strategy, which facilitates the end-to-end learning on the contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Part-based Representation Learning</head><p>The local feature has been widely studied in the design of hand-crafted algorithms <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Ojala et al. <ref type="bibr" target="#b33">[34]</ref> propose a local binary pattern (LBP) descriptor to extract the rotation-invariant feature. Lowe et al. <ref type="bibr" target="#b34">[35]</ref> develop a Scale Invariant Feature Transform (SIFT) descriptor for the image-based match. SIFT is invariant to translations, rotation, and scaling transformations by summarizing description of the local image structures in a local neighborhood around each interest point. In the spirit of the conventional part-based descriptor, some researchers also explore the local pattern learning in the deep-leaned models. One line of works divides the features based on an extra estimator, such as landmark detection, human pose estimated, and human parsing. Spindle Net <ref type="bibr" target="#b35">[36]</ref> leverages the landmark points of the human body to obtain semantic features from different body regions. Xu et al. <ref type="bibr" target="#b36">[37]</ref> propose a pose-guide part attention module to learn a confidence map. Guo et al. <ref type="bibr" target="#b37">[38]</ref> acquire the accurate human part-aligned representation by the human parsing model to enhance the robustness of the feature. Another line of works does not need an extra pose estimator and deploys a coarse part alignment, such as horizontal matching. Li et al. <ref type="bibr" target="#b38">[39]</ref> capture the three parts information corresponding to the head-shoulder, upper body, and lower body by Spatial Transformer Network (STN) <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Zhao et al. <ref type="bibr" target="#b41">[42]</ref> utilize the attention mechanism to learn aligned part information from the input image automatically. A strong Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b42">[43]</ref> shows a uniform partition strategy to extract high-level features. Then, by correcting within-part inconsistency of all the column vectors according to their similarities to each part, the performance of this work becomes better. Currently, some state-of-the-art works <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> extend the PCB with more partitions or optimization losses. Our work also studies a part-based representation learning on the convolutional layer, but is different in two aspects: Different from works of the first line <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, the proposed method does not need an extra part estimator. Different from works of the second line <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, our partition method makes the network have good scalability to image rotation (see <ref type="figure" target="#fig_0">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we introduce the Local Pattern Network (LPN) (see <ref type="figure" target="#fig_2">Figure 3</ref>). We first illustrate the network architecture for feature extraction, followed by the partition strategy for feature maps and the optimization objective. Finally, we provide a discussion on our intuition and special cases for different datasets.</p><p>Problem formulation. Given one geo-localization dataset, we denote the input image as x, and y represents the corresponding label. We apply the subscript j to denote the platform where the data x j is collected, and j ? {1, 2, 3}. In particular, x 1 denotes the sample from the satellite view, x 2 denotes the drone-view data, and x 3 denotes the ground-view image. The label y ? [1, C], where C indicates the number of categories. For instance, a dataset includes 701 buildings and each building contains multiple images. We number 701 buildings into 701 different indexes. Each index represents a category, i.e., the label y ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">701]</ref>. For cross-view geolocalization, we intend to learn one mapping function, which could project images from different platforms to one shared semantic space. The images of the same location are close, while the images from different location are apart from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Pattern Network</head><p>Feature extraction. The proposed model, i.e., Local Pattern Network (LPN), contains three branches, which extends from the Siamese network <ref type="bibr" target="#b23">[24]</ref>. From top to bottom in <ref type="figure" target="#fig_2">Figure 3</ref>, the three branches are the satellite-view branch, the drone-view branch and the ground-view branch respectively. LPN can deploy various network architectures as backbones to extract features, such as VGG <ref type="bibr" target="#b50">[51]</ref>, and ResNet <ref type="bibr" target="#b4">[5]</ref>. For illustration, we choose ResNet-50 <ref type="bibr" target="#b4">[5]</ref> as the network architecture of each branch if not specified. ResNet-50 contains five blocks named conv1, conv2, conv3, conv4, conv5, one average pooling layer, and one fully connected layer. We remove the final average pooling layer and the fully connected layer, and obtain intermediate feature maps for subsequent partition processing. Following <ref type="bibr" target="#b1">[2]</ref>, we share weights between the satellite-view branch and the drone-view branch, since input images of both branches are from the aerial viewpoint. Three branches have the same feature extraction manner. Specifically, given an input image of size 256 ? 256, we can acquire feature maps with the shape of 16 ? 16 ? 2048 in each branch. We denote this  Given one input image, we first extract feature maps. Since we study the cross-view geo-localization, the input image can be from different platforms. The proposed LPN contains three branches, i.e., the satellite-view branch, the drone-view branch and the ground-view branch, respectively, to deal with different kinds of inputs. The satellite-view branch and the drone-view branch share weights since images from the satellite view and the drone view have similar patterns. Then, the output feature maps from each branch are sliced according to the square-ring partition strategy. Next, the average pooling layer is used to transform each part-level feature maps into a column feature descriptor. Finally, all these feature descriptors are fed into a classifier module to get prediction vectors. In addition to the classification layer (Cls), the classifier module also contains other three type layers, which are the fully-connected layer (FC), the batch normalization layer (BN), and the dropout layer (Dropout). During training, we leverage the classifier module to predict the geo-tag of each part. The network is optimized by minimizing the sum of the cross-entropy losses over all parts. When testing, we obtain the part-level image representation before the classification layer in the classifier module. Then we concatenate part-level features as the final visual descriptor of the input image, and the dimension of the feature is 2048. In (A) (a green dotted line), we show the square-ring partition strategy. Note that here we display the framework for the University-1652 dataset of input data from three platforms. For two-view datasets, e.g., CVUSA, we use two CNN branches. function as F backbone , and the process of feature extraction can be formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Entropy Loss</head><formula xml:id="formula_0">f j = F backbone (x j ),<label>(1)</label></formula><p>where f j stands for the extracted feature map of the input image x j . Feature partition strategy. To explicitly take advantage of contextual information, we apply the square-ring partition strategy to divide feature maps. We observe that the geographic target is generally distributed in the center of the image, and the contextual information is radiantly distributed around. Based on this assumption of semantic information distribution, the center of the square-ring partition can be approximately aligned at the center of the feature maps. As shown in <ref type="figure" target="#fig_2">Figure 3</ref> (A) (green box), we separate images into four parts according to the distance to the image center. In practice, we separate the global feature maps f j to four feature parts f i j (i ? {1, 2, 3, 4}). The superscript i represents the i-th part from the center. Then we apply the average pooling layer to transform each part f i j with different shapes into a 2048-dim part feature g i j . The process can be formulated as:</p><formula xml:id="formula_1">f i j = F slice (f j , i),<label>(2)</label></formula><formula xml:id="formula_2">g i j = Avgpool(f i j ),<label>(3)</label></formula><p>where F slice indicates the square-ring partition, and Avgpool represents the average pooling operation. Optimization objective. Now we have obtained part features from different sources. Since the features are extracted from different branches, they may have different distribution, which could not be directly used for matching. To solve this limitation, we set up a mapping function that maps features of all sources into one shared feature space. In this shared space, features of the same geo-tag will have a closer distance, while features of different geo-tags are apart from each others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polar-transformed Satellite Ground</head><p>Case ?</p><p>: Column partition (on CVUSA*/CVACT*) <ref type="figure">Fig. 4</ref>. Illustration of the square-ring partition (Case I), the sequential partition (Case II) and the column partition (Case III). The sequential partition strategy and the column partition strategy are two special cases of the square-ring partition strategy. The sequential partition considers the geometric correspondence for matching satellite-and-ground panorama image pair <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The column partition directly splits the feature maps vertically. All three partition strategies exploit the contextual information and achieve the spatial alignment of each part. The square-ring partition strategy is suitable for processing images that the contextual information is distributed around the geographic target, such as University-1652. When localizing panoramic image and the orientation of different view images is aligned, the sequential partition has a higher priority, such as CVUSA and CVACT. The column partition enables a fine-grained spatial alignment for pre-processed image pairs on CVUSA * and CVACT * [1], <ref type="bibr" target="#b6">[7]</ref> whose orientation and semantic information distribution are roughly aligned.</p><p>This classifier module consists of following layers: a fully connected layer (FC), a batch normalization layer (BN), a dropout layer (Dropout), and a classification layer (Cls), which is a fully-connected layer. The classifier module is deployed to predict the geo-tag of each image based on part features. Given the part features g i j as the input, the classifier module outputs a column vector z i j . The dimension of z i j equals the number of geo-tag categories C.</p><formula xml:id="formula_3">z i j = F classif ier (g i j ).<label>(4)</label></formula><p>The cross-entropy loss could be formulated as:</p><formula xml:id="formula_4">p(y|x i j ) = exp(z i j (y)) C c=1 exp(z i j (c)) ,<label>(5)</label></formula><formula xml:id="formula_5">Loss = i,j ?log(p(y|x i j ))),<label>(6)</label></formula><p>where z i j (y) is the logit score of the ground-truth geo-tag y. We apply the softmax function (Equation 5) to obtain the normalized probability scorep(y|x i j ) in [0, 1].p(y|x i j ) is the predicted probability that x i j belongs to the geo-tag y. In <ref type="figure" target="#fig_5">Equation 6</ref>, we accumulate the losses on the image of different parts and different platforms to optimize the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>Our method is inspired by the mechanism of the human visual system on matching images of different viewpoints. In the ancient time, people compare the map and the surrounding environment to know where they are. The contextual information plays an important role. Nowadays, cameras on different platforms typically use wide-angle lenses to obtain complete geographic targets, and the contextual information around the geographic target is also collected in the image. We argue that the contextual information, as a by-product, can facilitate the discriminative representation learning. For example, the neighbor building also could help to predict the target location. Instead of dividing images in the pixel level, we split the feature maps in practice, which could not only improve the efficiency but also enable the larger receptive fields as well as the part alignment. The square-ring partition strategy is also robust to the rotation variants. Case I (see <ref type="figure">Figure 4</ref>) shows the application of the square-ring partition strategy on three images of different views in University-1652, i.e., satellite view, drone view and ground view. The orientation of these three-view images is not aligned. However, we can observe that the geographic target (orange point) is generally located in the image center. Because of the random orientation of three-view images, the contextual information with the same semantics (blue point) may not be distributed in the same orientation but can be located in the same part. We note that the sequential partition strategy is a special case of the square-ring partition strategy. The sequential partition strategy takes into account the geometric correspondence <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref> for a north aligned satellite-and-ground panorama image pair, such as data on CVUSA <ref type="bibr" target="#b16">[17]</ref>/CVACT <ref type="bibr" target="#b3">[4]</ref>. Specifically, we apply the square-ring partition to satellite images and the row partition to ground panoramas. As shown in Case II of <ref type="figure">Figure 4</ref>, image pairs on CVUSA/CVACT have different visual appearances. But the same semantic information (e.g., the blue point pair) from the true-matched image pair can still be roughly located in the same part of the divided feature maps. Besides, the column partition is also a variation of our method. The column partition can be adapted when the orientation and the spatial semantics of the matching image pair are roughly aligned. For example, images have been pre-processed by Optimal Transport theory <ref type="bibr" target="#b6">[7]</ref> or the polar transform <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Case III (see <ref type="figure">Figure 4</ref>) provides an example that the column partition is applied to a polar-transformed satellite image and a ground panorama. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>We first introduce three large-scale cross-view geolocalization datasets, two small-scale landmark retrieval datasets and the evaluation protocol. Then Section IV-B describes the implementation detail. We provide the comparison with the state of the arts in Section IV-C, followed by the ablation study in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Evaluation Protocol</head><p>We mainly train and evaluate our method on three large-scale geo-localization datasets, i.e., University-1652 <ref type="bibr" target="#b1">[2]</ref>, CVUSA <ref type="bibr" target="#b16">[17]</ref> and CVACT <ref type="bibr" target="#b3">[4]</ref>. <ref type="table" target="#tab_0">Table I</ref> shows the image number of query and gallery sets for testing different tasks using these three datasets.</p><p>University-1652 [2] is a multi-view multi-source dataset containing satellite-view data, drone-view data and groundview data. It collects 1652 buildings of 72 universities around the world. The training set includes 701 buildings of 33 universities, and the testing set includes the other 951 buildings of the rest 39 universities. There are no overlapping universities in the training and test set. Since some buildings do not have enough ground-view images to cover different aspects of these buildings, the dataset also provides an additional training set. Images in the additional training set are collected from the Google Image, and they have a similar view as the ground-view images. Therefore, the additional training set can be used as a supplement of the ground-view images. The dataset is employed to study two new tasks, i.e., drone-view target localization (Drone ? Satellite) and drone navigation (Satellite ? Drone). There are 701 buildings with 50,218 images for training. In the drone-view target localization task (Drone ? Satellite), there are 37,855 drone-view images in the query set and 701 true-matched satellite-view images and 250 satellite-view distractors in the gallery. There is only one true-matched satellite-view image under this setting. In the drone navigation task (Satellite ? Drone), there are 701 satellite-view query images, and 37,855 true-matched droneview images and 13,500 drone-view distractors in the gallery. There are multiple true-matched drone-view images under this setting.</p><p>CVUSA <ref type="bibr" target="#b16">[17]</ref> provides the data collected from two views, i.e., the ground view and the satellite view. Specifically, it contains <ref type="bibr" target="#b34">35</ref> Evaluation protocol. In our experiments, we use the Re-call@K (R@K) and the average precision (AP) to evaluate the performance of our model. R@K represents the proportion of correctly matched images in the top-K of the ranking list. A higher recall score shows a better performance of the network. We also calculate the area under the Precision-Recall curve, which is known as the average precision (AP), which reflects the precision and recall rate of the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We employ the ResNet-50 <ref type="bibr" target="#b4">[5]</ref> with pre-trained weights on ImageNet <ref type="bibr" target="#b53">[54]</ref> to extract visual features. Following <ref type="bibr" target="#b1">[2]</ref>, we modify the stride of the second convolutional layer and the last down-sample layer in conv5 1 of the ResNet-50 from 2 to 1. The newly-added layers in LPN, i.e., the classifier module, are initialized with kaiming initialization <ref type="bibr" target="#b54">[55]</ref>. We resize each input image to a fixed size of 256 ? 256 pixels during training and testing. In training, we employ random cropping and flipping to augment the input data. For the optimizer, we adopt stochastic gradient descent (SGD) with momentum 0.9 and weight decay 0.0005 with a mini-batch of 32. The initial learning rate is 0.001 for backbone layers and 0.01 for the new layers. We train our model for 120 epochs, and the learning rate is decayed by 0.1 after 80 epochs. During testing, we utilize the Euclidean distance to measure the similarity between the query image and candidate images in the gallery. Our model is implemented on Pytorch <ref type="bibr" target="#b55">[56]</ref>, and all experiments are conducted on one NVIDIA RTX 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the State-of-the-arts</head><p>Results on University-1652. As shown in <ref type="table" target="#tab_0">Table II</ref>, we compare the proposed method with other competitive approaches on University-1652. The proposed LPN has achieved 74.18% Recall@1 accuracy and 77.39% AP on Drone ? Satellite and 85.16% Recall@1 accuracy and 73.68% AP on Satellite ? Drone without using the additional Google training data. The performance has surpassed the reported result of other competitive methods such as <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and the proposed method outperforms the best method, i.e., instance loss <ref type="bibr" target="#b1">[2]</ref> by a large margin, i.e., about 14% AP improvement. If the extra training data, i.e., noisy data collected from Google Image, is added into the training set <ref type="bibr" target="#b1">[2]</ref>, we could further boost the retrieval performance. In the droneview target localization task (Drone ? Satellite), the accuracy of Recall@1 increases from 74.18% to 75.93% and the value of AP goes up from 77.39% to 79.14%; in the drone navigation task (Satellite ? Drone), the accuracy of Recall@1 increases from 85.16% to 86.45% and the value of AP raises from 73.68% to 74.79%. For the drone-view target localization task, there are 951 satellite-view images in the gallery. To make this retrieval task more challenging, we add 8884 satelliteview images collected from the testing set of CVUSA into the gallery of University-1652 as the distractors. Although the distractors would decrease the overall performance, indicated by Rank@1 and AP accuracy, the results are still competitive. This demonstrates the robustness of our proposed method against distractors. Results on CVUSA. The comparison with other competitive methods on CVUSA is detailed in <ref type="table" target="#tab_0">Table III</ref>. Groundview images on CVUSA are panoramas, in which, the contextual information is generally distributed on both sides of the geographic target. Basing on the discussion in III-B, we deploy the sequential partition strategy to explicitly mine the contextual information on CVUSA (see <ref type="figure">Figure 4</ref>). The sequential partition strategy is a specific case of the squarering partition strategy. As shown in <ref type="table" target="#tab_0">Table III</ref>, we could observe two points. First, we deploy category recognition as the pretext task to conduct geo-localization on CVUSA. In particular, we regard 35,532 pairs as 35,532 location categories to train the model. The proposed method, whether using VGG16 <ref type="bibr" target="#b50">[51]</ref> or ResNet-50 <ref type="bibr" target="#b4">[5]</ref> as the backbone, surpasses most existing methods. Specifically, when using VGG16 as the backbone, our method achieves 79.69% R@1, 91.70% R@5, 94.55% R@10 and 98.50% R@Top1% on CVUSA. Since the feature expression capability of ResNet-50 is powerful than VGG16, our method with ResNet-50 backbone obtains 85.79% R@1, 95.38% R@5, 96.98% R@10 and 99.41% R@Top1% on CVUSA. Second, the proposed method is complementary to existing methods. For instance, our method can combine with the CVFT <ref type="bibr" target="#b6">[7]</ref> and the SAFA <ref type="bibr" target="#b0">[1]</ref> orthogonally. We re-implement CVFT and SAFA. Specifically, we keep the VGG16 as the backbone on both models unchanged and divide feature maps basing on Case III. For CVFT, we divide the final aligned feature maps into 8 parts and compute the loss of each corresponding part. For SAFA, the polar transform is retained. We first divide the feature maps into 8 parts, and then we use one SPE in SAFA to deal with each part separately before computing the loss. Combined with our partition strategy, CVFT+Ours boosts the R@1 accuracy from 61.43% to 68.20% (+6.77%) and the R@Top1% accuracy from 99.02% to 99.30% (+0.28%). Similarly, SAFA+Ours can further improve the R@1 accuracy from 89.84% to 92.83% (+2.99%) and the R@Top1% accuracy from 99.64% to 99.78% (+0.14%).</p><p>Significant performance improvements suggest that our method helps to mine more contextual information, yielding discriminative features.</p><p>Results on CVACT. CVACT has a similar data pattern with CVUSA. Our method with ResNet-50 <ref type="bibr" target="#b4">[5]</ref> backbone achieves 79.99% R@1, 90.63% R@5, 92.56% R@10 and 97.03% R@Top1% on CVACT. For a fair comparison, our method using VGG16 as the backbone also acquires competitive results. CVFT <ref type="bibr" target="#b6">[7]</ref>+Ours obtains the improvement with the R@1 accuracy from 61.05% to 62.90% (+1.85%) and the R@Top1% accuracy from 95.93% to 97.22% (+1.29%). SAFA <ref type="bibr" target="#b0">[1]</ref>+Ours increases the R@1 accuracy from 81.03% to 83.66% (+2.63%) and the R@Top1% accuracy from 98.17% to 98.41% (+0.24%). The experimental results demonstrate that our method is still effective on CVACT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>To verify the effectiveness of components in our model, we design several ablation studies.</p><p>Effect of the number of parts. The number of parts n is one of the key parameters in our network. By default, we deploy n = 4. When n = 1, the model is employed to global average pooling. At this time, the model equals the baseline with global attention <ref type="bibr" target="#b1">[2]</ref>. As shown in <ref type="figure">Figure 5</ref>, with the increment of n, both the Recall@1 and AP values have a significant improvement, since more contextual information has been taken into consideration. Intuitively, concatenating more contextual information parts can improve the discriminability of the final feature descriptor. However, we note that, as n increases, each part contains less receptive fields with limited semantic information. As a result, a higher value of n compromises the discriminability of the image representation. When n = 6 or 8, the performance gains slowly or even slightly degrades. Therefore, we use n = 4 as the default choice for our network, which balances the mining of the contextual information and the appropriate size of the receptive field.  <ref type="table" target="#tab_0">RESULT IN THE SCHEME OF INSTANCE LOSS, WHILE, IN THE DEEP METRIC LEARNING  SCHEME, SAFA [1] IS A STATE-OF-THE-ART WORK. WE OBSERVE THAT THROUGH COMBINING OUR METHOD TO THESE TWO METHODS, THE  OFF-THE-SHELF NETWORK CAN ACHIEVE A SIGNIFICANT PERFORMANCE BOOST.  ? : THE METHOD UTILIZES EXTRA ORIENTATION INFORMATION AS</ref> INPUT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Publication <ref type="table" target="#tab_6">Backbone  CVUSA  CVACT val  R@1  R@5  R@10  R@Top1%  R@1</ref> R@5 R@10 R@Top1% MCVPlaces <ref type="bibr" target="#b21">[22]</ref> ICCV'15 AlexNet ---34.40 ----Zhai <ref type="bibr" target="#b16">[17]</ref> CVPR'17 VGG16 ---43.20 ----Vo <ref type="bibr" target="#b25">[26]</ref> ECCV "@1(%) ()(%) <ref type="figure">Fig. 5</ref>. The effect of the number of parts n on two tasks of the University-1652 dataset, i.e., Drone ? Satellite and Satellite ? Drone. The red line refers to the task of drone-view target localization (Drone ? Satellite). The blue line shows the task of drone navigation (Satellite ? Drone). We show the effect of the number of parts for R@1 accuracy (a), and AP accuracy (b). We observe that LPN achieves the best performance when the number of parts n =4.  Effect of the input image size. A small training size compresses the fine-grained information of the input image, which compromises the discriminative representation learning. In contrast, a larger input size introduces more memory costs during training. To balance the input image size with the memory usage, we study the effect of the input image size. We just change the image size and the covered region of the image is not changed in the experiment. As shown in <ref type="table" target="#tab_0">Table  IV</ref>, in both tasks, i.e., (Drone ? Satellite) and (Satellite ? Drone), as the input image size from 224 to 384, we observe that the performance gradually improves. When we continue to enlarge the input size to 512, the improvement is not clear on the Drone ? Satellite task. We hope this study could help the real-world application in selecting the appropriate input size, when computation sources are limited.</p><p>Is LPN robust to rotation variants? Satellite-view images in University-1652 are north aligned and the orientation of drone-view images is random. In the training phase, the rotation augmentation is applied in the satellite-view branch but not in other branches. To verify the scalability of LPN for    square-ring partition strategy divides the feature maps into four parts in LPN. We use the numbers 1, 2, 3, and 4 to represent the four parts of the feature maps, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Subsequently, we conduct experiments by choosing one or a combination of the four parts. The experimental results demonstrate LPN has learned complementary features (see <ref type="table" target="#tab_0">Table VI</ref>). We observe that using only one part has obtained one fairly good result in two tasks, i.e., (Drone ? Satellite) and (Satellite ? Drone). When we further concatenate two or three parts, the accuracy of Recall@1 and AP gradually increases. When all parts are leveraged, we obtain the best performance in both tasks. It demonstrates that LPN has learned complementary part feature, enriching the model capability.</p><p>But the Recall@1 accuracy and AP drop dramatically in both tasks if query features and gallery features cover different parts. The results reflect from the side that the learned semantic information between the parts is complementary. Because of the complementary in each part, the semantic information contained in different parts is not overlapping. When there are different parts in the query and gallery feature (i.e., the true-matched image pair covers significant different areas), the different parts become the distractors for the final visual feature, resulting in terrible retrieval performance. Is LPN robust to the shifted query image? In the realistic scenario, there is usually an offset in the geographic target location of the query image and true-matched images in the gallery. To explore whether LPN can cope with the offset of the geographic target location in a true-matched image pair, we carry out experiments on shifting the query image during testing. Specifically, we shift the query image to the right in pixels and keep images in the gallery intact (see <ref type="figure" target="#fig_6">Figure 7)</ref>. <ref type="table" target="#tab_0">Table VII</ref> shows the experimental results. 0 indicates that the input query image is not offset. When the input query image is shifted 10 pixels, we can hardly observe a performance drops for drone navigation and drone-view target localization tasks. While the shifted pixels is 20, the performance on both tasks decreases slightly. The experimental results suggest that LPN is robust when there is a small offset of the geographic target location for a true-matched image pair in retrieval.</p><p>Geo-localization between satellite-view images and ground-view images. In University-1652, geo-localization between satellite-view images and ground-view images is a challenging task. We can sum up the difficulties in the  <ref type="bibr" target="#b1">[2]</ref>. But using drone-view images in LPN obtains better results than without using these. Effect of the drone distance to the geographic target. The scale of the satellite-view image in University-1652 is fixed, while the scale of the drone-view image changes dynamically with the drone distance to the geographic target. We adopt drone-view images with different distances to the geographic target as queries to study the impact of the changed scale for LPN. As shown in <ref type="table" target="#tab_0">Table IX</ref>, when the drone-view image is captured in a middle distance to the geographic target, we obtain the best performance. When the drone distance is short to the geographic target, we can observe that the results are still competitive compared with using all drone-view query images. The scales of these drone-view images are close to satellite-view images. Another reason is that these drone-view images mainly contain the target building without extra trees and other buildings.</p><p>Transfer learning from University-1652 to small-scale datasets. To study the generalization ability of LPN trained on University-1652, we evaluate three models on two smallscale landmark retrieval datasets, i.e., Oxford5k <ref type="bibr" target="#b51">[52]</ref> and Paris6k <ref type="bibr" target="#b52">[53]</ref>. The first model is ResNet-50 [5] trained on ImageNet <ref type="bibr" target="#b53">[54]</ref>. The second model is baseline <ref type="bibr" target="#b1">[2]</ref> and LPN is  the third model. During the evaluation, three models have not been fine-tuned on these two datasets. For baseline and LPN, we choose two different branches, i.e., F s and F g to extract features, since these two branches focus on different low-level patterns of input images. Weights on F s are trained by the satellite-view images, while F g is learned on the ground-view images. From <ref type="table" target="#tab_9">Table X</ref>, we observe that the extracted feature from LPN shows better performance on both two datasets than features obtained from ResNet-50 and baseline. This result also demonstrates that the square-ring partition strategy can enhance the generalization ability of our model. We also note that in the same model, F g has a superior generalization ability than F s . It is because that images in Oxford5k and Pairs6k are closer to the Google Street View images, which are similar to the ground-view images collected from Google Image. Besides, F s is trained by the aerial-view data, which viewpoint is perpendicular to the ground plane. In contrast, the data viewpoint in Oxford5k or Paris6k is parallel to the ground plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Result</head><p>As an additional qualitative evaluation, we visualize some heatmaps created by our and compared methods. <ref type="figure" target="#fig_7">Figure 8 (a)</ref> shows heatmaps generated by baseline <ref type="bibr" target="#b1">[2]</ref> and LPN in the drone and satellite platforms. Compared with the baseline, our approach activates the region of the geographic target and neighbor areas containing the contextual information. <ref type="figure" target="#fig_7">Figure 8</ref> (b) shows the ground-view heatmaps generated by the original SAFA <ref type="bibr" target="#b0">[1]</ref> and SAFA fused our partition strategy (Ours). SAFA only activates the position of the road, while our method further places emphasis on contextual information next to the road position. Our method is more consistent with the processing of the human visual system to locate an unfamiliar road.</p><p>Moreover, we show some retrieval results for different tasks on University-1652 and CVUSA in <ref type="figure" target="#fig_8">Figure 9</ref>. On University-1652, we observe that LPN can adapt to retrieve the reasonable images based on the content in both drone-view localization and drone navigation tasks. One failure case is also shown in the second row of <ref type="figure" target="#fig_8">Figure 9</ref> (I), in which LPN can not recall the matched image in top-1. We notice that it is challenging in that the recalled top-1 image has a very similar pattern with the query image, especially the appearance of the geographic target in two images. On CVUSA, we observe a similar result. Our method with SAFA <ref type="bibr" target="#b0">[1]</ref> has successfully retrieved the relevant satellite-view images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we identify the challenge in cross-view geolocalization, and propose a simple and effective deep neural network, called Local Pattern Network (LPN), to explicitly mine the contextual information. Specifically, we introduce a square-ring partition strategy for learning complementary spatial features according to the distance to the image center. The contextual information enhances the discriminability of the image representation with more fine-grained patterns. Our approach has achieved competitive accuracy on three cross-view geo-localization benchmarks, i.e., University-1652, CVUSA and CVACT. Moreover, the proposed LPN has good scalability to rotation variation, which is close to the real-world application. The square-ring partition strategy also can be easily embedded into other frameworks to boost performance. In the future, we will investigate applying a module, such as STN <ref type="bibr" target="#b40">[41]</ref>, to estimate the scale of the drone-view image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The simplified diagram of our partition strategy, which is invariant to the rotation. The region of part A represents the geographic target in the center. According to the distance from the geographic target, the region of part B can be viewed as the first hierarchical contextual information, and the region of part C is the second hierarchical contextual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of the proposed LPN framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Case ?: Sequential partition (on CVUSA/CVACT) Geographic targetContextual informationCase ?: Square-ring partition (on University-1652)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The feature maps are first divided into four parts in LPN. Then, an average pooling layer transforms these four parts into four column vectors which are treated as subsequent feature descriptors. For each part, we use the numbers 1, 2, 3, 4 to represent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>The first and last images are the original query and gallery images in the test set. We pad 10 and 20 pixels in the left of the query image in the way of reflection, respectively. Then, we crop the padded image to the original image size in a left-aligned manner. Thus we can obtain the second and third images, which have an offset of the geographic target with the gallery image. The left space of the red dotted line is the extra padded pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of heatmaps. (a) Heatmaps generated by baseline<ref type="bibr" target="#b1">[2]</ref> and ours in different platforms of University-1652. (b) Ground-view heatmaps learned from SAFA<ref type="bibr" target="#b0">[1]</ref> and ours (SAFA + ours) on CVUSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative image retrieval results. (I) Top-5 retrieval results of drone-view target localization on University-1652. (II) Top-5 retrieval results of drone navigation on University-1652. (III) Top-3 retrieval results of geographic localization on CVUSA. The true matches are in yellow boxes, while the false matches are displayed in blue boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THREE DIFFERENT TEST SETS, INCLUDING THE IMAGE NUMBER OF QUERY SET AND GALLERY SET FOR DIFFERENT GEO-LOCALIZATION TASKS.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Task</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Drone ? Satellite</cell><cell cols="2">Satellite ? Drone</cell></row><row><cell></cell><cell>Query</cell><cell>Gallery</cell><cell>Query</cell><cell>Gallery</cell></row><row><cell cols="2">University-1652 [2] 37,855</cell><cell>951</cell><cell>701</cell><cell>51,355</cell></row><row><cell></cell><cell cols="4">Ground ? Satellite Satellite ? Ground</cell></row><row><cell></cell><cell>Query</cell><cell>Gallery</cell><cell>Query</cell><cell>Gallery</cell></row><row><cell>CVUSA [17]</cell><cell>8884</cell><cell>8884</cell><cell>8884</cell><cell>8884</cell></row><row><cell>CVACT val [4]</cell><cell>8884</cell><cell>8884</cell><cell>8884</cell><cell>8884</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>,532 ground-and-satellite image pairs for training and 8884 image pairs for testing. All ground-view panoramic images are collected from Google Street View. Meanwhile, corresponding satellite-view images are downloaded from Microsoft Bing Maps.</figDesc><table><row><cell>CVACT [4] is a large-scale cross-view dataset. Same as</cell></row><row><cell>CVUSA, CVACT provides 35,532 ground-and-satellite image</cell></row><row><cell>pairs for training, and ground-view images are panoramas.</cell></row><row><cell>Besides, CVACT provides a validation set with 8884 image</cell></row><row><cell>pairs named CVACT val and a testing set with 92,802 image</cell></row><row><cell>pairs denoted as CVACT test. A query image only has one</cell></row><row><cell>true-matched image in the gallery for CVACT val, while for</cell></row><row><cell>CVACT test, a query image may correspond to several true-</cell></row><row><cell>matched images in the gallery.</cell></row><row><cell>Oxford5k [52] &amp; Paris6k [53] are two prevailing landmark</cell></row><row><cell>retrieval datasets collected from Flickr. Oxford5k consists of</cell></row><row><cell>5062 images that belong to 11 different Oxford landmarks, and</cell></row><row><cell>Paris6k contains 6412 images of 12 particular Paris buildings.</cell></row><row><cell>There are 55 query images in Oxford5k and 12 queries in</cell></row><row><cell>Paris6k.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH THE STATE-OF-THE-ART RESULTS REPORTED ON UNIVERSITY-1652. M STANDS FOR THE MARGIN OF THE TRIPLET LOSS. (W/O GOOGLE) INDICATES THAT THE EXTRA TRAINING SET COLLECTED FROM GOOGLE IMAGE IS NOT DEPLOYED IN TRAINING PHASE. (W/ CVUSA DISTRACTORS) DENOTES THAT ALL SATELLITE-VIEW IMAGES COLLECTED FROM THE TESTING SET OF CVUSA ARE ADDED INTO THE SATELLITE-VIEW GALLERY OF UNIVERSITY-1652 AS THE DISTRACTORS.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">University-1652</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Drone ? Satellite Satellite ? Drone</cell></row><row><cell></cell><cell>R@1</cell><cell>AP</cell><cell>R@1</cell><cell>AP</cell></row><row><cell>Instance Loss [2]</cell><cell>58.49</cell><cell>63.31</cell><cell>71.18</cell><cell>58.74</cell></row><row><cell>Contrastive Loss [23]</cell><cell>52.39</cell><cell>57.44</cell><cell>63.91</cell><cell>52.24</cell></row><row><cell cols="2">Triplet Loss (M = 0.3) [57] 55.18</cell><cell>59.97</cell><cell>63.62</cell><cell>53.85</cell></row><row><cell cols="2">Triplet Loss (M = 0.5) [57] 53.58</cell><cell>58.60</cell><cell>64.48</cell><cell>53.15</cell></row><row><cell cols="2">Soft Margin Triplet Loss [9] 53.21</cell><cell>58.03</cell><cell>65.62</cell><cell>54.47</cell></row><row><cell>Ours (w/o Google)</cell><cell>74.18</cell><cell>77.39</cell><cell>85.16</cell><cell>73.68</cell></row><row><cell>Ours</cell><cell>75.93</cell><cell>79.14</cell><cell>86.45</cell><cell>74.79</cell></row><row><cell cols="2">Ours (w/ CVUSA distractors) 70.61</cell><cell>73.53</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON CVUSA, LIST SHOWS COMPARISONS OF VARIOUS METHODS. THERE ARE TWO SCHEMES TO OPTIMIZE THE NETWORK, i.e., INSTANCE LOSS AND DEEP METRIC LEARNING. ZHENG et al. [2] GET THE BEST</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON THE EFFECT OF DIFFERENT INPUT SIZES ON UNIVERSITY-1652.</figDesc><table><row><cell>Image Size</cell><cell cols="2">Drone ? Satellite R@1 AP</cell><cell cols="2">Satellite ? Drone R@1 AP</cell></row><row><cell>224</cell><cell>69.28</cell><cell>72.98</cell><cell>82.45</cell><cell>68.92</cell></row><row><cell>256</cell><cell>75.93</cell><cell>79.14</cell><cell>86.45</cell><cell>74.79</cell></row><row><cell>320</cell><cell>77.65</cell><cell>80.56</cell><cell>85.31</cell><cell>75.36</cell></row><row><cell>384</cell><cell>78.02</cell><cell>80.99</cell><cell>86.16</cell><cell>76.56</cell></row><row><cell>512</cell><cell>77.71</cell><cell>80.80</cell><cell>90.30</cell><cell>78.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="6">ABLATION STUDY ON ROTATING IMAGES DURING INFERENCE ON</cell></row><row><cell></cell><cell></cell><cell cols="2">UNIVERSITY-1652.</cell><cell></cell><cell></cell></row><row><cell cols="2">Rotation Angle</cell><cell cols="2">Drone ? Satellite</cell><cell cols="2">Satellite ? Drone</cell></row><row><cell>Query</cell><cell>Gallery</cell><cell>R@1</cell><cell>AP</cell><cell>R@1</cell><cell>AP</cell></row><row><cell>0 ?</cell><cell>0 ?</cell><cell>75.93</cell><cell>79.14</cell><cell>86.45</cell><cell>74.79</cell></row><row><cell>16 ?</cell><cell>0 ?</cell><cell>75.64</cell><cell>78.86</cell><cell>85.16</cell><cell>72.78</cell></row><row><cell>45 ?</cell><cell>0 ?</cell><cell>72.04</cell><cell>75.62</cell><cell>85.16</cell><cell>72.27</cell></row><row><cell>67 ?</cell><cell>0 ?</cell><cell>70.39</cell><cell>74.09</cell><cell>85.73</cell><cell>73.06</cell></row><row><cell>90 ?</cell><cell>0 ?</cell><cell>68.80</cell><cell>72.67</cell><cell>86.31</cell><cell>75.31</cell></row><row><cell>180 ?</cell><cell>0 ?</cell><cell>70.76</cell><cell>74.47</cell><cell>85.45</cell><cell>74.03</cell></row><row><cell>204 ?</cell><cell>0 ?</cell><cell>69.92</cell><cell>73.68</cell><cell>84.45</cell><cell>72.22</cell></row><row><cell>270 ?</cell><cell>0 ?</cell><cell>69.06</cell><cell>72.49</cell><cell>86.73</cell><cell>75.12</cell></row><row><cell>317 ?</cell><cell>0 ?</cell><cell>72.29</cell><cell>75.87</cell><cell>84.17</cell><cell>71.85</cell></row><row><cell>32 ?</cell><cell>75 ?</cell><cell>73.19</cell><cell>76.69</cell><cell>83.17</cell><cell>66.41</cell></row><row><cell>216 ?</cell><cell>87 ?</cell><cell>69.54</cell><cell>73.27</cell><cell>83.45</cell><cell>65.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original Query</cell><cell>Pad_10</cell><cell>Pad_20</cell><cell>Gallery</cell></row><row><cell cols="7">ABLATION STUDY OF USING ONE PART OR A COMBINATION OF DIFFERENT</cell></row><row><cell cols="7">PARTS DURING INFERENCE. 1, 2, 3, 4 INDICATE FOUR AVERAGED PARTS</cell></row><row><cell></cell><cell cols="5">WHICH ARE SLICED FROM FEATURE MAPS.</cell></row><row><cell cols="3">Part Combination</cell><cell cols="4">Drone ? Satellite Satellite ? Drone</cell></row><row><cell>Query</cell><cell cols="2">Gallery</cell><cell>R@1</cell><cell>AP</cell><cell>R@1</cell><cell>AP</cell></row><row><cell>1</cell><cell>1</cell><cell></cell><cell>71.95</cell><cell>75.50</cell><cell>84.02</cell><cell>70.24</cell></row><row><cell>2</cell><cell>2</cell><cell></cell><cell>71.94</cell><cell>75.49</cell><cell>83.74</cell><cell>71.32</cell></row><row><cell>3</cell><cell>3</cell><cell></cell><cell>71.97</cell><cell>75.62</cell><cell>85.45</cell><cell>71.61</cell></row><row><cell>4</cell><cell>4</cell><cell></cell><cell>70.75</cell><cell>74.41</cell><cell>82.03</cell><cell>69.50</cell></row><row><cell>1 + 2</cell><cell>1 + 2</cell><cell></cell><cell>74.85</cell><cell>78.14</cell><cell>85.59</cell><cell>73.69</cell></row><row><cell>1 + 2 + 3</cell><cell cols="2">1 + 2 + 3</cell><cell>75.74</cell><cell>78.97</cell><cell>86.31</cell><cell>74.76</cell></row><row><cell cols="4">1 + 2 + 3 + 4 1 + 2 + 3 + 4 75.93</cell><cell>79.14</cell><cell>86.45</cell><cell>74.79</cell></row><row><cell>1 + 2 + 3</cell><cell cols="2">2 + 3 + 4</cell><cell>0.07</cell><cell>0.38</cell><cell>0.14</cell><cell>0.21</cell></row><row><cell>1 + 2</cell><cell>2 + 3</cell><cell></cell><cell>0.08</cell><cell>0.39</cell><cell>0.00</cell><cell>0.17</cell></row><row><cell>1 + 2</cell><cell>3 + 4</cell><cell></cell><cell>0.13</cell><cell>0.50</cell><cell>0.00</cell><cell>0.21</cell></row><row><cell cols="7">image rotation, we conduct experiments on rotating the query</cell></row><row><cell cols="7">image to retrieval the true-matched images. We do not rotate</cell></row><row><cell cols="7">gallery images but query images. The experimental results are</cell></row><row><cell cols="7">shown in Table V. The 0 ? denotes the input query image</cell></row><row><cell cols="7">without rotation. For the task of drone navigation (Satellite ?</cell></row><row><cell cols="7">Drone), LPN obtains robust features for unseen satellite-view</cell></row><row><cell cols="7">query images against different rotation angles. In contrast, for</cell></row><row><cell cols="7">the task of drone-view target localization (Drone ? Satellite),</cell></row><row><cell cols="7">we do not train the model on the rotated drone-view data.</cell></row><row><cell cols="7">The LPN still achieves one competitive performance without</cell></row><row><cell cols="7">a significant performance drop. In addition, we also attempt to</cell></row><row><cell cols="7">rotate different angles on query and gallery images to further</cell></row><row><cell cols="7">test our model. The experimental results suggest that LPN has</cell></row><row><cell cols="5">good scalability to rotation variations.</cell><cell></cell></row><row><cell cols="7">Does LPN learn complementary part features? The</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="7">ABLATION STUDY ON SHIFTING QUERY IMAGES DURING INFERENCE ON</cell></row><row><cell></cell><cell></cell><cell cols="3">UNIVERSITY-1652.</cell><cell></cell></row><row><cell cols="2">Shifted Pixel</cell><cell cols="3">Drone ? Satellite R@1 AP</cell><cell cols="2">Satellite ? Drone R@1 AP</cell></row><row><cell>0</cell><cell></cell><cell>75.93</cell><cell>79.14</cell><cell></cell><cell>86.45</cell><cell>74.79</cell></row><row><cell>10</cell><cell></cell><cell>75.26</cell><cell>78.57</cell><cell></cell><cell>85.16</cell><cell>72.84</cell></row><row><cell>20</cell><cell></cell><cell>72.37</cell><cell>76.04</cell><cell></cell><cell>83.02</cell><cell>70.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII THE</head><label>VIII</label><figDesc>PERFORMANCE OF GEO-LOCALIZATION BETWEEN SATELLITE-VIEW IMAGES AND GROUND-VIEW IMAGES. (W/O DRONE) INDICATES THAT LPN IS TRAINED WITHOUT DRONE-VIEW IMAGES. Unlike ground-view images in CVUSA and CVACT, which are panoramas, the ground-view image in University-1652 only covers part of the whole building.(3) Ground-view images in University-1652 contains vast obstacles, e.g., trees and cars. From Table VIII, we can notice that geo-localization between satellite-view and ground-view images do not work well. Using satellite-view images to retrieve the ground-view images, our method achieves the best results in Recall@1 accuracy and AP. Whether employing drone-view images or not, there is a performance decrease in the ground-to-satellite localization compared with baseline</figDesc><table><row><cell>Method</cell><cell cols="4">Satellite ? Ground Ground ? Satellite R@1 AP R@1 AP</cell></row><row><cell>Baseline [2]</cell><cell>1.14</cell><cell>1.14</cell><cell>1.20</cell><cell>2.52</cell></row><row><cell>Ours (w/o drone)</cell><cell>1.43</cell><cell>1.31</cell><cell>0.74</cell><cell>1.83</cell></row><row><cell>Ours</cell><cell>1.85</cell><cell>1.66</cell><cell>0.81</cell><cell>2.21</cell></row><row><cell></cell><cell></cell><cell>TABLE IX</cell><cell></cell><cell></cell></row><row><cell cols="5">ABLATION STUDY ON USING DRONE-VIEW IMAGES WITH DIFFERENT</cell></row><row><cell cols="5">DISTANCE TO THE GEOGRAPHIC TARGET TO CONDUCT RETRIEVAL. "ALL"</cell></row><row><cell cols="5">INDICATES THAT WE APPLY ALL DRONE-VIEW QUERY IMAGES.</cell></row><row><cell>Distance</cell><cell></cell><cell cols="2">Drone ? Satellite R@1 AP</cell><cell></cell></row><row><cell>All</cell><cell></cell><cell>75.93</cell><cell cols="2">79.14</cell></row><row><cell>Long</cell><cell></cell><cell>60.20</cell><cell cols="2">65.01</cell></row><row><cell>Short</cell><cell></cell><cell>75.04</cell><cell cols="2">78.35</cell></row><row><cell>Middle</cell><cell></cell><cell>80.03</cell><cell cols="2">82.77</cell></row><row><cell cols="5">following points. (1) Ground and satellite images that are</cell></row><row><cell cols="5">collected from different viewpoints naturally have a distinct</cell></row><row><cell cols="2">visual appearance. (2)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X TRANSFER</head><label>X</label><figDesc>LEARNING FROM UNIVERSITY-1652 TO SMALL-SCALE DATASETS, i.e., OXFORD5K [52] AND PARIS6K [53]. WE SHOW THE AP (%) ACCURACY ON TWO DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>ResNet-50</cell><cell>baseline [2] Fs Fg</cell><cell>Fs</cell><cell>Ours</cell><cell>Fg</cell></row><row><cell>Oxford5k [52]</cell><cell>8.43</cell><cell cols="4">15.62 41.12 27.02 51.71</cell></row><row><cell>Paris6k [53]</cell><cell>27.93</cell><cell cols="4">38.18 59.00 45.81 67.73</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial-aware feature aggregation for image based cross-view geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">University-1652: A multi-view multisource benchmark for drone-based geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413896</idno>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the location dependence of convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lending orientation to neural networks for cross-view geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dualpath convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383184</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal feature transport for cross-view image geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Where am i looking at? joint location and orientation estimation by cross-view matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cvm-net: Cross-view matching network for image-based ground-to-aerial geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sta: Spatial-temporal attention for large-scale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Control of goal-directed and stimulusdriven attention in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehiclenet: Learning robust visual representation for vehicle reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.3014488</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting groundlevel scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic cross-view matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A N</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A framework for global vehicle localization using stereo images and satellite and road maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Senlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geolocalization of street views with aerial image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical temporal modeling with mutual distance matching for video based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.405</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pop: Patchwork of parts models for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trouv?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial priors for part-based recognition using statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards automatic discovery of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2018.2873599</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3037" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalizable person re-identification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pose-invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Glad: Globallocal-alignment descriptor for scalable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="999" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Triplet-based deep hashing network for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3893" to="3903" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bridging the domain gap for ground-to-aerial image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ground-to-aerial image geo-localization with a hard exemplar reweighting triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
