<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">F 3 Net: Fusion, Feedback and Focus for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
							<email>jun.wei@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
							<email>wangshuhui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">F 3 Net: Fusion, Feedback and Focus for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of existing salient object detection models have achieved great progress by aggregating multi-level features extracted from convolutional neural networks. However, because of the different receptive fields of different convolutional layers, there exists big differences between features generated by these layers. Common feature fusion strategies (addition or concatenation) ignore these differences and may cause suboptimal solutions. In this paper, we propose the F 3 Net to solve above problem, which mainly consists of cross feature module (CFM) and cascaded feedback decoder (CFD) trained by minimizing a new pixel position aware loss (PPA). Specifically, CFM aims to selectively aggregate multilevel features. Different from addition and concatenation, CFM adaptively selects complementary components from input features before fusion, which can effectively avoid introducing too much redundant information that may destroy the original features. Besides, CFD adopts a multi-stage feedback mechanism, where features closed to supervision will be introduced to the output of previous layers to supplement them and eliminate the differences between features. These refined features will go through multiple similar iterations before generating the final saliency maps. Furthermore, different from binary cross entropy, the proposed PPA loss doesnt treat pixels equally, which can synthesize the local structure information of a pixel to guide the network to focus more on local details. Hard pixels from boundaries or error-prone parts will be given more attention to emphasize their importance. F 3 Net is able to segment salient object regions accurately and provide clear local details. Comprehensive experiments on five benchmark datasets demonstrate that F 3 Net outperforms state-of-the-art approaches on six evaluation metrics. Code will be released at https://github.com/weijun88/F3Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Salient object detection (SOD) aims to estimate the visual significant regions of images or videos and often serves as the pre-processing step for many downstream vision tasks <ref type="bibr" target="#b22">(Wang et al. 2019a)</ref>. Earlier SOD algorithms mainly rely on heuristic priors (e.g., color, texture and contrast) to generate saliency maps. However, these hand-craft features can hardly capture high-level semantic relations and context information, thus they are not robust enough to complex scenarios. Recently, convolutional neural networks (CNNs) have demonstrated its powerful feature extraction capability in visual feature representation <ref type="bibr" target="#b31">(Zhang et al. 2018c;</ref><ref type="bibr" target="#b18">Szegedy et al. 2015;</ref><ref type="bibr" target="#b7">Huang et al. 2017;</ref><ref type="bibr" target="#b6">Hu, Shen, and Sun 2018)</ref>. Many CNNs-based models <ref type="bibr" target="#b15">Qin et al. 2019;</ref><ref type="bibr" target="#b0">Chen et al. 2018;</ref><ref type="bibr" target="#b25">Wu, Su, and Huang 2019;</ref><ref type="bibr" target="#b11">Li et al. 2018;</ref><ref type="bibr" target="#b30">Zhang et al. 2018b;</ref><ref type="bibr" target="#b3">Feng, Lu, and Ding 2019;</ref><ref type="bibr" target="#b20">Wang et al. 2017b;</ref><ref type="bibr" target="#b28">Zhang et al. 2017)</ref> have achieved remarkable progress and pushed the performance of SOD to a new level. These models adopt the encoder-decoder architecture, which is simple in structure and computationally efficient. The encoder usually is made up of a pretrained classification model (e.g. ResNet <ref type="bibr" target="#b4">(He et al. 2016)</ref> and VGG <ref type="bibr" target="#b17">(Simonyan and Zisserman 2015)</ref>), which can extract multiple features of different semantic levels and resolutions. In the decoder, extracted features are combined to generate saliency maps.</p><p>However, there still remains two big challenges in accurate SOD. First, features of different levels have different distribution characteristics. High level features have rich se-mantics but lack accurate location information. Low level features have rich details but full of background noises. To generate better saliency maps, multi-level features are combined. However, without delicate control of the information flow in the model, some redundant features, including noises from low level layers and coarse boundaries from high level layers will pass in and possibly result in performance degradation. Second, most of existing models use binary cross entropy that treats all pixels equally. Intuitively, different pixels deserve different weights, e.g., pixels at the boundary are more discriminative and should be attached with more importance. Various boundary losses <ref type="bibr" target="#b15">(Qin et al. 2019;</ref><ref type="bibr" target="#b3">Feng, Lu, and Ding 2019)</ref> have been proposed to enhance the boundary detection accuracy, but considering only the boundary pixels is not comprehensive enough, since there are lots of pixels near the boundaries prone to wrong predictions. These pixels are also important and should be assigned with larger weights. In consequence, it is essential to design a mechanism to reduce the impact of inconsistency between features of different levels and assign larger weights to those truly important pixels.</p><p>To address above challenges, we proposed a novel SOD framework, named F 3 Net, which achieves remarkable performance in producing high quality saliency maps. First, to mitigate the discrepancy between features, we design cross feature module (CFM), which fuses features of different levels by element-wise multiplication. Different from addition and concatenation, CFM takes a selective fusion strategy, where redundant information will be suppressed to avoid the contamination between features and important features will complement each other. Compared with traditional fusion methods, CFM is able to remove background noises and sharpen boundaries, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Second, due to downsampling, high level features may suffer from information loss and distortion, which can not be solved by CFM. Therefore, we develop the cascaded feedback decoder (CFD) to refine these features iteratively. CFD contains multiple sub-decoders, each of which contains both bottom-up and top-down processes. For bottom-up process, multi-level features are aggregated by CFM gradually. For top-down process, aggregated features are feedback into previous features to refine them. Third, we propose the pixel position aware loss (PPA) to improve the commonly used binary cross entropy loss which treats all pixels equally. In fact, pixels located at boundaries or elongated areas are more difficult and discriminating. Paying more attention to these hard pixels can further enhance model generalization. PPA loss assigns different weights to different pixels, which extends binary cross entropy. The weight of each pixel is determined by its surrounding pixels. Hard pixels will get larger weights and easy pixels will get smaller ones.</p><p>To demonstrate the performance of F 3 Net, we report experiment results on five popular SOD datasets and visualize some saliency maps. We conduct a series of ablation studies to evaluate the effect of each module. Quantitative indicators and visual results show that F 3 Net can obtained significantly better local details and improved saliency maps. Codes has been released. In short, our main contributions can be summarized as follows:</p><p>? We introduce the cross feature module to fuse features of different levels, which is able to extract the shared parts between features and suppress each other's background noises and complement each other's missing parts.</p><p>? We propose the cascaded feedback decoder for SOD, which can feedback features of both high resolutions and high semantics to previous ones to correct and refine them for better saliency maps generation.</p><p>? We design pixel position aware loss to assign different weights to different positions. It can better mine the structure information contained in the features and help the network focus more on detail regions.</p><p>? Experimental results demonstrate that the proposed model F 3 Net achieves the state-of-the-art performance on five datasets in terms of six metrics, which proves the effectiveness and superiority of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Early SOD methods mainly rely on intrinsic cues, such as color contrast <ref type="bibr" target="#b0">(Cheng et al. 2015)</ref>, texture <ref type="bibr" target="#b26">(Yan et al. 2013)</ref> and center prior <ref type="bibr" target="#b8">(Jiang and Davis 2013)</ref> to extract saliency maps, which mainly focus on low-level information and ignore rich contextual semantic information. Recently, CNNs has been used to extract multi-level features from original images and aggregate the extracted features to produce saliency maps. Among these methods, ) introduced short connections in fully convolutional networks <ref type="bibr" target="#b14">(Long, Shelhamer, and Darrell 2015)</ref> to integrate features from different layers. <ref type="bibr" target="#b1">(Deng et al. 2018)</ref> and <ref type="bibr" target="#b20">(Wang et al. 2017b</ref>) adopted an iterative strategy to refine the saliency maps step-bystep, using features both from deep layers and shallow layers. <ref type="bibr" target="#b13">(Liu, Han, and Yang 2018)</ref> proposed to generate attention over the context regions for each pixel, which can help suppress the interference of background noises. <ref type="bibr" target="#b0">(Chen et al. 2018)</ref> and <ref type="bibr" target="#b30">(Zhang et al. 2018b</ref>) used attention-guided network to select and extract supplementary features and integrate them to enhance saliency maps. <ref type="bibr" target="#b15">(Qin et al. 2019)</ref> designed hybrid loss to make full use of boundary information and <ref type="bibr" target="#b3">(Feng, Lu, and Ding 2019)</ref> used a two-branch network to simultaneously predict the contours and saliency maps. <ref type="bibr" target="#b29">(Zhang et al. 2018a</ref>) designed a bi-directional message passing model for better feature selection and integration. <ref type="bibr" target="#b12">(Liu et al. 2019</ref>) utilized simple pooling and feature aggregation module to build fast and high performance model. <ref type="bibr" target="#b32">(Zhao and Wu 2019)</ref> introduced the channel-wise attention and spatial attention to extract valuable features and suppress background noise.</p><p>However, the discrepancy between features of different levels has not been comprehensively studied. How to design more effective fusion strategies to reduce this discrepancy has become an important problem in SOD. In addition, apart from boundaries, there are lots of hard pixels deserving more attention. Increasing their weights in loss function can further improve the discriminating ability. Based on above mentioned problems, we design F 3 Net to generate saliency maps accurately and efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>We design cross feature module to selectively integrate features, which can prevent the introduction of redundant features. To refine the saliency maps, we propose a cascaded feedback decoder to refine multi-level features by multiple iterations. To guide the network to focus more on local details, we introduce pixel position aware loss which assigns different weights to different pixels. See <ref type="figure" target="#fig_2">Fig.2</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Feature Module</head><p>We propose cross feature module (CFM) to refine both high level features f h ? R H?W ?C and low level features f l ? R H?W ?C . f l preserves rich details as well as background noises, due to the restriction of the receptive field. These features have clear boundaries, which are important to generate accurate saliency maps. In contrast, f h is coarse in boundaries, because of multiple downsamplings. Despite of losing too much detailed information, f h still has consistent semantics and clear background. There exists big statistical discrepancy between these two kinds of features. Some examples have been shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>CFM performs feature crossing to mitigate the discrepancy between features. It firstly extracts the common parts between f l and f h by element-wise multiplication and then combines them with original f l and f h respectively by element-wise addition. Compared with direct addition or concatenation employed in existing study, CFM avoids redundant information introduced to f l and f h , which may "pollute" the original features and bring adverse effect to the generation of saliency maps. By multiple feature crossings, f l and f h will gradually absorb useful information from each other to complement themselves, i.e., noises of f l will be suppressed and boundaries of f h will be sharpened.</p><p>Specifically, CFM contains two branches, one for f l and the other for f h , as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. At first, one 3x3 convolutional layer is applied to f l and f h respectively to adapt them for follow-up processing. Then these features are transformed and fused by multiplication. The fused features share the properties of both f l and f h , i.e., clear boundaries and consistent semantics. Finally, the fused features will be added to the original f l and f h for refine representations. The whole process could be shown as follows.</p><formula xml:id="formula_0">f l = f l + M l (G l (f l ) * G h (f h )) (1) f h = f h + M h (G l (f l ) * G h (f h )) (2) where each of M h (?), M l (?), G h (?), G l (?)</formula><p>is the combination of convolution, batchnorm and relu. After getting the refined features, 3x3 convolution is applied to restore the original dimensions. The whole module presents a completely symmetric structure, where f l embeds its details to f h and f h filters the background noises of f l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cascaded Feedback Decoder</head><p>Cascaded feedback decoder (CFD) is built upon CFM which refines the multi-level features and generate saliency maps iteratively. For SOD, traditional methods aim to directly aggregate multi-level features to produce the final saliency maps. In fact, features of different levels may have missing or redundant parts because of downsamplings and noises. Even with CFM, these parts are still difficult to identify and restore, which may hurt the final performance. Considering the output saliency map is relatively complete and approximate to ground truth, we propose to propagate the features of the last convolution layer back to features of previous layers to correct and refine them. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the architecture of CFD which contains multiple decoders. Each decoder consists of two processes, i.e., bottom-up and top-down. For bottom-up process, features are gradually aggregated by CFM from high level to low level. The aggregated features will be supervised and produce a coarse saliency map. For top-down process, features aggregated by last process are directly downsampled and added to previous multi-level features exported by CFM to refine them. These refined features will be sent to the next decoder to go through the same processes. In fact, inside CFD, two processes of multiple decoders are linked one-byone and form a grid net. Multi-level features are flowing and refined in this net iteratively. At last, these features will be complete enough to generate finer saliency maps.</p><p>Specifically, we build CFD on ResNet-50 <ref type="bibr" target="#b4">(He et al. 2016</ref>), a widely used backbone in SOD tasks. For an input image with size HxW , ResNet-50 will extract its features at five levels, denoted as</p><formula xml:id="formula_1">{f i |i = 1, ..., 5} with resolu- tions [ H 2 i?1 , W 2 i?1 ].</formula><p>Because low level features bring too much computational cost but little performance improvement <ref type="bibr" target="#b25">(Wu, Su, and Huang 2019)</ref>, we only use features of the last four levels f 2 , f 3 , f 4 , f 5 , which have lower resolutions and cost less computation. The whole process of CFD can be formulated as Alg. 1, where De i (?) is the i-th sub-decoder and Ds i (?) means the downsampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Cascaded Feedback Decoder</head><p>Input: multi-level features {f i |i = 2, ..., 5} iteration times N Output: saliency map {m i |i = 1, ..., N } 1 f2, f3, f4, f5, p ? De1(f2, f3, f4, f5); 2 m1 ? Conv1(p); 3 for i = 2; i ? N ; i ? i + 1 do 4 p2, p3, p4, p5 ? Ds2(p), Ds3(p), Ds4(p), Ds5(p); 5 f2, f3, f4, f5, p ? Dei(f2+p2, f3+p3, f4+p4, f5+p5); 6 mi ? Convi(p); 7 end 8 return {mi|i = 1, ..., N };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel Position Aware Loss</head><p>In SOD, binary cross entropy (BCE) is the most widely used loss function. However, BCE loss has three drawbacks. First, it calculates the loss for each pixel independently and ignores the global structure of the image. Second, in pictures where the background is dominant, loss of foreground pixels will be diluted. Third, it treats all pixels equally. In fact, pixels located on cluttered or elongated areas (e.g., pole and horn) are prone to wrong predictions and deserve more attention and pixels located areas, like sky and grass, deserve less attention. So we propose a weighted binary cross en-  </p><formula xml:id="formula_2">L s wbce = ? H i=1 W j=1 (1+??ij) 1 l=0 1(g s ij = l)logPr(p s ij = l|?) H i=1 W j=1 ??ij<label>(3)</label></formula><p>where 1(?) is the indicator function and ? is a hyperparameter. The notation l ? {0, 1} indicates two kinds of the labels. p s ij and g s ij are prediction and ground truth of the pixel at location (i, j) in an image. ? represents all the parameters of the model and Pr(p s i,j = l|?) denotes the predicted probability.</p><p>In L s wbce , each pixel will be assigned with a weight ?. Hard pixel corresponds to larger ? and simple pixel will be assigned a smaller one. ? could be regarded as the indicator of pixel importance, which is calculated according to the difference between the center pixel and its surroundings, Eq. 4. </p><p>where A ij represents the area that surrounds the pixel (i, j). For all pixels, ? s ij ? [0, 1]. If ? s ij is large, pixel at (i, j) is very different from its surroundings. So it is an important pixel (e.g., edge or hole) and deserves more attention. On the contrary, if ? s ij is small, we think it is a plain pixel and deserve less attention. <ref type="figure" target="#fig_5">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>has shown some examples.</head><p>Compared with BCE, L s wbce pays more attention hard pixels. In addition, local structure information has been encoded into L s wbce , which may help the model focus on a larger receptive field rather than on a single pixel. To further make the network focus on global structure, we introduce weighted IoU (wIoU) loss, as shown in Eq. 5.</p><formula xml:id="formula_4">L s wiou = 1 ? H i=1 W j=1 (gt s ij * p s ij ) * (1 + ?? s ij ) H i=1 W j=1 (gt s ij + p s ij ? gt s ij * p s ij ) * (1 + ?? s ij )<label>(5)</label></formula><p>IoU loss has been widely used in image segmentation (Rahman and Wang 2016). It aims to optimize the global structure instead of focusing on single pixel and it is not affected by the unbalanced distribution. Recently, it has been introduced into SOD <ref type="bibr" target="#b15">(Qin et al. 2019)</ref> to make up for the deficiency of BCE. But it still treats all pixels equally and ignores the difference between pixels. Different from IoU loss, our wIoU loss assigns more weights to hard pixels to emphasize their importance. Based on above discussion, the pixel position aware loss is shown in Eq. 6. It synthesizes local structure information to generate different weights for all pixels and introduce both pixel restriction (L s wbce ) and global restriction (L s wiou ), which can better guide the network learning and produce  Each sub-decoder in CFD corresponds to one L s ppa . Besides, multi-level supervision (MLS) is added as an auxiliary loss to facilitate sufficient training, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Given N sub-decoders in CFD and M levels in total, the whole loss is defined in Eq. 7</p><formula xml:id="formula_5">L s = 1 N N i=1 L si ppa + 5 j=2 1 2 j?1 L sj ppa (7)</formula><p>The first item corresponds to the mean of all sub-decoders' loss and the second corresponds to the weighted sum of auxiliary loss where high level loss has smaller weight because of its larger error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Evaluation Metrics</head><p>The performance of F 3 Net is evaluated on five popular datasets, including ECSSD <ref type="bibr" target="#b26">(Yan et al. 2013)</ref> with 1000 images, PASCAL-S <ref type="bibr" target="#b10">(Li et al. 2014)</ref> with 850 images, DUT-OMRON <ref type="bibr" target="#b27">(Yang et al. 2013)</ref> with 5168 images, HKU-IS <ref type="bibr" target="#b9">(Li and Yu 2015)</ref> with 4,447 images and DUTS <ref type="bibr" target="#b19">(Wang et al. 2017a</ref>) with 15,572 images. All datasets are human-labeled with pixel-wise ground-truth for quantitative evaluations. DUTS is currently the largest SOD dataset, which are divided into 10,553 training images (DUTS-TR) and 5,019 testing images (DUTS-TE). We follow <ref type="bibr" target="#b25">(Wu, Su, and Huang 2019;</ref><ref type="bibr" target="#b15">Qin et al. 2019)</ref> to use DUTS-TR as the training dataset and others as testing datasets.</p><p>In addition, six metrics are used to evaluate the performance of F 3 Net and existing state-of-the-art methods. The first metric is the mean absolute error (MAE), as shown in Eq. 8, which is widely adopted in <ref type="bibr" target="#b13">Liu, Han, and Yang 2018)</ref>. Mean F-measure (mF ), structural similarity measure (S ? , ? = 0.5) <ref type="bibr">(Fan et al. 2017</ref>) and E-measure (E ? ) <ref type="bibr" target="#b2">(Fan et al. 2018)</ref> are also widely used to evaluate salient maps. In addition, precision-recall (PR) and F-measure curves are drawn to show the whole performance.</p><formula xml:id="formula_6">M AE = 1 H ? W H i=1 W j=1 |P (i, j) ? G(i, j)| (8)</formula><p>where P is the predicted map and G is the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>DUTS-TR is used to train F 3 Net and other above mentioned datasets are used to evaluate F 3 Net. For data augmentation, we use horizontal flip, random crop and multi-scale input images. ResNet-50 <ref type="bibr" target="#b4">(He et al. 2016</ref>), pre-trained on Ima-geNet, is used as the backbone network. Maximum learning rate is set to 0.005 for ResNet-50 backbone and 0.05 for other parts. Warm-up and linear decay strategies are used to adjust the learning rate. The whole network is trained end-to-end, using stochastic gradient descent (SGD). Momentum and weight decay are set to 0.9 and 0.0005, respectively. Batchsize is set to 32 and maximum epoch is set to 32. We use Pytorch 1.3 to implement our model. An RTX 2080Ti GPU is used for acceleration. During testing, we resized each image to 352 x 352 and then feed it to F 3 Net to predict saliency maps without any post-processing. Codes has been released at https://github.com/weijun88/F3Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Before analyzing the influence of each module, there are two hyper parameters (i.e., ? and N ) to be determined. ? is used in PPA loss to adjust the proportion of hard pixels. Tab.3 lists the scores of M AE, mF , S ? and E ? when ? is given different values. As can be seen, when ? equals 5, these indicators reach highest scores. In addition, N represents the number of sub-decoders in CFD. We increase N gradually from 1 to 4 and measure the corresponding scores of above metrics, as shown in Tab. 4. When N =2, the model achieves the best performance. Both of these experiments are conducted on DUT-OMRON and DUTS. To investigate the importance of different modules in F 3 Net, we conduct a series of controlled experiments on DUTS, as shown in Tab.2. First, we test the effect of different loss functions, inlcuding BCE, IoU and PPA. Among them, PPA loss achieves the best performance on three evaluation metrics. Furthermore, we keep adding the multi-level supervision, cross feature module and cascaded feedback decoder to evaluate their performance. As we can see, all these modules boost the model performance. When these modules are combined, we can get the best SOD results. It demonstrates that all components are necessary for the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-arts</head><p>Quantitative Comparison. To demonstrate the effectiveness of the proposed F 3 Net, we compare it against 12 stateof-the-art SOD algorithms, including AFNet <ref type="bibr" target="#b3">(Feng, Lu, and Ding 2019)</ref>   <ref type="bibr">059 .853 .882 .906 .086 .761 .822 .835 .066 .710 .817 .841 .051 .839 .873 .919 .079 .664 .780 .817 RAS(ECCV2018)</ref> . <ref type="bibr">055 .890 .894 .916 .102 .782 .792 .832 .060 .750 .838 .861 .045 .874 .888 .931 .063 .711 .812 .843 R 3 Net(IJCAI2018)</ref> . <ref type="bibr">051 .883 .910 .914 .101 .775 .809 .824 .067 .716 .837 .827 .047 .853 .894 .921 .073 .690 .819 .814 PiCA-R(CVPR2018)</ref> . <ref type="bibr">046 .886 .917 .913 .075 .798 .849 .833 .051 .759 .869 .862 .043 .870 .904 .936 .065 .717 .832 .841 BMPM(CVPR2018)</ref> . <ref type="bibr">044 .894 .911 .914 .073 .803 .840 .838 .049 .762 .861 .859 .039 .875 .906 .937 .063 .698 .809 .839 DGRL(CVPR2018)</ref> . <ref type="bibr">043 .903 .906 .917 .074 .807 .834 .836 .051 .764 .846 .863 .037 .881 .896 .941 .063 .709 .810 .843 PAGE(CVPR2019) .042 .906 .912 .920 .077 .810 .835 .841 .052 .777 .854 .869 .037 .882 .903 .940 .062 .736 .824 .853 AFNet(CVPR2019) .042 .908 .913 .918 .070 .821 .844 .846 .046 .792 .867 .879 .036 .888 .905 .942 .057 .738 .826 .853 TDBU(CVPR2019)</ref> . <ref type="bibr">041 .880 .918 .922 .071 .779 .844 .852 .048 .767 .865 .879 .038 .878 .907 .942 .061 .739 .837 .854 PoolNet(CVPR2019)</ref> . <ref type="bibr">039 .915 .921 .924 .074 .822 .845 .850 .040 .809 .883 .889 .032 .899 .916 .949 .055 .747 .835 .863 BASNet(CVPR2019) .037 .880 .916 .921 .076 .775 .832 .847 .048 .791 .866 .884 .032 .895 .909 .946 .056 .756 .836 .869 CPD-R(CVPR2019) .037 .917 .918 .925 .072 .824 .842 .849 .043 .805 .869 .886 .034 .891 .905 .944 .056 .747 .825 .866</ref> F 3 Net(ours   . For fair comparison, we use all saliency maps provided by the authors and evaluate them with the same code. As shown in Tab.1, our approach achieves the best scores across five datasets with respect to four metrics, compared with other counterparts. It demonstrates the superior performance of the proposed F 3 Net. In addition, <ref type="figure" target="#fig_7">Fig. 4</ref> shows the precision-recall curves of above mentioned algorithms on five datasets, which can evaluate the holistic performance of models. From these curves, we can observe that F 3 Net consistently outperforms all other models under different thresholds, which means that our method have a good capability to detect salient regions as well as generate accurate saliency maps.</p><p>Visual Comparison. In order to evaluate the proposed F 3 Net, we visualize some saliency maps produced by our model and other approaches in <ref type="figure">Fig. 5</ref>. We observe that the proposed method not only highlights the salient object regions clearly, but also well suppresses the background noises. It excels in dealing with various challenging scenarios, including cluttered backgrounds (row 2 and 6), small objects (row3), inverted reflection in water (row1) and occlusion (row 2). Compared with other counterparts, the saliency maps produced by our method are clearer and more accurate. Most importantly, our method achieves these results without any post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>Ours PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R <ref type="figure">Figure 5</ref>: Visual comparison of the proposed model with nine state-of-the-art methods. Apparently, saliency maps produced by our model are clearer and more accurate than others and our results are more consistent with the ground truths.</p><p>BCE IoU PPA MLS CFM CFD DUTS-TE MAE mF S ? E ? .051 . <ref type="bibr">779 .861 .871 .047 .783 .864 .874 .045 .789 .867 .875 .043 .808 .872 .880 .040 .812 .875 .882 .036 .831 .884 .893 .035 .840 .888 .902</ref> Table 2: Ablation study for different modules. BCE and IoU are two kinds of loss functions above mentioned. MSL means multi-level supervision. CFM and CFD are the main modules in F 3 Net. PPA is the proposed loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel SOD framework named F 3 Net. First, considering the difference between features of different levels, we propose CFM to selectively integrate features, which prevents the improper influence of redundant features. To further get finer details, we introduce CFD to refine multi-level features iteratively with feedback mechanisms. Besides, we design PPA loss to pay more attention to hard pixels and guide the network focus more on error-prone parts. The whole framework demonstrates remarkable feature extraction capability, which makes it robust and effective in various challenging scenarios. Experimental results on five datasets demonstrate that F 3 Net outperforms state-   <ref type="table">Table 4</ref>: The effect of sub-decoder number. When N = 2, the model achieves the best results. of-the-art methods under six evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Fusing features of different levels. (c) represents the low level features. (d) means the high level features. (e) is the fused features by F 3 Net. Clearly, the fused features have clear boundaries as well as few background noises.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of proposed F 3 Net. ResNet-50 is used as the backbone encoder. Cross feature module (CFM) is used as the basic module to fuse features of different layers. Cascaded feedback decoder (CFD) contains multiple sub-decoders to feedback and refine multi-level features. Multi-level supervision (MLS) helps to ease the optimization of F 3 Net. tropy (wBCE) loss as shown in Eq. 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Some examples of the calculated weight ?. (d) shows the superposition of original image and its corresponding ?. We can see that pixels located at boundaries, elongated areas or holes, have larger ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, BASNet (Qin et al. 2019), CPD-R (Wu, Su, and Huang 2019), BMPM (Zhang et al. 2018a), R 3 Net (Deng et</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Performance comparison with 12 state-of-the-art methods over 5 datasets. The first row shows comparison of precision-recall curves. The second row shows comparison of F-measure curves over different thresholds. As the figure shows, F 3 Net achieves the best performance on all datasets. al. 2018), PiCA-R (Liu, Han, and Yang 2018), DGRL (Wang et al. 2018), TDBU (Wang et al. 2019b), PoolNet (Liu et al. 2019), PAGE (Wang et al. 2019c), RAS (Chen et al. 2018) and C2SNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with 12 state-of-the-art methods over 5 datasets. MAE (smaller is better), mean Fmeasure (mF , larger is better), Smeasure (S ? , larger is better) and Emeasure (E ? , larger is better) are used to measure the model performance. The best results are highlighted in bold. Our model ranks first on all datasets and metrics. MAE mF S? E ? MAE mF S? E ? MAE mF S? E ? MAE mF S? E ? C2SNet(ECCV2018) .</figDesc><table><row><cell></cell><cell>ECSSD</cell><cell>PASCAL-S</cell><cell>DUTS-TE</cell><cell>HKU-IS</cell><cell>DUT-OMRON</cell></row><row><cell>Algorithm</cell><cell>1,000 images</cell><cell>850 images</cell><cell>5,019 images</cell><cell>4,447 images</cell><cell>5,168 images</cell></row><row><cell></cell><cell>MAE mF S? E ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) .033 .925 .924 .927 .062 .840 .855 .859 .035 .840 .888 .902 .028 .910 .917 .953 .053 .766 .838 .870    </figDesc><table><row><cell></cell><cell>1.000</cell><cell></cell><cell></cell><cell>ECSSD</cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell>PASCAL-S</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>DUTS</cell><cell></cell><cell></cell><cell>1.000</cell><cell></cell><cell></cell><cell>HKU-IS</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell>DUT-OMRON</cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell cols="2">0.0 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975</cell><cell>0.2 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>0.4 Recall 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell cols="2">0.0 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90</cell><cell>0.2 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>0.4 Recall 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell cols="2">0.0 0.5 0.6 0.7 0.8 0.9</cell><cell>0.2 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>0.4 Recall 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell cols="2">0.0 0.800 0.825 0.850 0.875 0.900 0.925 0.950 0.975</cell><cell>0.2 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>0.4 Recall 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell cols="2">0.0 0.3 0.4 0.5 0.6 0.7 0.8</cell><cell>0.2 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>0.4 Recall 0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell>0.950</cell><cell></cell><cell></cell><cell>ECSSD</cell><cell></cell><cell></cell><cell>0.875</cell><cell></cell><cell></cell><cell>PASCAL-S</cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell>DUTS</cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell>HKU-IS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DUT-OMRON</cell><cell></cell><cell></cell></row><row><cell>F-measure</cell><cell>0.750 0.775 0.800 0.825 0.850 0.875 0.900 0.925</cell><cell>0</cell><cell>50 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>100 Threshod 150</cell><cell>200</cell><cell>250</cell><cell>0.700 0.725 0.750 0.775 0.800 0.825 0.850</cell><cell>0</cell><cell>50 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>100 Threshod 150</cell><cell>200</cell><cell>250</cell><cell>0.60 0.65 0.70 0.75 0.80 0.85</cell><cell>0</cell><cell>50 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>100 Threshod 150</cell><cell>200</cell><cell>250</cell><cell>0.70 0.75 0.80 0.85 0.90</cell><cell>0</cell><cell>50 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>100 Threshod 150</cell><cell>200</cell><cell>250</cell><cell>0.60 0.65 0.70 0.75 0.80</cell><cell>0</cell><cell>50 C2SNet RAS R3Net PiCA-R BMPM DGRL PAGE AFNet TDBU PoolNet BASNet CPD-R Ours</cell><cell>100 Threshod 150</cell><cell>200</cell><cell>250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>DUT-OMRON DUTS-TE MAE mF S ? E ? MAE mF S ? E ??=3.058 .755 .835 .857 .038 .835 .888 .898 ?=4 .057 .758 .837 .859 .037 .837 .888 .900 ?=5 .053 .766 .838 .870 .035 .840 .888 .902 ?=6 .060 .752 .833 .855 .038 .834 .887 .897</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with different ?. When ? = 5, the model achieves the best results.DUT-OMRON DUTS-TE MAE mF S ? E ? MAE mF S ? E ? N =1 .055 .760 .834 .866 .037 .838 .886 .897  N =2 .053 .766 .838 .870 .035 .840 .888 .902  N =3 .057 .762 .837 .867 .036 .837 .887 .900  N =4 .059 .758 .833 .863 .038 .835 .885 .896    </figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by National Natural Science Foundation of China: 61672497, 61620106009, 61931008, U1636214 and 61836002, and in part by Key Research Program of Frontier Sciences, CAS: QYZDJ-SSW-SYS013.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
	<note>TPAMI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R 3 net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI, 684-690. ijcai.org. Fan, D.; Cheng, M</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Submodular salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contour knowledge transfer for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (15)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing intersection-over-union in deep neural networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISVC (1)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10072</biblScope>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5968" to="5977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11214</biblScope>
			<biblScope unit="page" from="273" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
