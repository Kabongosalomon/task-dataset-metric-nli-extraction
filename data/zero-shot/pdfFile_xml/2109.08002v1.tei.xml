<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAFRAN: An interpretable, rule-based link prediction method outperforming embedding models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ott</surname></persName>
							<email>simon.ott@meduniwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Artificial Intelligence and Decision Support</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data and Web Science Research Group, University Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Samwald</surname></persName>
							<email>matthias.samwald@meduniwien.ac.at</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence and Decision Support</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAFRAN: An interpretable, rule-based link prediction method outperforming embedding models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural embedding-based machine learning models have shown promise for predicting novel links in knowledge graphs. Unfortunately, their practical utility is diminished by their lack of interpretability. Recently, the fully interpretable, rule-based algorithm AnyBURL yielded highly competitive results on many general-purpose link prediction benchmarks. However, current approaches for aggregating predictions made by multiple rules are affected by redundancies. We improve upon AnyBURL by introducing the SAFRAN rule application framework, which uses a novel aggregation approach called Non-redundant Noisy-OR that detects and clusters redundant rules prior to aggregation. SAFRAN yields new state-of-the-art results for fully interpretable link prediction on the established generalpurpose benchmarks FB15K-237, WN18RR and YAGO3-10. Furthermore, it exceeds the results of multiple established embedding-based algorithms on FB15K-237 and WN18RR and narrows the gap between rule-based and embedding-based algorithms on YAGO3-10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Link prediction in knowledge graphs is a vibrant area of research and development and encompasses a wide range of different approaches. Currently, most state-of-the-art approaches use low-dimensional representations (embeddings) of knowledge graphs to predict new knowledge. However, as such approaches are black boxes, the validity of their predictions can be questioned due to lack of interpretability. Recent research has focused on learning symbolic rules from knowledge graphs that try to detect correlations of entities in knowledge bases and express them as first-order Horn rules. In contrast to neural models, such rule-based methods are fully transparent and predictions can be explained intuitively.</p><p>However, while current research on rule-based approaches puts a lot of emphasis on designing rule-mining algorithms, research on methods for applying learned rules and aggregating their predictions in a meaningful way is comparatively neglected.</p><p>AnyBURL <ref type="bibr" target="#b1">[Meilicke et al., 2019</ref><ref type="bibr" target="#b2">[Meilicke et al., , 2020</ref> is currently one of the best performing symbolic rule learners and demonstrated results competitive with embedding methods on link prediction benchmarks such as FB15k-237. However, the application of rules generated by Any-BURL is currently limited by functional redundancies, making it difficult to meaningfully aggregate predictions made by different rules and thereby limiting predictive performance. arXiv:2109.08002v1 [cs.AI] 16 Sep 2021</p><p>In this paper, we introduce the high-performance rule application framework SAFRAN 1 ('Scalable And fast non-redundant rule application') that employs a novel aggregation approach. We propose an algorithm called Non-redundant Noisy-OR that detects redundant rules prior to aggregation, mitigating their negative effects and improving predictive performance. This algorithm for aggregating predictions is not necessarily restricted to rules learned by AnyBURL and could also be applied to other rule-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Link prediction</head><p>Knowledge graphs contain facts, such as "Amsterdam is the capital of the Netherlands" or "Marie Curie was born in Warsaw", which are represented as semantic triples containing a subject (head h), predicate (relation r) and object (tail t). Formally, a knowledge graph is a directed heterogeneous multigraph G = (E, R, T ) where E and R are the set of entities and relations respectively and T is a set of triples {(h, r, t)} ? E ? R ? E. The task of link prediction is to predict either the tail of a given head and relation <ref type="bibr">(h, r, ?)</ref> or the head of a given tail and relation <ref type="bibr">(?, r, t)</ref> such that the resulting triple describes a correct fact which has not been in G before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AnyBURL</head><p>We now give an overview about the main AnyBURL algorithm, which is explained in detail in <ref type="bibr" target="#b1">[Meilicke et al., 2019]</ref> and <ref type="bibr" target="#b2">[Meilicke et al., 2020]</ref>. AnyBURL (Anytime Bottom Up Rule Learning) is a novel walk-based method and is currently one of the best symbolic rule learners, competing with current state-of-the-art embedding approaches <ref type="bibr" target="#b5">[Rossi et al., 2021]</ref>. <ref type="bibr">Walk-based (bottom-up)</ref> methods are based on the idea that sampled paths from a knowledge graph (random walks) are examples of very specific rules, which can be transformed into more general rules. Learned rules can then be applied to the knowledge graph for the prediction of novel links between entities. AnyBURL learns rules in the form of first-order logic Horn rules of length n. In each iteration of the algorithm for mining rules the algorithm samples a random triple h(c 0 , c 1 ), f.e. lives(max, uk) from the training set. Starting from either the head or the tail of this triple, AnyBURL performs a random walk of length n, resulting in a ground path in the form of h(c 0 , c 1 ) ? b 1 (c 1 , c 2 ), . . . , b n (c n , c n+1 ), f.e. speaks(max, english) ? lives <ref type="bibr">(max, uk), lang(uk, english)</ref>. The sampled ground path is subsequently generalized into three predefined types of rules shown in <ref type="table" target="#tab_0">Table 1</ref>. Cyclic rules can be generalized from cyclic paths (c 0 = c n+1 ), while AC2 rules can be generalized from acyclic paths (c 0 = c n+1 ). AC1 rules are hybrid, as they can be both generalized from cyclic (if c 0 = c n+1 ) and acyclic (if c 0 = c n+1 ) paths. All rules that can be generalized from the previous example ground path can be seen in <ref type="table" target="#tab_1">Table 2</ref>. The following applies for inferring a set of head triples? r from the body of a rule r:</p><formula xml:id="formula_0">Cyclic (C) h(Y, X) ? b 1 (X, A 2 ), . . . , b n (A n , Y ) Acyclic 1 (AC1) h(c 0 , X) ? b 1 (X, A 2 ), . . . , b n (A n , c n+1 ) Acyclic 2 (AC2) h(c 0 , X) ? b 1 (X, A 2 ), . . . , b n (A n , A n+1 )</formula><formula xml:id="formula_1">speaks(Y, X) ? lives(X, A), lang(A, Y ) speaks(english, X) ? lives(X, A), lang(A, english) speaks(Y, max) ? lives(max, A), lang(A, Y )</formula><formula xml:id="formula_2">H r = ? ? ? ? ? {h(e y , e x )|e x , e y ? E ? ?e 2 , ..., e n b 1 (e x , e 2 ), . . . , b n (e n , e y ) ? T }, if type = C {h(c 0 , e x )|e x ? E ? ?e 2 , ..., e n b 1 (e x , e 2 ), . . . , b n (e n , c n+1 ) ? T }, if type = AC1 {h(c 0 , e x )|e x ? E ? ?e 2 , ..., e n+1 b 1 (e x , e 2 ), . . . , b n (e n , e n+1 ) ? T }, if type = AC2</formula><p>AnyBURL calculates the confidence of a rule as the size of the union of inferred triples by a rule and the triples of the training set divided by the number of inferred triples conf (r) = |? r ? T |/|? r |, where T is the set of triples contained in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aggregation</head><p>Rule-based link prediction methods, such as AnyBURL, produce many rules with different confidences that are subsequently applied to the knowledge graph in order to generate candidates for a link prediction task. Upon a link prediction task p(s, ?), rule r generates the set of predictions {y|p(s, y) ?? r } that could act as substitutions for the question mark. Rules can either predict zero, one or multiple entities. As the same entities can be proposed by multiple rules which may differ in confidence, an aggregation of these confidences is required to assess the final confidence of a prediction. <ref type="table">Table 3</ref> shows the confidences and results of five fictional rules. <ref type="table">Table 3</ref>: Example of different rules generating either zero, one or multiple predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule Results Confidence</head><formula xml:id="formula_3">R 0 {a, b} 0.9 R 1 {c} 0.8 R 2 {c} 0.7 R 3 {b} 0.3 R 5 {a} 0.1 R 5 ? 0.1</formula><p>One aggregation approach called Maximum aggregation is using the maximum of these confidences <ref type="bibr" target="#b1">[Meilicke et al., 2019]</ref>. The score of an entity score(e) = max{conf (r 1 ), . . . , conf(r k )} is the maximum confidence of all rules r 1 , . . . , r k that predict entity e. If the maximum confidences of two or more entities are the same, these entities are further ranked by their second best confidence and so on, until all top-k candidates can be distinguished or all rules are processed. This approach results in the following list of ranked candidates when applied to the solutions of <ref type="table">Table 3</ref>: ranking max = (b, 0.9), (a, 0.9), (c, 0.8) . As the second best rule that proposes b has a higher confidence than the second best rule which proposes a, b is ranked before a.</p><p>Unfortunately, the Maximum aggregation is constrained to relatively simple predictions: Each prediction is primarily informed by only a single rule with the highest confidence; it is not possible to make predictions based on a weighted combination of different rules. These limitations of maximum aggregation are potentially addressed by noisy-or aggregation, first proposed in this context by <ref type="bibr">[Gal?rraga et al., 2015]</ref>. Instead of taking the maximum of confidences, the probability that at least one of the rules proposed the correct candidate is used for generating a list of ranked candidates. The score of an entity e can thus be calculated as</p><formula xml:id="formula_4">score(e) = 1 ? k i=1 (1 ? conf (r i ))</formula><p>where r 1 , . . . , r k are rules that predict entity e. Applying the Noisy-OR approach to the results of <ref type="table">Table 3</ref>, the entities are ranked as follows: ranking noisy = (c, 0.94), (b, 0.93), (a, 0.9) . This ranking differs from the maximum approach. As c is proposed by the second and third most confident rules, its aggregated confidence is higher than a or b, entities proposed by the most confident rule.</p><p>In theory, Noisy-OR allows for more sophisticated confidence aggregation. However, it assumes independence/non-redundancy of all rules. As most rules that can be derived from real-world knowledge bases are in some way redundant with other rules, Noisy-OR was found to perform worse than maximum aggregation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The detrimental effect of redundancy on Noisy-OR aggregation</head><p>Redundancies in rules can lead to overestimation of confidences of predicted entities when aggregating using Noisy-OR. As two redundant rules generate the same predictions for the same reasons, the confidences of entities predicted by both rules are overestimated due to double counting. Consider the rules shown in <ref type="table">Table 4</ref> where this problem becomes apparent. If the training set contains the ground path lives(john, uk), lang(uk, english), each rule would predict speaks(john, english) upon the prediction task speaks(john, ?) for the same reasons. Aggregating the confidences of all three rules using Noisy-OR would result in an overestimated confidence of 0.988 for the entity english.</p><p>While these redundancies are apparent, redundancies between rules are often less obvious in practice. Redundancy between two rules can arise from special relationships between relations, such as relations that are nearly equivalent, symmetric relations or relations entailing other relations. Consider the rules shown in <ref type="table">Table 5</ref>. The first example shows two</p><formula xml:id="formula_5">speaks(X, Y ) ? lives(X, A), lang(A, Y ) 0.9 speaks(john, Y ) ? lives(john, A), lang(A, Y ) 0.7</formula><p>speaks(X, english) ? lives(X, A), lang(A, english) 0.6 <ref type="table">Table 4</ref>: Examples of obvious redundancies between rules.</p><p>rules being redundant based on entailment, as every capital of a country is a city within that country. The second example shows redundancy based on the symmetric relationship married. Both rules generate predictions for languages spoken by a person, based on what language its spouse speaks. The third example shows that such relationships are not restricted to single relations, but can arise from a combination of relations, as the combination parent ? brother is a duplicate of the relation uncle. <ref type="table">Table 5</ref>: Examples of less apparent redundancies between rules.</p><formula xml:id="formula_6">citizen(X, Y ) ? born(X, A), city(A, Y ) 0.7 citizen(X, Y ) ? born(X, A), capital(A, Y ) 0.7 speaks(X, Y ) ? married(X, A), speaks(A, Y ) 0.6 speaks(X, Y ) ? married(A, X), speaks(A, Y ) 0.6 lives(X, Y ) ? parent(X, A), brother(A, B), lives(B, Y ) 0.6 lives(X, Y ) ? uncle(X, A), lives(A, Y ) 0.6</formula><p>It is important to understand that there is usually no explicit information available about the symmetry of married nor do we know that capital is more specific than city. It is not even possible to grasp this information from the dataset itself as it might be incomplete and noisy. This makes it impossible to decide redundancy via a procedure that makes use of relevant background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm</head><p>To overcome the problem of redundancy when using the Noisy-OR aggregation method, we propose an approach to cluster rules based on their redundancy degree prior to aggregation. Predictions of rules in a cluster are aggregated using the Maximum approach, as this approach is not susceptible to redundancies. Predictions of the different clusters are then further aggregated using the Noisy-OR approach. We call the resulting aggregation technique Non-redundant Noisy-OR. A pseudocode of Non-redundant Noisy-OR can be seen in Appendix G.</p><p>As a metric for redundancy between two rules r i , r j the Jaccard Index sim(r i , r j ) = |? r i ?? r j |/|? r i ?? r j | of the sets of inferred triples is used. As the calculation of the Jaccard coefficient is very inefficient for large sets, the Jaccard coefficient is estimated using the MinHash scheme <ref type="bibr">[Broder, 1997]</ref>, which makes time complexity linear and memory usage constant.</p><p>Naturally rules are pre-clustered by their relation of the head atom, as two rules having a different relation in the head are never used in the same prediction task and the union of their inferred triples is always empty h r i = h r j ?? r i ?? r j = ?. Thus, similarity matrices of rules are calculated for each head relation and used to cluster the rules.</p><p>Two rules r i , r j are considered redundant and assigned to the same cluster if sim(r i , r j ) &gt; t, where t is a threshold. However a global threshold t is not capable of defining the cutoff point for redundancy for all relations, rule types and directions of prediction (prediction of head entities or tail entities). As the sets of rules that predict for different relations may vary in distribution of type, length or specificity, the threshold that generates the optimal clustering for a relation may not generate the optimal clustering for a different relation. Furthermore as relations may vary in multiplicity the direction of the prediction has to taken into account. Therefore, a distinct threshold for the prediction of heads and prediction of tails for each relation is required in order to be able to optimally cluster an entire rule set. Furthermore, within a set of rules that predict for the same relation, optimal thresholds may vary between types of rules. As can be seen by the examples in <ref type="table">Table 4</ref>, the type combination of C and AC1 often tends to form set-subset relation. The Jaccard Index is very insensitve to such relationships. While e.g. a Jaccard index of 0.25 between a rule of type C and a rule of type AC1 may indicate a high redundancy due to the dominance of set-subset relationships within rules of this type combination, it may be considered as a fairly low evidence level for a redundancy between two rules of type C, where such relationships are not as prominent. Thus a differentiation of rule types is needed when defining the cutoff value for redundancy between two rules of a certain relation. For each combination of rule types an independent threshold parameter is used to determine redundancy. Conclusively, the threshold of similarity for rules that predict entities of direction d (head or tail) of a h-triple (we use h to refer to a relation, while r is used to refer to rules) is given by t hd = [t C/C , t C/AC1 , t C/AC2 , t AC1/AC2 , t AC1/AC1 , t AC2/AC2 ]. Note that if t hd = [0, 0, 0, 0, 0, 0] only the Maximum aggregation is used and if t hd = [1, 1, 1, 1, 1, 1] only Noisy-OR.</p><p>An optimal clustering of rules that predict h-triples is given by the type combination thresholds t hd that maximize the fitness of the clusters for predicting new links. This fitness is evaluated on the validation set using the mean-reciprocal rank based on the topk predicted entities. SAFRAN uses one of two search strategies for finding the optimal thresholds: grid search (parameter sweep) and random search. For grid search the range of possible thresholds [0.0, 1.0] is divided by n equally distant steps and each threshold is subsequently used for clustering. For the grid search, we do not distinguish between rule-type specific parameters but use a single relation-specific fix parameter t hd to limit the search space. Contrary to this, the random search randomly samples the six rule-typespecific thresholds of t hd . <ref type="figure">Figure 1</ref> demonstrates the effect of our algorithm. It has two parts that both illustrate the list of predicted entities for the prediction task genre artist(?, Bryan Adams). The upper half of the figure shows the result of applying the Non-redundant Noisy-OR approach, while the lower part presents the results of the maximum aggregation. In addition to the candidate lists some explanations in terms of the rules or rule clusters that predict these entities are SAFRAN <ref type="figure">Figure 1</ref>: Comparing the list of predicted entities for the prediction task genre artist(?, Bryan Adams) for Non-redundant Noisy-OR and maximum approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Example and its Explanation</head><p>shown. For the Non-redundant Noisy-OR we depict the two clusters that have the highest maximum confidence. By aggregating these clusters via the Noisy-OR multiplication, the highest rank candidate is Pop rock. A different result can be observed for the maximum aggregation. The correct entity Pop rock is ranked at the third position, while the first position is occupied by Heavy Metal. The example illustrates nicely that the influence of a single rule can be too strong in the maximum approach, while the Non-redundant Noisy-OR approach allows to aggregate the results in a more meaningful way. This holds in particular if the clusters make sense and combine similar reasons for a prediction within the same cluster. Therefore, in addition to improving predictive performance, our algorithm provides explanations that are easier to understand and that provide deeper insights about the underlying data and the rules derived from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted experiments with three established link prediction datasets, including FB15k-237 <ref type="bibr" target="#b9">[Toutanova and Chen, 2015]</ref>, <ref type="bibr">WN18RR [Dettmers et al., 2018]</ref> and <ref type="bibr">YAGO3-10 [Dettmers et al., 2018</ref>]. An overview of these datasets can be seen in <ref type="table" target="#tab_2">Table 6</ref>. For our experiments we selected only datasets without train-test leakage. We abstained from performing experi- <ref type="table" target="#tab_0">FB15k-237  14,541  237  272,115  17,535  20,466  WN18RR  40,943  11  86,845  3,034  3,134  YAGO3-10  123,182  37</ref> 1,079,040 5,000 5,000 Training, inferencing and evaluations were run on a machine with 24 physical (48 logical) Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz cores and 264 GB of RAM. As SAFRAN and AnyBURL do not utilize GPU-accelerated computing, only CPUs were used. For optimal comparison with the best results using the maximum aggregation approach presented in <ref type="bibr" target="#b2">[Meilicke et al., 2020]</ref>, the same learning parameters were adopted for the same datasets. Rulesets used for the results were learned for 1000 seconds each, using 22 threads. For rules learned from WN18RR the maximum length of cyclic rules was set to 5, while for all other datasets a maximum length of 3 was used. The maximum length of acyclic rules was set to 1 for all datasets. As FB15k-237 contains reflexive triples in the form of r(c, c) a flag was set that allows AnyBURL to use such reflexive triples for learning rules. The same rulesets were used for inference and evaluation with AnyBURL and SAFRAN. Random search was performed using a n of 10 and 10000 iterations, while grid search was performed with a n of 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Entities # Relations # Training # Test # Validation</head><p>We compare our results with the established latent models RESCAL <ref type="bibr" target="#b3">[Nickel et al., 2011]</ref>, TransE <ref type="bibr">[Bordes et al., 2013]</ref>, DistMult <ref type="bibr" target="#b12">[Yang et al., 2015]</ref>, ComplEx <ref type="bibr">[Trouillon et al., 2016]</ref>, <ref type="bibr">ConvE [Dettmers et al., 2018]</ref>, RotatE <ref type="bibr" target="#b8">[Sun et al., 2019]</ref>, TuckER <ref type="bibr">[Balazevic et al., 2019]</ref> and the rule-based methods AMIE+ <ref type="bibr">[Gal?rraga et al., 2015]</ref>, RuleN <ref type="bibr">[Meilicke et al., 2018]</ref>, C-NN <ref type="bibr">[Ferr?, 2020]</ref>, Neural LP <ref type="bibr" target="#b13">[Yang et al., 2017]</ref>, <ref type="bibr">RLvLR [Omran et al., 2018]</ref>, DRUM <ref type="bibr" target="#b7">[Sadeghian et al., 2019]</ref>, <ref type="bibr">GPFL [Gu et al., 2020b]</ref>. For details on these approaches we refer to their respective original papers. For each approach we report the filtered mean reciprocal rank (MRR), filtered hits@1 and filtered hits@10, where predicted triples that are already known are filtered out, except the test triple itself. The results for FB15k-237, WN18RR and YAGO3-10 can be seen in <ref type="table" target="#tab_4">Table 7</ref>. All results reported by us were evaluated according to the average policy <ref type="bibr" target="#b5">[Rossi et al., 2021]</ref> (see Appendix A), where the rank of a target entity within a group of same score entities is the average rank of this group. Reported results from <ref type="bibr" target="#b5">[Rossi et al., 2021]</ref>, <ref type="bibr" target="#b6">[Ruffinelli et al., 2020]</ref> were evaluated using the same protocol. Results of C-NN, GPFL, AMIE+, RuleN were evaluated using ordinal (random) policy which results in very similar results than average policy, while to the best of our knowledge there is no information available about the policy used for evaluating RLvLR. Results of DRUM and Neural LP were evaluated using the top policy which is biased towards producing better evaluations results and cannot be compared directly to other results.</p><p>Our aggregation method in the more exhaustive random search outperforms both symbolic and subsymbolic approaches on FB15k-237 and WN18RR. This is a surprising result as both data sets have been used for years to evaluate and improve performance of latent methods that are build on the concept of embeddings. On YAGO3-10 the results are slightly worse. For this data set ComplEx achieves an MRR that is 0.012 higher. However, AnyBURL together with the Non-redundant Noisy-OR is still among the top performing techniques. With our approach we offer a fully interpretable symbolic approach, that allows to explain the results in terms of the rules that generated them, while we are still able to compete with or even outperform latent approaches. The improvement against the AnyBURL default maximum aggregation technique differs a lot between datasets. While we observe a clear improvement of more than 3% in terms of MRR for FB15k-237, the improvement for the other datasets is around 0.5% to 1%. Currently we do not understand the reason for this differences. The most probable explanation is a lower degree of redundancy in the rules that describe the regularities encoded in these datasets.</p><p>Another reason for this may be the small validation set sizes of WN18RR and YAGO3-10. While the training set of YAGO3-10 is the biggest of the three datasets used in the experiments, the size of its validation set is only 0.46% of the dataset, compared to 6.6% in FB15k-237. However, non-redundant Noisy-OR needs an appropriate sized validation set that represents a broad spectrum of relations and entities to be able to fully recognize redundancies. The validation set size of WN18RR is 3.4% of the dataset. However as the overall dataset is considerably small, the absolute size of its validation set is only 3,134 triples. As rules generated by AnyBURL can generalize to unseen entities <ref type="bibr">[Ferr?, 2020]</ref>, this size may not be sufficient to generate the most optimal clustering.   <ref type="bibr" target="#b6">[Ruffinelli et al., 2020]</ref>, ? from <ref type="bibr" target="#b5">[Rossi et al., 2021]</ref>, from <ref type="bibr">[Ferr?, 2020]</ref>, ? from <ref type="bibr" target="#b7">[Sadeghian et al., 2019]</ref>, ? from <ref type="bibr">[Gu et al., 2020b]</ref>, ? from <ref type="bibr" target="#b1">[Meilicke et al., 2019]</ref>, from <ref type="bibr">[Omran et al., 2018]</ref> and from <ref type="bibr" target="#b16">[Zhang et al., 2020]</ref>.</p><p>In our experiments we used two different search techniques. The random search performs better in most cases. As explained above, it learns thresholds that distinguish between different rule types. Thus, only a random selection of all possible threshold combinations is visited during the search. Nevertheless, type-specific thresholds seem to make sense as the results are in most cases slightly better compared to the grid search, which learns a single threshold per relation.</p><p>To support our claim that Non-redundant Noisy-OR could potentially be applied to other rule-based approaches, we performed experiments on all used data sets using rules learned by AMIE+. The results can be seen in Appendix F. We used AMIE in its default setting. This means that it mines only cyclic rules. This could be the reason that we observed no or only a very limited improvement on each data sets with the exception of FB15k-237.</p><p>To ascertain the importance of optimal thresholds, we performed another experiment. We evaluated each relation in the validation set using Noisy-OR and Maximum aggregation and apply the approach with the maximum MRR of each relation to the test set. Results for the datasets can be seen in <ref type="table" target="#tab_4">Table 7</ref> denoted as VS. All metrics improved compared to the Maximum aggregation but could not achieve the gain of Non-redundant Noisy-OR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>One of the most prominent rule mining systems is AMIE, which has been described first in <ref type="bibr">[Gal?rraga et al., 2015]</ref> and later, including several extension and improvements in <ref type="bibr">[Lajus et al., 2020]</ref>. These papers focus mainly on mining rules (including the confidence computation) rather than their application. This is typical for most rule-mining or inductive logic programming (ILP) papers. The authors touch the topic of how to aggregate confidences only in a single experiment where they compare maximum and the Noisy-OR aggregation. According to their results the Noisy-OR approach works better.</p><p>In <ref type="bibr">[Ferr?, 2020]</ref> another interesting variation of Noisy-OR, the Dempster-Shafer score <ref type="bibr">[Denoeux, 1995]</ref>, has been applied. This approach is not based on rules, but relies on another symbolic representation using concepts of nearest neighbours. In their experiments the authors found that the aggregation using the Dempster-Shafer score performs at best 1.3 % worse than the maximum score <ref type="bibr">[Ferr?, 2020]</ref>.</p><p>It might make sense to decouple the learning of rules from the model used for their application. ProbLogic <ref type="bibr">[De Raedt et al., 2007]</ref> offers a well-defined model for applying probabilistic rules to a given set of facts (or triples). ProbLog evaluates probabilistic logic programs by combining Selective Linear Definite clause resolution (SLD-resolution) with methods for computing the probability of Boolean formulae. Using ProbLog for applying the rules learned by a rule miner would be similar to the Noisy-OR aggregation we described above. As ProbLog is based on a model-theoretic notion of entailment, predictions created by a rule might enable other rules to fire, which might again result into further predictions (and so on). While this makes sense from a conceptual point of view, such an approach results in runtime problems for larger datasets.</p><p>In <ref type="bibr">[Ku?elka and Davis, 2020]</ref> the authors proposed Markov Logic as a powerful framework for solving knowledge base completion tasks. Instead of learning rules, the authors propose to learn the weights of a Markov logic network. As a consequence, dependencies between regularities are modeled in a complex way, which would lead to a non-trivial aggregation of evidences. However, the analysis remains entirely theoretical and, so it is doubtful whether the approach is applicable to larger knowledge graphs.</p><p>The concept of rule redundancy is crucial for rule aggregation. If two rules predict the same candidate for the same reason, a Noisy-OR aggregation makes no sense. In <ref type="bibr">[Gu et al., 2020a]</ref> a rule hierarchy was used to detect redundant rules. However, the application model is based on the maximum aggregation strategy, which means that the removal has no positive impact on the predictive quality. Indeed, the authors focus their experiments on runtime performance and report about a positive impact without a decrease in the predictive quality.</p><p>It is important to understand that the notion of rule redundancy is not limited to logical or extentional equivalence. Approaches to detect equivalent duplicate rules are well known in ILP, and have for example been implemented in QuickFOIL <ref type="bibr" target="#b14">[Zeng et al., 2014]</ref>. Instead of that, we have to deal with problems where two rules are only partially or nearly redundant.</p><p>We note that there are a few approaches that try to generate post-hoc explanations of predictions made by latent models such as knowledge graph embeddings. However, as opposed to AnyBURL/SAFRAN which provides ad-hoc explanations for every predicted triple, such approaches often fail to find explanations. F.e. the approach proposed in <ref type="bibr" target="#b15">[Zhang et al., 2019]</ref> applied to the knowledge graph embedding CrossE can only find explanations for 40% of triples in the test set of the FB15K dataset. Furthermore it is not fully transparent if generated explanations actually represent the reason for a prediction of a latent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Interpretable models drastically improve the trust and practical utility of predictions, and help to address current concerns about intransparency and unwanted bias in machine learning models. We introduced a novel method for fully interpretable link prediction that yields new state-of-the-art results among interpretable methods and outperforms state-of-the-art embedding models on some of the established benchmarks. Furthermore, our approach provides explanations that reveal clustering patterns in underlying data and derived rulesets. While our approach can significantly improve predictive performance of rule-based approaches, a more in-depth analysis is needed to assess the quality of generated clusters, which we plan to do in future work. We hope that beyond their direct practical utility, our results help to reinvigorate interest into novel rule-based approaches to complement the embedding-and deep learning-based approaches that have been at the center of attention in machine learning research over the past years. There are multiple policies for dealing with entities that have the same score as the correct entity <ref type="bibr" target="#b5">[Rossi et al., 2021</ref>]:</p><p>? Top: The correct entity is given the best rank among a group of same score entities.</p><p>? Bottom: The correct entity is given the worst rank among a group of same score entities. This policy is the most conservative.</p><p>? Average: The rank of the correct entity is the mean rank of all predictions that tie their score with the correct entity.</p><p>? Ordinal : The rank of the correct entity is given the rank in which it appears in the group of same score entities. This policy usually corresponds to the random policy.</p><p>? Random: The rank of the correct entity is given a random rank of the ranks within the group of same score entities. This policy usually corresponds to the average policy.</p><p>When comparing results from different sources it is important to use comparable policies for dealing with same score entities. F.e. results evaluated with the top policy can be artificially boosted and thus they cannot be compared to results evaluated with the ordinal/random or average policy.  Appendix C. Runtime comparison AnyBURL -SAFRAN <ref type="table" target="#tab_8">Table 9</ref> shows a comparison of the runtimes of AnyBURL and SAFRAN, when inferencing filtered predictions of all relevant rules for each prediction task in the test set of a dataset.</p><p>The experiment was performed with 8 physical <ref type="formula">(16 logical</ref>    <ref type="figure">0)</ref>, Non-redundant Noisy-OR (0 &lt; t &lt; 1) and Noisy-OR (t = 1) produced the best results in the validation set. ? is the number of relations that appear in the training set, but not in the validation set. As the different thresholds could not be evaluated for these relations, the Maximum aggregation is used. <ref type="table" target="#tab_0">Table 12</ref> shows the results of Non-redundant Noisy-OR applied to rules learned with AMIE+. Rules were learned with default settings, in which AMIE+ only learns cyclic rules. The results can be seen in <ref type="table" target="#tab_0">Table 12</ref>. As with rules learned by AnyBURL, Nonredundant Noisy-OR could achieve the biggest performance gain on the FB15k-237 dataset. However, the gains on other datasets are significantly small due to the restriction of the default setting to only cyclic rules. As the rule set only contains only rules of one type (i.e. cyclic rules), only a grid search was performed with the same settings as in  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) AMD Ryzen 7 PRO 4750U @ 1700 MHz cores and 16 GB of RAM. SAFRAN could drastically improve the runtime of rule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different types of rules that can be learned by AnyBURL. Uppercase letters represent variables and lowercase letters represent constants. h(. . . ) is called the head, while b 1 (. . . ), . . . , b n (. . . ) is called the body of a rule. Note that variables or constants of a body atom can be flipped.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Rules that can be generalized from the ground path speaks(max, english) ? lives(max, uk), lang(uk, english)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics. Number of entities, relations and triples in each dataset.ments on the parent datasets FB15k (WN18)[Bordes et al., 2013]  of FB15k-237 (WN18RR) as they were found to suffer from heavy testing leakage (f.e. some triples in the test data are present as inverse triples in the training data).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 :</head><label>7</label><figDesc>MRR, Hits@1, Hits@10 results for FB15K-237, WN18RR and YAGO3-10. Best results for each metric and dataset are marked in bold. *Denotes our approach. Results were evaluated with top policy for dealing with same score entities (see Appendix A) and are not directly comparable to other approaches. Results marked with are from</figDesc><table /><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185-5194, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1522. URL https://www.aclweb.org/anthology/D19-1522. Yulong Gu, Yu Guan, and Paolo Missier. Building rule hierarchies for efficient logical rule learning from knowledge graphs, 2020a. Yulong Gu, Yu Guan, and Paolo Missier. Towards learning instantiated logical rules from knowledge graphs, 2020b. Ond?ej Ku?elka and Jesse Davis. Markov logic networks for knowledge base completion: A theoretical analysis under the mcar assumption. In Ryan P. Adams and Vibhav Gogate, editors, Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115 of Proceedings of Machine Learning Research, pages 1138-1148. PMLR, 22-25 Jul 2020. URL http://proceedings.mlr.press/v115/kuzelka20a.html. Jonathan Lajus, Luis Gal?rraga, and Fabian Suchanek. Fast and exact rule mining with amie 3. In European Semantic Web Conference, pages 36-52. Springer, 2020.</figDesc><table><row><cell>Antoine</cell><cell>Bordes,</cell><cell>Nicolas</cell><cell>Usunier,</cell><cell>Alberto</cell><cell>Garcia-Duran,</cell><cell>Jason</cell><cell>We-</cell></row><row><cell>ston,</cell><cell cols="3">and Oksana Yakhnenko.</cell><cell cols="4">Translating embeddings for modeling</cell></row><row><cell cols="3">multi-relational data.</cell><cell>2013.</cell><cell cols="4">URL https://papers.nips.cc/paper/</cell></row><row><cell cols="7">5071-translating-embeddings-for-modeling-multi-relational-data.</cell><cell></cell></row><row><cell cols="8">1 Andrei Zary Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), page 21-29. rank i</cell></row><row><cell cols="6">IEEE Comput. Soc, 1997. doi: 10.1109/SEQUEN.1997.666900.</cell><cell></cell><cell></cell></row><row><cell cols="8">Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. Problog: A probabilistic prolog</cell></row><row><cell cols="8">and its application in link discovery. In IJCAI, volume 7, pages 2462-2467. Hyderabad,</cell></row><row><cell>2007.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">T. Denoeux. A k-nearest neighbor classification rule based on dempster-shafer theory. IEEE</cell></row><row><cell cols="8">Transactions on Systems, Man, and Cybernetics, 25(5):804-813, 1995. doi: 10.1109/21.</cell></row><row><cell>376493.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional</cell></row><row><cell cols="8">2d knowledge graph embeddings. In Thirty-Second AAAI Conference on Artificial Intel-</cell></row><row><cell cols="2">ligence, 2018.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">S?bastien Ferr?. Application of concepts of neighbours to and knowledge graph completion.</cell></row><row><cell cols="8">Data Science, Preprint:1-28, 2020. ISSN 2451-8492. doi: 10.3233/DS-200030. URL</cell></row><row><cell cols="5">https://doi.org/10.3233/DS-200030. Preprint.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Luis Gal?rraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. Fast rule mining</cell></row><row><cell cols="8">in ontological knowledge bases with amie+. The VLDB Journal, 24(6):707-730, Dec</cell></row><row><cell cols="8">2015. ISSN 0949-877X. doi: 10.1007/s00778-015-0394-1. URL https://doi.org/10.</cell></row><row><cell cols="3">1007/s00778-015-0394-1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Appendix B. Ruleset Statistics</figDesc><table><row><cell></cell><cell># C</cell><cell># AC1</cell><cell cols="2"># AC2 # Total</cell></row><row><cell cols="5">FB15K-237 53,578 1,413,806 502,593 1,969,977</cell></row><row><cell>WN18RR</cell><cell>7,329</cell><cell>38,816</cell><cell>25393</cell><cell>71,538</cell></row><row><cell>YAGO3-10</cell><cell cols="4">3,699 1,955,166 333,443 2,292,308</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Total number of rules and rule type distributions of rule sets used for the different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Comparison of the runtimes</figDesc><table><row><cell cols="4">Appendix D. Threshold Statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">t = 0</cell><cell cols="2">0 &lt; t &lt; 1</cell><cell cols="2">t = 1</cell><cell>?</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>#R</cell><cell>#T</cell><cell>#R</cell><cell>#T</cell><cell cols="4">#R #T #R #T</cell></row><row><cell cols="2">FB15K-237 Grid</cell><cell cols="2">Head 38</cell><cell cols="3">4890 181 15393</cell><cell>4</cell><cell>170</cell><cell>14</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>44</cell><cell cols="3">4479 178 15944</cell><cell>1</cell><cell>30</cell><cell>14</cell><cell>13</cell></row><row><cell cols="4">FB15K-237 Random Head 21</cell><cell cols="3">2910 202 17543</cell><cell>0</cell><cell>0</cell><cell>14</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>34</cell><cell cols="3">1312 189 19141</cell><cell>0</cell><cell>0</cell><cell>14</cell><cell>13</cell></row><row><cell cols="2">FB15K-237 VS</cell><cell cols="3">Head 130 14288</cell><cell>0</cell><cell>0</cell><cell cols="3">93 6165 14</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell cols="2">137 14801</cell><cell>0</cell><cell>0</cell><cell cols="3">86 5652 14</cell><cell>13</cell></row><row><cell>WN18RR</cell><cell>Grid</cell><cell>Head</cell><cell>2</cell><cell>125</cell><cell>9</cell><cell>3009</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>2</cell><cell>27</cell><cell>9</cell><cell>3107</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>WN18RR</cell><cell cols="2">Random Head</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>3131</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>1</cell><cell>3</cell><cell>10</cell><cell>3131</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>WN18RR</cell><cell>VS</cell><cell>Head</cell><cell>7</cell><cell>2659</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>475</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>6</cell><cell>1368</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>1766</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">YAGO3-10 Grid</cell><cell>Head</cell><cell>6</cell><cell>1516</cell><cell>27</cell><cell>3478</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>5</cell><cell>38</cell><cell>28</cell><cell>4956</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>6</cell></row><row><cell cols="3">YAGO3-10 Random Head</cell><cell>7</cell><cell>1835</cell><cell>26</cell><cell>3159</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>9</cell><cell>1597</cell><cell>24</cell><cell>3397</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>6</cell></row><row><cell cols="2">YAGO3-10 VS</cell><cell cols="2">Head 19</cell><cell>4510</cell><cell>0</cell><cell>0</cell><cell>14</cell><cell>484</cell><cell>4</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>Tail</cell><cell>18</cell><cell>4210</cell><cell>0</cell><cell>0</cell><cell>15</cell><cell>784</cell><cell>4</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Number of relations #R and the number of affected test triples #T of each direction (head or tail) for which the Maximum aggregation (t =</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Results of Non-redundant Noisy-OR on rules learned by AMIE+.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work received funding from netidee grant 5171 ('OpenBioLink').</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Evaluation metrics and same score policy</head><p>We now formally define used metrics in our experiments. Let t = 2 * |T | be the number of prediction tasks in test set T and rank i be the rank of the correct entity in the predicted list of entities for completion task i, then Hits@k and mean-reciprocal rank (MRR) are defined as follows:</p><p>Appendix E. Clustering Statistics  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">Hospedales</forename><surname>Tucker ; Manuel Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00671-6\1</idno>
		<ptr target="https://madoc.bib.uni-mannheim.de/45092/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods Christian Meilicke</title>
		<editor>Denny Vrande?i?</editor>
		<meeting>the 2019 Conference on Empirical Methods Christian Meilicke<address><addrLine>Monterey, CA, USA; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anytime bottom-up rule learning for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Melisachew Wudage Chekol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conferences on Artificial Intelligence</title>
		<editor>Sarit Kraus</editor>
		<meeting>International Joint Conferences on Artificial Intelligence<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="3137" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforced anytime bottom up rule learning for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Melisachew Wudage Chekol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuckenschmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.04412" />
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">9781450306195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable rule learning via learning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Pouya Ghiasnezhad Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/297</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/297" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding for link prediction: A comparative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donatella</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Matinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3424672</idno>
		<ptr target="https://doi.org/10.1145/3424672" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkxSmlBFvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drum: End-to-end differentiable rule mining on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/0c72cb7ee1512f800abe27823a792d03-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<editor>Maria Florina Balcan and Kilian Q</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quickfoil: Scalable inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jignesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="197" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interaction embeddings for prediction and explanation in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3291014</idno>
		<ptr target="https://doi.org/10.1145/3289600.3291014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i03.5701</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
