<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallelizing Legendre Memory Unit Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narsimha</forename><surname>Chilkuri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
						</author>
						<title level="a" type="main">Parallelizing Legendre Memory Unit Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a new recurrent neural network (RNN) named the Legendre Memory Unit (LMU) was proposed and shown to achieve state-of-the-art performance on several benchmark datasets. Here we leverage the linear time-invariant (LTI) memory component of the LMU to construct a simplified variant that can be parallelized during training (and yet executed as an RNN during inference), thus overcoming a well known limitation of training RNNs on GPUs. We show that this reformulation that aids parallelizing, which can be applied generally to any deep network whose recurrent components are linear, makes training up to 200 times faster. Second, to validate its utility, we compare its performance against the original LMU and a variety of published LSTM and transformer networks on seven benchmarks, ranging from psMNIST to sentiment analysis to machine translation. We demonstrate that our models exhibit superior performance on all datasets, often using fewer parameters. For instance, our LMU sets a new state-of-the-art result on psMNIST, and uses half the parameters while outperforming Dis-tilBERT and LSTM models on IMDB sentiment analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>LSTMs <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref>, the most popular class of recurrent neural networks (RNNs), process information in an inherently sequential manner. This prevents parallelization within training examples, and thus, they cannot fully leverage today's commodity GPU hardware. This sequential nature of RNNs is one of the critical reasons for the shift towards purely self-attention based architectures such as the transformer and its derivatives <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b8">Devlin et al., 2018;</ref><ref type="bibr" target="#b5">Brown et al., 2020)</ref>, especially in the domain of Natural Language Processing (NLP). Paral-lelization makes training such networks far more efficient on GPUs, and thus allows them to be applied to enormous datasets -for example, the Colossal Clean Crawled Corpus (C4) which comprises 750GB of English text <ref type="bibr" target="#b24">(Raffel et al., 2019)</ref>. Hence, such models, via unsupervised pre-training, make it possible for us to exploit resources such as the internet, 1 which produces 20TB of text data each month. A feat such as this, from the training perspective, would be unimaginable using LSTM or other RNN based models.</p><p>In this paper, we exploit the idea that linear recurrence relations can be solved. More specifically, a linear timeinvariant (LTI) system's state update equation can be written in a non-sequential fashion <ref type="bibr">(?str?m &amp; Murray, 2010)</ref>. This allows for the computation of the hidden state of an LTI system to be done in parallel, thus overcoming the fundamental constraint that traditional RNNs suffer from. Despite this desirable property, to our knowledge, such systems have remained underexplored. This perhaps has to do with the general belief that nonlinear dynamics are critical for solving complicated problems, especially AI level tasks. Here, we challenge that notion by focussing our attention on a specific LTI system, the memory component of the recently proposed Legendre Memory Unit <ref type="bibr" target="#b31">(Voelker et al., 2019)</ref>, and demonstrating superior performance of our model on a variety of tasks, including predicting a nonlinear, chaotic dynamical system. The Legendre Memory Unit (LMU) is an RNN cell that is constructed by coupling an LTI system to a nonlinear one. The LTI system, known as the Delay Network <ref type="bibr" target="#b32">(Voelker &amp; Eliasmith, 2018)</ref>, is the core component of the LMU that projects a sliding window of length ? of the input signal onto the Legendre basis -hence the name Legendre Memory Unit. This construction, which employs the Delay Network (DN) to act as the memory and a nonlinear recurrent layer to compute arbitrary functions across time, has been shown to outperform LSTMs and other RNNs on various tasks. Of special interest to us is the ability of the LMU to handle temporal dependencies across 100,000 time-steps, which is orders of magnitude better than the LSTM.</p><p>In the following pages, we start by simplifying the LMU such that recurrence exists only in the linear system. After showing that the training of this simplified variant can be 1 https://commoncrawl.org arXiv:2102.11417v2 <ref type="bibr">[cs.</ref>LG] 10 May 2021 parallelized, we turn to the question of its effectiveness. We do so by first considering the two synthetic tasks that the original LMU was validated on: psMNIST and Mackey-Glass. We show that our variant exceeds the original LMU's performance on both of these tasks, while also establishing a new state-of-the-art result for RNNs on psMNIST. We also test our models against LSTM models of comparable sizes on several NLP tasks. We look at sentiment classification (IMDB), semantic similarity (QQP), and natural language inference (SNLI), and show that our models achieve better performance while using significantly fewer parameters (up to 650x). We then briefly investigate the transfer learning abilities of our model by training a language model on the (unlabelled) Amazon Reviews dataset and using that to improve the IMDB score. Here, we show that it outperforms DistilBert, a transfomer based model, while using 50% fewer parameters. We conclude our NLP experiments by demonstrating superior performance on language modelling (text8) and machine translation (IWSLT'15 En-Vi). Finally, we show that these architectural modifications result in training times that are up to 200 times faster, relative to the original LMU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been recent studies focused on the application of LMU to new problems and on the modification of the original architecture. For example, <ref type="bibr" target="#b2">Blouw et al. (2020)</ref> used an LMU based architecture to achieve SotA results on the problem of efficient key-word spotting, and <ref type="bibr" target="#b9">Gu et al. (2020)</ref> recently proposed a generalization of the original architecture, which they use to beat the SotA result set by the LMU on psMNIST.</p><p>Our model, however, is more directly inspired by the success of self-attention. Self-attention based architectures have come to replace RNN based approaches for problems such as language modelling, machine translation, and a slew of other NLP tasks <ref type="bibr" target="#b23">(Radford et al., 2018;</ref><ref type="bibr" target="#b24">Raffel et al., 2019)</ref>. Three properties that make self-attention desirable over RNNs are: (1) it is better at handling the challenging problem of long-range dependencies; (2) it is purely feedforward; and (3) when the sequence length is smaller than the dimension of representation, which is not uncommon in NLP applications, self-attention is computationally cheaper than an RNN.</p><p>Of these three desirable properties, our model inherits the first one from the original LMU, and satisfies properties (2) and (3) by construction. Additionally, the capability to run our model in a recurrent manner during inference can be advantageous in situations where there are memory constraints or where low-latency is critical.</p><p>It should also be noted that prior works such as <ref type="bibr" target="#b1">Balduzzi &amp; Ghifary (2016)</ref> and <ref type="bibr" target="#b15">Martin &amp; Cundy (2017)</ref> have explored the use of linear RNNs for sequence modelling. Our work differs from theirs in a few ways. First, we do not learn the recurrent connections, but instead make use of an LTI system, the DN, that optimally implements the delay operation; the use of frozen connections also leads to a more efficient parallelization scheme. Second, while previous works restrict the recurrent relations to be elementwise (recurrent weight matrix is diagonal), we use full matrices in the linear system. Finally, we test our model on a wide range of tasks, well beyond the synthetic datasets <ref type="bibr" target="#b15">(Martin &amp; Cundy, 2017)</ref> and language modelling <ref type="bibr" target="#b1">(Balduzzi &amp; Ghifary, 2016)</ref> considered previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background and LMU Variants</head><p>In this section we start by introducing the delay problem and show how a delay is optimally realized by the DN, an LTI system. We then introduce the LMU which employs a single-input DN coupled to a nonlinear dynamical system to process sequential data. Finally, we introduce our model, which is obtained by simplifying the original architecture. We show how this simplification allows for training to be parallelized (often reducing the computation complexity), while also retaining the ability to handle long range dependencies of the original formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Delay Network</head><p>Ideal Delay A system is said be an ideal delay system if it takes in an input, u(t), and outputs a function, y(t), which is the delayed version of the input. Mathematically, this can be described in the following manner:</p><formula xml:id="formula_0">y(t) = D[u(t)] = 0 t &lt; ? u(t ? ?) t ? ? ,<label>(1)</label></formula><p>where D is the ideal delay operator and ? ? R is the length of the delay. There are two things of note here: (1) the ideal delay system is linear, i.e., for any two functions, f (t) and g(t), and any a, b ? R, it respects the following equation:</p><formula xml:id="formula_1">D[af (t) + bg(t)] = aD[f (t)] + bD[g(t)];<label>(2)</label></formula><p>and (2) although this problem looks deceptively simple, in fact it takes a system with infinite memory to take in a continuous stream of input (with unspecified frequency content), store it for ? seconds and then reproduce the entire input without error.</p><p>These two considerations tell us that the optimal system that implements delay must be linear and that even the most optimal physical implementation can only approximately realize the delay.</p><p>Approximating Delay If we are interested in constructing a dynamical system that implements delay, thanks to the linearity constraint, we can narrow our search space from the general system of ODEs of the form:</p><formula xml:id="formula_2">m = f(m, u), (3) y = g(m, u),<label>(4)</label></formula><p>to just finding the four matrices (A, B, C, D) that define an LTI system:?</p><formula xml:id="formula_3">= Am + Bu,<label>(5)</label></formula><formula xml:id="formula_4">y = Cm + Du.<label>(6)</label></formula><p>Following the derivation of the Delay Network in <ref type="bibr" target="#b32">Voelker &amp; Eliasmith (2018)</ref>, we start by considering the transfer function of the delay system, which for a SISO system is defined as:</p><formula xml:id="formula_5">G(s) = y(s) u(s) = e ??s ,<label>(7)</label></formula><p>where y(s) and u(s) are found by taking the Laplace transform of the input and output functions in time. As expected, this defines an infinite dimensional transfer function, capturing the intuitive difficulty of constructing a continuous delay.</p><p>The transfer function can be converted to a finite, causal state space realization if and only if it can be written as a proper 2 ratio of finite dimensional polynomials in s <ref type="bibr" target="#b4">(Brogan, 1991)</ref>. G(s), however, is irrational, i.e., it cannot be written as a proper, finite dimensional ratio. Therefore, making an approximation is necessary.</p><p>We can achieve an optimal convergence rate (in the leastsquare error sense) for rational approximants by means of Pad? approximants <ref type="bibr" target="#b19">(Partington, 2004)</ref>. Choosing the order of the numerator to be one less than the order of the denominator and accounting for numerical issues in the state-space realization (see <ref type="bibr" target="#b32">Voelker &amp; Eliasmith (2018)</ref> for details), gives the following canonical realization:</p><formula xml:id="formula_6">A i,j = (2i + 1) ? ?1 i &lt; j (?1) i?j+1 i ? j ,<label>(8)</label></formula><formula xml:id="formula_7">B i = (2i + 1)(?1) i ? ,<label>(9)</label></formula><formula xml:id="formula_8">C i = (?1) i i l=0 i l i + l j (?1) l ,<label>(10)</label></formula><formula xml:id="formula_9">D = 0, i, j ? [0, d ? 1],<label>(11)</label></formula><p>where we refer to d as the order of the system. The LTI system (A, B, C, D) is what is known as a Delay Network (DN). The order of the system, d, and the delay length, ? are the main hyperparameters to choose when using a DN.</p><p>Higher order systems require more resources, but provide a more accurate emulation of the ideal delay. Because we have used Pad? approximants, each order is optimal for that dimension of state vector m.</p><p>Legendre Polynomials Suppose we construct a system using the (A, B, C, D) matrices define above, and provide it with an input signal, u(t). Given the state m t , we can use C to decode u(t ? ?) to a degree of accuracy determined by the order of our system, i.e.,</p><formula xml:id="formula_10">u(t ? ?) ? C T m t .<label>(12)</label></formula><p>Intuitively, given m t , it seems possible to decode not only</p><formula xml:id="formula_11">u(t ? ?) but also u(t ? ? ) ? 0 ? ? ? ?.</formula><p>This can be done using a slightly modified C for a given ? :</p><formula xml:id="formula_12">u(t ? ? ) ? C(? ) T m t ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_13">C i (? ) = (?1) i i l=0 i l i + l j ? ? ? l , 0 ? ? ? ?,<label>(14)</label></formula><p>and C(? = ?) corresponds to the C defined in equation <ref type="formula" target="#formula_0">(10)</ref>. Interestingly, the functions in (14) turn out to be the shifted Legendre polynomials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Legendre Memory Unit</head><p>The LMU is obtained by coupling a single-input delay network to a nonlinear dynamical system. The DN orthogonalizes the input signal across a sliding window of length ?, whereas the nonlinear system relies on this memory to compute arbitrary functions across time. The state update equations that define the LMU are given below:</p><formula xml:id="formula_14">u t = e T x x t + e T h h t?1 + e T m m t?1 , (15) m t =?m t?1 +Bu t , (16) h t = f (W x x t + W h h t?1 + W m m t ),<label>(17)</label></formula><p>where? andB are the discretized versions 3 of A and B. These matrices are usually frozen during training, although they need not be. The input to the LTI system, u(t), is computed by projecting the input to the RNN cell, x t ? R dx , the hidden state, h t ? R d h , and the memory state, m t ? R d , onto their respective encoding weights (e x , e h , and e m ). The memory state, hidden state and the input to the cell are then combined using the weight matrices (W x , W h , and W m ) and passed through the nonlinearity, f . The encoding vectors and the kernel matrices are learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Our Model</head><p>In this paper, we propose a modified LMU architecture by making two changes to the equations defined above. First, we remove certain connections in <ref type="formula" target="#formula_0">(15)</ref> and <ref type="formula" target="#formula_0">(17)</ref>, namely (e T h , e T m ) and (W h ); this is done to aid parallelization. Second, instead of projecting the input to the RNN cell, x t down to a scalar as in <ref type="formula" target="#formula_0">(15)</ref>, we implement a general affine transformation followed by an element-wise nonlinearity; the original architecture is better suited for dealing with 1D or low-dimensional inputs, and this is a straightforward generalization of the encoding equation for higher dimensional inputs. Additionally, adding a bias term to equation <ref type="formula" target="#formula_0">(17)</ref>, we end up with the following:</p><formula xml:id="formula_15">u t = f 1 (U x x t + b u ), (18) m t =?m t?1 +Bu t , (19) o t = f 2 (W m m t + W x x t + b o ).<label>(20)</label></formula><p>Note that equations <ref type="formula" target="#formula_0">(18)</ref> and <ref type="formula" target="#formula_1">(20)</ref> are equivalent to having time-distributed dense layers before and after equation <ref type="formula" target="#formula_0">(19)</ref>.</p><p>In general, we expect our model to be modified and used in combination with other feed-forward layers, including self-attention. For example, we found a gated architecture <ref type="bibr" target="#b29">(Srivastava et al., 2015)</ref> where equation <ref type="formula" target="#formula_0">(18)</ref> is modified to</p><formula xml:id="formula_16">u t = f 1 (W u x t + b u ) ? g + x t ? (1 ? g) ,</formula><p>to work well for the addition problem (results not shown). The gate is defined as g = ?(W g x t + b g ), i.e., a sigmoidactivated affine transformation where the bias is initialized to -1.</p><p>Additionally, now that the input to the LTI system in this case is a vector, u t ? R 1?du , we have that m t ? R d?du , and equation <ref type="formula" target="#formula_0">(16)</ref> can be thought of as implementing the following:</p><formula xml:id="formula_17">m t = reshape(? reshape(m t?1 , (d, d u )) +Bu t , d ? d u ).<label>(21)</label></formula><p>Parallel Training One of the motivations for the above mentioned architectural changes is that the model now has only one recurrent connection: m t 's dependence on itself from the past in equation <ref type="formula" target="#formula_0">(19)</ref>. But because this is an LTI system, standard control theory <ref type="bibr">(?str?m &amp; Murray, 2010)</ref> gives us a non-iterative way of evaluating this equation as shown below 4 </p><formula xml:id="formula_18">m t = t j=1? t?jB u j .<label>(22)</label></formula><formula xml:id="formula_19">RNN O(n ? d 2 x ) ? Convolution O(k ? n ? d 2 x ) ? Attention O(n 2 ? d x ) ? DN (19) O(n ? d 2 ? d x ) ? DN (24) O(n 2 ? d ? d x ) ? DN (25) O(n ? d ? d x ) ? DN (26) O(n ? log n ? d ? d x ) ? Defining H = ? 0B?B . . . ? R d?n and U = ? ? ? ? ? ? ? u 1 u 2 u 3 . . . u n u 1 u 2 . . . u n?1 u 1 . . . u n?2 . . . . . . u 1 ? ? ? ? ? ? ? ? R n?n ,<label>(23)</label></formula><p>the above convolution equation can alternatively be written as a matrix multiplication:</p><formula xml:id="formula_20">m 1:n = HU ,<label>(24)</label></formula><p>where n is the sequence length. Given that H is the impulse response of the LTI system, in practice we compute H by feeding in an impulse to the RNN version of the DN (equation <ref type="formula" target="#formula_0">(19)</ref>). Note that in our method the? andB matrices are frozen during training, so the impulse response need only be computed once. In case of multi-dimensional inputs, we would repeat the above computation several times, once for each dimension of the input. It is also evident from the structure of the U matrix that although this reformulation turns the DN into a feedforward layer, it still respects causality. In other words, the state m t depends only on the inputs seen until that point of time, i.e., u i : i ? t.</p><p>Complexity With the new formulation, we immediately see that it is computationally (i.e., in terms of the number of operations) advantageous in situations where we only need the final state (return_sequences=False in Keras terminology). Instead of using (19) to explicitly simulate the first n ? 1 steps in-order to compute the state at the time-step n, we can instead just compute m n = HU :n ,</p><p>thus reducing the complexity of computing the final state,</p><formula xml:id="formula_22">m n , from O(n ? d 2 ? d x ) to O(n ? d ? d x ),</formula><p>where n is the sequence length, d is the order, and d x is the dimension of the input. We show in Section (4.6) that using this implementation results in up to 200x speedup.</p><p>The more general computation (24), although parallelizable, results in a complexity of O(n 2 ? d ? d x ). This can be made more efficient by employing the convolution theorem which gives us an equivalent way of evaluating the convolution in the Fourier space as 5</p><formula xml:id="formula_23">m 1:n = F ?1 {F{H} ? F {U :n }}.<label>(26)</label></formula><p>Thanks to the fast Fourier transform, the above operation has a complexity of O(n ? log 2 n ? d ? d x ). These, other algorithms, and their complexities are reported in <ref type="table" target="#tab_0">Table (1)</ref>.</p><p>It was argued in <ref type="bibr" target="#b30">Vaswani et al. (2017)</ref> that a self-attention layer is cheaper than an RNN when the representation dimension of the input, d x , is much greater than the length of the sequence, n, which is seen in NLP applications. For example, standard word or sub-word based machine translation systems work with sequence lengths of about 100 and representation dimension ranging from 300 to 512. The same argument holds for the DN layer too, since we have found the inequality log 2 n ? d d x to hold in all our wordbased NLP experiments -this excludes text8, which works at the level of characters. More specifically, we use d ? 4 for our word-based models.</p><p>Recurrent Inference Machine learning algorithms are usually optimized for training rather than deployment <ref type="bibr" target="#b7">(Crankshaw, 2019)</ref>, and because of that models need to be modified, sometimes non-trivially, to be more suitable for inference. For example, one of the metrics that is crucial for applications such as online Automatic Speech Recognition (ASR) is low latency. Transformers have shown faster training times and better performance than RNNs on ASR, but since they employ (global) self-attention, which requires the entire input to begin processing, they are not natural fits for such a task <ref type="bibr" target="#b33">(Wang et al., 2020)</ref>. Look-ahead  and chuck-based <ref type="bibr" target="#b16">(Miao et al., 2020)</ref> approaches are usually used to alter the architecture of self-attention for such purposes. While our model can be trained in parallel, because of the equivalence of equations <ref type="formula" target="#formula_0">(19)</ref> and <ref type="formula" target="#formula_1">(26)</ref>, it can also be run in an iterative manner during inference, and hence can process data in an online or streaming fashion during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following experiments we compare our model against the LMU, LSTMs and transformers. With these experiments, we focus on benchmarking rather than establishing new state-of-the-art results. Hence, we stick to simple architectures, constrain ourselves to train all the models, with the exception of text8, using the Adam optimizer <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2014)</ref> with all the default parameter settings. For text8, we found it helpful to reduce the learning rate by a factor of 10 halfway into training. Although unusual, we use the default optimization settings even when transfer learning.</p><p>With models we compare against, we use results found in published work, even when they make use of more sophisticated architectures, learning schedules or regularization schemes.</p><p>We do not consider the capacity task from the original LMU paper here as they employ just the delay layer without nonlinear dynamics in order to deal with this problem, which makes their architecture essentially the same as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">psMNIST</head><p>The permuted sequential MNIST (psMNIST) dataset was introduced by <ref type="bibr" target="#b12">Le et al. (2015)</ref> to test the ability of RNNs to model complex long term dependencies. psMNIST, as the name suggests, is constructed by permuting and then flattening the (28 ? 28) MNIST images. The permutation is chosen randomly and is fixed for the duration of the task, and in order to stay consistent, we use the same permutation as <ref type="bibr" target="#b6">Chandar et al. (2019)</ref> and <ref type="bibr" target="#b31">Voelker et al. (2019)</ref>. The resulting dataset is of the form (samples, 784, 1), which is fed into a recurrent network sequentially, one pixel at a time. We use the standard 50k/10k/10k split.</p><p>Architecture As was pointed out in <ref type="bibr" target="#b31">Voelker et al. (2019)</ref>, in order to facilitate fair comparison, RNN models being tested on this task should not have access to more the 28 2 = 784 internal variables. Keeping that in mind, and in order to make direct comparisons to the original LMU model, we consider a model with d = 468 dimensions for memory. We set the dimension of the output state to 346 and use ? = 784. Our model uses 165k parameters, the same as all the models reported in <ref type="table">Table (</ref>2), except for the original LMU model, which uses 102k parameters, and the HiPPO-LegS model, which is reported to use 512 hidden dimensions (number of parameters is unknown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results &amp; Discussion</head><p>Test scores of various models on this dataset are reported in <ref type="table">Table (</ref>2). Our model not only surpasses the LSTM model, but also beats the current stateof-the result of 98.3% set by HiPPO-LegS <ref type="bibr" target="#b9">(Gu et al., 2020)</ref> recently. Thus, our model sets a new state-of-the art result for RNNs of 98.49% on psMNIST. It is interesting that our model, despite being simpler than the original LMU, outperforms it on this dataset. This suggests that the main advantage of the LMU over past models is the quality of its temporal memory, implemented by the DN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Mackey-Glass</head><p>Mackey-Glass equations are a set of two nonlinear differential equations that were originally developed to model the quantity of mature cells in the blood. The second of these equations is interesting because it can result in chaotic attractors and is used to construct the dataset at hand. This is a time-series prediction task where we need to predict 15 time-steps into the future.</p><p>Architecture <ref type="bibr" target="#b32">Voelker &amp; Eliasmith (2018)</ref> compare three architectures on this dataset. The first one uses 4 layers of LSTMs (h = 28), the second one replaces LSTMs with LMUs (d = 4, ? = 4), and the final one replaces the first and third layers in the first model with LMUs. We use a relatively simple architecture where we combine our model (single layer) with an additional dense layer. We use d = 40, ? = 50, and 1 and 140 units in the input and output layers, with the additional dense layer containing 80 units. We did not try other variations. All the models contain about 18k parameters and are run for 500 epochs.</p><p>Results &amp; Discussion NRMSE scores on the test set are presented in <ref type="table">Table (</ref>3). We see that our model outperforms the other three models in accuracy and training time. In the original paper <ref type="bibr" target="#b32">Voelker &amp; Eliasmith (2018)</ref> hypothesize that the superior performance of the hybrid model is due to the presence of gates, but given that our model lacks gating mechanisms, we think that it might have to do with the LMU model being misparametrized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sentiment, Semantics and Inference</head><p>In this section, we explore the supervised and semisupervised capabilities of our model. More specifically, we first look at the tasks of sentiment analysis (IMDB), semantic similarity (QQP) and natural language inference (SNLI), and then improve upon the IMDB score using a language model pre-trained on the (unlabelled) Amazon Reviews dataset <ref type="bibr" target="#b18">(Ni et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPERVISED</head><p>IMDB The IMDB dataset <ref type="bibr" target="#b14">(Maas et al., 2011</ref>) is a standard sentiment classification task containing a collection of 50k highly polar reviews, with the training and testing sets containing 25k reviews each. We use the standard pre-processed dataset available from the Keras website, 6 consider a vocabulary of 20k words, and set the maximum sequence length to 500 words.</p><p>QQP For Quora Question Pairs (QQP), given a pair of sentences, the task is to identify whether the two sentences are semantically similar. In this case, we experiment on two train/dev/test splits: 390k/8k/8k like in <ref type="bibr" target="#b28">Shen et al. (2018)</ref>, and 280k/80k/40k like in <ref type="bibr" target="#b27">Sharma et al. (2019)</ref>. We use a vocabulary of 20k words and truncate the sentences to be less than 25 words.</p><p>SNLI The Stanford Natural Language Inference 7 was released to serve as a benchmark for evaluating machine learning systems on the task of natural language inference. Given a premise, the task is to determine whether a hypothesis is true (entailment), false (contradiction), or independent (neutral). We use the standard 550k/10k/10k split, consider a vocabulary of 20k words, and set the maximum sequence length to 25 words.</p><p>Architecture In our experiments, confusingly, we found the use of the DN, without any nonlinearities, to work well. Therefore, we construct parameter efficient models that employ just the DN layer, with d = 1 and ? = maxlen. We use 300D Glove embeddings (840B Common Crawl; Pennington et al. <ref type="formula" target="#formula_0">(2014)</ref>) for all our models. For the IMDB task, which is a single sentence task, we encode the sentence and pass the encoded vector to the final classification layer. For two-sentence tasks, QQP and SNLI, we encode the two sentences to produce two vectors, and then pass the vector obtained by concatenating the two vectors, their absolute difference, and their element-wise product to the final classification layer. We compare our models against the LSTM models described in <ref type="bibr" target="#b9">(Gu et al., 2020)</ref> for IMDB, both <ref type="bibr" target="#b28">(Shen et al., 2018)</ref> and <ref type="bibr" target="#b27">(Sharma et al., 2019)</ref> for QQP, and <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref> for SNLI. They all use at least an order of magnitude more parameters than our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results &amp; Discussion</head><p>We present the results from the three experiments in <ref type="table" target="#tab_2">Table (4)</ref>. As we can see, our simple models based on the DN alone do indeed outperform the LSTM models. It is also noteworthy that our models use significantly fewer parameters than the LSTM models: 160x on IMDB, 650x on QQP and 60x on SNLI.  <ref type="bibr" target="#b9">(Gu et al., 2020)</ref>, QQP results are from <ref type="bibr" target="#b28">(Shen et al., 2018)</ref> and <ref type="bibr" target="#b27">(Sharma et al., 2019)</ref> respectively, and SNLI is from <ref type="bibr" target="#b3">(Bowman et al., 2015</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEMI-SUPERVISED</head><p>Amazon Reviews NLP over the past couple of years has been dominated by transfer learning approaches, where language models pre-trained on large corpora are used to initialize general purpose fine-tuning architectures to help with various downstream tasks. We therefore consider using a pre-trained language model on the Amazon Reviews dataset to improve the previously achieved score on IMDB. Here, we make direct comparisons to the LSTM model described in <ref type="bibr" target="#b22">Radford et al. (2017)</ref>. While they use the entire dataset (82 million reviews) for pre-training and train their model for a month on 4 GPUs, due to resource constraints, we use less than 5% (3.7 million reviews) and train our model for about 12 hours on a single GPU. We use a vocabulary of 30k words.</p><p>Architecture We have found the repeating block architecture, where each block is composed of our model, a few highway layers, and a dense layer, when used with skip connections to work well (see <ref type="figure">Figure in</ref> supplementary materials). Since the inputs, d x , are high dimensional, using a large ?, which would have to be accompanied by a large order d to maintain resolution, would result in vectors that are d x ? d dimensional, which is not desirable. Therefore, we instead work with smaller ? (and hence smaller d) and use several repeating blocks to take long-term dependencies into account. This is similar to how convolutional networks are used: small kernel sizes but many convolutional layers. If ? i is the setting we use with the DN in the i th block, then the effective delay, ? e = i ? i . In this specific case, we have ? i = 6 ? i, and ? e = 30. Thus, the model has access to the past 30 tokens when making a prediction. In terms of the fine-tuning performance, like <ref type="bibr" target="#b21">Peters et al. (2018)</ref>, we found that using deep representations, i.e., a linear combination of representations from all the blocks, to be better than using just the output of the top block. For fine-tuning, we compute a weighted sum of the outputs from the language model, and use that to classify a given review as expressing a positive or negative sentiment. Even during fine-tuning, we use the the Adam optimizer with all the default settings. We did not experiment with smaller learning rates or other learning schedules. Results &amp; Discussion Results for this experiment are presented in <ref type="table">Table (</ref> <ref type="bibr">5)</ref>. Despite using a much smaller pretraining dataset, training for just 12 hours, and using less than 50% of the parameters, our model outperforms the LSTM model described in <ref type="bibr" target="#b22">Radford et al. (2017)</ref>. We also include a self-attention based model, DistilBert <ref type="bibr" target="#b25">(Sanh et al., 2019)</ref>, for comparison; it must be noted however that Distil-Bert was trained on a more general yet much larger dataset (concatenation of English WikiPedia and Toronto Book Corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Language Modelling</head><p>text 8 We evaluate our model on character level language modelling using the text8 dataset 8 . It contains 100MB of clean text from Wikipedia and has an alphabet size of 27.</p><p>As is standard, we use the first 90MB as the training set, the next 5MB as the validation set and the final 5MB as the test set. Following <ref type="bibr" target="#b17">Mikolov et al. (2012)</ref> and <ref type="bibr" target="#b35">Zhang et al. (2016)</ref>, we set the sequence length to 180.</p><p>Architecture This architecture is similar in essence to the language model used in the above section, except for the use of deep representations; we simply work with the output from the top block. 9 For text8, minor changes were made to adapt the model to deal with longer sequences, which is a consequence of modelling at the level of characters, and to parameter match it to the LSTM model in <ref type="bibr" target="#b35">Zhang et al. (2016)</ref>, which uses around 3.2 million weights. We employ three blocks in this case and use all three DNs with the setting ? = 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results &amp; Discussion</head><p>We report the scores in bits per character in <ref type="table">Table (</ref> <ref type="bibr">6)</ref>. We see that our model performs better than the LSTM model of similar size. A couple of observations regarding model optimization: 1) we noticed that the training loss plateaus around the twelfth epoch, so we found it helpful it to decrease the learning rate by a factor of 10 around then (this is the only dataset for which we alter the optimization parameters); 2) despite that, the training slows down after a few more epochs, and we stop the optimization after 20 epochs; we believe that a more carefully designed learning schedule might be able to help with faster convergence. We also observed that using a selfattention layer after the final block helps with generalization; this perhaps points to the fact that the model needs a context that is longer 45 tokens in order to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Translation</head><p>IWSLT'15 En-Vi IWSLT'15 English to Vietnamese is a medium-resource language translation task containing 133k sentence pairs. Following <ref type="bibr" target="#b13">Luong &amp; Manning (2015)</ref>, we do not perform any pre-processing other than replacing words that occur less frequently than 5 by the &lt;unk&gt; token, which leaves us with vocabulary sizes of 17k and 7.7k for English and Vietnamese, respectively. We use the TED tst2012 as the validation set and TED tst2013 as the test set. We use 300D representation for source and target embeddings.</p><p>Architecture For translation, we use a standard encoderdecoder architecture inspired by the Amazon reviews language model, and we also employ an attention layer to help with translation. Our model's about the same size as the the 24 million LSTM model described in <ref type="bibr" target="#b13">Luong &amp; Manning (2015)</ref>. Due to time constraints, the architecture and hyperparamters for this problem were relatively underexplored.</p><p>Results &amp; Discussion Case sensitive BLEU scores are reported in <ref type="table">Table (</ref>6). Our model brings in an improvement of 2.3 BLEU over the LSTM model. When tested on lower-cased text (without altering the training procedure), we obtained a higher score of 26.2 BLEU. One major limiting factor of this analysis is that we use a small, fixed vocabulary (17k and 7.7k words), with no way of dealing with out-of-vocabulary words. For future work, we intend to experiment with an open-vocabulary encoding scheme such as byte pair encoding <ref type="bibr" target="#b26">(Sennrich et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Training Time</head><p>Here we explore the effect of parallelization on training time. We refer to architectures that implement the DN using equation <ref type="formula" target="#formula_0">(19)</ref> as the LTI version and the ones that use either (25) or (26), depending on whether return_sequences is false or true, as the parallel version.</p><p>Results are presented in <ref type="figure" target="#fig_0">Figure (1)</ref>. On the left, we compare the speedup we obtain by going from the original LMU to our model, in both LTI and parallel forms, on psMNIST and Mackey-Glass. We first notice that switching from the LMU to the LTI version results in non-trivial gains. This is solely due to the reduction in the number of recurrent connections. On top of this, switching to the parallel version, owing to long sequences (784 and 5000) and, in the case of psMNIST, a reduction in the computational burden, results in substantial improvements in training times of 220x and 64x respectively. 10</p><p>On the right of <ref type="figure" target="#fig_0">Figure (1)</ref>, we look at how varying the sequence length of the inputs effects the time it takes to complete one epoch. We see that the LTI version exhibits a linear growth whereas the parallel one essentially stays constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>With the intention of alleviating the long standing problem of speeding up RNN training, we introduce a variant of the LMU architecture that can process data in a feedforward or sequential fashion. When implemented as a feedforward layer, it can utilize parallel processing hardware such as GPUs and thus is suitable for large scale applications, and when run as an RNN, it is useful in applications where the amount of available memory is a limitation. We demonstrate the effectiveness of this architecture by experimenting on a range of tasks, of varying scale and difficulty. We also briefly consider the question of computational complexity of our model, and argue for its suitability to large scale applications in the domain of NLP, a direction we will pursue in the future. <ref type="bibr">10</ref> We note that for Mackey-Glass we use a (parameter matched) one layer LMU model instead of the 4 layer model used in <ref type="bibr" target="#b31">Voelker et al. (2019)</ref>; the difference with respect to the original architecture is more drastic, with our model (parallel) being about 200x faster.</p><p>We note that our method of parallelization applies to all deep architectures with linear recurrent dependencies, and although we focus on a specific LTI system throughout, we hope that our analysis highlights the utility of linear systems for the purposes of machine learning. In sum, we believe that linear systems offer a scalable solution to many time series problems without sacrificing accuracy. <ref type="figure">Figure 2</ref>. Illustration of the language model used for pre-training on the Amazon Reviews dataset. Although the actual model uses five blocks (combination of DN, Dense and Highway), we only show two blocks in the above <ref type="figure">figure.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(left) The plot shows the speedup we obtain from switching from the LMU to the LTI (in orange) and parallel implementation (in blue) of our model. (right) This show the effect of increasing the sequence length on the LTI and parallel versions of our model. All results were measured using a single GTX 1080.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Complexity per layer and minimum number of sequential operations of various architectures. n is the sequence length, dx is the input dimension, d is the order, and k is the size of the kernel. First three rows are as reported in<ref type="bibr" target="#b30">Vaswani et al. (2017)</ref>.</figDesc><table><row><cell>Layer Type</cell><cell>Complexity</cell><cell>Sequential Ops</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>psMNIST results. The first three rows are from<ref type="bibr" target="#b31">Voelker et al. (2019)</ref>, and the fourth row is from<ref type="bibr" target="#b9">Gu et al. (2020)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Table 3. Mackey-Glass</cell></row><row><cell></cell><cell></cell><cell>results.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>NRMSE</cell></row><row><cell>Model</cell><cell>Accuracy</cell><cell>LSTM</cell><cell>0.059</cell></row><row><cell>LSTM</cell><cell>89.86</cell><cell>LMU</cell><cell>0.049</cell></row><row><cell>NRU</cell><cell>95.38</cell><cell>Hybrid</cell><cell>0.045</cell></row><row><cell>LMU</cell><cell>97.15</cell><cell>Our Model</cell><cell>0.044</cell></row><row><cell>HiPPO-LegS</cell><cell>98.3</cell><cell></cell><cell></cell></row><row><cell>Our Model</cell><cell>98.49</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>IMDB, QQP and SNLI results. IMDB result is from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>IMDB results with pre-training. First row is from<ref type="bibr" target="#b22">Radford et al. (2017)</ref>, and the second row is from<ref type="bibr" target="#b25">Sanh et al. (2019)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2"># parameters (Millions) Accuracy</cell></row><row><cell>LSTM</cell><cell>75</cell><cell>92.88</cell></row><row><cell>DistilBERT</cell><cell>66</cell><cell>92.82</cell></row><row><cell>Our Model</cell><cell>34</cell><cell>93.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Language modelling and translation results. text8 score is from<ref type="bibr" target="#b35">Zhang et al. (2016)</ref>, and IWSLT score is from<ref type="bibr" target="#b13">Luong &amp; Manning (2015)</ref>. a (case sensitive), b (case insensitive).</figDesc><table><row><cell>Model</cell><cell cols="2">text8 IWSLT'15</cell></row><row><cell>LSTM</cell><cell>1.65</cell><cell>23.3</cell></row><row><cell cols="3">Our Model 1.61 25.5 a /26.2 b</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Center for Theoretical Neuroscience, University of Waterloo 2 Applied Brain Research. Correspondence to: Narsimha Chilkuri &lt;narasimha.chilkuri@gmail.com&gt;.Preprint -Accepted for publication in ICML2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A ratio a(s) b(s) is said be proper if the order of the numerator does not exceed the order of the denominator.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Using zero-order hold and dt = 1, exact discretization gives A = e A andB = A ?1 (e A ? I)B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Restricting ourselves to 1D input for the purposes of illustration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Assuming a padded Fourier transform across the appropriate axis and automatic broadcasting when computing element-wise multiplication. See LMUFFT code for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://keras.io/api/datasets/imdb/. 7 Dataset and published results are available at https://nlp.stanford.edu/projects/snli/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://mattmahoney.net/dc/textdata.9  We did not test the use of deep representations in this context.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Aaron Voelker, Vincenzo Heska, Alison Shi, Andreas Stockel, and Terry Stewart for discussions that helped improve this paper. This work was supported by CFI and OIT infrastructure funding as well as the Canada Research Chairs program, NSERC Discovery grant 261453, AFOSR grant FA9550-17-1-0026 and OGS graduate funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Feedback systems: an introduction for scientists and engineers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Astr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Princeton university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strongly-typed recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hardware aware training for efficient keyword spotting on general purpose and specialized hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04465</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modern control theory. Pearson education india</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Brogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards non-saturating recurrent units for modelling long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3280" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Design and Implementation of Low-Latency Prediction Serving Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hippo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07669</idno>
		<title level="m">Recurrent memory with optimal polynomial projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Parallelizing linear recurrent neural nets over sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cundy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04057</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformer-based online ctc/attention end-to-end speech recognition architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6084" to="6088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Some frequency-domain approaches to the model reduction of delay systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Partington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Reviews in Control</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Distilbert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01041</idno>
		<title level="m">Natural language understanding with the quora question pairs dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09843</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15544" to="15553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving spiking dynamical networks: Accurate delays, higher-order synapses, and time cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="609" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reducing the latency of end-to-end streaming speech recognition models with a scout network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10369</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7829" to="7833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1822" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
