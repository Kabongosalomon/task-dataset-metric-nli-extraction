<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Bibas</surname></persName>
							<email>kobybibas@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meir</forename><surname>Feder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<email>talhassner@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the predictive normalized maximum likelihood (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6% over recent leading methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important concern that limits the adoption of deep neural networks (DNN) in critical safety systems is how to assess our confidence in their predictions, i.e, quantifying their generalization capability <ref type="bibr" target="#b16">(Kaufman et al., 2019;</ref><ref type="bibr" target="#b38">Willers et al., 2020)</ref>. Take, for instance, a machine learning model for medical diagnosis . It may produce (wrong) diagnoses in the presence of test inputs that are different from the training set rather than flagging them for human intervention <ref type="bibr" target="#b34">(Singh et al., 2021)</ref>. Detecting such unexpected inputs had been formulated as the out-of-distribution (OOD) detection task <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref>, as flagging test inputs that lie outside the training classes, i.e., are not in-distribution (IND).</p><p>Previous learning methods that designed to offer such generalization measures, include VCdimension <ref type="bibr" target="#b43">(Zhong et al., 2017;</ref><ref type="bibr" target="#b36">Vapnik and Chervonenkis, 2015)</ref> and norm based bounds <ref type="bibr" target="#b27">(Neyshabur et al., 2018;</ref><ref type="bibr" target="#b0">Bartlett et al., 2017)</ref>. As a whole, these methods characterized the generalization ability based on the properties of the parameters. However, they do not consider the test sample that is presented to the model <ref type="bibr" target="#b15">(Jiang et al., 2020)</ref>, which makes them useless for OOD detection. Other approaches build heuristics over the empirical risk minimization (ERM) learner, by post-processing the model output <ref type="bibr" target="#b33">(Sastry and Oore, 2020)</ref> or modifying the training process <ref type="bibr" target="#b28">(Papadopoulos et al., 2021;</ref><ref type="bibr" target="#b37">Vyas et al., 2018)</ref>. Regardless of the approach, these methods choose the learner that minimizes the loss over the training set. This may lead to a large generalization error because the ERM estimate may be wrong on unexpected inputs; especially with large models such as DNN <ref type="bibr" target="#b1">(Belkin et al., 2019)</ref>.</p><p>To produce a useful generalization measure, we exploit the individual setting framework <ref type="bibr" target="#b23">(Merhav and Feder, 1998)</ref>. In the individual setting, there is no assumption about how the training and the test data are generated, nor about their probabilistic relationship. Moreover, the relationship between the labels and data can be deterministic and may therefore be determined by an adversary. The generalization error in this setting is often referred to as the regret <ref type="bibr" target="#b23">(Merhav and Feder, 1998)</ref>. This regret is defined by the log-loss difference between a learner and the genie: a learner that knows the specific test label, yet is constrained to use an explanation from a set of possible models. The individual setting is the most general framework and so the result holds for a wide range of scenarios. Specifically, the result holds to OOD detection where the distribution of the OOD inputs is unknown.</p><p>The pNML learner <ref type="bibr" target="#b7">(Fogel and Feder, 2018)</ref> was proposed as the min-max solution of the regret, where the minimum is over the learner choice and the maximum is for any possible test label value. Intuitively, the pNML assigns a probability for a potential outcome as follows: Add the test sample to the training set with an arbitrary label, find the ERM solution of this new set, and take the probability it gives to the assumed label. Follow this procedure for every label and normalize to get a valid probability assignment. The pNML was developed before for linear regression <ref type="bibr" target="#b4">(Bibas et al., 2019b)</ref> and was evaluated empirically for DNN <ref type="bibr" target="#b8">(Fu and Levine, 2021;</ref><ref type="bibr" target="#b3">Bibas et al., 2019a)</ref>.</p><p>We derive an analytical solution of the pNML learner and its generalization error (the regret) for a single layer NN. We analyze the derived regret and show it obtains low values when the test input either (i) lies in a subspace spanned by the eigenvectors associated with the large eigenvalues of the training data empirical correlation matrix or (ii) is located far from the decision boundary. Crucially, although our analysis focuses on a single layer NN, our results are applicable to the last layer of DNNs without changing the network architecture or the training process: We treat the pretrained DNN as a feature extractor with the last layer as a single layer NN classifier. We can therefore show the usage of the pNML regret as a confidence score for the OOD detection task.</p><p>To summarize, we make the following contributions.</p><p>1. Analytical derivation of the pNML regret. We derive an analytical expression of the pNML regret, which is associated with the generalization error, for a single layer NN.</p><p>2. Analyzing the pNML regret. We explore the pNML regret characteristics as a function of the test sample data, training data, and the corresponding ERM prediction. We provide a visualization on low dimensional data and demonstrate the situations in which the pNML regret is low and the prediction can be trusted.</p><p>3. DNN adaptation. We propose an adaptation of the derived pNML regret to any pretrained DNN that uses the softmax function with neither additional parameters nor extra data.</p><p>Applying our pNML regret to a pretrained DNN does not require additional data, it is efficient and can be easily implemented. The derived regret is theoretically justified for OOD detection since it is the individual setting solution for which we do not require any knowledge on the test input distribution. Our evaluation includes 74 IND-OOD detection benchmarks using DenseNet-BC-100, ResNet-34, and WideResNet-40 trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30. Our approach outperforms leading methods in nearly all 74 OOD detection benchmarks up to a remarkable +15.2%</p><p>2 Related work OOD detection. Many recent work use additional OOD set in training <ref type="bibr" target="#b22">(Malinin and Gales, 2018;</ref><ref type="bibr" target="#b12">Hendrycks et al., 2019a;</ref><ref type="bibr" target="#b25">Nandy et al., 2020;</ref><ref type="bibr" target="#b24">Mohseni et al., 2020)</ref>. A well-known method for OOD detection, named ODIN <ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, manipulates the input image based on the gradient of the loss function. This input manipulation increases the margin between the maximum probability of images with known classes and images with unknown objects. However, the manipulation strength is defined by OOD data, which might be unavailable or different from the OOD samples in inference.</p><p>Different approaches propose to manipulate the loss function in the training phase. <ref type="bibr" target="#b19">Lee et al. (2018)</ref> proposed to use the Mahalanobis distance between the test input representations and the class conditional distribution in the intermediate layers to train a logistic regression classifier to determine if the input sample is OOD. The Energy method <ref type="bibr" target="#b21">(Liu et al., 2020)</ref> adds a cost function to the training phase to shape the energy surface explicitly for OOD detection. <ref type="bibr" target="#b28">Papadopoulos et al. (2021)</ref> suggested the outlier exposure with confidence control (OECC) method in which a regularization term is added to the loss function such that the model produces a uniform distribution for OOD samples.</p><p>All mentioned methods require manipulating the DNN architecture or adding additional data. The Baseline method <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref> is one of the earliest approaches designed to identify whether a test input is IND and does not require changing the pretrained model. This method uses the maximum probability of the DNN output as the OOD score. The Gram method <ref type="bibr" target="#b33">(Sastry and Oore, 2020)</ref>  Other work suggested using Bayesian techniques to estimate the prediction confidence <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b18">Lakshminarayanan et al., 2017;</ref><ref type="bibr" target="#b35">van Amersfoort et al., 2020)</ref>. However, the Bayesian techniques add extensive compute overhead to the prediction.</p><p>The pNML learner. The pNML learner is the min-max solution of the supervised batch learning in the individual setting <ref type="bibr" target="#b7">(Fogel and Feder, 2018)</ref>. For sequential prediction it is termed the conditional normalized maximum likelihood <ref type="bibr" target="#b30">(Rissanen and Roos, 2007;</ref><ref type="bibr" target="#b31">Roos and Rissanen, 2008)</ref>.</p><p>Several methods deal with obtaining the pNML learner for different hypothesis sets. <ref type="bibr" target="#b4">Bibas et al. (2019b)</ref> and <ref type="bibr" target="#b2">Bibas and Feder (2021)</ref> showed the pNML solution for linear regression. <ref type="bibr" target="#b32">Rosas et al. (2020)</ref> proposed an NML based decision strategy for supervised classification problems and showed it attains heuristic PAC learning. <ref type="bibr" target="#b8">Fu and Levine (2021)</ref> used the pNML for model optimization based on learning a density function by discretizing the space and fitting a distinct model for each value.</p><p>For the DNN hypothesis set, <ref type="bibr" target="#b3">Bibas et al. (2019a)</ref> estimated the pNML distribution with DNN by fine-tuning the last layers of the network for every test input and label combination. This approach is computationally expensive since training is needed for every test input. <ref type="bibr" target="#b44">Zhou and Levine (2020)</ref> suggested a way to accelerate the pNML computation in DNN by using approximate Bayesian inference techniques to produce a tractable approximation to the pNML. <ref type="bibr" target="#b29">Pesso et al. (2021)</ref> used the pNML with adversarial target attack as a defense mechanism against adversarial perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notation and preliminaries</head><p>In the supervised machine learning scenario, a training set consisting of N pairs of examples is given</p><formula xml:id="formula_0">D N = {(x n , y n )} N n=1 , x n ? X , y n ? Y,<label>(1)</label></formula><p>where x n is the n-th data instance and y n is its corresponding label. The goal of a learner is to predict the unknown test label y given a new test data x by assigning probability distribution q(?|x) to the unknown label. The performance is evaluated using the log-loss function</p><formula xml:id="formula_1">(q; x, y) = ? log q(y|x).<label>(2)</label></formula><p>For the problem to be well-posed, we must make further assumptions on the class of possible models or hypothesis set that is used to find the relation between x and y. Denote ? as a general index set, this class is a set of conditional probability distributions</p><formula xml:id="formula_2">P ? = {p ? (y|x), ? ? ?}.<label>(3)</label></formula><p>The ERM is the learner from the hypothesis set that attains the minimal log-loss on the training set.</p><p>The individual setting. An additional assumption required to solve the problem is related to how the data and the labels are generated. In this work we consider the individual setting <ref type="bibr" target="#b23">(Merhav and Feder, 1998;</ref><ref type="bibr" target="#b7">Fogel and Feder, 2018;</ref><ref type="bibr">Bibas et al., 2019b,a)</ref>, where the data and labels, both in the training and test, are specific individual quantities: We do not assume any probabilistic relationship between them, the labels may even be assigned in an adversarial manner.</p><p>The genie. In the individual setting the goal is to compete with a reference learner, a genie, that has to following properties: (i) knows the test label value, (ii) is restricted to use a model from the given hypotheses set P ? , and (iii) does not know which of the samples is the test. This reference learner then chooses a model that attains the minimum loss over the training set and the test sampl?</p><formula xml:id="formula_3">?(D N ; x, y) = arg min ??? (p ? ; x, y) + N n=1 (p ? ; x n , y n ) .<label>(4)</label></formula><p>The regret is the log-loss difference between a learner q and this genie:</p><formula xml:id="formula_4">R(q; D N ; x, y) = ? log q(y|x) ? ? log p? (D N ;x,y) (y|x) .<label>(5)</label></formula><p>Theorem 1 <ref type="bibr" target="#b7">(Fogel and Feder (2018)</ref>). The universal learner, denoted as the pNML, minimizes the regret for the worst case test label</p><formula xml:id="formula_5">? = R * (D N , x) = min q max y?Y R(q; D N ; x, y).<label>(6)</label></formula><p>The pNML probability assignment and regret are</p><formula xml:id="formula_6">q pNML (y|x) = p? (D N ;x,y) (y|x) y ?Y p? (D N ;x,y ) (y |x) , ? = log y ?Y p? (D N ;x,y ) (y |x).<label>(7)</label></formula><p>Proof. The regret is equal for all choices of y. If we consider a different probability assignment, it should assign a smaller probability for at least one of the outcomes. If the true label is one of those outcomes it will lead to a higher regret. For more information see <ref type="bibr" target="#b7">Fogel and Feder (2018)</ref>.</p><p>The pNML regret is associated with the model complexity <ref type="bibr" target="#b42">(Zhang, 2012)</ref>. This complexity measure formalizes the intuition that a model that fits almost every data pattern very well would be much more complex than a model that provides a relatively good fit to a small set of data. Thus, the pNML incorporates a trade-off between goodness of fit and model complexity as measured by the regret.</p><p>Online update of a neural network. Let X N and Y N be the data and label matrices of N training points respectively</p><formula xml:id="formula_7">X N = [x 1 x 2 . . . x N ] ? R N ?M , Y N = [y 1 y 2 . . . y N ] ? R N ?C ,<label>(8)</label></formula><p>such that the number of input features and model outputs are M and C respectively. Denote X + N as the Moore-Penrose inverse of the data matrix</p><formula xml:id="formula_8">X + N = (X N X N ) ?1 X N Rank(X N X N ) = M X N (X N X N ) ?1 otherwise,<label>(9)</label></formula><p>f (?) and f ?1 (?) as the activation and inverse activation functions, and ? ? R M ?C as the learnable parameters. The ERM, that minimizes the training set mean squared error (MSE), i?</p><formula xml:id="formula_9">? N = X + N f ?1 (Y N ) = arg min ? ||Y N ? f (X N ?)|| 2 2 .<label>(10)</label></formula><p>Recently, <ref type="bibr" target="#b45">Zhuang et al. (2020)</ref> suggested a recursive formulation of the DNN weights. Using their scheme only one training sample is processed at a time: Denote the projection of a sample x onto the orthogonal subspace of the training set correlation matrix as</p><formula xml:id="formula_10">x ? = I ? X + N X N x,<label>(11)</label></formula><p>the update rule when receiving a new training sample with data x with label y i?</p><formula xml:id="formula_11">?(D N ; x, y) =? N + g f ?1 (y) ? x ? N , g 1 x ? 2 x ? x ? = 0 1 1+x X + N X + N x X + N X + N x, x ? = 0 .<label>(12)</label></formula><p>In their paper, <ref type="bibr" target="#b45">Zhuang et al. (2020)</ref> applied this formulation for a DNN, layer by layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The pNML for a single layer NN</head><p>Intuitively, the pNML as stated in <ref type="formula" target="#formula_6">(7)</ref> can be described as follows: To assign a probability for a potential outcome, (i) add it to the training set with an arbitrary label, (ii) find the best-suited model, and (iii) take the probability it gives to the assumed label. Follow this procedure for every label and normalize to get a valid probability assignment. Use the log normalization factor as the confidence measure. This method can be extended to any general learning procedure that generates a prediction based on a training set. One such method is a single layer NN.</p><p>A single layer NN maps an input x ? R M ?1 using the softmax function to a probability vector which represents the probability assignment to one of C classes</p><formula xml:id="formula_12">p ? (i|x) = f (x ?) i = e ? i x C j=1 e ? j x , i ? {1, . . . , C}.<label>(13)</label></formula><p>To align with the recursive formulation of (12), the label y is a one-hot vector with C elements and is represented by a row vector in the standard base {e i } C i=1 (the i-th element is 1 and all others are 0). c stands for the true test label such that e c is the one-hot vector of the true label. Also, the learnable</p><formula xml:id="formula_13">parameters {? i } C i=1</formula><p>are the columns of the parameter matrix of (10). We would like that the probability assignment of the true label to be 1. We define the following MSE minimization objective</p><formula xml:id="formula_14">min ??? N n=1 1 ? y n f ? x n 2 .<label>(14)</label></formula><p>Minimizing this MSE objective is equivalent to constrain the genie to the following Gaussian family</p><formula xml:id="formula_15">P ? = p ? (y|x) = 1 ? 2?? 2 exp ? 1 2? 2 e c y ? f (x ?) 2 , ? ? R M ?C<label>(15)</label></formula><p>and minimizing the log loss with respect to it. More details are in appendix A. The inverse of the activation function is</p><formula xml:id="formula_16">z f ?1 (p ? (i|x)) = ln p ? (i|x) + ln C j=1 e ? j x .<label>(16)</label></formula><p>To compute the genie, we assume that the label of the test sample is known and add it to the training set. We fit the model by optimizing the learnable parameters to minimize the loss of this new set. Lemma 1. Given test data x with label y = e c , the genie prediction of the true label is</p><formula xml:id="formula_17">p? (D N ;x,y=ec) (c|x) = p c p c + p x g c (1 ? p c ) ,<label>(17)</label></formula><p>where p c is the probability assignment of the ERM model of the label c, and g is as defined in <ref type="formula" target="#formula_0">(12)</ref>.</p><p>Proof. Using <ref type="formula" target="#formula_0">(12)</ref>, the probability assignment of the genie can be written as follows.</p><formula xml:id="formula_18">p? (D N ;x,y=ec) (c|x) = e? (D N ;x,y=ec) x C j=1 j =c e ? j x + e? (D N ;x,y=ec) x = e x [?c+g(z?? c x)] C j=1 e ? j x ? e ? c x + e x [?c+g(z?? c x)]</formula><p>.</p><p>(18) The genie knows the true test label e c . The inverse activation function can be written as z = ln S where S C j=1 e ? j x . The simplified numerator is</p><formula xml:id="formula_19">e ? c x e x g(z?? c x) = e ? c x Se ?? c x x g = Sp ?x g c p c .<label>(19)</label></formula><p>Substituting to (18) and divide the numerator and denominator by S provides the result.</p><p>The true test label is not available to a legit learner. Therefore, in the pNML process, every possible label is taken into account. The pNML regret is the logarithm of the sum of models' prediction, each one trained with a different test label value.</p><p>Theorem 2. Denote p i as the ERM prediction of label i, the pNML regret of a single layer NN is</p><formula xml:id="formula_20">? = log C i=1 p i p i + p x g i (1 ? p i )</formula><p>.</p><p>(20)</p><p>Proof. The normalization factor is the sum of the probabilities assignment of models that were trained with a specific value of the test sample K = C i=1 p? (D N ;x,y=ei) (i|x). Using Theorem 1, the log normalization factor is the pNML regret. With lemma 1, we get the explicit expression.</p><p>The pNML probability assignment of label i ? {1, . . . , C} is the probability assignment of a model that was trained with that label divided by the normalization factor q pNML (i|x) = 1 K p? (D N ;x,y) (y|x). Let u m and h m be the m-th eigenvector and eigenvalue of the training set data matrix X N such that for x ? = 0, the quantity x g is</p><formula xml:id="formula_21">x g = x X + N X + N x 1 + x X + N X + N x = 1 N M m=1 1 h 2 m x u m 2 1 + 1 N M i=1 1 h 2 m (x u m ) 2 .<label>(21)</label></formula><p>We make the following remarks.</p><p>1. If the test sample x lies in the subspace spanned by the eigenvectors with large eigenvalues, x g is small and the corresponding regret is low lim</p><formula xml:id="formula_22">x g? ?0 ? = log C i=1 p i = 0.</formula><p>In this case, the pNML prediction is similar to the genie and can be trusted.</p><p>2. Test input that resides is in the subspace that corresponds to the small eigenvalues produces</p><p>x g = 1 and a large regret is obtained lim</p><formula xml:id="formula_23">x g? ?1 ? = log C i=1 1 2?p 2 i .</formula><p>The prediction for this test sample cannot be trusted. In section 5 we show that in this situation the test sample can be classified as an OOD sample.</p><p>3. As the training set size (N ) increases x g becomes smaller and the regret decreases.</p><p>4. If the test sample is far from the decision boundary, the ERM assigns to one of the labels probability 1. In this case, the regret is 0 no matter in which subspace the test vector lies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The pNML regret characteristics using a low-dimensional dataset</head><p>We demonstrate the characteristics of the derived regret and show in what situations the prediction of the test sample can be trusted. To visualize the pNML regret on a low-dimensional dataset, we use the Iris flower data set <ref type="bibr" target="#b6">(Fisher, 1936)</ref>. We utilize two classes and two features and name them c 1 , c 2 , and feature 1, feature 2 respectively. <ref type="figure" target="#fig_0">Figure 1a</ref> shows the ERM probability assignment of class c 2 for a single layer NN that was fitted to the training data, which are marked in red. At the top left and bottom right, the model predicts with high probability that a sample from these areas belongs to class c 1 and c 2 respectively. <ref type="figure" target="#fig_0">Figure 1b</ref> presents the analytical pNML regret. At the upper left and lower right, the regret is low: Although there are no training samples there, these regions are far from the decision boundary, adding one sample would not alter the probability assignment significantly, thus the pNML prediction is close to the genie. At the top right and bottom left, there are no training points therefore the regret is relatively high and the confidence in the prediction is low. In section 5, we show that these test samples, which are associated with high regret, can be classified as OOD samples.</p><p>In addition, we visualize the regret for overlapping classes. In <ref type="figure" target="#fig_0">figure 1c</ref>, the ERM probability assignment for inseparable class split is shown. The ERM probability is lower than 0.7 for all test feature values. <ref type="figure" target="#fig_0">Figure 1d</ref> presents the corresponding pNML regret. The pNML regret is small in the training data surroundings (including the mixed label area). The regret is large in areas where the training data is absent, as in the figure edges.   <ref type="bibr" target="#b6">(Fisher, 1936)</ref>). (a) and (c) show the ERM probability assignment of class c 2 for a separable split and inseparable split respectively. The derived pNML regret for the separable split is shown in (b) and for the inseparable split is shown in (d). The training data of class c 1 and c 2 are marked with red circles and red crosses respectively. Low regret is associated with the training data surroundings. See section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep neural network adaptation</head><p>In previous sections, we derived the pNML for a single layer NN. We next show that our derivations can, in fact, be applied to any pretrained NN, without requiring additional parameters or extra data.</p><p>First extract the embeddings of the training set: Denote ?(?) as the embedding creation (feature extraction) using a pretrained ERM model, propagate the training samples through the DNN up to the last layer and compute the inverse of the data matrix ?(X N ) + ?(X N ) + . Then, given a specific test example x, extract its embedding ?(x) and its ERM probability assignment {p i } C i=1 . Finally calculate the regret as described in Theorem 1 using the training set and test embedding vectors.</p><p>We empirically found that norms of OOD embeddings are lower than those of IND samples. The regret depends on the norm of the test sample: For 0 &lt; a &lt; b, the regret of ax is lower than the regret of bx. Hence, we normalize all embeddings (training, IND, and OOD) to have L 2 norms equal to 1.0.</p><p>Samples with a high regret value are considered samples with a large distance from the genie, the learner that knows the true label, and therefore the prediction cannot be trusted. Our proposed method utilizes this regret value to determine whether a test data item represents a known or an unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application to out-of-distribution detection</head><p>We rigorously test the effectiveness of the pNML regret for OOD detection 1 . The motivation for using the individual setting and the pNML as its solution for OOD detection is that in the individual setting there is no assumption on the way the data is generated. The absence of assumption means that the result holds for a wide range of scenarios (PAC, stochastic, and even adversary) and specifically to OOD detection, where the OOD samples are drawn from an unknown distribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>We follow the standard experimental setup <ref type="bibr" target="#b21">(Liu et al., 2020;</ref><ref type="bibr" target="#b33">Sastry and Oore, 2020;</ref><ref type="bibr" target="#b19">Lee et al., 2018)</ref>. All the assets we used are open-sourced with either Apache-2.0 License or Attribution-NonCommercial 4.0 International licenses. We ran all experiments on NVIDIA K80 GPU.</p><p>IND sets. For datasets that represent known classes, we use CIFAR-100, CIFAR-10 <ref type="bibr" target="#b17">(Krizhevsky et al., 2014)</ref> and SVHN <ref type="bibr" target="#b26">(Netzer et al., 2011)</ref>. These sets contain RGB images with 32x32 pixels. In addition, to evaluate higher resolution images, we use ImageNet-30 set <ref type="bibr" target="#b13">(Hendrycks et al., 2019b)</ref>.</p><p>OOD sets. The OOD sets are represented by TinyImageNet <ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, LSUN <ref type="bibr" target="#b40">(Yu et al., 2015)</ref>, iSUN <ref type="bibr" target="#b39">(Xu et al., 2015)</ref>, Uniform noise images, and Gaussian noise images. We use two variants of TinyImageNet and LSUN sets: a 32x32 image crop that is represented by "(C)" and a resizing of the images to 32x32 pixels that termed by "(R)". We also used CIFAR-100, CIFAR-10, and SVHN as OOD for models that were not trained with them.</p><p>Evaluation methodology. We benchmark our approach by adopting the following metrics <ref type="bibr" target="#b33">(Sastry and Oore, 2020;</ref><ref type="bibr" target="#b19">Lee et al., 2018)</ref>: (i) AUROC: The area under the receiver operating characteristic curve of a threshold-based detector. A perfect detector corresponds to an AUROC score of 100%. (ii) TNR at 95% TPR: The probability that an OOD sample is correctly identified (classified as negative) when the true positive rate equals 95%. (iii) Detection accuracy: Measures the maximum possible classification accuracy over all possible thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We build upon the existing leading methods: Baseline <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref>, ODIN <ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, Gram <ref type="bibr" target="#b33">(Sastry and Oore, 2020)</ref>, OECC <ref type="bibr" target="#b28">(Papadopoulos et al., 2021)</ref>, and Energy <ref type="bibr" target="#b21">(Liu et al., 2020)</ref>. We use the following pretrained models: ResNet-34 <ref type="bibr" target="#b10">(He et al., 2016)</ref>, DenseNet-BC- 100 <ref type="bibr" target="#b14">(Huang et al., 2017)</ref> and WideResNet-40 <ref type="bibr" target="#b41">(Zagoruyko and Komodakis, 2016)</ref>. Training was performed using CIFAR-100, CIFAR-10 and SVHN, each training set used separately to provide a complete picture of our proposed method's capabilities. Notice that ODIN, OECC, and Energy methods use OOD sets during training and the Gram method requires IND validation samples.  <ref type="table" target="#tab_5">Table 3</ref> shows the results of the Energy method and our method when combined with the pretrained Energy model (Energy+pNML) with WideResNet-40 model on CIFAR-100 and CIFAR-10 as IND sets. Evidently, our method improves the AUROC of the OOD detection task in 14 out of 16 IND-OOD combinations. The most significant improvement is in CIFAR-100 as IND and ImageNet (R) and iSUN as the OOD sets. In these sets, we improve the AUROC by 15.6% and 15.2% respectively. For TNR at TPR 95%, the pNML regret enhances the CIFAR-100 and Gaussian combination by 90.4% and achieves a perfect separation of IND-OOD samples.</p><p>For high resolution images, we use Resnet-18 and ResNet-101 models trained on ImageNet. We utilize the ImageNet-30 training set for computing ?(X N ) + ?(X N ) + . All images were resized to 254 ? 254 pixels. We compare the result to the Baseline method in <ref type="table" target="#tab_6">Table 4</ref>. The table shows that the pNML outperforms Baseline by up to 9.8% and 8.23% for ResNet-18 and ResNet-101 respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We derived the analytical expression of the pNML regret for a single layer NN. We showed that the model generalizes well when the test data resides in either a subspace spanned by the eigenvectors associated with the large eigenvalues of the training data correlation matrix or far from the decision boundary. We showed how to apply the pNML for any pretrained DNN that uses the softmax layer with neither additional parameters nor extra data. We demonstrated the effectiveness of our pNML regret-based approach on 74 IND-OOD detection benchmarks. Compared with the recent state of the art methods, our approach elevates absolute AUROC values by up to 15.6%</p><p>For future work, in our regret derivation we constrained the genie hypothesis set to the Gaussian family and a single layer NN. We would like to extend the derivation to larger hypothesis sets and more layers, believing it would improve performance. Furthermore, the pNML regret can be used for additional tasks such as active learning, probability calibration, and adversarial attack detection.</p><p>Societal impacts. The negative social impact of this work depends on the application. For instance, in a surveillance camera scenario, a person from a minority group can be flagged as OOD if the minority group was not included in the training set. We recommend that when OOD is flagged a human will intervene rather than an algorithmic response since the nature of the OOD is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MSE minimization is equivalent to log-loss minimization</head><p>We use the same notations as in section 4.</p><p>Denote e c as a one-hot row vector of the true label, we define the hypothesis set that genie is allowed to choose from as</p><formula xml:id="formula_24">P ? = p ? (y|x) = 1 ? 2?? 2 exp ? 1 2? 2 y ? f (x n ?) e c 2 .</formula><p>(1)</p><p>The genie chooses the learner from the hypothesis set that minimizes the log-loss. Let x n ? R M ?1 be the n-th data with the label c n ? {1, 2, . . . , C}, y n be a row vector where y ncn is its c n element. We show that the log-loss minimizer of this hypothesis set is equal to the MSE minimizer:</p><p>arg min</p><formula xml:id="formula_25">??R M ?C (p ? , X N , Y N ) = arg min ??R M ?C ? log N n=1 1 ? 2?? 2 exp ? 1 2? 2 y ncn ? f (x n ?) cn 2 = arg min ??R M ?C N n=1</formula><p>y ncn ? f (x n ?) cn 2 .</p><p>(2)</p><p>We know that the training set label are one-hot vector y n = e cn such that y ncn = 1:</p><p>arg min</p><formula xml:id="formula_26">??R M ?C (p ? , X N , Y N ) = arg min ??R M ?C N n=1 1 ? f (x n ?) cn 2 = arg min ??R M ?C N n=1 1 ? f (x n ?)e cn 2 2 = arg min ??R M ?C N n=1 1 ? y n f (x n ?) 2 2 (3)</formula><p>which is the MSE minimization objective we defined in section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Single layer NN pNML regret simulation</head><p>We simulate the response of the pNML regret for two classes (C=2) and divide it by log C to have the regret bounded between 0 and 1. <ref type="figure" target="#fig_0">Figure 1</ref> shows the regret behaviour for different p 1 (the ERM probability assignment of class 1) as a function of x g.</p><p>For an ERM model that is certain on the prediction (p 1 = 0.99 that is represented by the purple curve), a slight variation of x g causes a large response of the regret comparing to p 1 that equals 0.55 and 0.85. All curves converging to the maximal regret for x g greater than 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The spectrum of real dataset</head><p>We provide a visualization of the training data spectrum when propagated to the last layer of a DNN.</p><p>We feed the training data through the model up to the last layer to create the training embeddings. Next, we compute the correlation matrix of the training embeddings and perform an SVD decomposition. We plot the eigenvalues for different training sets in figure 2.  <ref type="figure" target="#fig_2">Figure 2a</ref> shows the eigenvalues of DenseNet-BC-100 model when ordered from the largest to smallest. For the SVHN training set, most of the energy is located in the first 50 eigenvalues and then there is a significant decrease of approximately 10 3 . The same phenomenon is also seen in figure 2a that shows the eigenvalues of ResNet-40 model. In our derived regret, if the test sample is located in the subspace that is associated with small eigenvalues (for example indices 50 or above for DenseNet trained with SVHN) then x g is large and so is the pNML regret.</p><p>For both DensNet and ResNet models, the values of the eigenvalues of CIFAR-100 seem to be spread more evenly compared to CIFAR-10, and the CIFAR-10 are more uniform than the SVHN. How much the eigenvalues are spread can indicate the variability of the set: SVHN is a set of digits that is much more constrained than CIFAR-100 which has 100 different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Gram vs. Gram+pNML</head><p>We further explore the benefit of the pNML regret in detecting OOD samples over the Gram approach. We focus on the DenseNet model with CIFAR-100 as the training set and LSUN (C) as the OOD set. <ref type="figure" target="#fig_3">Figure 3a</ref> shows the 2D histogram of the IND set based on the pNML regret values and Gram scores.</p><p>In addition, we plotted the best threshold for separating the IND and OOD of these sets. pNML regret values less than 0.0024 and Gram scores below 0.0017 qualify as IND samples by both the pNML and Gram scores. Gram and Gram+pNML do not succeed to classify 1205 and 891 out of a total 10,000 IND samples respectively. <ref type="figure" target="#fig_3">Figure 3b</ref> presents the 2D histogram of the LSUN (C) as OOD set. For regret values greater than 0.0024 and Gram score lower than 0.0017, the pNML succeeds to classify as IND but the Gram fails: There are 473 samples that the pNML classifies as OOD but the Gram fails, in contrast to 76 samples classified as such by the Gram and not by the pNML regret. Most of the pNML improvement is in assigning a high score to OOD samples while there is not much change in the rank of the IND ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional out of distribution metrics</head><p>The additional OOD metrics, TNR at 95% FPR and Detection Accuracy, for the DensNet model are shown in <ref type="table" target="#tab_2">table 1 and table 2 respectively and for the ResNet are presented in table 3 and table 4</ref>. We improve the compared methods for all IND-OOD sets except for 6 experiments of ODIN method with the TNR at 95% metric. We show the TNR vs FPR of these experiments in <ref type="figure">figure 4</ref>. We state that for most of the TNR values, the pNML regret outperforms the ODIN method, as also shown in the AUROC metric.   <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref>, ODIN <ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, Gram (Sastry and Oore, 2020), and OECC <ref type="bibr" target="#b28">(Papadopoulos et al., 2021)</ref>   <ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref>, ODIN <ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, Gram <ref type="bibr" target="#b33">(Sastry and Oore, 2020)</ref>, and OECC <ref type="bibr" target="#b28">(Papadopoulos et al., 2021)</ref>    <ref type="figure">Figure 4</ref>: The TNR as a function of the TPR of IND-OOD sets for which the ODIN method is better than the pNML at TPR of 95%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A single layer NN fitted to low dimensional data (the Iris flower set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The pNML regret for a two class predictor. p 1 is the ERM prediction of class c 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The spectrum of the training embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>2D histogram of the pNML regret and the Gram score of a DenseNet model trained with CIFAR-100 as IND set and LSUN (C) as OOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>DenseNet CIFAR-10 LSUN (R)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>OOD detection for DenseNet-BC-100 model, comparing our pNML-based approach to leading methods. Results reported using AUROC show our method enhances previous work up to 69.4%, 49.4%, 3.1%, and 2.2%. See section 5.2 for more details.</figDesc><table><row><cell>IND</cell><cell>OOD</cell><cell cols="4">Baseline/+pNML ODIN/+pNML Gram/+pNML OECC/+pNML</cell></row><row><cell></cell><cell>iSUN</cell><cell>69.7 / 96.4</cell><cell>84.5 / 96.7</cell><cell>99.0 / 99.5</cell><cell>99.2 / 99.5</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>70.8 / 96.6</cell><cell>86.0 / 96.9</cell><cell>99.3 / 99.7</cell><cell>99.4 / 99.6</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>80.1 / 93.1</cell><cell>91.5 / 93.1</cell><cell>91.4 / 94.5</cell><cell>93.9 / 96.1</cell></row><row><cell>CIFAR-100</cell><cell>Imagenet (R) Imagenet (C)</cell><cell>71.6 / 97.4 76.2 / 95.7</cell><cell>85.5 / 97.6 88.8 / 96.0</cell><cell>99.0 / 99.5 97.7 / 98.7</cell><cell>99.0 / 99.5 98.2 / 99.0</cell></row><row><cell></cell><cell>Uniform</cell><cell>43.3 / 100</cell><cell>83.7 / 100</cell><cell>100 / 100</cell><cell>99.9 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>30.6 / 100</cell><cell>50.6 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>SVHN</cell><cell>82.6 / 96.2</cell><cell>92.5 / 96.2</cell><cell>97.3 / 98.4</cell><cell>97.0 / 97.5</cell></row><row><cell></cell><cell>iSUN</cell><cell>94.8 / 98.7</cell><cell>98.9 / 98.9</cell><cell>99.8 / 100</cell><cell>99.9 / 100</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>95.5 / 98.9</cell><cell>99.2 / 99.2</cell><cell>99.9 / 100</cell><cell>99.9 / 100</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>93.0 / 96.4</cell><cell>95.8 / 96.4</cell><cell>97.5 / 98.7</cell><cell>98.9 / 99.9</cell></row><row><cell>CIFAR-10</cell><cell>Imagenet (R) Imagenet (C)</cell><cell>94.1 / 98.8 93.8 / 97.7</cell><cell>98.5 / 99.0 97.6 / 97.9</cell><cell>99.7 / 99.9 99.3 / 99.7</cell><cell>99.8 / 99.9 99.5 / 99.9</cell></row><row><cell></cell><cell>Uniform</cell><cell>96.6 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>97.6 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>SVHN</cell><cell>89.9 / 98.4</cell><cell>94.6 / 98.7</cell><cell>99.1 / 99.6</cell><cell>99.6 / 100</cell></row><row><cell></cell><cell>iSUN</cell><cell>94.4 / 98.7</cell><cell>92.8 / 99.1</cell><cell>99.8 / 99.9</cell><cell>100 / 100</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>94.1 / 98.4</cell><cell>92.5 / 98.9</cell><cell>99.8 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>92.9 / 98.0</cell><cell>88.6 / 98.1</cell><cell>98.6 / 99.4</cell><cell>99.8 / 100</cell></row><row><cell></cell><cell>Imagenet (R)</cell><cell>94.8 / 98.6</cell><cell>93.3 / 99.0</cell><cell>99.7 / 99.9</cell><cell>100 / 100</cell></row><row><cell>SVHN</cell><cell>Imagenet (C)</cell><cell>94.6 / 98.6</cell><cell>92.8 / 98.8</cell><cell>99.4 / 99.8</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Uniform</cell><cell>93.2 / 99.8</cell><cell>91.6 / 100</cell><cell>99.9 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>97.4 / 99.8</cell><cell>98.9 / 99.9</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>91.8 / 96.7</cell><cell>88.9 / 97.8</cell><cell>95.4 / 97.3</cell><cell>99.5 / 100</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>91.4 / 96.7</cell><cell>88.2 / 97.8</cell><cell>96.4 / 98.0</cell><cell>99.6 / 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>A comparison of our pNML regret based detection to leading methods for ResNet-34 model. Results measured by the AUROC metric show that our technique offers significant improvements over previous work of up to 41.5%, 11.9%, 2.4%, and 2.1%. See section 5.2 for more details.</figDesc><table><row><cell>IND</cell><cell>OOD</cell><cell cols="4">Baseline/+pNML ODIN/+pNML Gram/+pNML OECC/+pNML</cell></row><row><cell></cell><cell>iSUN</cell><cell>75.7 / 83.0</cell><cell>85.6 / 87.6</cell><cell>98.8 / 99.1</cell><cell>99.0 / 99.3</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>75.6 / 83.8</cell><cell>85.4 / 88.0</cell><cell>99.2 / 99.4</cell><cell>99.3 / 99.6</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>75.5 / 83.1</cell><cell>82.6 / 88.1</cell><cell>92.2 / 94.6</cell><cell>95.7 / 97.8</cell></row><row><cell>CIFAR-100</cell><cell>Imagenet (R) Imagenet (C)</cell><cell>77.1 / 84.4 79.6 / 85.8</cell><cell>87.7 / 88.5 85.6 / 88.6</cell><cell>98.9 / 99.2 97.7 / 98.4</cell><cell>98.7 / 98.9 97.9 / 98.1</cell></row><row><cell></cell><cell>Uniform</cell><cell>85.2 / 98.1</cell><cell>99.0 / 99.4</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>45.0 / 86.5</cell><cell>83.8 / 95.7</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>SVHN</cell><cell>79.3 / 90.9</cell><cell>94.0 / 95.4</cell><cell>96.0 / 97.9</cell><cell>97.0 / 97.6</cell></row><row><cell></cell><cell>iSUN</cell><cell>91.0 / 96.4</cell><cell>94.0 / 97.5</cell><cell>99.8 / 100</cell><cell>99.9 / 99.9</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>91.1 / 96.6</cell><cell>94.1 / 97.7</cell><cell>99.9 / 100</cell><cell>100 / 99.9</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>91.8 / 95.4</cell><cell>93.6 / 95.6</cell><cell>97.9 / 99.1</cell><cell>99.1 / 99.5</cell></row><row><cell>CIFAR-10</cell><cell>Imagenet (R) Imagenet (C)</cell><cell>91.0 / 95.4 91.4 / 95.4</cell><cell>93.9 / 96.6 93.3 / 96.2</cell><cell>99.7 / 99.9 99.3 / 99.7</cell><cell>99.9 / 99.9 99.7 / 99.8</cell></row><row><cell></cell><cell>Uniform</cell><cell>96.1 / 99.8</cell><cell>99.9 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>97.5 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>SVHN</cell><cell>89.9 / 95.1</cell><cell>95.8 / 97.9</cell><cell>99.5 / 99.8</cell><cell>99.8 / 99.8</cell></row><row><cell></cell><cell>iSUN</cell><cell>92.2 / 97.1</cell><cell>91.4 / 98.0</cell><cell>99.8 / 99.9</cell><cell>100 / 100</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>91.5 / 96.7</cell><cell>90.6 / 97.7</cell><cell>99.8 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>92.8 / 97.0</cell><cell>92.3 / 97.1</cell><cell>98.8 / 99.6</cell><cell>99.7 / 99.9</cell></row><row><cell></cell><cell>Imagenet (R)</cell><cell>93.5 / 97.5</cell><cell>92.8 / 98.3</cell><cell>99.8 / 99.9</cell><cell>100 / 100</cell></row><row><cell>SVHN</cell><cell>Imagenet (C)</cell><cell>94.2 / 97.5</cell><cell>93.7 / 98.2</cell><cell>99.5 / 99.9</cell><cell>99.9 / 100</cell></row><row><cell></cell><cell>Uniform</cell><cell>96.0 / 98.5</cell><cell>95.5 / 99.5</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>Gaussian</cell><cell>96.1 / 98.4</cell><cell>96.1 / 99.6</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>93.0 / 97.4</cell><cell>92.0 / 98.0</cell><cell>97.4 / 99.3</cell><cell>99.4 / 99.8</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>92.5 / 97.1</cell><cell>91.7 / 97.8</cell><cell>97.5 / 99.2</cell><cell>99.4 / 99.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 and</head><label>1</label><figDesc>Table 2show the AUROC of different OOD sets for DenseNet and ResNet models respectively. Our approach improves all the compared methods in nearly all combinations of IND-OOD sets. The largest AUROC gain over the current state-of-the-art is of CIFAR-100 as IND and LSUN (C) as OOD: For the DenseNet model, we improve Gram and OECC method by 3.1% and 2.2% respectively. For the ResNet model, we improve this combination by 2.4% and 2.1% respectively. The additional metrics (TNR at 95% FPR and detection accuracy) are shown in appendix E.The Baseline method uses a pretrained ERM model with no extra data. Combining the pNML regret with the standard ERM model as shown in the Baseline+pNML column surpasses Baseline by up to 69.4% and 41.5% for DensNet and ResNet, respectively. Also, Baseline+pNML is comparable to the more sophisticated methods: Although it lacks tunable parameters and does not use extra data, Baseline+pNML outperforms ODIN in most DenseNet IND-OOD set combinations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>OOD detection for WideResNet-40 model. Results show our method improves the Energy<ref type="bibr" target="#b21">(Liu et al., 2020</ref>) method up to 15.6%, 90.4%, and 15.9% for AUROC, TNR at TPR 95%, and Detection accuracy respectively. See section 5.2 for more details.</figDesc><table><row><cell>IND</cell><cell>OOD</cell><cell>AUROC</cell><cell cols="2">TNR at TPR 95% Detection Acc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Energy/+pNML</cell></row><row><cell></cell><cell>iSUN</cell><cell>78.4 / 93.6</cell><cell>30.7 / 62.5</cell><cell>71.1 / 87.0</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>80.3 / 94.1</cell><cell>31.2 / 65.5</cell><cell>73.1 / 87.5</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>95.9 / 95.5</cell><cell>80.0 / 79.3</cell><cell>89.3 / 89.1</cell></row><row><cell>CIFAR-100</cell><cell cols="2">Imagenet (R) 71.4 / 87.0 Imagenet (C) 79.7 / 87.3</cell><cell>22.1 / 44.8 36.9 / 49.4</cell><cell>66.1 / 79.9 72.8 / 79.7</cell></row><row><cell></cell><cell>Uniform</cell><cell>97.9 / 99.8</cell><cell>95.2 / 100</cell><cell>95.8 / 99.6</cell></row><row><cell></cell><cell>Gaussian</cell><cell>92.0 / 99.8</cell><cell>9.6 / 100</cell><cell>92.3 / 99.8</cell></row><row><cell></cell><cell>SVHN</cell><cell>96.5 / 96.4</cell><cell>79.2 / 82.8</cell><cell>90.5 / 91.3</cell></row><row><cell></cell><cell>iSUN</cell><cell>99.3 / 99.4</cell><cell>98.3 / 98.7</cell><cell>96.7 / 97.0</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>99.3 / 99.5</cell><cell>98.6 / 99.0</cell><cell>97.0 / 97.3</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>99.4 / 99.5</cell><cell>98.6 / 98.6</cell><cell>97.0 / 97.1</cell></row><row><cell>CIFAR-10</cell><cell cols="2">Imagenet (R) 98.1 / 98.1 Imagenet (C) 98.6 / 98.6</cell><cell>92.0 / 92.4 94.4 / 94.6</cell><cell>94.0 / 94.0 94.9 / 94.9</cell></row><row><cell></cell><cell>Uniform</cell><cell>99.0 / 99.9</cell><cell>100 / 100</cell><cell>98.7 / 99.8</cell></row><row><cell></cell><cell>Gaussian</cell><cell>99.1 / 99.9</cell><cell>100 / 100</cell><cell>98.7 / 99.8</cell></row><row><cell></cell><cell>SVHN</cell><cell>99.3 / 99.6</cell><cell>98.3 / 98.9</cell><cell>96.9 / 97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>OOD detection with ImageNet-30 as the IND set. Results show that the pNML outperforms the Baseline<ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017</ref>) method up to 9.8% and 8.23% for ResNet-18 and ResNet-101 models respectively. See section 5.2 for more details.</figDesc><table><row><cell>IND</cell><cell>OOD</cell><cell>ResNet-18</cell><cell>ReseNet-101</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline/+pNML Baseline/+pNML</cell></row><row><cell></cell><cell>iSUN</cell><cell>95.58 / 99.74</cell><cell>96.26 / 99.54</cell></row><row><cell></cell><cell>LSUN (R)</cell><cell>95.51 / 99.72</cell><cell>95.77 / 99.43</cell></row><row><cell></cell><cell>LSUN (C)</cell><cell>96.89 / 99.77</cell><cell>98.00 / 99.86</cell></row><row><cell>ImageNet-30</cell><cell>Uniform Gaussian</cell><cell>99.35 / 99.99 98.78 / 100</cell><cell>98.70 / 100 98.61 / 100</cell></row><row><cell></cell><cell>SVHN</cell><cell>99.18 / 99.99</cell><cell>98.94 / 99.98</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>89.99 / 99.79</cell><cell>91.24 / 99.47</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>92.15 / 92.15</cell><cell>93.39 / 99.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1 :</head><label>1</label><figDesc>DenseNet-BC-100 model TNR at TPR95% comparison. The compared methods are Baseline</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>DenseNet-BC-100 model Detection Acc. comparison. The compared methods are Baseline</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>ResNet-34 model Detection Acc. comparison. The compared methods are Baseline<ref type="bibr" target="#b11">(Hendrycks and Gimpel, 2017)</ref>, ODIN<ref type="bibr" target="#b20">(Liang et al., 2018)</ref>, Gram<ref type="bibr" target="#b33">(Sastry and Oore, 2020)</ref>, and OECC<ref type="bibr" target="#b28">(Papadopoulos et al., 2021)</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Uniform</cell><cell></cell><cell>92.9 / 95.7</cell><cell>92.3 / 97.4</cell><cell>99.9 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Gaussian</cell><cell></cell><cell>92.9 / 95.4</cell><cell>93.0 / 97.5</cell><cell>100 / 100</cell><cell>100 / 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell>90.0 / 93.1</cell><cell>89.4 / 93.4</cell><cell>92.2 / 96.2</cell><cell>96.9 / 98.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell>89.6 / 92.5</cell><cell>89.0 / 93.1</cell><cell>92.4 / 96.1</cell><cell>97.0 / 98.5</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TNR</cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0 0.2</cell><cell>ODIN ODIN+pNML TPR=95%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>TPR</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available in https://github.com/kobybibas/pnml_ood_detection</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="6240" to="6249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The predictive normalized maximum likelihood for overparameterized linear regression with norm constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07181</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Regret and double descent. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12286</idno>
		<title level="m">Deep pnml: Predictive normalized maximum likelihood for deep neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new look at an old problem: A universal learning approach to linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Symp. on Information Theory</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning rotation invariant features for cryogenic electron microscopy image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss-Dicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal batch learning with log-loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symp. on Information Theory</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Offline model-based optimization via normalized maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Balancing specialization, generalization, and compression for detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chertok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-ofdistribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Energy-based out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2124" to="2147" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised learning for generalizable out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B S</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards maximizing the representation gap between in-domain &amp; out-of-distribution examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Inform. Process. Syst. Workshops</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A pac-bayesian approach to spectrallynormalized margin bounds for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Outlier exposure with confidence control for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-A</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Rajati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Utilizing adversarial targeted attacks to boost adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional nml universal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Theory and Applications Workshop</title>
		<imprint>
			<biblScope unit="page" from="337" to="341" />
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On sequentially normalized maximum likelihood models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compare</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning, compression, and leakage: Minimizing classification error via meta-universal compression principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gastpar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07382</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting out-of-distribution examples with Gram matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Uncertainty aware and explainable diagnosis of retinal disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2021: Imaging Informatics for Healthcare</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>and Applications</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measures of complexity</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Out-ofdistribution detection using an ensemble of self supervised leave-out classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="560" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Safety concerns and mitigation approaches regarding the use of deep learning in safety-critical perception tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Willers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raafatnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abrecht</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Safety, Reliability, and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="336" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06755</idno>
		<title level="m">Turkergaze: Crowdsourcing saliency with webcam based eye tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Model selection with informative normalized maximum likelihood: Data prior and model prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Descriptive and normative approaches to human behavior</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="303" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovery guarantees for one-hidden-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4140" to="4149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02696</idno>
		<title level="m">Amortized conditional normalized maximum likelihood</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training a multilayer network with low-memory kernel-and-range projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="522" to="550" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
