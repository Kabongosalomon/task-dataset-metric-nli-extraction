<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Range Graph Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Parviz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">NJIT</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Universit? de Montr?al Guy Wolf Mila</orgName>
								<orgName type="institution" key="instit2">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Valence Discovery</orgName>
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Range Graph Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) 1 with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP-GNNs and Graph Transformer architectures that are intended to capture LRI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Considering a graph as a collection of nodes where arbitrary relations between nodes are represented as edges, there are numerous real-world instances with data structures where complex and irregular interactions among objects can be represented as edges where the objects themselves are denoted as nodes. This has led to a rapid rise of interest in the development of graph neural networks (GNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b51">52]</ref> for deep learning on geometric and graph domains.</p><p>The popularly used class of GNNs is based on the message passing paradigm <ref type="bibr" target="#b19">[20]</ref> where a node's feature representation, at each layer, is updated using a trainable function that receives and combines feature information from all its neighboring nodes. A network that has L GNN layers, stacked sequentially, can iteratively aggregate feature information from up to L hops for the update of a node's representation. If a long-range information to a node from its L-hop neighbor is needed for a task (say, for a large L), the same number of GNN layers is ideally required. However, with the <ref type="table" target="#tab_7">Table 1</ref>: Overview of the datasets in the proposed LRGB. Note: 'Pixels+Coord' denotes the feature vector consisting of 12-dim statistics of each superpixel (for each RGB color: the average, standard deviation, maximum, and minimum of pixel intensities in the superpixel) and 2-dim coordinates of the center of mass of its X,Y pixel locations. 'Edge Weight' corresponds to the weight assigned between two superpixel nodes w.r.t. the construction method. The 'Atom Encoder' and 'Bond Encoder' are OGB molecular feature encoders <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. All tasks are inductive tasks.  increasing L the size of the L-hop neighborhood grows exponentially and so does the amount of information that needs to be encoded into one vector by the network. This leads to 'information oversquashing' as the message aggregation step continues to be iteratively applied at each layer in order to propagate the information <ref type="bibr" target="#b1">[2]</ref>. Consequently, such GNNs fail at capturing long-range dependencies as a significant amount of distant information may get lost due to the squashing.</p><p>In order to factor in distant information when message passing GNNs are used, <ref type="bibr" target="#b1">[2]</ref> used a fully connected graph at the final layer as an intuitive remedy. The primary rationale behind this approach is to enable each node in a graph to connect to every other node at some stage in the network to pass the information that otherwise would get squashed, thus breaking the bottleneck. Consequently, several recent works propose Graph Transformers that leverage full-connections among all nodes in the graph to capture long-range dependencies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b36">37]</ref>. However, it is often the case that these models are evaluated on datasets where the corresponding tasks primarily rely on local structural information rather than the distant information propagation between nodes. This observation is prevalent for many existing datasets such as ZINC <ref type="bibr" target="#b12">[13]</ref>, ogbg-molpcba, or ogbg-molhiv <ref type="bibr" target="#b23">[24]</ref> that are among the most frequently used benchmarks. The Spectral Attention Network (SAN) <ref type="bibr" target="#b29">[30]</ref> has Rshown insignificant contribution of full attention in these benchmarks. In fact, leaderboards of these benchmarks are topped by local MP-GNN based models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b31">32]</ref>, albeit these GNNs are non trivial extensions and are augmented with higher-order structural information, among other model improvements. At the same time, these molecular benchmarks largely consist of graphs of small sizes, i.e., the number of nodes in a graph. Nonetheless, on the contrary, graphs with large number of nodes may not necessarily imply that they require models with long-range dependencies for the learning task.</p><p>Contribution. In this work, we focus on these shortcomings of existing popular graph learning benchmark datasets and propose characterizing factors in a dataset that can be studied for the exploration of new GNN and Graph Transformer architectures that possess long-range interaction (LRI) capabilities. Note that our characterization henceforth is not directed at proposing 'provable LRI' benchmarks, which would often lead to toy datasets (that are useful for quick prototyping of ideas) such as the shortest path prediction task <ref type="bibr" target="#b46">[47]</ref> or the color connectivity dataset <ref type="bibr" target="#b41">[42]</ref> which rely on LRI. Instead, our aim is to propose real-world datasets that require LRI, and the factors we consider for a LRGB dataset characterization could be understood as implications which suggest that the learning task(s) in the graphs would depend on long range signal propagation. Consequently, we introduce 5 benchmarking datasets -PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct from the domains of Computer Vision and Chemistry which we incorporate in LRGB, see <ref type="table" target="#tab_1">Tables 1-2</ref> for an overview and <ref type="figure" target="#fig_0">Figure 1</ref> for a sample illustration. The learning tasks that we propose in these datasets depend on some degree of long-range signal handling given the nature of task, contribution of global graph structure to the task, and the sizes of graphs in these datasets. Fittingly, in our baseline experiments, these datasets show that the fully-connected models which enable LRI propagation perform considerably better than local message passing based GNNs.</p><p>Existing attempts towards LRI benchmarks. Thanks to the understanding of the limitations of message passing based GNNs <ref type="bibr" target="#b19">[20]</ref> with respect to the 1-Weisfeiler Leman (WL) isomorphism test <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b38">39]</ref> and the information oversquashing <ref type="bibr" target="#b1">[2]</ref>, there has been several developments of GNN and Graph Transformer architectures which have strictly greater representation power than 1-WL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">55]</ref>. By design, fully connected Graph Transformers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37</ref>] are able to model long-range dependencies in the graphs and alleviate information bottleneck to some extent <ref type="bibr" target="#b44">[45]</ref>. However, most of such architectures are evaluated on benchmarks where it is not clear whether long-range interactions are required for the corresponding learning tasks.</p><p>Nevertheless, we believe there is a consensus in the community towards the development of specific benchmarks that can assist LRI enabled-GNNs, including full-graph operable Graph Transformers. This can be observed in existing independent attempts at proposing new graph benchmarks to evaluate LRI. In <ref type="bibr" target="#b46">[47]</ref>, a new Graph MNIST benchmark with an increased average graph diameter compared to MNIST superpixels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> was used to evaluate the ability of Spectral Graph Network in incorporating long range signals. Similarly, a synthetic color connectivity task that, by construction, requires LRI propagation to differentiate between its classes was used in <ref type="bibr" target="#b41">[42]</ref> to demonstrate a hierarchical graph network's ability in modeling long-range signals. Apart from the aforementioned synthetic and semi-real tasks, MalNet <ref type="bibr" target="#b17">[18]</ref>, a real-world dataset of large function call graphs (avg. 15k nodes) was recently proposed. MalNet could be a potential LRI task given its graph sizes, which we discuss in Section 2 as a characterizing aspect of LRI benchmarks, along with other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Characterizing Long-Range Interactions</head><p>We now proceed to study the key characteristics that can help to determine how a graph dataset could be appropriate to guage whether a GNN can or cannot model LRIs. Note that our discussion here is based on datasets with inductive tasks, which contain many graphs, rather than a single (large) graph.</p><p>Graph Size. The number of nodes in a graph is critical to determine if any visible effect of information oversquashing would occur if a local message passing based GNN (MP-GNN) is used to learn on this graph. If r is a hypothetical estimate of the learning problem's radius in the graph or the problem's range of interaction <ref type="bibr" target="#b1">[2]</ref>, and L ? r denotes the number of layers that are stacked in a GNN to learn the task, the number of nodes in a node's receptive field grows exponentially, i.e., O(exp(L)) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. However, if the graph size is small, such as ogbg-mol* <ref type="bibr" target="#b23">[24]</ref> or ZINC <ref type="bibr" target="#b12">[13]</ref> datasets with average graph size in the range of 23-26 nodes, then r would effectively be small as well. As a consequence, the effect of squashing of the information from the node's receptive field will be diminishing, and any local MP-GNN would succeed to learn the task to a great extent without being influenced by the information bottleneck. Therefore, a direct conclusion of this condition is that for a LRI benchmark, the graph sizes should be sufficiently large in order to separate the local MP-GNNs' performance from those models which model LRIs. However, this condition of graph size alone may not be enough to determine a LRGB dataset as the problem radius r may be small for some tasks even if the graph size is large, which brings us to the following factors.</p><p>Nature of Task. The nature of task can be understood to be directly related to the problem's range of interaction, r. In broad sense, the task can be either short-range, i.e., requiring information exchange among nodes in local or near-local neighborhood, or long-range, where interactions are required far away from the near-local neighborhood. For instance, the task in the ZINC molecular dataset <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref> is associated with counting local structures and it has been revealed that a substructurecounting based model <ref type="bibr" target="#b6">[7]</ref> would optimally require counts of 7-length substructures for the best performance. Any increment above this length does not further show a gain in the ZINC task. It may therefore be interpreted that such a benchmarking task does not require long-range signal propagation. Additionally, note that the graph sizes in ZINC are small (9-37 nodes) which functionally makes it a non-LRI benchmark if we also factor in the nature of ZINC's task.</p><p>However, even if the graph size of a dataset is considerably large, it may not warrant that models with long-range signal propagation are best suited unless the nature of the task determines so. A recent example to this is MalNet-Tiny dataset <ref type="bibr" target="#b17">[18]</ref> consisting of graphs up to 5,000 nodes, where there is a scarce improvement of performance of fully-connected GNN modules <ref type="bibr" target="#b42">[43]</ref>. Finally, there exist tasks in graphs which are prone to bottleneck if local MP-GNNs are used while this bottleneck is substantially reduced if LRI enabled non-local MP-GNNs are used, see <ref type="table" target="#tab_3">Table 3</ref> in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Contribution of global graph structure to task. Since MP-GNNs rely on information aggregated from a local neighborhood to update a node's features, it is subject to miss global structural information, such as global positional encoding (PE) <ref type="bibr" target="#b42">[43]</ref>. Additionally, MP-GNNs are also susceptible to lose out critical node signals coming from distant nodes if the graph size is large enough <ref type="bibr" target="#b1">[2]</ref>. Such signals are conveniently propagated in a fully-connected Transformer-like networks modeling LRI. The contribution of global structure to a task thus becomes a distinctive property desired in a LRI benchmark. MP-GNNs are often augmented with positional encodings (PE) carrying global structural information to assist tasks requiring some degree of LRI. A dataset where the learning task benefits from global PE can hence be a potential LRI benchmark. Similarly, if the learning task in a dataset is dependent on some form of distance information, or is directly a function of distance, coupled with graph feature information, the dataset can be a strong candidate for LRGB since the distance information would require global structural information. Examples of this can be molecular datasets where the learning task is related to prediction of 2D or 3D distance and structure properties.</p><p>3 Proposed LRGB Datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PascalVOC-SP</head><p>PascalVOC-SP is a node classification dataset, based on the Pascal VOC 2011 image dataset <ref type="bibr" target="#b15">[16]</ref>, where each node corresponds to a region of the image belonging to a particular class. The original dataset is available on a Custom License (respecting Flickr terms of use) <ref type="bibr" target="#b40">[41]</ref>. Similar to the recent superpixels (SP) datasets such as MNIST and CIFAR10 <ref type="bibr" target="#b12">[13]</ref>, we extract superpixels nodes in PascalVOC-SP by using the SLIC algorithm <ref type="bibr" target="#b0">[1]</ref> and construct a rag-boundary graph that interconnects these nodes. Unlike MNIST and CIFAR10 superpixels which have up to 75 and 150 nodes respectively, we extract a maximum of 500 superpixel nodes for SLIC compactness value of 30 <ref type="bibr" target="#b1">2</ref> in PascalVOC-SP in order to satisfy the 'graph size' characteristic to make it a LRGB benchmark. Effectively, it results in the PascalVOC-SP dataset to have an average shortest path length of 10.74?0.51 and average diameter of 27.62?2.13 (see <ref type="table" target="#tab_1">Table 2</ref>) which is significantly larger than that of MNIST with 3.03?0.17, 6.03?0.47 and CIFAR10 with 3.97?0.08, 8.46?0.50 average shortest path and diameters respectively. We argue that these properties, along with the task of predicting the node label of the superpixel region, which is analogous to the semantic segmentation task in Computer Vision, makes PascalVOC-SP a suitable LRGB dataset fulfilling major characteristics discussed in Section 2. We also prepare other variants of PascalVOC-SP with different values of SLIC compactness and graph construction options, which are included in Appendix A.</p><p>Statistics. There are 11,355 graphs with a total of 5.4 million nodes in PascalVOC-SP where each graph corresponds to an image in Pascal VOC 2011. The graphs prepared after the superpixels extraction have on average 479.40 nodes with complete statistics reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Task. The task in PascalVOC-SP is node classification which predicts a semantic segmentation label for each superpixel node out of 21 classes. We label each superpixel node with the same class label of the original pixel ground truth which is on the mean coordinates of the superpixel region. <ref type="bibr" target="#b2">3</ref> Splitting. In the original Pascal VOC 2011 dataset, there are only training and validation splits that we can use. For PascalVOC-SP, we maintain the train set as it is, and split the original validation set into new validation and test sets. For this splitting, we divide the original validation set in 50:50 ratio using a stratified split proportionate to the original distribution of the data with respect to a meta label that depends on the node classes. This meta label is a ground truth class value obtained by a majority voting of non-background ground truth node labels. The splitting decision with this meta-label is taken to preserve a similar distribution of node labels in both the new validation and the test set. Thus, we have 8,498, 1,428 and 1,429 graphs in the final training, validation and test sets, respectively.</p><p>Construction. After the superpixels extraction, we prepare the rag-boundary graph as illustrated step-wise in <ref type="figure" target="#fig_1">Figure 2</ref>. Two superpixels nodes are connected with an edge if the node regions share a common boundary. We use the rag_boundary functionality from skimage <ref type="bibr" target="#b5">[6]</ref> to extract the region boundaries. By construction, the dataset in this rag-boundary graph format has nodes with variying number of neighbors. This construction format also makes the graph more sparse with an average node degree of 5.6 for graphs averaging 479.40 node sizes. The initial feature of each superpixel node is 12 dimensional RGB feature value (mean, std, max, min) and that of an edge between two nodes is a 2 dimensional vector where the first value is 'weight' denoting the average of the Sobel filter <ref type="bibr" target="#b14">[15]</ref> pixel values along the boundary between the 2 adjacent regions, and the second value is 'count' denoting the count of all pixels along this boundary.</p><p>Performance Metric. The performance metric is the macro weighted F1 score for the predicted node label and the ground truth node label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COCO-SP</head><p>Similar to PascalVOC-SP, COCO-SP is a node classification dataset based on the MS COCO image dataset <ref type="bibr" target="#b32">[33]</ref> where each superpixel node denotes an image region belonging to a particular class. The original MS COCO image dataset is available under CC BY 4.0 License. We follow the same steps as in Section 3.1 for the preparation of superpixels and the graphs in COCO-SP for the rag-boundary graph format. Additional optional variants of the COCO-SP datasets are included in Appendix A.</p><p>Statistics. There are 123,286 graphs with a total of 58.7 million nodes in COCO-SP where each graph corresponds to an image in MS COCO dataset <ref type="bibr" target="#b32">[33]</ref>. The graphs prepared after the superpixels extraction have on average 476.88 nodes with complete statistics reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Task. The learning task in COCO-SP is node classification to predict a semantic segmentation label for each superpixel node out of 81 classes. We label each superpixel node with the same class label of the original pixel ground truth which is on the mean coordinates of the superpixel region.</p><p>Splitting. In the MS COCO image dataset there are only train and validation sets available that we can use. In COCO-SP, we maintain the original validation set as the new test set, while we sample 5,000 images from the original training set to generate the new validation set. Finally, there are 113,286, 5,000 and 5,000 graphs in the resultant training, validation and test set, respectively.</p><p>Performance Metric. Similar to PascalVOC-SP, the performance metric is the macro weighted F1 score for the predicted node label and the groundtruth node label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PCQM-Contact</head><p>Molecular property prediction is one of the most popular tasks for benchmarking GNNs. The usual task is to predict a biochemical property <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. Molecular datasets are very interesting for the study of Graph Transformers since their properties do not only depend on local graph structure defined by covalent bonds, but inter alia also on long-range interactions that define the 3D folding of the molecules, their surface area, or the energy of their electronic orbitals <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>However, existing benchmarks do not necessarily depend on long-range interactions and are noisy to properly evaluate Graph Transformers. For instance, the task in ZINC dataset from <ref type="bibr" target="#b12">[13]</ref> depends on a linear combination of local structures <ref type="bibr" target="#b26">[27]</ref>. Thus, it is unclear whether there is any benefit in using a fully-connected Transformer instead of a standard message passing network <ref type="bibr" target="#b42">[43]</ref>. OGB <ref type="bibr" target="#b23">[24]</ref> offers a variety of datasets from biological assays, but they are often small, noisy, and it is unclear whether they would benefit from the long-range interactions of a Transformer. In PCQM-Contact, we design a task that explicitly requires LRI since it needs to understand the interaction between distant atoms.</p><p>Statistics. There are 529,434 graphs with a total of 15 million nodes in PCQM-Contact where each graph corresponds to a molecular graph with explicit hydrogens, and more details in <ref type="table" target="#tab_1">Table 2</ref>. All graphs were taken from the PCQM4M training set with available 3D structure <ref type="bibr" target="#b22">[23]</ref> and filtered to only keep those with at least one contact.</p><p>Task. The task is to predict pairs of distant nodes (more than 5 hops away from each other in a molecule graph) that will be contacting with each other in the 3D space, i.e., the 3D distance between atoms will be smaller than 3.5?. The threshold of 3.5? is chosen to account for hydrogen bonds, one of the most common non-covalent interaction with a typical distance of 2.7 to 3.3 ? <ref type="bibr" target="#b35">[36]</ref>. The 5-hop distance is chosen to avoid trivial predictions between atoms that are close in the molecular graph and force the network to learn properties related to the 3D structure. Note that, contrarily to most benchmarks that represent hydrogens implicitly with node features, PCQM-Contact makes the hydrogens atoms explicit by adding a node.</p><p>Contact map prediction is therefore framed as inductive link prediction.  Splitting. We randomly split the dataset into 90% (476,490 molecules) training split, 5% (26,472 molecules) validation split, and 5% (26,472 molecules) testing split.</p><p>Performance Metric. In the absence of true negatives, we resort to the ranking metrics common in the knowledge graph link prediction literature <ref type="bibr" target="#b3">[4]</ref>. Given a query (h, ?), we compute a scalar score for each other node in a graph as a tail (h, t i ), and look for a rank of a true positive link. The true link has rank 1 if its score is the highest among all other links. We use standard ranking metrics Hits@1, Hits@3, and Mean Reciprocal Rank (MRR, aka Inverse Harmonic Mean Rank <ref type="bibr" target="#b21">[22]</ref>) in the filtered setting <ref type="bibr" target="#b3">[4]</ref>. That is, if there exist several true links sharing the same head (or tail), i.e., (h, t 1 ), (h, t 2 ), . . . , (h, t k ), evaluating each link separately, we filter out (mask) scores of other true tails setting their scores to ?? such that they do not interfere with the ranking procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Peptides molecular graphs</head><p>Peptides are short chains of amino acids that are abundant in nature as they serve many important biological functions <ref type="bibr" target="#b45">[46]</ref>, but they are much shorter than proteins <ref type="bibr" target="#b39">[40]</ref>. Since each amino acid is composed of many heavy atoms, the molecular graph of a peptide is much larger than that of a small drug-like molecule.</p><p>Peptides have about 6 times large diameter and 5 times more atoms than the PCQM-Contact dataset, but similar avg. degree of~2. See <ref type="figure" target="#fig_2">Figure 3</ref> for an illustration. This makes them ideal for testing long-range dependencies in GNNs while still being able to fit an entire mini-batch on a single GPU.</p><p>Here we propose Peptides-func and Peptides-struct datasets, derived from 15,535 peptides retrieved from SAT-Pdb <ref type="bibr" target="#b45">[46]</ref>. Both datasets use the same set of graphs but differ in their prediction tasks.</p><p>Construction. The graphs are derived such that the nodes correspond to the heavy (non-hydrogen) atoms of the peptides while the edges represent the bonds between them. We reuse the OGB molecular featurization <ref type="bibr" target="#b23">[24]</ref> that computes rich node and edge features from molecular SMILES.</p><p>In both Peptides-func and Peptides-struct, recognizing local structures is very important for the model to even identify the original amino acids. Further, we do not include any 2D or 3D peptide structure information. The graphs correspond to 1D amino acids chains, which means it is important for the model to identify the location of an amino acid in the graph. Finally, with the peptides chains having different lengths and a strong variability in their graph diameters, any used graph positional or structural encoding needs to generalize well across various sizes and be computationally efficient.</p><p>Statistics. Both Peptides-func and Peptides-struct consist of 15,535 graphs with a total of 2.3 million nodes, <ref type="table" target="#tab_1">Table 2</ref>. All peptides were obtained from the SATPdb <ref type="bibr" target="#b45">[46]</ref> database (an aggregate of multiple public-domain sources) that includes the sequence, molecular graph, function, and 3D structure of the peptides.</p><p>Previously introduced ENZYMES and PROTEINS datasets <ref type="bibr" target="#b4">[5]</ref>, that use the 3D structure of the folded proteins to build a graph of amino acids, are notably different from those that we propose here. In addition to more complex prediction tasks, our datasets are also larger in multiple ways. First, we derive 15,535 graphs, compared to theirs 600 and 1,113, respectively. Second, we use heavy atoms as nodes and not the amino acids, resulting in larger graphs: on average 150.94 nodes per graph, compared to theirs 32.63 and 39.06, respectively. In terms of graph diameter, our graphs average 56.99 compared to theirs 10.92 and 11.62, respectively. Thus, the proposed Peptides-func and Peptides-struct are better suited to benchmarking of graph Transformers or other expressive GNNs, as they contain larger graphs, more data points, and challenging tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Peptides-func</head><p>Task. Peptides-func is a multi-label graph classification dataset. There is a total of 10 classes based on the peptide function, e.g., Antibacterial, Antiviral, cell-cell communication, and others. We treat it as a multi-label classification as a peptide can belong to several classes simultaneously; on average to 1.65 of the 10 classes. The labels are imbalanced, only 16.5% of the data is in the positive class, with the richest class having 62.7% positives and the poorest 1.9%. The correlation between individual classes is shown in <ref type="figure" target="#fig_0">Figure A.1</ref>.</p><p>Splitting. For the purpose of split computation, the data is first aggregated into meta-classes by considering the concatenation of all 10 original labels of a data point as its meta-class. Meta-classes with less than 10 occurrences are pooled into one meta-class. Then we apply stratified splitting to generate balanced train-valid-test dataset splits; we use the ratio of 70%-15%-15%, respectively.</p><p>Performance Metric. We choose the unweighted mean Average Precision (AP). This metric measures the area under the precision-recall curve, and is also used for ogbg-molpcba in OGB, a dataset with similar imbalanced multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Peptides-struct</head><p>Task. Peptides-struct is a multi-label graph regression dataset based on the 3D structure of the peptides. It consists of the same graphs as Peptides-func, but with different task. Here we aim to predict aggregated 3D properties of the peptides at the graph level. The properties (normalized to zero mean and unit standard deviation) include: Inertia_mass, Inertia_valence, Length, sphericity, and plane_best_fit. How we derive these properties is described in Appendix A.2 and their correlations are shown in <ref type="figure">Figure A.</ref>1. These new tasks are expected to directly benefit from the full-connectivity of a Transformer, because they require implicit understanding of complex 3D interactions. To adapt these tasks to the general graph learning setting, we avoided the prediction of pairwise node distances, which would require specialized methods from the conformer generation literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Splitting. The data splits are identical to Peptides-func. Since structure is related to functionality, this also ensures that the structures in the testing set represent well the training and validation sets.</p><p>Performance Metric. For the Peptides-struct dataset, we use the Mean Absolute Error (MAE), typically selected for molecular property regression datasets. We further track the Coefficient of Determination (R 2 ) on each task, and compute its unweighted mean across tasks. We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers. We adopt fair and rigorous experimental settings for all our experiments in order to present reliable comparison between the two GNN classes. The former models do not directly include any mechanism to model LRI, while the latter are by design fully connected and can propagate long-range signals, which are required for the proposed benchmarks. For the baselines, we select GCN <ref type="bibr" target="#b28">[29]</ref>, GINE <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b24">25]</ref> and GatedGCN <ref type="bibr" target="#b7">[8]</ref> models from the local MP-GNN class, and fully connected Transformer <ref type="bibr" target="#b48">[49]</ref> with Laplacian PE (LapPE) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> and SAN <ref type="bibr" target="#b29">[30]</ref> models from the Transformer class. In order to facilitate fair comparison and reliable discussion of the observed trends, we choose hyperparameters of the aforementioned baselines while keeping to a budget of 500k learnable parameters. Detailed experimental setup and hyperparameters are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported in <ref type="table" target="#tab_3">Table 3</ref>, for Peptides-func and Peptides-struct in <ref type="table" target="#tab_4">Table 4</ref>, and for PCQM-Contact in <ref type="table" target="#tab_5">Table 5</ref>. Our aim is to address the following main questions through the analysis of these results:</p><p>(i) Do we observe a visible separation in learning and generalization when models equipped with LRI dependencies are evaluated against local MP-GNNs on the proposed benchmark?</p><p>(ii) Does the use of positional encodings, contributing critical structural information, improve MP-GNN performance on the proposed datasets?</p><p>(iii) What are the challenges and future discoveries that can be facilitated by the new benchmarks?</p><p>Simple instances of local MP-GNNs perform poorly on the proposed LRGB datasets. As shown by the results in <ref type="table" target="#tab_3">Tables 3, 4</ref> and 5, GCN and GINE, which depend on local feature aggregation from node neighborhoods using simple aggregation functions, perform poorly on all datasets except Peptides-func. This is consistent with the empirical findings in <ref type="bibr" target="#b1">[2]</ref> where GCN and GIN suffer from over-squashing to a greater extent than GAT, an attention based MP-GNN <ref type="bibr" target="#b49">[50]</ref>.</p><p>Transformers operating on fully-connected graph show the best performance. It can be observed that the Transformer model and the SAN, which is an improved Transformer, rank among the best performing baselines in <ref type="table" target="#tab_3">Tables 3, 4</ref> and 5. The gap in performance seems the most distinct for Peptides-func and Peptides-struct among all datasets. This can be attributed to the long-range design of the task, the contribution of non-local information and the graph size statistics of these datasets as discussed in Section 3.4. On PascalVOC-SP and PCQM-Contact, SAN performs com-  paratively better than vanilla Transformer+LapPE, suggesting that full connections in graphs should be used in non-trivial manner for LRI enabled models to do well. Finally, the gain with Transformers observed in COCO-SP is minor which points towards training models with larger parameters for such a large dataset.</p><p>Discussion on LRI characterizing factors. To a major extent, the reasons behind the fullyconnected Transformer baselines being able to excel in the proposed LRGB datasets can be linked to one or all of the characterizing factors that were discussed in Section 2. For instance, the nature of task of Peptides-* datasets, along with their substantial graph statistics (i.e., avg. nodes, avg. shortest paths and avg. diameter as reported in <ref type="table" target="#tab_1">Table 2</ref>) can help explain how long range dependencies are a must to do well on such tasks. Similarly, for PCQM-Contact, even if the graph sizes are small, the task of predicting pairs of distant nodes makes it a suitable LRGB dataset as shown by, e.g., Test Hits@3 scores of SAN against the local MP-GNNs in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>Challenges and future directions. First, the use of positional encoding alone contributes to little or no gain in performance on the proposed datasets. See the scores of GatedGCN augmented with LapPE or RWSE in <ref type="table" target="#tab_3">Tables 3, 4</ref> and 5 to this end. We hope such results to influence further exploration of powerful approaches to incorporate global structural and positional encoding in LRI enabled models, where the proposed LRGB can be used to conveniently evaluate the novel approaches. Second, the scores against each performance metrics in <ref type="table" target="#tab_3">Tables 3-5</ref> exhibit the current limitations of Transformers for graph learning and suggest that there is still a large window to fulfil by the better design of Graph Transformers that can make use of irregular sparse structure information, as well as propagate long range interactions. Finally, it must be noted that as we proceed towards evaluating Graph Transformers on long range benchmarks, such as our proposed LRGB with up to 479.40 avg. nodes, 58.79 million total nodes and 332.09 million total edges in a dataset, trivial O(N 2 ) Transformers may be computationally inefficient to scale. To this end, research is also imperative on efficient or linear Transformers for graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present the Long Range Graph Benchmark (LRGB) consisting of 5 datasets for node, edge and graph-level prediction tasks. Through our study of multiple characterizing factors, we argue that the proposed datasets' size and tasks makes these ideal to evaluate and develop models enabled with long-range dependencies. This is empirically verified with extensive baseline experiments using both local and non-local GNN classes showing that Transformers significantly outperform message passing on the proposed datasets. The increasing interest in the development of Transformers for graph representation learning raised the need for the creation of a dedicated LRGB and we fulfil this gap through our work. We believe our proposed benchmark can be leveraged to prototype new ideas and provide an accurate ranking of a model's capturing of LRIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extended Dataset Description and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Optional variants of PascalVOC-SP and COCO-SP datasets</head><p>For PascalVOC-SP and COCO-SP datasets, the graphs that we keep by default are the rag-boundary graphs which are based on SLIC superpixels extraction with compactness value of 30. Additionally, we provide optional variants of both these datasets which are based on SLIC superpixels extraction with compactness value of 10, and two other graph construction formats, coo and coo-feat. In this section, we include the description and results of baseline experiments of these optional datasets as well. Note that any of these version of the SP dataset can be used as independent LRGB dataset.</p><p>Construction of coo and coo-feat graphs: Under these two construction methods, the resultant graphs are 8 nearest neighbor graphs where the pairwise adjacency weights for two nodes are first constructed based on coordinates (for coo) or based on coordinates and feature intensities (for coo-feat) of the superpixels nodes, and then each node is directly connected to 8 other nodes with the highest weight scores. The weights computation is based on the Eqn. 1 for coo and Eqn. 2 for coo-feat:</p><formula xml:id="formula_0">W 8?nn ij = exp ? x i ? x j ? 2 x (1) W 8?nn ij = exp ? x i ? x j ? 2 x ? f i ? f j ? 2 f<label>(2)</label></formula><p>where, x i , x j are the 2 dimensional coordinates, and f i , f j are the 12 dimensional (3 dimensions each of mean, std, max, min) RGB feature values of superpixels i, j respectively, ? 2 x is a scaling parameter defined as the average distance x k of the k = 8 nearest neighbors for each node. The initial feature of each superpixel node is 12 dimensional RGB feature value (mean, std, max, min) and that of an edge between two nodes is a 1 dimensional edge weight that is given by Eqn. 1 or 2 for the respective graph format.</p><p>Statistics and Baseline Results. The complete statistics of the aforementioned optional versions of the PascalVOC-SP and COCO-SP datasets are included in <ref type="table" target="#tab_7">Table A</ref>.1. Note that that versions with the options SLIC: 30 and Graph: rag-boundary is the default dataset for both PascalVOC-SP and COCO-SP that we present in the main Section 3. The results of the baseline experiments on all the dataset versions are reported in <ref type="table" target="#tab_1">Table A.2.   Table A</ref>.1: Statistics of 6 tried versions of PascalVOC-SP and COCO-SP datasets, each derived with a different combination of Options in terms of: (i) SLIC, which denotes the value of compactness parameter used during the extraction of superpixels by SLIC algorithm <ref type="bibr" target="#b0">[1]</ref> and (ii) Graph, which denotes the graph format that was used to construct the adjacency matrix. The Graph options 'coo' refers to a 8-nn graph where the edge weight is based on superpixel coordinates (Eqn. 1), 'coo-feat' refers to a 8-nn graph where the edge weight is based on superpixel coordinates as well as feature intensities (Eqn. 2), 'rag-boundary' refers to a region boundary graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Extended description of Peptides-struct</head><p>Below, we describe the 11 tasks from the Peptides-struct dataset, which represented properties computed from the 3D structure, then normalized to zero mean and unit standard deviation.</p><p>? Inertia_mass The inertia of the molecules according to its 3 principal components, using the mass of the atoms and their distances to the centroid. ? Inertia_valence The inertia of the molecules according to its 3 principal components, using the valence of the atoms and their distances to the centroid. ? Length The maximum distance between each atom-pairs in each of its 3 main axes.</p><p>? Sphericity A measure of how much the molecule looks like a sphere: the ratio of the molecule's surface area to the surface area of a sphere with similar volume. ? Plane_best_fit The average distance of all heavy atoms from the plane of best fit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Extended Results for Peptides-struct</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Correlation of labels in Peptides-func and Peptides-struct</head><p>The Peptides-func is a multi-label classification and Peptides-struct is a multi-label regression. Evaluating the correlation between the labels will ensure that labels are not redundant and provide a variety of information, <ref type="figure">Figure A.</ref>1. We observe that there is very little correlation between classes of peptides. For the structural dataset, there are some correlations, which are expected since intertia is related to length and spherocity, but in general, the correlation remains limited, which motivates using a multi-label regression.   Shown is the size of the hidden node representation d and the number of layers L. Where applicable, the type of positional/structural embedding is shown: LapPE-k denotes Laplacian positional encoding <ref type="bibr" target="#b29">[30]</ref> with first k non-trivial eigenvectors (with original Transformer-based encoder for SAN <ref type="bibr" target="#b29">[30]</ref> and more parameter-efficient DeepSet encoder for GatedGCN); RWSE-m denotes random-walk structural encoding <ref type="bibr" target="#b13">[14]</ref> with 1..m steps and a linear encoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Details on Baseline Experiments Setup</head><p>Models. We use GCN <ref type="bibr" target="#b28">[29]</ref>, GINE <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b24">25]</ref> and GatedGCN <ref type="bibr" target="#b7">[8]</ref> models from the local MP-GNNs class, and fully connected Transformer <ref type="bibr" target="#b48">[49]</ref> with Laplacian PE (LapPE) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> and SAN <ref type="bibr" target="#b29">[30]</ref> models among the Transformer class. GCN (Graph Convolutional Network) is the most popularly used local MP-GNN baseline, GINE (Graph Isomorphism Network) is a 1-WL expressive MP-GNN with ability to incorporate edge features into its update equation <ref type="bibr" target="#b24">[25]</ref>, and GatedGCN (Gated Graph Convolutional Network) is a soft-attention based GCN which uses learned edge gates to improve the aggregation procedure. Transformer with LapPE is a generalization of the vanilla Transformer network <ref type="bibr" target="#b48">[49]</ref> from Natural Language Processing (NLP) domain to graphs and SAN (Spectral Attention Network) is a powerful fully-connected Graph Transformer which includes a learned PE module based on Laplacian eigenvectors and eigenvalues, alongside separate treatment of real and non-real graph edges <ref type="bibr" target="#b29">[30]</ref>. We use SAN with LapPE as well RWSE (Random Walk Structural Encoding) <ref type="bibr" target="#b13">[14]</ref>. The collection of above baseline models allows us to show performance trends using simple, straightforward models such as GCN and Transformer to advanced ones such as GatedGCN and SAN. We believe this baseline collection, albeit small, represents a diverse representation of the course of action graph deep learning has evolved to, reaching at a stage where we can embark conveniently towards the development of GNNs that learn efficiently to propagate long-range dependencies.</p><p>Experimental Setup. In order to facilitate fair comparison and reliable discussion of the observed trends, we select the hyperparameters of the aforementioned baselines such that they yield models within a budget of approx. 500k learnable parameters. To this end, we configure 4-8 layers deep models and adjust their hidden dimension size accordingly to the 500k parameter budget. For a list of hyperparameters used in each baseline see <ref type="table" target="#tab_7">Table C</ref>.1. We run each experiment 4 times with different random seeds and report the mean and standard deviation of the respective performance metrics.</p><p>For optimization, we use Adam <ref type="bibr" target="#b27">[28]</ref> with default settings. We set the starting learning rate between 0.0003 and 0.001 depending on the model and dataset, and decay it by 0.5 factor upon reaching a validation loss plateau. We limit the training time up to 60h, which is adequate for the models to converge, except SAN on COCO-SP. SAN is particularly computationally intensive and may require a week of single NVidia A100 computation time to converge on COCO-SP. Individual configuration files with exact hyperparameters for all 7 models and 5 datasets are provided with the source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Computing environment and used resources</head><p>Our implementation uses GraphGPS <ref type="bibr" target="#b42">[43]</ref> built on PyG and its GraphGym module <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref> that are all released under MIT License. All presented experiments were executed in a shared computing cluster environment with multiple CPU and GPU architectures: NVidia V100 (32GB), NVidia RTX8000 (48GB), and NVidia A100 (40GB). The resource budget for each experiment was 1 GPU, 4 CPUs, and up to 32GB system RAM. Except COCO-SP, which required up to 72GB RAM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Molecule with LRIs (dotted lines showing 3D atomic contact) that are not trivially captured by the graph structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>boundary` graph overlay on SLIC SP final `rag-boundary` graph Visualization of a sample image, its SLIC SP regions and rag-boundary graph from COCO-SP dataset. In this figure, the extracted SP are &lt; 50 for better visualization. For the actual graphs with a maximum of 500 SP and nodes in PascalVOC-SP and COCO-SP, refer to Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Top: 3D Visualization of "GLLGPLLKIAAKVGKNLL" peptide. Bottom: The molecular graph for the same peptide.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A. 1 :</head><label>1</label><figDesc>Left: Visualization of pearson correlation between classes in peptides-func dataset. Right: Visualization of correlation between geometric properties in peptides-struct dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the five proposed LRGB datasets.</figDesc><table><row><cell>Dataset</cell><cell>Total Graphs</cell><cell>Total Nodes</cell><cell>Avg Nodes</cell><cell>Mean Deg.</cell><cell>Total Edges</cell><cell>Avg Edges</cell><cell>Avg Short.Path.</cell><cell>Avg Diameter</cell></row><row><cell>PascalVOC-SP</cell><cell>11,355</cell><cell cols="2">5,443,545 479.40</cell><cell>5.65</cell><cell cols="3">30,777,444 2,710.48 10.74?0.51</cell><cell>27.62?2.13</cell></row><row><cell>COCO-SP</cell><cell cols="3">123,286 58,793,216 476.88</cell><cell cols="4">5.65 332,091,902 2,693.67 10.66?0.55</cell><cell>27.39?2.14</cell></row><row><cell>PCQM-Contact</cell><cell cols="2">529,434 15,955,687</cell><cell>30.14</cell><cell>2.03</cell><cell>32,341,644</cell><cell>61.09</cell><cell>4.63?0.63</cell><cell>9.86?1.79</cell></row><row><cell>Peptides-func</cell><cell>15,535</cell><cell cols="2">2,344,859 150.94</cell><cell>2.04</cell><cell>4,773,974</cell><cell cols="3">307.30 20.89?9.79 56.99?28.72</cell></row><row><cell>Peptides-struct</cell><cell>15,535</cell><cell cols="2">2,344,859 150.94</cell><cell>2.04</cell><cell>4,773,974</cell><cell cols="3">307.30 20.89?9.79 56.99?28.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>That is, training molecules only have true positive links without hard negative labels whereas at validation and test time we predict contact links over new, unseen molecules. The molecules are treated as relational graphs with learnable edge types (standard BondEncoder in OGB<ref type="bibr" target="#b22">[23]</ref> e.g., single bond, double bond, triple bond), but the predictable contact link does not have an explicit edge type, so a link prediction decoder is a function f (h, t) of probed head and tail nodes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Baseline experiments for PascalVOC-SP and COCO-SP with rag-boundary graph on SLIC compactness 30 for node classification task (Extended results for all graph formats inTable A.2). Performance metric is macro F1 on the respective splits (Higher is better). All experiments are run 4 times with 4 different seeds. The MP-GNN models are 8 layers deep, while the transformer-based models have 4 layers in order to maintain comparable hidden representation size at the fixed parameter budget of 500k. *The SAN model under-fitted the COCO-SP dataset since it required more budget than the 60 hours allowed on A100 GPUs. Bold: Best score.</figDesc><table><row><cell>Model</cell><cell># Params</cell><cell cols="2">PascalVOC-SP</cell><cell># Params</cell><cell>COCO-SP</cell></row><row><cell></cell><cell></cell><cell>Train F1</cell><cell>Test F1 ?</cell><cell>Train F1</cell><cell>Test F1 ?</cell></row><row><cell>GCN</cell><cell cols="3">496k 0.1450?0.0125 0.1268?0.0060</cell><cell cols="2">509k 0.0948?0.0014 0.0841?0.0010</cell></row><row><cell>GINE</cell><cell cols="3">505k 0.2088?0.0268 0.1265?0.0076</cell><cell cols="2">515k 0.2100?0.0041 0.1339?0.0044</cell></row><row><cell>GatedGCN</cell><cell cols="3">502k 0.3552?0.0451 0.2873?0.0219</cell><cell cols="2">509k 0.3167?0.0059 0.2641?0.0045</cell></row><row><cell cols="4">GatedGCN+LapPE 502k 0.3512?0.0167 0.2860?0.0085</cell><cell cols="2">509k 0.3102?0.0112 0.2574?0.0034</cell></row><row><cell cols="4">Transformer+LapPE 501k 0.7170?0.0048 0.2694?0.0098</cell><cell cols="2">508k 0.3912?0.0098 0.2618?0.0031</cell></row><row><cell>SAN+LapPE</cell><cell cols="3">531k 0.5723?0.0427 0.3230?0.0039</cell><cell cols="2">536k 0.2830?0.0246* 0.2592?0.0158*</cell></row><row><cell>SAN+RWSE</cell><cell cols="3">468k 0.5819?0.0331 0.3216?0.0027</cell><cell cols="2">474k 0.2657?0.0224* 0.2434?0.0156*</cell></row><row><cell cols="3">4 Experiments and Discussion</cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Baseline experiments</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Baselines for Peptides-func (graph classification) and Peptides-struct (graph regression). Performance metric is Average Precision (AP) for classification and MAE for regression (seeTable A.3 for extended results with R 2 metric). Each experiment was run with 4 different seeds. All MP-GNN models have 5 layers, while the Transformer-based models have 4 layers. Bold: Best score.</figDesc><table><row><cell>Model</cell><cell># Params.</cell><cell cols="2">Peptides-func</cell><cell>Peptides-struct</cell></row><row><cell></cell><cell></cell><cell>Train AP</cell><cell>Test AP ?</cell><cell>Train MAE</cell><cell>Test MAE ?</cell></row><row><cell>GCN</cell><cell>508k</cell><cell cols="3">0.8840?0.0131 0.5930?0.0023 0.2939?0.0055 0.3496?0.0013</cell></row><row><cell>GINE</cell><cell>476k</cell><cell cols="3">0.7682?0.0154 0.5498?0.0079 0.3116?0.0047 0.3547?0.0045</cell></row><row><cell>GatedGCN</cell><cell>509k</cell><cell cols="3">0.8695?0.0402 0.5864?0.0077 0.2761?0.0032 0.3420?0.0013</cell></row><row><cell>GatedGCN+RWSE</cell><cell>506k</cell><cell cols="3">0.9131?0.0321 0.6069?0.0035 0.2578?0.0116 0.3357?0.0006</cell></row><row><cell>Transformer+LapPE</cell><cell>488k</cell><cell cols="3">0.8438?0.0263 0.6326?0.0126 0.2403?0.0066 0.2529?0.0016</cell></row><row><cell>SAN+LapPE</cell><cell>493k</cell><cell cols="3">0.8217?0.0280 0.6384?0.0121 0.2822?0.0108 0.2683?0.0043</cell></row><row><cell>SAN+RWSE</cell><cell>500k</cell><cell cols="3">0.8612?0.0219 0.6439?0.0075 0.2680?0.0038 0.2545?0.0012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>GCN</cell><cell>504k</cell><cell>0.1321?0.0007 0.3791?0.0004 0.8256?0.0006 0.3234?0.0006</cell></row><row><cell>GINE</cell><cell>517k</cell><cell>0.1337?0.0013 0.3642?0.0043 0.8147?0.0062 0.3180?0.0027</cell></row><row><cell>GatedGCN</cell><cell>527k</cell><cell>0.1279?0.0018 0.3783?0.0004 0.8433?0.0011 0.3218?0.0011</cell></row><row><cell>GatedGCN+RWSE</cell><cell>524k</cell><cell>0.1288?0.0013 0.3808?0.0006 0.8517?0.0005 0.3242?0.0008</cell></row><row><cell>Transformer+LapPE</cell><cell>502k</cell><cell>0.1221?0.0011 0.3679?0.0033 0.8517?0.0039 0.3174?0.0020</cell></row><row><cell>SAN+LapPE</cell><cell>499k</cell><cell>0.1355?0.0017 0.4004?0.0021 0.8478?0.0044 0.3350?0.0003</cell></row><row><cell>SAN+RWSE</cell><cell>509k</cell><cell>0.1312?0.0016 0.4030?0.0008 0.8550?0.0024 0.3341?0.0006</cell></row></table><note>Baseline performance on PCQM-Contact (link prediction). Each experiment was repeated with 4 different random seeds. The evaluated models have 5 (MP-GNN models) or 4 (Transformer- based models) layers with approximately 500k learnable parameters. Bold: Best score.Model # Params. Test Hits@1 ? Test Hits@3 ? Test Hits@10 ? Test MRR ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>,693.67 10.66?0.55 27.39?2.14 Table A.2: Baseline experiments for PascalVOC-SP and COCO-SP for node classification task. Performance metric is macro F1 on the respective splits (Higher is better). All experiments are run 4 times with 4 different seeds. All models have approximately 500k learnable parameters for fair comparison. The MP-GNN models are 8 layers deep, while the transformer-based models have 4 layers in order to maintain comparable hidden representation size at the fixed parameter budget. 0914?0.0056 0.0797?0.0026 0.1003?0.0043 0.0843?0.0019 0.0948?0.0014 0.0841?0.0010 GINE 515k 0.1742?0.0186 0.1168?0.0053 0.1646?0.0081 0.1003?0.0022 0.2100?0.0041 0.1339?0.0044 GatedGCN 508k 0.3024?0.0043 0.2441?0.0035 0.2926?0.0154 0.2285?0.0069 0.3167?0.0059 0.2641?0.0045 GatedGCN+LapPE 509k 0.3101?0.0062 0.2454?0.0015 0.2894?0.0060 0.2283?0.0036 0.3102?0.0112 0.2574?0.0034 Transformer+LapPE 508k 0.3855?0.0185 0.2579?0.0057 0.3750?0.0224 0.2589?0.0069 0.3912?0.0098 0.2618?0.0031 SAN+LapPE 536k 0.3376?0.0455 0.2781?0.0143 0.2941?0.0810 0.2498?0.0513 0.2830?0.0246 0.2592?0.0158 SAN+RWSE 474k 0.3652?0.0104 0.2817?0.0047 0.3754?0.0074 0.2869?0.0067 0.2657?0.0224 0.2434?0.0156</figDesc><table><row><cell>Dataset PascalVOC-SP COCO-SP (SLIC) Model PascalVOC-SP 10 GCN GINE GatedGCN SLIC 10 30 10 30 GatedGCN+LapPE 502k 0.4390?0.0144 0.2803?0.0031 0.3535?0.0376 0.2241?0.0035 0.3553?0.0396 0.2722?0.0149 Options Total Graphs Total Nodes Avg Nodes Mean Deg. Total Edges Avg Edges Avg Short.Path. Avg Diameter Graph coo 11,355 4,747,374 418.09 8.00 37,978,992 3,344.69 7.50?0.76 17.89?2.10 coo-feat 8.00 37,978,992 3,344.69 7.50?0.76 17.89?2.10 reg-bound 5.62 26,659,158 2,347.79 9.08?1.23 22.99?3.70 coo 11,355 5,443,545 479.40 8.00 43,548,360 3,835.17 8.05?0.18 19.40?0.65 coo-feat 8.00 43,548,360 3,835.17 8.05?0.18 19.40?0.65 reg-bound 5.65 30,777,444 2,710.48 10.74?0.51 27.62?2.13 coo 123,286 49,732,322 403.39 8.00 397,858,524 3,227.12 7.39?0.77 17.61?2.12 coo-feat 8.00 397,858,524 3,227.12 7.39?0.77 17.61?2.12 reg-bound 5.61 278,816,918 2,261.55 8.85?1.23 22.40?3.70 coo 123,286 58,793,216 476.88 8.00 470,345,728 3,815.08 8.06?0.18 19.42?0.70 coo-feat 8.00 470,345,728 3,815.08 8.06?0.18 19.42?0.70 reg-bound 5.65 # Params coo coo-feat reg-bound Train F1 Test F1 ? Train F1 Test F1 ? Train F1 Test F1 ? 496k 0.1559?0.0079 0.1281?0.0025 0.1956?0.0202 0.1321?0.0043 0.1530?0.0048 0.1306?0.0025 505k 0.2178?0.0382 0.1127?0.0039 0.3007?0.0461 0.1078?0.0035 0.2278?0.0224 0.1231?0.0052 502k 0.4319?0.0187 0.2788?0.0032 0.3560?0.0567 0.2289?0.0137 0.3574?0.0573 0.2705?0.0251 Transformer+LapPE 501k 0.6140?0.0635 0.2661?0.0129 0.5594?0.0445 0.2667?0.0060 0.5925?0.0447 0.2627?0.0086 SAN+LapPE 531k 0.6691?0.0339 0.2904?0.0031 0.5555?0.0650 0.2808?0.0047 0.5636?0.0506 0.3031?0.0046 SAN+RWSE 468k 0.6200?0.0502 0.2841?0.0090 0.5726?0.0615 0.2764?0.0184 0.5968?0.0487 0.3113?0.0072 30 GCN 496k 0.1469?0.0068 0.1262?0.0031 0.1742?0.0042 0.1326?0.0015 0.1450?0.0125 0.1268?0.0060 GINE 505k 0.2575?0.0283 0.1203?0.0045 0.2479?0.0318 0.1035?0.0015 0.2088?0.0268 0.1265?0.0076 GatedGCN 502k 0.4311?0.0325 0.2916?0.0058 0.3379?0.0107 0.2410?0.0015 0.3552?0.0451 0.2873?0.0219 GatedGCN+LapPE 502k 0.4223?0.0356 0.2890?0.0057 0.3110?0.0706 0.2317?0.0217 0.3512?0.0167 0.2860?0.0085 Transformer+LapPE 501k 0.6213?0.0393 0.2633?0.0056 0.6607?0.0684 0.2697?0.0081 0.7170?0.0048 0.2694?0.0098 SAN+LapPE 531k 0.6485?0.0711 0.3218?0.0160 0.5242?0.0480 0.3003?0.0046 0.5723?0.0427 0.3230?0.0039 SAN+RWSE 468k 0.6240?0.0866 0.3227?0.0084 0.5869?0.0349 0.3124?0.0091 0.5819?0.0331 0.3216?0.0027 10 GCN 509k 0.0852?0.0030 0.0770?0.0017 0.0919?0.0058 0.0780?0.0026 0.0885?0.0078 0.0809?0.0043 GINE 515k 0.1874?0.0071 0.1109?0.0048 0.1605?0.0090 0.0846?0.0045 0.1812?0.0155 0.1196?0.0053 GatedGCN 508k 0.3009?0.0078 0.2280?0.0032 0.2842?0.0077 0.2130?0.0036 0.3149?0.0099 0.2542?0.0044 GatedGCN+LapPE 509k 0.3018?0.0057 0.2307?0.0014 0.2789?0.0080 0.2110?0.0036 0.3184?0.0144 0.2529?0.0063 30 332,091,902 2Dataset Transformer+LapPE 508k 0.3700?0.0141 0.2455?0.0036 0.3775?0.0082 0.2492?0.0036 0.3758?0.0205 0.2478?0.0068 SAN+LapPE 536k 0.3437?0.0096 0.2605?0.0062 0.3278?0.0041 0.2596?0.0015 0.2541?0.0394 0.2325?0.0191 SAN+RWSE 474k 0.3557?0.0264 0.2675?0.0126 0.3270?0.0145 0.2585?0.0046 0.2815?0.0371 0.2442?0.0231 COCO-SP GCN 509k 0.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>3: Extended evaluation metrics for Peptides-struct. The training and testing performance is quantified in terms of the Coefficient of Determination R 2 , in addition to MAE reported inTable 4.</figDesc><table><row><cell>Model</cell><cell cols="2"># Params. Train MAE</cell><cell>Test MAE ?</cell><cell>Train R2</cell><cell>Test R2 ?</cell></row><row><cell>GCN</cell><cell></cell><cell cols="4">508k 0.2939?0.0055 0.3496?0.0013 0.6513?0.0078 0.6019?0.0027</cell></row><row><cell>GINE</cell><cell></cell><cell cols="4">476k 0.3116?0.0047 0.3547?0.0045 0.6494?0.0108 0.5943?0.0067</cell></row><row><cell>GatedGCN</cell><cell></cell><cell cols="4">509k 0.2761?0.0032 0.3420?0.0013 0.6907?0.0058 0.6254?0.0013</cell></row><row><cell>GatedGCN+RWSE</cell><cell></cell><cell cols="4">506k 0.2578?0.0116 0.3357?0.0006 0.7204?0.0149 0.6329?0.0034</cell></row><row><cell cols="2">Transformer+LapPE</cell><cell cols="4">488k 0.2403?0.0066 0.2529?0.0016 0.8027?0.0108 0.7743?0.0053</cell></row><row><cell>SAN+LapPE</cell><cell></cell><cell cols="4">493k 0.2822?0.0108 0.2683?0.0043 0.6887?0.0153 0.7581?0.0057</cell></row><row><cell>SAN+RWSE</cell><cell></cell><cell cols="4">500k 0.2680?0.0038 0.2545?0.0012 0.7112?0.0049 0.7716?0.0034</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Visualizations of a sample image and its SP graphs from PascalVOC-SP dataset with 465 nodes and 3,720 edges each for coo, coo-feat graph and 2,628 edges for reg-bound graph. Unique colors on the nodes denote the corresponding node labels. Visualizations of a sample image and its SP graphs from COCO-SP dataset with 470 nodes and 3,760 edges each for coo, coo-feat graph and 2,662 edges for reg-bound graph. Unique colors on the nodes denote the corresponding node labels.</figDesc><table><row><cell>B Visualizations B.1 PascalVOC-SP `coo` graph overlay on SLIC SP Original Image `coo-feat` graph overlay on SLIC SP `rag-boundary` graph overlay on SLIC SP Original Image `coo` graph overlay on SLIC SP `coo-feat` graph overlay on SLIC SP `rag-boundary` graph overlay on SLIC SP Figure B.1: B.2 COCO-SP Figure B.2:</cell><cell>SLIC SP (compactness=30) final `coo` graph final `coo-feat` graph final `rag-boundary` graph SLIC SP (compactness=30) final `coo` graph final `coo-feat` graph final `rag-boundary` graph</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table C.1: Baseline hyperparameters of 7 evaluated models on the 5 new LRGB benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>PascalVOC-SP COCO-SP PCQM-Contact Peptides-func Peptides-struct</figDesc><table><row><cell>GCN</cell><cell>d=220, L=8</cell><cell>d=220, L=8</cell><cell>d=275, L=5</cell><cell>d=300, L=5</cell><cell>d=300, L=5</cell></row><row><cell>GINE</cell><cell>d=166, L=8</cell><cell>d=166, L=8</cell><cell>d=208, L=5</cell><cell>d=208, L=5</cell><cell>d=208, L=5</cell></row><row><cell>GatedGCN(+PE/SE)</cell><cell>d=108, L=8</cell><cell>d=108, L=8</cell><cell>d=138, L=5</cell><cell>d=138, L=5</cell><cell>d=138, L=5</cell></row><row><cell>used PE/SE</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>RWSE-16</cell><cell>RWSE-16</cell><cell>RWSE-16</cell></row><row><cell>Transformer+LapPE</cell><cell>d=120, L=4</cell><cell>d=120, L=4</cell><cell>d=120, L=4</cell><cell>d=120, L=4</cell><cell>d=120, L=4</cell></row><row><cell></cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell></row><row><cell>SAN+LapPE</cell><cell>d=88, L=4</cell><cell>d=88, L=4</cell><cell>d=84, L=4</cell><cell>d=84, L=4</cell><cell>d=84, L=4</cell></row><row><cell></cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell><cell>LapPE-10</cell></row><row><cell>SAN+RWSE</cell><cell>d=96, L=4</cell><cell>d=96, L=4</cell><cell>d=100, L=4</cell><cell>d=100, L=4</cell><cell>d=100, L=4</cell></row><row><cell></cell><cell>RWSE-16</cell><cell>RWSE-16</cell><cell>RWSE-16</cell><cell>RWSE-16</cell><cell>RWSE-16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The compactness parameter balances spatial and color information when extracting superpixels in SLIC<ref type="bibr" target="#b0">[1]</ref>.<ref type="bibr" target="#b2">3</ref> The labels are based on the annotations provided in Semantic Boundary Dataset (SBD) version of Pascal VOC 2011: https://github.com/shelhamer/fcn.berkeleyvision.org/tree/master/data/pascal</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Dataset Licenses.</head><p>The information on the dataset sources that we used for the proposed LRGB datasets' preparation, the original licenses of use and the release licenses are in <ref type="table">Table A</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC Superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: CW networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scikit-image: Image processing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuelle Neil</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stefanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Workshop on Deep Learning on Graphs: Methods and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks with learnable structural and positional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time volume graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">E</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><forename type="middle">Rezk</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Siggraph 2004 Course Notes</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A large-scale database for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen Horng</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems: Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GeoMol: Torsional geometric generation of molecular 3D conformer ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lagnajit</forename><surname>Octavian-Eugen Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">W</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klavs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Process Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Unified Framework for Rank-based Evaluation Metrics for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Gaklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gyori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Learning Benchmarks Workshop at The WebConf 2022</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 2</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ZINC: a free tool to discover chemistry for biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Sign and basis invariant networks for spectral graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13013</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pre-training molecular graph representation with 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duncan E Mcree</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>Practical protein crystallography. 2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<title level="m">GraphiT: Encoding graph structure in transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21824" to="21840" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<ptr target="https://www.nature.com/scitable/definition/peptide-317/" />
	</analytic>
	<monogr>
		<title level="j">Nature.com. Learn Science at Scitable: Peptide</title>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascalvoc</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html" />
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical graph neural nets can capture long-range interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 31st International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12454</idno>
		<title level="m">Recipe for a General, Powerful, Scalable Graph Transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9558" to="9568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04810</idno>
		<title level="m">Benchmarking graphormer on large-scale molecular modeling datasets</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SATPdb: a database of structurally annotated therapeutic peptides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumardeep</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Kumar Dhanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Bhalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><forename type="middle">Sadullah</forename><surname>Usmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Tuknait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gajendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1119" to="1126" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graph networks with spectral message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Stachenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00079</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d infomax improves gnns for molecular property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prudencio</forename><surname>Tossou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTI, Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Neural Networks: Foundations, Frontiers, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning neural generative dynamics for molecular conformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nested graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">3 Peptides-func and Peptides-struct peptide sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno>GLLGPLLKIAAKVGKNLL&quot; Length_a: -0.39817</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">3: Large size visualization of Figure 3. Top: 3D Visualization of &quot;GLLGPLLKI-AAKVGKNLL&quot; peptide. Bottom: The molecular graph for the same peptide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Figure</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
