<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxue</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Scene Parsing and Reconstruction ? Analysis-by-Synthesis ? Holistic Scene Grammar ? Markov chain Monte Carlo</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a computational framework to jointly parse a single RGB image and reconstruct a holistic 3D configuration composed by a set of CAD models using a stochastic grammar model. Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene structure, which characterizes a joint distribution over the functional and geometric space of indoor scenes. The proposed HSG captures three essential and often latent dimensions of the indoor scenes: i) latent human context, describing the affordance and the functionality of a room arrangement, ii) geometric constraints over the scene configurations, and iii) physical constraints that guarantee physically plausible parsing and reconstruction. We solve this joint parsing and reconstruction problem in an analysis-by-synthesis fashion, seeking to minimize the differences between the input image and the rendered images generated by our 3D representation, over the space of depth, surface normal, and object segmentation map. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable solution space, jointly optimizing object localization, 3D layout, and hidden human context. Experimental results demonstrate that the proposed algorithm improves the generalization ability and significantly outperforms prior methods on 3D layout estimation, 3D object detection, and holistic scene understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The complexity and richness of human vision are not only reflected by the ability to recognize visible objects, but also to reason about the latent actionable information <ref type="bibr">[1]</ref>, including inferring latent human context as the functionality of a scene <ref type="bibr">[2,</ref><ref type="bibr">3]</ref>, reconstructing 3D hierarchical geometric structure <ref type="bibr">[4,</ref><ref type="bibr">5]</ref>, and complying with the physical constraints that guarantee the physically plausible scene configurations <ref type="bibr">[6]</ref>. Such rich understandings of an indoor scene are the essence for building an intelligent computational system, which transcends the prevailing appearance-and geometry-based recognition tasks to account also for the deeper reasoning of observed images or patterns.  <ref type="figure" target="#fig_2">Fig. 1</ref>: Illustration of the proposed holistic 3D indoor scene parsing and reconstruction in an analysis-by synthesis fashion. A 3D representation is initialized by individual vision modules (e.g., object detection, 2D layout estimation). A joint inference algorithm compares the differences between the rendered normal, depth, and segmentation map with the ones estimated directly from the input RGB image, and adjust the 3D structure iteratively.</p><p>One promising direction is analysis-by-synthesis <ref type="bibr">[7]</ref> or "vision as inverse graphics" <ref type="bibr">[8,</ref><ref type="bibr">9]</ref>. In this paradigm, computer vision is treated as an inverse problem as opposed to computer graphics, of which the goal is to reverse-engineer hidden factors occurred in the physical process that produces observed images.</p><p>In this paper, we embrace the concept of vision as inverse graphics, and propose a holistic 3D indoor scene parsing and reconstruction algorithm that simultaneously reconstructs the functional hierarchy and the 3D geometric structure of an indoor scene from a single RGB image. <ref type="figure" target="#fig_2">Figure 1</ref> schematically illustrates the analysis-by-synthesis inference process. The joint inference algorithm takes proposals from various vision modules and infers the 3D structure by comparing various projections (i.e., depth, normal, and segmentation) rendered from the recovered 3D structure with the ones directly estimated from an input image.</p><p>Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the hierarchical structure of a scene. As illustrated in <ref type="figure" target="#fig_4">Figure 2</ref>, our HSG decomposes a scene into latent groups in the functional space (i.e., hierarchical structure including activity groups) and object instances in the geometric space (i.e., CAD models). For the functional space, in contrast to the conventional method that only models the object-object relations, we propose a novel method to model human-object relations by imagining latent human in activity groups to further help explain and parse the observed image. For the geometric space, the geometric attributes (e.g., size, position, orientation) of individual objects are taken into considerations, as well as the geometric relations (e.g., supporting relation) among them. In addition, physical constraints (e.g., collision among the objects, violations of the layout) are incorporated to generate a physically plausible 3D parsing and reconstruction of the observed image.</p><p>Here, an indoor scene is represented by a parse graph (pg) of a grammar, which consists of a hierarchical structure and a Markov random field (MRF) over terminal nodes that captures the rich contextual relations between objects and room layout (i.e., the room configuration of walls, floors, and ceilings).</p><p>A maximum a posteriori probability (MAP) estimate is designed to find the optimal solution that parses and reconstructs the observed image. The likelihood measures the similarity between the observed image and the rendered images projected from the inferred pg onto various 2D image spaces. Thus, the pg can be iteratively refined by sampling an MCMC with simulated annealing based on posterior probability. We evaluate our method on a large-scale RGB-D dataset by comparing the reconstructed 3D indoor rooms with the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Scene Parsing: Existing scene parsing approaches fall into two streams. i) Discriminative approaches <ref type="bibr">[10]</ref><ref type="bibr">[11]</ref><ref type="bibr">[12]</ref><ref type="bibr">[13]</ref><ref type="bibr">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr">[16]</ref> classify each pixel to a semantic label. Although prior work has achieved high accuracy in labeling the pixels, these methods lack a general representation of visual vocabulary and a principle approach to exploring the semantic structure of a general scene. ii) Generative approaches <ref type="bibr">[17]</ref><ref type="bibr">[18]</ref><ref type="bibr">[19]</ref><ref type="bibr">[20]</ref><ref type="bibr">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr">[23]</ref><ref type="bibr">[24]</ref> can distill scene structure, making it closer to human-interpretable structure of a scene, enabling potential applications in robotics, VQA, etc. In this paper, we combine those two streams in an analysis-by-synthesis framework to infer the hidden factors that generate the image. Scene Reconstruction from a Single Image: Previous approaches [25-27] of indoor scene reconstruction from a single RGB image can be categorized into three streams. i) 2D or 3D room layout prediction by extracting geometric features and ranking the 3D cuboids proposals <ref type="bibr">[28]</ref><ref type="bibr">[29]</ref><ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref>. ii) By representing objects via geometric primitives or CAD models, previous approaches [36-44] utilize 3D object recognition or pose estimation to align object proposals to a RGB or depth image. iii) Joint estimation of the room layout and 3D objects with contexts <ref type="bibr">[18, 19, 22-24, 33, 45, 46]</ref>. In particular, <ref type="bibr">Izadinia et al. [33]</ref> show promising results in inferring the layout and objects without the contextual relations and physical constraints. In contrast, our method jointly models the hierarchical scene structure, hidden human context and physical constraints, providing a semantic representation for holistic scene understanding. Furthermore, the proposed method presents a joint inference algorithm using MCMC, which in theory can achieve a global optimal. Scene Grammar: Scene grammar models have been used to infer the 3D structure and functionality from a RGB image <ref type="bibr">[3,</ref><ref type="bibr">17,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b22">47]</ref>. Our HSG differs from <ref type="bibr">[17,</ref><ref type="bibr">18]</ref> in two aspects: i) Our model represents the 3D objects with CAD models rather than geometric primitives, capable of modeling detail contextual relations (e.g., supporting relation), which provides better realization of parsing  <ref type="figure" target="#fig_4">Fig. 2</ref>: An indoor scene represented by a parse graph (pg ) of the HSG that spans across the functional space and the geometric space. The functional space characterizes the hierarchical structure and the geometric space encodes the spatial entities with contextual relations. and reconstruction. ii) We infer hidden human and activity groups in the HSG, which helps the explanation and parsing. Compared to <ref type="bibr">[3,</ref><ref type="bibr" target="#b22">47]</ref>, we model and parse the 3D structure of objects and layouts from a single RGB image, rather than the labelled point-clouds using RGB-D images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>This paper makes five major contributions:</p><p>1. We integrate geometry and physics to interpret and reconstruct indoor scenes with CAD models. We jointly optimize 3D room layouts and object configurations, largely improving the performance of scene parsing and reconstruction on SUN RGB-D dataset [45].</p><p>2. We incorporate hidden human context (i.e., functionality) into our grammar, enabling to imagine latent human pose in each activity group by grouping and sampling. In this way, we can optimize the joint distribution of both visible and invisible <ref type="bibr" target="#b23">[48]</ref> components of the scene.</p><p>3. We propose a complete computational framework to combine generative model (i.e., a stochastic grammar), discriminative models (i.e., direct estimations of depth, normal, and segmentation maps), and graphics engines (i.e., rendered images) in scene parsing and reconstruction. 4. To the best of our knowledge, ours is the first work to use the inferred depth, surface normal and object segmentation map to assist parsing and reconstructing 3D scenes (both room layout and multiple objects). Note that <ref type="bibr" target="#b24">[49]</ref> uses similar intermediate representation for a single object.</p><p>5. By learning the supporting relations among objects, the proposed method eliminates the widely adopted assumption in previous work that all objects must stand on the ground. Such flexibility of the model yields better parsing and reconstruction of the real-world scenes with complex object relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Holistic Scene Grammar</head><p>We represent the hierarchical structure of indoor scenes by a Holistic Scene Grammar (HSG). An HSG consists of a latent hierarchical structure in the functional space F and terminal object entities in the geometric space G. The intuition is that, for man-made environments, the object arrangement in the geometric space should be a "projection" from the functional space (i.e., human activities). The functional space as a probabilistic context free grammar (PCFG) captures the hierarchy of the functional groups, and the geometric space captures the spatial contexts among objects by defining an MRF on the terminal nodes. The two spaces together form a stochastic context-sensitive grammar (SCSG). The HSG starts from a root scene node and ends with a set of terminal nodes. An indoor scene is represented by a parse graph pg as illustrated in <ref type="figure" target="#fig_4">Figure 2</ref>.</p><p>Definition: The stochastic context-sensitive grammar HSG is defined as a 5tuple S, V, R, E, P . S denotes the root node of the indoor scene. V is the vertex set that includes both non-terminal nodes V f ? F and terminal nodes V g ? G. R denotes the production rule, and E the contextual relations among the terminal nodes, which are represented by the horizontal links in the pg. P is the probability model defined on the pg.</p><formula xml:id="formula_0">Functional Space F: The non-terminal nodes V f = {V c f , V a f , V o f , V l f } ? F consist of the scene category nodes V c f , activity group nodes V a f , objects nodes V o f , and layout nodes V l f .</formula><p>Geometric Space G: The terminal nodes V g = {V o g , V l g } ? G are the CAD models of object entities and room layouts. Each object v ? V o g is represented as a CAD model, and the object appearance is parameterized by its 3D size, location, and orientation. The room layout v ? V l g is represented as a cuboid which is further decomposed into five planar surfaces of the room (left wall, right wall, middle wall, floor, and ceiling with respect to the camera coordinate).</p><p>Production Rule R: The following production rules are defined for HSG:</p><formula xml:id="formula_1">? S ? V c f : scene ? category 1 | category 2 | . . . (e.g., scene ? office | kitchen) ? V c f ? V a f ? V l f</formula><p>: category ? activity groups ? layout (e.g., office ? (walking, reading) ? layout)</p><p>? V a f ? V o f : activity group ? functional objects (e.g., sitting ? (desk, chair)) where ? denotes the deterministic decomposition, | alternative explanations, and () combination. Contextual relations E capture relations among objects, including their relative positions, relative orientations, grouping relations, and supporting relations. The objects could be supported by either other objects or the room layout; e.g., a lamp could be supported by a night stand or the floor.</p><p>Finally, a scene configuration is represented by a pg, whose terminals are room layouts and objects with their attributes and relations. As shown in <ref type="figure" target="#fig_4">Figure 2</ref>, a pg can be decomposed as pg = (pg f , pg g ), where pg f and pg g denote the functional part and geometric part of the pg, respectively. E ? pg g denotes the contextual relations in the terminal layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Probabilistic Formulation</head><p>The objective of the holistic scene parsing is to find an optimal pg that represents all the contents and relations observed in the scene. Given an input RGB image I, the optimal pg could be derived by an MAP estimator,</p><formula xml:id="formula_2">p(pg|I) ? p(pg) ? p(I|pg) (1) ? p(pg f ) ? p(pg g |pg f ) ? p(I|pg g ) (2) = 1 Z exp ?E(pg f ) ? E(pg g |pg f ) ? E(I|pg g ) ,<label>(3)</label></formula><p>where the prior probability p(pg) is decomposed into p(pg f )p(pg g |pg f ), and p(I|pg) = p(I|pg g ) since the image space is independent of the functional space given the geometric space. We model the joint distribution with a Gibbs distribution; E(pg f ), E(pg g |pg f ) and E(I|pg g ) are the corresponding energy terms.</p><p>Functional Prior E(pg f ) characterizes the prior of the functional aspect in a pg, which models the hierarchical structure and production rules in the functional space. For production rules of alternative explanations | and combination (), each rule selects child nodes and the probability of the selections is modeled with a multinomial distribution. The production rule ? is deterministically expanded with probability 1. Given the production rules R, the energy can be written as E(pg f ) = r i ?R ? log p(r i ).</p><p>Geometric Prior E(pg g |pg f ) is the prior of the geometric aspect in a pg. Besides modeling the size, position and orientation distribution of each object, we also consider two types of contextual relations E = {E s , E a } among the objects: i) relations E s between supported objects and their supporting objects; ii) relations E a between imagined human and objects in an activity group.</p><p>We define different potential functions for each type of contextual relations, constructing an MRF in the geometric space including four terms:</p><formula xml:id="formula_3">E(pg g |pg f ) = E sc (pg g |pg f ) + E spt (pg g |pg f ) + E grp (pg g |pg f ) + E phy (pg g ). (4) ? Size Consistency E sc constrains the size of an object. E sc (pg g |pg f ) = v i ?V o g ? log p(s i |V o f ),</formula><p>where s i denotes the size of object v i . We model the distribution of object scale in a non-parametric way, i.e., kernel density estimation (KDE).</p><p>? Supporting Constraint E spt characterizes the contextual relations between supported objects and supporting objects (including floors, walls and ceilings). We model the distribution with their relative heights and overlapping areas:</p><formula xml:id="formula_4">E spt (pg g |pg f ) = (v i ,v j )?E s K o (v i , v j ) + K h (v i , v j ) ? ? s log p v i , v j | V l f , V o f ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">K o (v i , v j ) = 1 ? area(v i ? v j )/area(v i )</formula><p>defines the overlapping ratio in xy-plane, and K h (v i , v j ) defines the relative height between the lower surface of v i and the upper surface of v j . K o (?) and K h (?) is 0 if supporting object is floor and wall, respectively.</p><formula xml:id="formula_6">p(v i , v j |V l f , V o f )</formula><p>is the prior frequency of the supporting relation modeled by multinoulli distributions. ? s is a balancing constant.</p><p>? Human-Centric Grouping Constraint E grp . For each activity group, we imagine the invisible and latent human poses to help parse and understand the scene. The intuition is that the indoor scenes are designed to serve human daily activities, thus the indoor images should be jointly interpreted by the observed entities and the unobservable human activities. This is known as the Dark Matter <ref type="bibr" target="#b23">[48]</ref> in computer vision that drives the visible components in the scene. Prior methods on scene parsing often merely model the object-object relations. In this paper, we go beyond passive observations to model the latent human-object relations, thereby proposing a human-centric grouping relationship and a joint inference algorithm over both the visible scene and the invisible latent human context. Specifically, for each activity group v ? V a f , we define correspondent imagined human with a six tuple &lt; y, ?, t, r, s,? &gt;, where y is the activity type, ? ? R 25?3 is the mean human pose (represented by 25 joints) of activity type y, t denotes the translation, r denotes the rotation, s denotes the scale, and? is the imagined human skeleton:? = ? ? r ? s + t. The energy among the imagined human and objects is defined as:</p><formula xml:id="formula_7">E grp (pg g |pg f ) = v i ?V a f E grp (? i |v i ) = v i ?V a f v j ?ch(v i ) D d (? i , ? j ;d) + D h (? i , ? j ;h) + D o (? i , ? j ;?),<label>(6)</label></formula><p>where ch(v i ) denotes the set of child nodes of v i , ? j denotes the 3D position of v j . D d (?), D h (?) and D o (?) denote geometric distances, heights and orientation differences, respectively, calculated by the center of the imagined human pose to the object center subtracted by their mean (i.e.,d,h and?).</p><p>? Physical Constraints: Additionally, in order to avoid violating the physical laws during parsing, we define the physical constraints E phy (pg g ) to penalize physical violations. Exceeding the room cuboid or overlapping among the objects are defined as violations. This term is formulated as:</p><formula xml:id="formula_8">E phy (pg g ) = v i ?V o g ( v j ?V o g \v i O o (v i , v j ) + v j ?V l g O l (v i , v j )),<label>(7)</label></formula><p>where O o (?) denotes the overlapping area between objects, and O l (?) denotes the area of objects exceeding the layout. Likelihood E(I|pg g ) characterizes the similarity between the observed image and the rendered image generated by the parsing results. Due to various lighting conditions, textures, and material properties, there will be an inevitable difference between the rendered RGB images and the observed scenes. Here, instead of using RGB images, we solve this problem in an analysis-by-synthesis fashion by comparing the depth, surface normal, and object segmentation map.</p><p>By combining generative models and discriminative models, the proposed approach tries to reverse-engineer the hidden factors that generate the observed image. Specifically, we first use discriminative methods to project the observed image I to various feature spaces. In this paper, we directly estimate three intermediate images-depth map ? d (I), surface normal map ? n (I) and object segmentation map ? m (I), as the feature representation of the observed image I.</p><p>Meanwhile, a pg inferred by our method represents the 3D structure of the observed image, which is used to reconstruct image I to recover the correspond-ing depth map ? d (I ), surface normal map ? n (I ), and object segmentation map ? m (I ) through a forward graphics rendering.</p><p>Finally, we compute the likelihood term by comparing these rendered results from the generative model with the directly estimated results calculated by the discriminative models. Specifically, the likelihood is computed by the pixel-wise differences between the two sets of maps,</p><formula xml:id="formula_9">E(I|pg g ) = D p (? d (I), ? d (I )) + D p (? n (I), ? n (I )) + D p (? m (I), ? m (I )),<label>(8)</label></formula><p>where D p (?) is the sum of pixel-wise Euclidean distances between two maps. Note a weight is associated with each energy term, which is learned by cross-validation or set empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>Given a single RGB image as the input, the goal of inference is to find the optimal pg that best explains the hidden factors that generate the observed image while recovering the 3D scene structure. The inference includes three major steps:</p><p>? Room geometry estimation: estimate the room geometry by predicting the 2D room layout and the camera parameter, and by projecting the estimated 2D layout to 3D. Details are provided in subsection 4.1.</p><p>? Objects initialization: detect objects and retrieve CAD models correspondingly with the most similar appearance, then roughly estimate their 3D poses, positions, sizes, and initialize the support relations. See subsection 4.2.</p><p>? Joint inference: optimize the objects, layout and hidden human context in the 3D scene in an analysis-by-synthesis fashion by maximizing the posterior probability of the pg. Details are provided in subsection 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Room Geometry Estimation</head><p>Although recent approaches <ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref> are capable of generating a relatively robust prediction of the 2D room layout using CNN features, 3D room layout estimations are still inaccurate due to its sensitivity to camera parameter estimation in clusttered scenes. To address the inconsistency between the 2D layout estimation and camera parameter estimation, we design a deep neural network to estimate the 2D layout, and use the layout heatmap to estimate the camera parameter.</p><p>2D Layout Estimation: Similar to [34], we represent the 2D layout with its room layout type and keypoint positions. The network structure is provided in the supplementary material. The network optimizes the Euclidean loss for layout heatmap regression and the cross-entropy loss for room type estimation.</p><p>Camera Parameter: Traditional geometry-based method [28] computes the camera parameter by estimating the vanishing points from the observed image, which is sensitive and unstable in cluttered indoor scenes with heavy occlusions. Inspired by [43], we propose a learning-based method that uses the keypoints heatmaps to predict the camera parameters, i.e., focal length, together with the yaw, pitch, and roll angles of the camera. Since the yaw angle has already been incorporated into the evaluation of room layout, we estimate the remaining three variables (focal length, pitch and roll) by stacking four FC layers (1024-128-16-3) on the keypoint heatmaps.</p><p>3D Layout Initialization: Using the estimated 2D layout and camera parameters, we project the corners of the 2D layout to 3D in order to obtain a 3D room cuboid. We assume the cameras and the ceilings are 1.2m and 3.0m high, respectively. For simplicity, we translate and rotate the 3D rooms so that one of the visible room corners is at the origin of the world coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Objects Initialization</head><p>We fine-tune the Deformable Convolutional Networks <ref type="bibr" target="#b25">[50]</ref> using Soft-NMS <ref type="bibr" target="#b26">[51]</ref> to detect 2D bounding boxes. To initialize the 3D objects, we retrieve the most similar CAD models and initialize their 3D poses, sizes, and positions.</p><p>Model Retrieval: We consider all the models in the ShapeNetSem repository <ref type="bibr" target="#b27">[52,</ref><ref type="bibr" target="#b28">53]</ref> and render each model from 48 viewpoints consisting of uniformly sampled 16 azimuth and 3 elevation angles. We extract 7 ? 7 features from the ROI-pooling layer of the fine-tuned detector of images in the detected bounding boxes and candidate rendered images. By ranking the cosine distance between each detected object feature and rendered image feature in the same object category, we obtain the top-10 CAD models with corresponding poses.</p><p>Geometric Attributes Estimation: The geometric attributes of an object are represented by a 9D vector of 3D pose, position, and size, where 3D poses are initialized from the retrieval procedure. Prior work roughly projected 2D points to 3D, and recovered the 3D position and size by assuming that all the objects are on the floor. Such approach shows limitations in complex scenarios.</p><p>Without making the above assumption, we estimate the depth of each object by computing the average depth value of the pixels that are in both the detection bounding box and the segmentation map. We then compute its 3D position using the depth value. Empirically, this approach is more robust since per-pixel depth estimation error is small even in cluttered scenes. To avoid the alignment problem of the 2D bounding boxes, we initialize the object size by sampling object sizes from a learned distribution and choose the one with the largest probability.</p><p>Supporting Relation Estimation: For each object v i ? V o f , we find its supporting object v * j of minimal supporting energy from objects or layout:</p><formula xml:id="formula_10">v * j = arg min v j K o (v i , v j ) + K h (v i , v j ) ? ? s log p(v i , v j |V l f , V o f ), v j ? (V l f , V o f ). (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Inference</head><p>Given an image I, we first estimate the room geometry, object attributes and relations as described in the above two subsections. As summarized in Alg.1, the joint inference includes: (1) optimize the objects and layout ( <ref type="figure">Figure 3</ref>); (2) group objects, assign activity label and imagine human pose in each activity group; and (3) optimize the objects, layout and human pose iteratively. proposes rotation of the object with a specified angle. Each dynamic can diffuse in two directions, e.g., each object can translate in direction of '+x' and '?x', or rotate in direction of clockwise and counterclockwise. By computing the local gradient of P (pg|I), the dynamics propose to move following the direction of the gradient with a proposal probability of 0.8, or the inverse direction of the gradient with proposal probability of 0.2.</p><p>? Layout Dynamics: Dynamics q l 1 translates the faces of the layout, which also optimizes the camera height when translating the floor. Dynamics q l 2 rotates the layout.</p><p>? Human pose Dynamics q h 1 , q h 2 and q h 3 are designed to translate, rotate and scale the human pose, respectively.</p><p>Given the current pg, each dynamic will propose a new pg according to a proposal probability p(pg |pg, I). The proposal is accepted according to an acceptance probability ?(pg ? pg ) defined by the Metropolis-Hasting algorithm <ref type="bibr" target="#b29">[54]</ref>: ?(pg ? pg ) = min(1, p(pg|pg , I)p(pg |I) p(pg |pg, I)p(pg|I) ).</p><p>In step (2), we group objects and assign activity labels. For each type of activity, there is a object category which has the highest occurrence frequency (i.e., chair in activity 'reading'). Intuitively, the correspondence between objects and activities should be n-to-n but not n-to-one, which means each object can belong to several activity groups. In order to find out all possible activity groups, for each type of activity, we define an activity group around each major object and incorporate nearby objects (within a distance threshold) with prior larger Algorithm 1 Joint inference algorithm 1: Given Image I, initialized parse graph pg init 2: procedure Step1(V o g , V l g ) Inference without hidden human context</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for different temperatures do Different temperatures are adopted in simulated annealing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for ? 1 iterations do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>randomly choose layout, apply layout dynamics to optimize layout V l g 6:</p><p>for each object v i ? V o g do 7:</p><p>for ? 2 iterations do 8:</p><p>randomly apply object dynamics to optimize object v i</p><formula xml:id="formula_12">9: procedure Step2(V a f , {?})</formula><p>Inference of hidden human context 10:</p><p>group objects and assign activity labels (see last paragraph in subsection 4.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>for each activity group v i ? V a f do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>repeat 13:</p><p>randomly apply human pose dynamics to optimize? i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>until E(? i |v i ) converges Maximizing grouping energy in Equation <ref type="formula" target="#formula_14">11</ref> 15</p><formula xml:id="formula_13">: procedure Step3(V o g , V l g , {?})</formula><p>Iterative inference of whole parse graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>for different temperatures do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>for ? 3 iterations do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>randomly choose layout, objects or human pose</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>apply random dynamics to minimize P (pg|I) 20: Return pg optimized <ref type="figure">Fig. 4</ref>: Sampled human poses in various indoor scenes. Objects in multiple activity groups have multiple poses. We visualize the pose with the highest likelihood.</p><p>than 0. For each activity group v i ? V a f , the pose of the imagined human is estimated by maximizing the likelihood p(v i |? i ), which is equivalent to minimize the grouping energy E grp (? i |v i ) defined in <ref type="bibr">Equation 6</ref>, <ref type="figure">Figure 4</ref> shows the results of sampled human poses in various indoor scenes.</p><formula xml:id="formula_14">y * i , m * i , t * i , r * i , s * i = arg min y i ,m i ,t i ,r i ,s i E grp (? i |v i ),<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We use the SUN RGB-D dataset [45] to evaluate our approach on 3D scene parsing, 3D reconstruction, as well as other 3D scene understanding tasks. The  dataset has 5050 testing images and 10,355 images in total. Although it provides RGB-D data, we only use the RGB images as the input for training and testing. <ref type="figure" target="#fig_0">Figure 5</ref> shows some qualitative parsing results (top 20%). We evaluate our method on three tasks: i) 3D layout estimation, ii) 3D object detection, and iii) holistic scene understanding with all the 5050 testing images of SUN RGB-D across all scene categories. The capability of generalization to all the scene categories is difficult for most of the conventional methods due to the inaccuracy of camera parameter estimation and severe sensitivity to the occlusions in cluttered scenes. In this paper, we alleviate it by using the proposed learning-based camera parameter estimation and a novel method to initialize the geometric attributes. In addition, we also achieve the state-of-the-art results in 2D layout estimation on LSUN dataset <ref type="bibr" target="#b30">[55]</ref> and Hedau dataset [28]. The implementation details, and additional results of camera parameter estimation and 2D layout estimation are summarized in the supplementary material.</p><p>3D Layout Estimation: The 3D room layout is optimized using the proposed joint inference. We compare the estimation by our method (with and without joint inference) with 3DGP <ref type="bibr">[19]</ref>. Following the evaluation protocol defined in [45], we calculate the average Intersection over Union (IoU) between the free space from the ground truth and the free space estimated by our method. <ref type="table" target="#tab_3">Table 1</ref> shows our method outperforms 3DGP by a large margin. We also improve the performance by 8.2% after jointly inferring the objects and layout, demonstrating the usefulness of integrating the joint inference process.</p><p>Since IM2CAD [33] manually selected 484 images from living rooms and bedrooms without releasing the image list, we compare our method with them on the entire set of living rooms and bedrooms. <ref type="table" target="#tab_3">Table 1</ref> shows our method surpasses IM2CAD, especially after incorporating the joint inference process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Object Detection:</head><p>We evaluate our 3D object detection results using the metrics defined in <ref type="bibr">[45]</ref>. We compute the mean average precision (mAP) using the 3D IoU between the predicted and ground truth 3D bounding boxes. In the absence of depth, we adjust threshold IoU from 0.25 (evaluation setting with depth as the input) to 0.15 and report our results in <ref type="table" target="#tab_4">Table 2</ref>. 15 out of 30 object categories are reported here due to the limited space; full table is reported in the supplementary material. The results indicate our method not only exceeds the detection score by a significant margin but also makes it possible to evaluate the entire object categories. Note that although IM2CAD also evaluates the detection, they use the metric related to a specified distance threshold. Here, we also compare with IM2CAD on the subset with this special metric rather than IoU threshold. We are able to obtain an mAP of 80.2%, higher than an mAP of 74.6% reported in the IM2CAD.</p><p>Holistic Scene Understanding: We estimate the detailed 3D scene including both objects and room layout. Using the metrics proposed in [45], we evaluate the geometric precision P g , geometric recall R g , and semantic recall R r with the IoU threshold set to 0.15. We also evaluate the IoU of the free space (3D voxels inside the room polygon but outside any object bounding box) between the ground truth and the estimation. <ref type="table" target="#tab_3">Table 1</ref> shows that the proposed method <ref type="table" target="#tab_8">Table 3</ref>: Ablative analysis of our method on SUN RGB-D dataset. We evaluate on holistic scene understanding under different settings. We denote support relation as C 1 , physical constraint as C 2 and human imagination as C 3 . Similarly, we denote the setting of only optimizing the layout during inference as S 4 , only optimizing the objects during inference as S 5 demonstrates a significant improvement. Moreover, we improve the initialization result by 12.2% on geometric precision, 7.5% on geometric recall, 6.1% on semantic recall, and 4.1% on free space estimation. The improvement of total scene understanding indicates that the joint inference can largely improve the performance of each task. Using the same setting with 3D layout estimation, we compare with IM2CAD [33] and improve the free space IoU by 3.1%.</p><formula xml:id="formula_15">Setting w/o C1 w/o C2 w/o C3 w/o (C1,</formula><p>Ablative Analysis: The proposed HSG incorporates several key components including supporting relations, physics constraints and latent human contextual relations. To analyze how each component would influence the final results, as well as how much the joint inference process would benefit each task, we conduct the ablative analysis on holistic scene understanding under different settings, through turning on and off certain components or skipping certain steps during joint inference. The experiments are tested on the subset of offices where we incorporate the latent human context. <ref type="table" target="#tab_8">Table 3</ref> summarizes the results. Among all the energy terms we incorporate, physical constraints influence the performance the most, which demonstrates the importance of the physical common sense during inference. It also reflects the efficiency of joint inference as the performances would drop by a large margin without the iterative joint inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present an analysis-by-synthesis framework to recover the 3D structure of an indoor scene from a single RGB image using a stochastic grammar model integrated with latent human context, geometry and physics. We demonstrate the effectiveness of our algorithm from three perspectives: i) the joint inference algorithm significantly improves results in various individual tasks and ii) outperforms other methods; iii) ablative analysis shows each of module plays an important role in the whole framework. In general, we believe this will be a step towards a unifying framework for the holistic 3D scene understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Learning of Prior Knowledge</head><p>The learning process of our method includes two steps: i) collecting the statistics of scene categories, object categories, object sizes, and supporting relations from SUN RGB-D dataset <ref type="bibr">[1]</ref>; ii) collecting the statistics of grouping occurrences and the geometric relations between objects and human from Watch-n-Patch <ref type="bibr">[2]</ref>. Using SUN RGB-D, we model the prior of scene types, object categories and support relations by multinoulli distributions. For example, a lamp is supported by the floor with a probability of 0.4 and by a desk with a probability of 0.2. The branching probability is simply counting the frequency of each alternative choice. The distribution of the object sizes is learned via non-parametric kernel density estimation.</p><p>The human-centric grouping occurrence and human-object interactions in 3D space are learned from the Watch-n-Patch. This dataset collects the RGB-D videos of human activities in o ces and kitchens. Since some activities are irrelevant with objects, we learn the activities of 'reading', 'play-computer', 'take-item' and 'put-down-item' in all the o ce videos. For each activity, we first extract key frames from each sequence with group activity labels. Then we compute the occurrence frequency of the objects around human within a distance threshold, and model the prior of object category using a multinomial distribution. The geometric relations between the objects and humans are similarly learned by fitting normal distributions of relative distance, height, and orientation between each joint of a human pose and the object center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">2D Room Layout Estimation</head><p>Similar to <ref type="bibr">[3]</ref>, we use a keypoint-based room layout representation to train our network. <ref type="figure" target="#fig_2">Figure 1</ref> shows the regular room types defined in <ref type="bibr">[4]</ref> with their respective keypoints.</p><p>Our model is able to predict both keypoint and room type from an input image using a single model. To achieve this goal, we increase the number of   channels in the output layer to match the total number of keypoints (in total 48) of all 11 room types. The cost function is the same as described in <ref type="bibr">[3]</ref>, which incorporates the Euclidean loss for layout heatmap regression and the cross-entropy loss for room type estimation. <ref type="figure" target="#fig_4">Figure 2</ref> shows our network architecture. Compared with <ref type="bibr">[3]</ref>, we use the "stacked hourglass" network <ref type="bibr">[6]</ref> as our basic network architecture rather than SegNet <ref type="bibr">[7]</ref>. Our network consists of multiple stacked hourglass modules which allow for repeated bottom-up, top-down inference.</p><p>The input to the network is 256x256. The output of the network is the room type keypoint heatmaps in a resolution 64x64 within a respect room type category label. We use the Adam optimizer <ref type="bibr">[8]</ref> with batch size 16, initial learning rate 0.0001. We train 150 epochs, which takes about 2 days on a 12GB NVIDIA Titan X GPU. We also degrade the gradient of background pixels by multiplying them with a factor of 0.2 to prevent the output converges to zero due to the imbalance between foreground and background distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation Details</head><p>For 2D object detection, we fine-tune the object detector on SUN RGB-D with 30 object categories. Since <ref type="bibr">[4]</ref> and <ref type="bibr">[9]</ref> have no ground-truth of the camera parameter, we train the 2D layout estimation module using <ref type="bibr">[4]</ref> as the initial model, followed by using the feature of the heatmap (stacking three FC layers (512-16-1)) to further train camera parameter and scene category on SUN RGB-D. During the initialization and joint inference process, we use the depth estimation model as described in <ref type="bibr">[10]</ref>, surface normal estimation in <ref type="bibr">[11]</ref>, and semantic segmentation in <ref type="bibr">[12]</ref>. These models are trained on the training set of the SUN RGB-D or NYU v2 dataset <ref type="bibr">[13]</ref> (included in the SUN RGB-D). In this paper, we further incorporate human context inference on the subset of o ces and skip it on other scenes. During joint inference, we fix the scene category, object categories and support relations to reduce the computational complexity. We used OpenGL <ref type="bibr">[14]</ref> to render the depth, surface normal and segmentation map. Rendering each map takes about 1 second. On average, our joint inference process takes about one hour for each image on a single CPU core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation of 2D Layout Estimation</head><p>We evaluate the 2D layout estimation without joint inference on LSUN dataset <ref type="bibr">[4]</ref> and Hedau dataset <ref type="bibr">[9]</ref>. The LSUN dataset consists of 4000 training, 394 validation and 1000 test images. The Hedau dataset contains 209 training, 56 validation and 105 test images. We follow the standard evaluation procedure <ref type="bibr">[17]</ref> and use pixel errors and keypoint errors as two evaluation metrics. Pixel errors compute the pixel-wise error between the ground truth and estimations of the surface label, and the keypoint errors only considers the average Euclidean distance between the annotated and estimated keypoints. As reported in <ref type="table" target="#tab_3">Table 1</ref>, our approach achieves 5.22% keypoint error, which outperforms all existing methods and comparable pixel error with the previous best results <ref type="bibr">[20]</ref> on both LSUN and Hedau dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Camera Parameter Estimation</head><p>We compute the mean absolute error between our estimation and the groundtruth on testing set of SUN RGB-D. As shown in <ref type="table" target="#tab_4">Table 2</ref>, comparing with the traditional geometry-based method <ref type="bibr">[9]</ref>, the proposed method gains a significant improvement. Quantitative results of the comparison over all the scene categories are shown in <ref type="figure">Figure 3</ref>. Empirically, geometry-based methods perform poorly in cluttered scenes (e.g., storage rooms) and perform well in clean scenes with clear orthogonal lines (e.g., receptions). Our method provides a good estimation which applies to most of the indoor scenes, improving the generalization ability of the monocular reconstruction algorithms. <ref type="figure">Figure 3</ref> shows the comparison in detail over all categories. We can see that the geometry-based method performs well over the scenes with clear lines in three orthogonal directions like receptions, but results in large errors over cluttered scenes like storage rooms. Our method provides a good estimation which applies to most of the indoor scenes, improve the generalization ability for the singleview reconstruction algorithms. <ref type="figure">Figure 4</ref> shows the comparison with 3DGP over all categories; we can also observe that 3DGP fails in some scene categories such as dinette and cafeteria, which further reflects the drawbacks of the geometry-based methods.    <ref type="table" target="#tab_8">Table 3</ref> shows the evaluation of 3D object detection over 30 categories of objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of 3D Layout Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of 3D Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">More Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of the proposed method on SUN RGB-D dataset. The joint inference significantly improves the performance over individual modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>23. Zhang, Y., Song, S., Yumer, E., Savva, M., Lee, J.Y., Jin, H., Funkhouser, T.:Physically-based rendering for indoor scene understanding using convolutional neural networks. In: CVPR. (2017) 24. Zou, C., Li, Z., Hoiem, D.: Complete 3d scene parsing from single rgbd image. arXiv preprint arXiv:1710.09490(2017)25. Hoiem, D., Efros, A.A., Hebert, M.: Automatic photo pop-up. ACM Transactions on Graphics (TOG) (2005) 26. Han, F., Zhu, S.C.: Bottom-up/top-down image parsing by attribute graph grammar. In: ICCV. (2005) 27. Saxena, A., Chung, S.H., Ng, A.Y.: Learning depth from single monocular images. In: Conference on Neural Information Processing Systems (NIPS). (2006) 28. Hedau, V., Hoiem, D., Forsyth, D.: Recovering the spatial layout of cluttered rooms. In: CVPR. (2009) 29. Lee, D.C., Hebert, M., Kanade, T.: Geometric reasoning for single image structure recovery. In: CVPR. (2009) 30. Mallya, A., Lazebnik, S.: Learning informative edge maps for indoor scene layout prediction. In: ICCV. (2015) 31. Dasgupta, S., Fang, K., Chen, K., Savarese, S.: Delay: Robust spatial layout estimation for cluttered indoor scenes. In: CVPR. (2016) 32. Ren, Y., Li, S., Chen, C., Kuo, C.C.J.: A coarse-to-fine indoor layout estimation (cfile) method. In: Asian Conference on Computer Vision (ACCV). (2016) 33. Izadinia, H., Shan, Q., Seitz, S.M.: Im2cad. In: CVPR. (2017) 34. Lee, C.Y., Badrinarayanan, V., Malisiewicz, T., Rabinovich, A.: Roomnet: Endto-end room layout estimation. In: ICCV. (2017) 35. Zhao, H., Lu, M., Yao, A., Guo, Y., Chen, Y., Zhang, L.: Physics inspired optimization on semantic transfer features: An alternative method for room layout estimation. In: CVPR. (2017) 36. Salas-Moreno, R.F., Newcombe, R.A., Strasdat, H., Kelly, P.H., Davison, A.J.: Slam++: Simultaneous localisation and mapping at the level of objects. In: CVPR. (2013) 37. Aubry, M., Maturana, D., Efros, A.A., Russell, B.C., Sivic, J.: Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In: CVPR. (2014) 38. Lim, J.J., Khosla, A., Torralba, A.: Fpm: Fine pose parts-based model with 3d cad models. In: ECCV. (2014) 39. Song, S., Xiao, J.: Sliding shapes for 3d object detection in depth images. In: ECCV. (2014) 40. Tulsiani, S., Malik, J.: Viewpoints and keypoints. In: CVPR. (2015) 41. Bansal, A., Russell, B., Gupta, A.: Marr revisited: 2d-3d alignment via surface normal prediction. In: CVPR. (2016) 42. Song, S., Xiao, J.: Deep sliding shapes for amodal 3d object detection in rgb-d images. In: CVPR. (2016) 43. Wu, J., Xue, T., Lim, J.J., Tian, Y., Tenenbaum, J.B., Torralba, A., Freeman, W.T.: Single image 3d interpreter network. In: ECCV. (2016) 44. Deng, Z., Latecki, L.J.: Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images. In: CVPR. (2017) 45. Song, S., Lichtenberg, S.P., Xiao, J.: Sun RGB-D: A RGB-D scene understanding benchmark suite. In: CVPR. (2015) 46. Song, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T.: Semantic scene completion from a single depth image. In: CVPR. (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Types of room layout. The room types are defined in[4]. These 11 room types cover most of the possible configurations of the indoor scenes under Manhattan world assumption[5].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Network Architecture. The "hourglass" modules work as encoder-decoders which allow for repeated bottom-up, top-down inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Estimation error of focal length.3DGPOurs Quantitative comparisons of 3D layout estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>More qualitative results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1808.02201v1 [cs.CV] 7 Aug 2018</figDesc><table><row><cell cols="2">Input 2D Image</cell><cell></cell><cell>3D Scene Configuration</cell></row><row><cell></cell><cell></cell><cell>3D</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Layout</cell><cell>Parse &amp;</cell></row><row><cell></cell><cell></cell><cell>Initialize</cell><cell>Reconstruct</cell></row><row><cell></cell><cell></cell><cell>Object</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Proposal</cell><cell>Iterate</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Render</cell></row><row><cell></cell><cell>Surface</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Normal</cell><cell></cell><cell></cell></row><row><cell>Direct</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Estimate</cell><cell>Depth</cell><cell>Compare</cell><cell>Project</cell></row><row><cell></cell><cell>Map</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Object</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Mask</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The process of joint inference of objects and layout by MCMC with simulated annealing. Top: depth maps. Middle: normal maps. Bottom: object segmentation maps. Objects and layout are optimized iteratively. Dynamics q o 1 adjusts the position of a random object, which translates the object center in one of the three Cartesian coordinate axes. Instead of translating the object center and changing the object size directly, Dynamics q o</figDesc><table><row><cell>Target</cell><cell>Initialization Iteration 150</cell><cell>Iteration 300 Iteration 500 Iteration 900 Iteration 1200</cell></row><row><cell cols="3">Fig. 3: In each step, we use distinct MCMC processes. Specifically, to traverse non-</cell></row><row><cell cols="3">differentiable solution spaces, we design Markov chain dynamics {q o 1 , q o 2 , q o 3 } for objects, {q l 1 , q l 2 } for layout, and {q h 1 , q h 2 , q h 3 } for human pose. Specifically,</cell></row></table><note>? Object Dynamics:2 translates one of the six faces of the cuboid to generate a smoother diffusion. Dynamics q o 3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>3D scene parsing and reconstruction results on SUN RGB-D dataset</figDesc><table><row><cell>Method</cell><cell># of image</cell><cell>3D Layout Estimation IoU</cell><cell cols="4">Holistic Scene Understanding Pg Rg Rr IoU</cell></row><row><cell>3DGP [19]</cell><cell>5050</cell><cell>19.2</cell><cell>2.1</cell><cell>0.7</cell><cell>0.6</cell><cell>13.9</cell></row><row><cell>Ours (init.)</cell><cell>5050</cell><cell>46.7</cell><cell>25.9</cell><cell>15.5</cell><cell>12.2</cell><cell>36.6</cell></row><row><cell>Ours (joint.)</cell><cell>5050</cell><cell>54.9</cell><cell>37.7</cell><cell>23.0</cell><cell>18.3</cell><cell>40.7</cell></row><row><cell>3DGP [19]</cell><cell>749</cell><cell>33.4</cell><cell>5.3</cell><cell>2.7</cell><cell>2.1</cell><cell>34.2</cell></row><row><cell>IM2CAD [33]</cell><cell>484</cell><cell>62.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.0</cell></row><row><cell>Ours (init.)</cell><cell>749</cell><cell>61.2</cell><cell>29.7</cell><cell>17.3</cell><cell>14.4</cell><cell>47.1</cell></row><row><cell>Ours (joint.)</cell><cell>749</cell><cell>66.4</cell><cell>40.5</cell><cell>26.8</cell><cell>21.7</cell><cell>52.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of 3D object detection on SUN RGB-D dataset</figDesc><table><row><cell>Method</cell><cell cols="13">bed chair sofa table desk toilet fridge sink bathtub bookshelf counter door dresser lamp tv mAP</cell></row><row><cell>[19]</cell><cell>5.62 2.31 3.24 1.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Ours (init.) 45.55 5.91 23.64 4.20 2.50 1.91 14.00 2.12 0.55</cell><cell>2.16</cell><cell cols="6">0.34 0.01 5.69 1.12 0.62 7.35</cell></row><row><cell cols="7">Ours (joint.) 58.29 13.56 28.37 12.12 4.79 16.50 15.18 2.18 2.84</cell><cell>7.04</cell><cell cols="6">1.6 1.56 13.71 2.41 1.04 12.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons of 2D layout estimation on LSUN [4] andHedau dataset[9]   </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Camera parameter estimation.</figDesc><table><row><cell>Method</cell><cell cols="2">Mean Absolute Error focal length pitch roll</cell></row><row><cell>Hedau et al. [9]</cell><cell>141.78</cell><cell>3.45 33.85</cell></row><row><cell>Ours</cell><cell>35.87</cell><cell>3.12 7.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of 3D object detection on SUN RGB-D dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">bed chair</cell><cell>sofa</cell><cell>table</cell><cell>desk</cell><cell>toilet</cell><cell>fridge</cell><cell>sink</cell><cell cols="6">bathtub bookshelf counter door dresser lamp</cell><cell>tv</cell></row><row><cell>[21]</cell><cell cols="2">5.62 2.31</cell><cell>3.24</cell><cell>1.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Ours (init.) 45.55 5.91</cell><cell>23.64</cell><cell>4.20</cell><cell>2.50</cell><cell>1.91</cell><cell>14.00</cell><cell>2.12</cell><cell>0.55</cell><cell>2.16</cell><cell>0.34</cell><cell>0.01</cell><cell>5.69</cell><cell>1.12</cell><cell>0.62</cell></row><row><cell cols="6">Ours (joint.) 58.29 13.56 28.37 12.12 4.79</cell><cell>16.50</cell><cell>15.18</cell><cell>2.18</cell><cell>2.84</cell><cell>7.04</cell><cell cols="5">1.60 1.56 13.71 2.41 1.04</cell></row><row><cell cols="15">nightstand books tvstand sofachair cabinet endtable dressermirror person recyclebin curtain whiteboard mirror picture paper computer</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>5.83</cell><cell cols="2">0.00 3.04</cell><cell>8.87</cell><cell>0.00</cell><cell>0.65</cell><cell>17.16</cell><cell>1.31</cell><cell>0.00</cell><cell>0.27</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>8.80</cell><cell cols="4">0.02 6.69 16.99 0.48</cell><cell>3.15</cell><cell>19.43</cell><cell>4.04</cell><cell>0.63</cell><cell>0.40</cell><cell>0.20</cell><cell cols="3">0.00 0.00 0.00</cell><cell>0.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of California, Los Angeles 2 International Center for AI and Robot Autonomy (CARA)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 5: More qualitative results (cont.)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Actionable information in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning for computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human-centric indoor scene synthesis using stochastic grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hallucinated humans as the hidden context for labeling 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Blocks world revisited: Image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Single-view 3d scene parsing by attributed grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting potential falling objects by inferring human action and natural disturbance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Joey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision as bayesian inference: analysis by synthesis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lectures in pattern theory i, ii and iii: Pattern analysis, pattern synthesis and regular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image parsing with stochastic scene grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling high-dimensional humans for activity anticipation using gaussian process latent crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Inferring &quot;dark matter&quot; and &quot;dark energy&quot; from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>MarrNet: 3D Shape Reconstruction via 2.5D Sketches</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantically-enriched 3d models for common-sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale scene understanding challenge: Room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. ; %)</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hedau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop. (2015) Method LSUN Hedau Keypoint Error (%) Pixel Error (%) Pixel Error</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>15] - - 14.50</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sun RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Watch-n-patch: Unsupervised understanding of actions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Roomnet: Endto-end room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale scene understanding challenge: Room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Se?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Physically-based rendering for indoor scene understanding using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">OpenGL programming guide: the ocial guide to learning OpenGL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shreiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T K O A W</forename><surname>Group</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A coarse-to-fine indoor layout estimation (cfile) method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">Im2cad. In: CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Physics inspired optimization on semantic transfer features: An alternative method for room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
