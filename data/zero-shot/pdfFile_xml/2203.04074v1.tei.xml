<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
							<email>zhangtao@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
							<email>jishunping@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contour-based instance segmentation methods have developed rapidly recently but feature rough and handcrafted front-end contour initialization, which restricts the model performance, and an empirical and fixed backend predicted-label vertex pairing, which contributes to the learning difficulty. In this paper, we introduce a novel contour-based method, named E2EC, for high-quality instance segmentation. Firstly, E2EC applies a novel learnable contour initialization architecture instead of handcrafted contour initialization. This consists of a contour initialization module for constructing more explicit learning goals and a global contour deformation module for taking advantage of all of the vertices' features better. Secondly, we propose a novel label sampling scheme, named multi-direction alignment, to reduce the learning difficulty. Thirdly, to improve the quality of the boundary details, we dynamically match the most appropriate predictedground truth vertex pairs and propose the corresponding loss function named dynamic matching loss. The experiments showed that E2EC can achieve a state-of-the-art performance on the KITTI INStance (KINS) dataset, the Semantic Boundaries Dataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for use in real-time applications, with an inference speed of 36 fps for 512?512 images on an NVIDIA A6000 GPU. Code will be released at https://github.com/zhang-tao-whu/e2ec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation is a fundamental computer vision task and the cornerstone of many downstream computer vision applications, such as autonomous driving and robotic grasping. The classic instance segmentation methods are based on a two-stage pipeline, where the bounding boxes (bboxes) of the instances are first generated, and then pixelwise segmentation is performed within the bboxes. Typi- cal examples are methods such as Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> and PANet <ref type="bibr" target="#b22">[23]</ref>. These methods can achieve an excellent accuracy, but are inefficient, which restricts their application in real-time tasks. With the rapid development of one-stage detectors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39]</ref>, many one-stage mask-based instance segmentation methods have now been proposed, such as YOLACT <ref type="bibr" target="#b1">[2]</ref>, BlendMask <ref type="bibr" target="#b2">[3]</ref>, TensorMask <ref type="bibr" target="#b3">[4]</ref>, and CenterMask <ref type="bibr" target="#b16">[17]</ref>. However, these one-stage methods consume a lot of storage, require costly post-processing, and hardly perform in real time. The quality of the instance boundary prediction is also unsatisfactory as these methods usually use limited feature information (for example, Mask R-CNN only segments instances in a 28?28 feature map). The contour-based methods have recently received renewed attention and have shown great potential. Examples of such methods are Curve GCN <ref type="bibr" target="#b20">[21]</ref>, Deep Snake <ref type="bibr" target="#b26">[27]</ref>, Point-Set Anchors <ref type="bibr" target="#b30">[31]</ref>, DANCE <ref type="bibr" target="#b24">[25]</ref>, PolarMask <ref type="bibr" target="#b32">[33]</ref>, and LSNet <ref type="bibr" target="#b8">[9]</ref>. The contour-based methods treat the instance segmentation as a regression task, i.e., regressing the vertex coordinates of a contour represented by a series of discrete vertices. A contour composed of N (e.g., N = 128) vertices is sufficient to describe most of the instances well <ref type="bibr" target="#b26">[27]</ref>. Compared with the mask-based methods, which require intensive processing of each pixel, the contour-based methods are simpler and require less calculation. The contour-based methods can also directly obtain the boundaries of the instances, without any complicated post-processing.</p><p>However, the existing contour-based methods still have many obvious shortcomings. First, all of the existing multistage methods adopt a manually designed shape for the initial contour. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the difference between the manually designed initial contour and the ground-truth instance boundary can lead to many unreasonable deformation paths (the route from initial to ground-truth vertex) and huge training difficulty. It is also impossible to sample the manually designed initial contour to achieve a uniform angle and uniform vertex spacing at the same time.</p><p>The Point-Set Anchors and DANCE methods attempt to address this problem by changing the intuitive vertex pairing method <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, but the results are not satisfactory.</p><p>Second, local or limited information is popularly applied in contour adjustment. For example, the one-stage Polar-Mask <ref type="bibr" target="#b32">[33]</ref> and LSNet <ref type="bibr" target="#b8">[9]</ref> methods directly regress the coordinates of the contour vertices based on only the limited features at the instance center, resulting in the loss of the predicted contour details. The multi-stage methods iteratively adjust the initial contour based on the features of the contour vertices to obtain a more refined segmentation result. However, Curve GCN and Deep Snake utilize a local information aggregation mechanism that propagates the features of the local adjacent contour vertices to refine the contour, which can fail in correcting large prediction errors. Moreover, the local aggregation has to be inefficiently repeated to access global information. Instead, we propose a global contour deformation method based on the features of all the contour vertices.</p><p>Third, the pairing of the ground-truth and predicted vertices in the current contour-based methods is fixed, regardless of the continuous position adjustment of a predicted vertex (e.g., it is already on the ground-truth boundary or close to another ground-truth vertex, but far from the given one). Hence, the pre-fixed vertex pairing is not optimal, and can result in a slower convergence speed, and even wrong predictions.</p><p>In this paper, we propose a multi-stage and highly efficient end-to-end contour-based instance segmentation model named E2EC, which can completely overcome these shortcomings. E2EC incorporates three novel components: 1) a learnable contour initialization architecture; 2) multidirection alignment (MDA); and 3) a dynamic matching loss (DML) function.</p><p>E2EC replaces the manually designed initial contour with a learnable contour initialization architecture, which handles the first and second problem. This architecture contains two novel modules: 1) a contour initialization module; and 2) a global contour deformation module. The contour initialization module directly regresses the complete initial contour based on the center point features, which differs from regressing lengths along given fixed rays <ref type="bibr" target="#b32">[33]</ref>. The global contour deformation module then refines the initial contour based on all of the features of the initial contour  <ref type="figure">Figure 2</ref>. Overview of E2EC. E2EC consists of a learnable contour initialization architecture including a contour initialization and a global deformation module that produces the coarse contour, and a contour refinement module that produces the final contour with the supervision of DML. vertices and center point instead of using features of local vertices. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the learnable initial contour architecture does not rely on a manually designed initial contour (e.g., the ellipse of Curve GCN or the octagon of Deep Snake), and directly deforms from the midpoint of the object instance to the contour with more reasonable paths.</p><p>The difficulty in predicted-label vertex pairing roots in the fact that no simple differentiable calculation can measure the distance between the predicted and ground-truth boundaries. To address the third problem, on the one hand, we propose multi-direction alignment (MDA), which fixes the directions of the selected multiple contour vertices with respect to the center point (the black points in <ref type="figure" target="#fig_0">Figure 1</ref> (E2EC)), and then uniformly samples between the fixed vertices to generate ground-truth vertices. MDA appropriately restricts the possible vertex pairing and deformation paths, and greatly reduces the difficulty of learning while ensuring the upper bound of the performance. The combination of the learnable initial contour architecture and MDA eliminates the unreasonable deformation paths that commonly exist in the current contour-based methods.</p><p>On the other hand, we propose a matching strategy that dynamically matches the predicted vertices and the most appropriate label vertices instead of fixed pairing, and the corresponding dynamic matching loss (DML) function. DML eliminates the problems of an over-smooth boundary and poor fitting of the inflection points in the contour-based methods, and greatly improves the quality of the predicted boundary details.</p><p>In the experiments conducted in this study, E2EC exhibited a state-of-the-art performance on the KITTI INStance (KINS) dataset <ref type="bibr" target="#b27">[28]</ref>, the Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b12">[13]</ref> and the Cityscapes <ref type="bibr" target="#b5">[6]</ref> dataset. For 512?512 images, E2EC achieved a 36 fps inference speed on an NVIDIA A6000 GPU. If the iterative deformation module is disabled, E2EC can reach a speed of 50 fps, with an accuracy comparable to that of Deep Snake.   <ref type="bibr" target="#b26">[27]</ref> (a). The green points represent the features of the contour vertices, the yellow points represent the local kernel function of the circular convolution, and the blue points represent the offsets of the contour vertices, and red is MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M=1</head><p>M=2 M=4 M=8 <ref type="figure">Figure 4</ref>. Multi-direction alignment. M is the number of vertices fixed in the direction with respect to the center point. When M increases, the learning difficulty of the task gradually decreases, but the unevenness of the vertex distribution also gradually increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mask-based instance segmentation methods. The classic mask-based instance segmentation methods, such as Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> and PANet <ref type="bibr" target="#b22">[23]</ref>, include a bbox extraction stage and a mask segmentation stage. These methods can achieve a good performance, but with a very slow speed. In recent years, one-stage methods such as Center-Mask <ref type="bibr" target="#b16">[17]</ref>, YOLACT <ref type="bibr" target="#b1">[2]</ref>, SOLO <ref type="bibr" target="#b29">[30]</ref>, and BlendMask <ref type="bibr" target="#b2">[3]</ref> that follow the above process have developed rapidly, and have greatly improved in speed. However, dense pixel-wise classification requires a huge amount of calculation. Although these methods try to sacrifice performance and perform segmentation on the down-sampled feature maps, to reduce the amount of calculation, they still cannot meet the requirement of real-time performance. The methods proposed in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> follow another pipeline, where they first perform semantic segmentation and then cluster the pixels to generate instances. However, these methods require complex post-processing and cannot be applied to amodal instance segmentation tasks.</p><p>Contour-based instance segmentation methods. Compared with the mask-based methods, the contourbased methods have an absolute advantage in speed. PolarMask <ref type="bibr" target="#b32">[33]</ref> and LSNet <ref type="bibr" target="#b8">[9]</ref> directly regress the coordinates of the instance vertices based on the features at the center point, and can reach a speed that is almost equivalent to that of the detector; however, the corresponding segmentation quality is quite rough. Curve GCN <ref type="bibr" target="#b20">[21]</ref>, Deep Snake <ref type="bibr" target="#b26">[27]</ref>, Point-Set Anchors <ref type="bibr" target="#b30">[31]</ref>, and DANCE <ref type="bibr" target="#b24">[25]</ref> use the vertex features of the contour for the boundary regression, which greatly improves the performance. These methods first initialize the contour, and then iteratively deform the initial contour to obtain the final instance contour. However, the initial contour shapes of these methods are all manually designed, such as the ellipse of Curve GCN, the octagonal of Deep Snake, and the rectangle of Point-Set Anchors and DANCE. The huge difference between the manually designed initial contour and the ground-truth contour leads to many inappropriate vertex pairs, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For example, there are many intersections between the deformation paths of Curve GCN and Deep Snake at different vertices that confuse the training process. The segment-wise matching strategy proposed by DANCE slightly alleviates the above problem, but the intersections still exist. The deformation paths of Point-Set Anchors seem reasonable, but its vertex paring strategy seriously reduces the upper bound of the performance. The proposed E2EC method eliminates the unreasonable deformation path of the contour-based methods and does not reduce the upper bound of the performance.</p><p>Other instance segmentation methods. Dense Rep-Points <ref type="bibr" target="#b36">[37]</ref> uses a discrete point set to model the instance, but the instance representation of Dense RepPoints cannot be directly converted to a mask or contour representation, and requires complex post-processing. Polytransform <ref type="bibr" target="#b18">[19]</ref> combines a mask-based method and a contourbased method. Poly Transform first generates the mask representation of the instance, then converts the mask into a contour through post-processing, and finally refines the contour by a deformation module. However, Poly Transform cannot be trained end-to-end, and the speed is too slow for it to be applied to real-time scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed E2EC method</head><p>In this section, we describe the three main parts of the proposed end-to-end contour-based (E2EC) instance segmentation method, i.e., the learnable contour initialization architecture, the multi-direction alignment (MDA), and the dynamic matching loss (DML) function. The workflow of E2EC is shown in <ref type="figure">Figure 2</ref>. E2EC first generates a heatmap to locate the instance centers, and then learns the initial contour by regressing the initial offsets based on the center point features. The initial contour is first deformed by a global deformation module, and evolves to the coarse contour. The deformation modules <ref type="bibr" target="#b26">[27]</ref> then deform the coarse contour twice to the final contour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learnable contour initialization architecture</head><p>The learnable contour initialization architecture includes a contour initialization module and a global deformation module.</p><p>Initial contour. Unlike the manually designed initialization used in the existing contour-based methods, it is not necessary to specify the shape of the initial contour as this is learned by the network. Inspired by Dense RepPoints <ref type="bibr" target="#b36">[37]</ref>, the offsets of each initial contour vertex are directly regressed with respect to the center point, based on the center point features, which is denoted as {(?x i init , ?y i init )|i = 1, 2, ..., N }, where N is the number of vertices of the initial contour. The initial contour vertices are computed by adding the center point coordinates and the offsets, which is denoted as {(x i init , y i init )|i = 1, 2, ..., N }. Dense Rep-Points regresses an unordered point set and then converts the point set into a contour or mask representation through complex post-processing. In contrast, E2EC directly regresses the contour (an ordered point set) without any requirement for post-processing. Compared with the other manually designed initial contours (e.g., ellipse or octagon), the learnable initial contour is closer to the ground-truth contour. In addition, the direction of the deformation path of the learnable initial contour is from the center point to the contour vertex (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>), guaranteeing that no unfavorable intersections between the deformation paths to impact the convergence.</p><p>Global deformation. It is challenging to directly regress the contour vertices with only the center point features. Meanwhile, it is also difficult to effectively deform the contour based only on the local features of a single contour vertex or several adjacent ones. The circular convolution proposed in Deep Snake uses a local aggregation mechanism to supplement the global information. However, the circular convolution operating on the local adjacent vertices needs to be repeated multiple times to aggregate the global information, and it cannot effectively correct large errors in the contour. We propose a simple but more effective global aggregation mechanism named global deformation to deform the initial contour based on both the center point features and all of the contour vertex features. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> output layer is N ? 2) to obtain the offset predictions of the contour vertices (a vector of length N ? 2, denoted as {(?x i coarse , ?y i coarse )|i = 1, 2, ..., N }). The offsets and the initial contour coordinates are summed to obtain the adjusted coarse instance contour, which is denoted as {(x i coarse , y i coarse )|i = 1, 2, ..., N }. In our experiments, we set N = 128 and C = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-direction alignment (MDA)</head><p>Due to the challenge of the contour initialization and predicted-label vertex pairing, deviation may exist between the actual vertex deformation path and the ideal deformation path, which leads to the adjustment of some vertices tending toward the along-contour direction with a slower convergence speed, and even wrong predictions. MDA addresses this problem by fixing the direction of several selected vertices with respect to the center point, and then samples the ground truth uniformly between the fixed vertices. The sampling results for different numbers of alignment vertices are shown in <ref type="figure">Figure 4</ref>. MDA can effectively reduce the learning difficulty of the contour adjustment, without reducing the upper bound of the performance. Interestingly, PolarMask and LSNet are two extreme cases of MDA. If we suppose that the number of contour vertices is N and the number of alignment vertices is M , when M = N , the strategy degenerates to PolarMask, which has the lowest learning difficulty but the lowest upper bound of the performance. When M = 0, the strategy degenerates to LSNet, which is the most difficult case to learn, with a high upper bound of the performance. We experimentally found that M = 4 obtains the best performance. When M = 4, the learning difficulty is significantly reduced, but the upper bound of the performance is not reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic matching loss (DML)</head><p>As the pre-fixed vertex pairing used in previous studies is not optimal and can cause learning difficulty, we propose DML, which dynamically adjusts the relationship of the vertex pairing to supervise output of the last deforma-tion module <ref type="bibr" target="#b26">[27]</ref> as shown in <ref type="figure">Figure 2</ref>. The loss consists of two parts: 1) the predicted vertex points toward the nearest points on the label boundary, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a), and then 2) the key label vertex pulls the nearest predicted vertex toward its position, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(b). The full details of DML are described below.</p><p>In the best case, the vertices should be adjusted to the target contour with the least cost. For each predicted vertex, it is a complicated process to dynamically find the nearest correspondence in the label contour line. Firstly, in order to simplify the calculation, the adjacent ground-truth vertices are split into 10 equal sub-segments. The problem is then transformed into discovering the nearest interpolated ground-truth contour vertex. Equation (1) describes the process of matching the nearest interpolated ground-truth vertex (gt ipt ) for each predicted contour vertex by minimizing the L2 distance of predicted i-th point and x-th (0 &lt; x &lt; N + 1) label point. Equation <ref type="formula">(2)</ref> is the corresponding and first component of DML. Secondly, the closest predicted vertex is dynamically matched to the key vertices (obtained by the Douglas-Peucker algorithm <ref type="bibr" target="#b7">[8]</ref>) of the label contour to preserve the details of the predicted contour. Equation <ref type="formula">(3)</ref> describes the process of matching each key vertex to the nearest predicted vertex to best preserve the details of a boundary. Equation <ref type="formula" target="#formula_0">(4)</ref> is the corresponding and second component of DML. DML is the average of the above two components, as shown in Equation <ref type="bibr" target="#b4">(5)</ref>. DML can greatly improve the quality of the predicted boundary and address the over-smoothing problem found in Deep Snake and DANCE. </p><p>L(pred, gt) = L1(pred, gt) + L2(pred, gt) 2 (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Detector. E2EC can be constructed based on any detector, and it is only necessary to change the output size of the bbox branch from H ? W ? 2 to H ? W ? (N ? 2) to directly regress the initial contour with N vertices. In the experiments conducted in this study, for a fair comparison with the other methods, CenterNet <ref type="bibr" target="#b38">[39]</ref> was used as the detector for E2EC.</p><p>Loss function. Smooth L1 loss is used to supervise the contour initialization branch, the global deformation branch and the first refinement deformation module. The losses are defined as:</p><formula xml:id="formula_1">Linit = 1 N N i=1 smooth l1(x init i ? x gt i )<label>(6)</label></formula><formula xml:id="formula_2">Lcoarse = 1 N N i=1 smooth l1(x coarse i ? x gt i ) (7) Liter1 = 1 N N i=1 smooth l1(x iter1 i , x gt i )<label>(8)</label></formula><p>Where N is the number of contour vertices,x init i is the predicted initial contour vertex,x coarse i is the predicted coarse contour vertex, x gt i is the label contour vertex, andx iter1 i is the contour vertex after being deformed with the first deformation module in the refinement step.</p><p>The DML function is used to supervise the last deformation module, as shown in equation <ref type="formula" target="#formula_3">(9)</ref>, wherex iter2 i is the contour vertex after being deformed with the second deformation module. The loss of refinement deformation modules is then defined as equation <ref type="formula" target="#formula_4">(10)</ref>.</p><formula xml:id="formula_3">Liter2 = LDML(x iter2 i , x gt i )<label>(9)</label></formula><formula xml:id="formula_4">Liter = Liter1 + Liter2<label>(10)</label></formula><p>The overall loss is as follows:</p><formula xml:id="formula_5">L overall = L det + ?Linit + ?Lcoarse + Liter<label>(11)</label></formula><p>Both ? and ? are set to 0.1. L det is the loss of the center point detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and metrics</head><p>Datasets. The KINS <ref type="bibr" target="#b27">[28]</ref>, SBD <ref type="bibr" target="#b12">[13]</ref>, Cityscapes <ref type="bibr" target="#b5">[6]</ref>, and COCO <ref type="bibr" target="#b19">[20]</ref> datasets were used in the experiments. The KINS dataset is used for amodal instance segmentation, and has seven instance classes, with 7,474 training images and 7,517 testing images. The SBD dataset has 20 instance classes and is split into 5,623 training images and 5,732 testing images. The SBD dataset is made up of 11,355 reannotated images from the PASCAL VOC [10] dataset, with instance-level boundaries. The Cityscapes dataset has eight instance classes and contains 2,975 training, 500 validation, and 1,525 testing images with high-quality annotations. The COCO dataset has eighty instance classes and contains 115k training, 5k validation, and 20k testing images.</p><p>Metrics. In this paper, the mask quality is evaluated in terms of the standard AP metric. To distinguish the standard AP metric from other metrics, it is denoted as AP msk . For all the datasets, all the settings of AP msk were the same as for Deep Snake.</p><p>The boundary quality is evaluated in terms of the boundary AP metric proposed by <ref type="bibr" target="#b4">[5]</ref>. This is denoted as AP bdy and focuses on the boundary quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation experiments</head><p>To quantitatively analyze the effect of each component of the proposed E2EC method and verify the design details, we conducted ablation experiments on the SBD dataset, with  <ref type="figure">Figure 2</ref> shows how E2EC generates the contours at these different stages.    Deep Snake as the baseline. The network was trained endto-end for 150 epochs with multi-scale data augmentation. The learning rate started from 1e-4 and was decayed by 0.5 at 80 and 120 epochs. The results of the ablation experiments are given in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>The learnable contour initialization architecture. When the learnable contour initialization architecture is used to replace the contour initialization method of the baseline method, this yields improvements of 3.1 AP msk , 2.2 AP msk 50 and 3.9 AP msk 70 . For the AP bdy metric, which is sensitive to the boundary quality, the learnable contour initialization architecture brings improvements of 6.2 AP bdy , 9.6 AP bdy 50 , and 6.8 AP bdy 75 . Such a huge improvement in AP fully proves the importance of a reasonable initial deformation path for the contour-based method.</p><p>Multi-direction alignment. When the uniform label sampling scheme used by the baseline method is replaced with the MDA strategy, this yields improvements of 1.3 AP msk , 1.1 AP msk 50 and 1.7 AP msk 70 over the baseline with the learnable contour initialization architecture. For the AP bdy metric, also results in improvements of 1.0 AP bdy , 1.1 AP bdy 50 and 1.3 AP bdy 75 . It can be seen in <ref type="figure" target="#fig_5">Figure 6</ref> that, as the number of alignments increases, the deviation between the actual and ideal deformation paths becomes smaller and smaller, and the actual deformation path becomes more optimal. We also conducted quantitative experiments with different alignment numbers, to explore the most appropriate number of alignment directions. The ablation results are shown in <ref type="table" target="#tab_1">Table 1b</ref>. The most appropriate alignment num-ber is 4, which achieves the highest AP bdy and the secondhighest AP msk . As the number increases further, the difficulty of the learning decreases, but too many alignment operations also affects the upper bound of the performance and exacerbates the unevenness of the vertex distribution, as shown in <ref type="figure">Figure 4</ref>. The proposed MDA label sampling scheme makes a good balance between the learning difficulty and the upper bound of the performance. <ref type="bibr" target="#b14">[15]</ref>, and DML were used to supervise the last deformation module. The results obtained with the different loss functions are shown in <ref type="table" target="#tab_1">Table 1c</ref>. Replacing smooth L1 loss with chamfer loss brings about a deterioration of 0.6 AP msk and an improvement of 0.4 AP bdy . Chamfer loss improves the quality of the predicted boundary, but also reduces the quality of the mask, because the order of the contour vertices has been destroyed. The proposed DML function can dynamically match all the predicted vertices with the most appropriate label vertices. DML yields improvements of 0.4 AP msk and 1.1 AP bdy . The prediction results obtained with and without DML are also illustrated in <ref type="figure" target="#fig_7">Figure 7</ref>, where it can be seen that DML significantly improves the fitting degree between the predicted contour and the instance (such as the motorcycle helmet). As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, the model supervised by DML predicts the best contour, without destroying the order of the vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic matching loss. Smooth L1 loss, chamfer loss</head><p>Speed vs. accuracy. The accuracy and inference speed of the contours at different stages are listed in <ref type="table" target="#tab_1">Table 1d</ref>, and <ref type="figure" target="#fig_7">Figure 7</ref> illustrates the predicted contour at different stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training data backbone fps AP val AP AP 50 preson rider car truck bus train mcycle bicycle mask-based SGN <ref type="bibr" target="#b21">[22]</ref> Fine+coarse VGG16 0. <ref type="bibr" target="#b5">6</ref>     E2EC can generate an initial contour at a high speed, but the quality of the result is rough for instance segmentation. The coarse contour is obtained from the global deformation of the initial contour. The coarse contour predicted by E2EC has the same accuracy as the final output of Deep Snake. The inference speed of 56 fps on a 512?512 image far exceeds the speed of Deep Snake (33 fps). The complete E2EC achieves an inference speed of 36 fps, which is slightly higher than Deep Snake, and the final contour predicted by E2EC has a huge advantage in terms of both the mask and boundary quality. The experimental results show that E2EC predicts the coarse contour 55.5% faster than the final contour. However, AP msk only reduced by 3.5. Therefore, in the case of the requirement for extremely high speed, removing the deformation components can be a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with the state-of-the-art methods</head><p>Performance on the KINS dataset. The KINS dataset is used for amodal instance segmentation, and is annotated with inference completion information for the occluded parts of the instances. E2EC was trained for 150 epochs with the Adam optimizer, and the learning rate started from 1e-4 and was decayed by 0.5 at 80 and 120 epochs. Multiscale training was conducted, and the models were tested at a single resolution of 768?2496.</p><p>In <ref type="table">Table 6</ref>, the performance of the different instance segmentation methods on the KINS dataset is compared. E2EC does not use the bbox branch to generate the initial contour, so the detection result is calculated from the predicted contour. E2EC achieves a state-of-the-art performance in both the detection and segmentation tasks. E2EC achieves 36.5 AP det , 34.0 AP msk and 33.3 AP bdy (see <ref type="bibr" target="#b3">4)</ref>, and outperforms Deep Snake by 3.7 AP det , 2.7 AP msk and 3.1 AP bdy . <ref type="figure" target="#fig_9">Figure 9</ref> shows some representative results of E2EC obtained on the KINS dataset.</p><p>Performance on the SBD dataset. For the SBD dataset, the details of the model training were the same as for the KINS dataset. Multi-scale training was conducted and the models were tested at a single resolution of 512?512.</p><p>The SBD dataset has more instance categories and more complicated instance contours than the KINS dataset, so that the advantage of the proposed E2EC method with more optimal deformation paths is more obvious. E2EC outper- forms Deep Snake by 4.8 AP msk (see <ref type="table" target="#tab_2">Table 2</ref>) and 8.3 AP bdy (see <ref type="table" target="#tab_4">Table 4</ref>). The contour details predicted by E2EC are remarkable, and AP msk 75 is 9.1 higher than Deep Snake. E2EC uses the lighter DLA-34 as the backbone, and it outperforms the fully convolutional instance-aware semantic segmentation (FCIS) method, which uses ResNet-101 as the backbone, by 2.0 AP msk 75 . <ref type="figure" target="#fig_9">Figure 9</ref> shows some examples of the results of E2EC on the SBD dataset. It can be seen that instances with complex contours, such as legs and chairs, are well segmented by the proposed method, but they cannot be well segmented by Deep Snake.</p><p>Performance on the Cityscapes dataset. The details of the model training for the Cityscapes dataset were the same as for the previous two datasets. It is worth mentioning that it is not necessary to train the detector separately with E2EC, but it is necessary to train the detector alone for 140 epochs and then train the whole network end-to-end for 200 epochs with Deep Snake. Multi-scale training was conducted, and the models were tested at a single resolution of 1216?2432. <ref type="table" target="#tab_6">Table 5</ref> compares the results of the proposed E2EC method with those of the other state-of-the-art methods on the Cityscapes validation and test sets. E2EC achieves 30.3 AP msk on the test set, without the multi-component detection used in <ref type="bibr" target="#b26">[27]</ref>, outperforming Deep Snake by 2.1 AP msk . With the multi-component detection strategy, the proposed method achieves 39.0 AP msk on the validation set and 32.9 AP msk on the test set, outperforming Deep Snake by 1.6 AP msk and 1.2 AP msk respectively. E2EC also outperforms the classic mask-based method of PANet by 2.5 AP msk on the validation set and 1.1 AP msk on the test set, and is almost five times faster. The performance of the proposed method is on a par with UPSNet <ref type="bibr" target="#b35">[36]</ref>; however, the latter was trained on the additional large Common Objects in Context (COCO) dataset <ref type="bibr" target="#b19">[20]</ref>. <ref type="table" target="#tab_4">Table 4</ref> compares the boundary quality of the results obtained by the proposed E2EC method and Deep Snake. E2EC achieves 36.3 AP bdy and 36.5 AP bdy 75 on the validation set, outperforming Deep Snake by 1.8 AP bdy and 4.4 AP bdy 75 . Some of the results obtained by E2EC are shown in <ref type="figure" target="#fig_9">Figure 9</ref>, where it can be seen that the proposed method segments the details of the instances well (such as the rearview mirror and tires), which cannot be achieved by Deep Snake.</p><p>Performance on the COCO dataset. For the COCO dataset, the model is trained end-to-end for 140 epochs with Adam optimizer and 20 epochs with SGD optimizer. The learning rate starts from 1e-4 and drops by half at 80 and 120 epochs, respectively. The model tested at the original image resolution without any tricks.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, E2EC achieves 33.8 AP msk with 30.1 fps, which outperforms Deep Snake by 3.5 AP msk . When the deformation modules are removed, E2EC * achieves 31.7 AP msk with 54.3 fps, which is almost 2? faster than Deep Snake and outperforms Deep Snake by 1.4 AP msk .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed an end-to-end contourbased instance segmentation method named E2EC. E2EC introduces three novel components: 1) the learnable contour initialization architecture; 2) the multi-direction alignment (MDA) label sampling scheme; and 3) the dynamic matching loss (DML) function. E2EC greatly improves the contour extraction quality of the contour-based instance segmentation. In this study, E2EC achieved state-of-the-art results on the KINS, SBD, and Cityscapes datasets, with a beyond real-time performance. We also introduced a faster variant, where, by only retaining the learnable contour initialization architecture, the accuracy can be comparable to that of Deep Snake, and the speed is almost as fast as that of the backbone one-stage detector CenterNet. The modules proposed in E2EC could be easily applied to other contour-based instance segmentation methods. We hope that E2EC will serve as a fundamental and strong baseline for contour-based instance segmentation research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The ideal deformation paths of several contour-based methods. White boundaries and points are the initial contours, blue lines are the deformation paths, and black points are the alignment points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Global deformation (b) vs. circular convolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Dynamic matching loss. The yellow points are the predicted contour vertices, the green points are the label vertices, the red points are the key label vertices, and the arrows represent the deformation path (the relationship of the pairing). (a) The first part of DML, where each prediction point is adjusted to the nearest point on the ground-truth boundary. (b) The second part of DML, where the key label point pulls the nearest prediction point toward its position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>(b), the features of the N initial contour vertices and the center point are first concatenated into a vector of length (N + 1) ? C (where C is the channel number of the vertex feature). The vector is then input into the MLP module (channels of hidden layer and M=1 M=4 M=2M=8 The actual deformation paths with different numbers (M) of fixed vertices. The blue line is the initial contour, the green line is the final contour, and the black lines are the deformation paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>The prediction contours at different stages. The initial contour of E2EC fits the instance better than the octagonal initialization of Deep Snake. The quality of the coarse contour predicted by E2EC is comparable to the final contour predicted by Deep Snake. The final contour predicted by E2EC with DML perfectly outlines the motorcycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>The prediction results supervised by different loss functions. Chamfer loss slightly improved the quality of boundary details but produced a serious jagged phenomenon (especially the red and purple instances). Dynamic matching loss heavily improved the quality of boundary details and did not cause any harm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>The qualitative results obtained by E2EC. The first row shows some examples of the prediction results for the SBD dataset, the second row shows some examples of the prediction results for the KINS dataset, and the third row shows some examples of the prediction results for the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Method AP msk AP msk 50 AP msk 70 AP bdy AP bdy Ablation experiments on the SBD dataset. Baseline method is Deep Snake<ref type="bibr" target="#b26">[27]</ref>. Arch denotes the learnable contour initialization architecture. MDA denotes multi-direction alignment. DML denotes dynamic matching loss.M AP msk AP msk 50 AP msk 70 AP bdy AP bdy Results with different alignment numbers (M). The highest accuracy is bolded and the second-hignest accuracy is italicized. The results of the last deformation module being supervised by different loss functions. The proposed DML outperforms smooth L1 loss and chamfer loss in terms of both the mask and boundary quality.Stage AP msk AP msk</figDesc><table><row><cell>50 35.3 44.9 46.8 47.9 (a) 50 AP bdy 75 Baseline 54.4 62.1 48.3 10.8 2.6 +Arch 57.5 64.3 52.2 17.0 9.4 +MDA 58.8 65.4 53.9 18.0 10.4 +DML 59.2 65.8 54.1 19.1 11.7 1 57.5 64.3 52.2 17.0 44.9 2 58.9 65.8 54.1 17.1 45.8 4 58.8 65.4 53.9 18.0 46.8 8 58.4 65.3 53.8 17.2 45.7 (b) Loss AP msk AP msk 50 AP msk 70 AP bdy AP bdy 50 AP bdy 75 smooth l1 58.8 65.4 53.9 18.0 46.8 10.4 Chamfer 58.2 65.0 52.9 18.4 47.4 11.1 DML 59.2 65.8 54.1 19.1 47.9 11.7 AP msk 70 AP bdy AP bdy 50 Initial 49.8 61.3 34.4 1.7 7.5 Coarse 55.7 64.6 49.5 8.7 29.4 Final 59.2 65.8 54.1 19.1 47.9 (c) 50 (d) Accuracy/speed trade-off.</cell><cell>AP bdy 75 9.4 9.3 10.4 9.3 AP bdy 75 0.2 2.6 11.7</cell><cell>FPS 58 56 36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation experiments for E2EC. All models are trained on SBD train set and tested on SBD val set, using DLA-34 backbone.</figDesc><table><row><cell>Method</cell><cell cols="3">AP msk AP msk 50 AP msk 70</cell></row><row><cell>MNC [7]</cell><cell>?</cell><cell>63.5</cell><cell>41.5</cell></row><row><cell>FCIS [18]</cell><cell>?</cell><cell>65.7</cell><cell>52.1</cell></row><row><cell>STS [16]</cell><cell>29.0</cell><cell>30.0</cell><cell>6.5</cell></row><row><cell>ESE-20 [36]</cell><cell>35.3</cell><cell>40.7</cell><cell>12.1</cell></row><row><cell cols="2">Deep Snake [27] 54.4</cell><cell>62.1</cell><cell>48.3</cell></row><row><cell>DANCE [25]</cell><cell>56.2</cell><cell>63.6</cell><cell>50.4</cell></row><row><cell>E2EC</cell><cell>59.2</cell><cell>65.8</cell><cell>54.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on SBD val set.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell cols="4">AP AP 50 AP 75 FPS</cell></row><row><cell>E2EC</cell><cell>DLA-34</cell><cell cols="4">33.8 52.9 35.9 30.1</cell></row><row><cell>E2EC  *</cell><cell>DLA-34</cell><cell cols="4">31.7 52.2 32.8 54.3</cell></row><row><cell cols="2">Deep Snake [27] DLA-34</cell><cell>30.3</cell><cell>-</cell><cell>-</cell><cell>27.3</cell></row><row><cell>PolarMask [33]</cell><cell cols="5">ResNet-101-FPN 30.4 51.9 31.0 15.0</cell></row><row><cell cols="6">PolarMask++ [34] ResNet-50-FPN 30.2 52.6 30.8 20.5</cell></row><row><cell>YOLACT [2]</cell><cell cols="5">ResNet-101-FPN 29.8 48.5 31.2 33.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results obtained on the COCO test-dev.</figDesc><table><row><cell cols="2">Datasets Method</cell><cell>AP bdy AP bdy 50 AP bdy 75</cell></row><row><cell>Kins</cell><cell cols="2">Deep Snake 30.2 53.0 31.2 E2EC 33.3 54.9 35.6</cell></row><row><cell>SBD</cell><cell cols="2">Deep Snake 10.8 35.3 2.6 E2EC 19.1 47.9 11.7</cell></row><row><cell>Cityscapes</cell><cell cols="2">Deep Snake 34.5 62.9 32.1 E2EC 36.3 62.3 36.5</cell></row></table><note>* means the two deformation modules are removed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of the boundary quality for the different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>29.2 25.0 44.9 21.8 20.1 39.4 24.8 33.2 30.8 17.7 12.4 Mask R-CNN [14] Fine ResNet50 2.2 31.5 26.2 49.9 30.5 23.7 46.9 22.8 32.2 18.6 19.1 16.0 GMIS [24] Fine+coarse ResNet101 --27.6 49.6 29.3 24.1 42.7 25.4 37.2 32.9 17.6 11.9 Spatial [26] Fine ERFNet 11 -27.6 50.9 34.5 26.1 52.4 21.7 31.2 16.4 20.1 18.9 PANet [23] Fine ResNet50 &lt;1 36.5 31.8 57.1 36.8 30.4 54.8 27.0 36.3 25.5 22.6 20.8</figDesc><table><row><cell>UPSNet [35]</cell><cell cols="12">Fine+COCO ResNet50 4.4 37.8 33.0 59.6 35.9 27.4 51.8 31.7 43.0 31.3 23.7 19.0</cell></row><row><cell>contour-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Polygon RNN++ [1] Fine</cell><cell cols="2">ResNet50 -</cell><cell cols="9">-25.5 45.5 29.4 21.8 48.3 21.1 32.3 23.7 13.6 13.6</cell></row><row><cell>Deep Snake [27]</cell><cell>Fine</cell><cell>DLA-34</cell><cell>-</cell><cell>-28.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>E2EC</cell><cell>Fine</cell><cell cols="11">DLA-34 6.2 34.0 30.3 54.0 40.7 27.9 55.4 28.4 35.8 20.1 20.9 13.2</cell></row><row><cell cols="2">Deep Snake* [27] Fine</cell><cell cols="11">DLA-34 4.6 37.4 31.7 58.4 37.2 27.0 56.0 29.5 40.5 28.2 19.0 16.4</cell></row><row><cell>E2EC*</cell><cell>Fine</cell><cell cols="11">DLA-34 4.9 39.0 32.9 59.2 39.0 27.8 56.0 29.5 41.2 29.1 21.3 19.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Results obtained on the Cityscapes test set. * means that the multi-component detection is used to integrate several components into a complete instance. The proposed E2EC method outperforms Deep Snake<ref type="bibr" target="#b26">[27]</ref> in all the categories.</figDesc><table><row><cell>Method</cell><cell cols="2">AP det AP msk</cell></row><row><cell>mask-based</cell><cell></cell><cell></cell></row><row><cell>MNC [7]</cell><cell cols="2">20.9 18.5</cell></row><row><cell>FCIS [18]</cell><cell cols="2">25.6 23.5</cell></row><row><cell>ORCNN [11]</cell><cell cols="2">30.9 29.0</cell></row><row><cell cols="3">Mask R-CNN [14] 31.3 29.3</cell></row><row><cell cols="3">Mask R-CNN* [28] 32.7 31.1</cell></row><row><cell>PANet [23]</cell><cell cols="2">32.3 30.4</cell></row><row><cell>PANet* [28]</cell><cell cols="2">33.4 32.2</cell></row><row><cell>VRS&amp;SP [32]</cell><cell>?</cell><cell>32.1</cell></row><row><cell>ARCNN [38]</cell><cell>?</cell><cell>32.9</cell></row><row><cell>contour-based</cell><cell></cell><cell></cell></row><row><cell>Deep Snake [27]</cell><cell cols="2">32.8 31.3</cell></row><row><cell>E2EC</cell><cell cols="2">36.5 34.0</cell></row><row><cell cols="3">Table 6. Results obtained on</cell></row><row><cell cols="3">the KINS test set. * denotes</cell></row><row><cell cols="3">with ASN proposed by [28].</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyang</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boundary iou: Improving object-centric image segmentation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15334" to="15342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithms for the reduction of the number of points required to represent a digitized line or its caricature. Cartographica: the international journal for geographic information and geovisualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="112" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Location-sensitive visual recognition with cross-iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04899</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to see the invisible: End-to-end trainable amodal instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Follmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>H?rtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Klostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>B?ttger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ssap: Single-shot instance segmentation with affinity pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent attention networks for structured online maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrinidhi</forename><surname>Kowshika Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3417" to="3426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Straight to shapes: real-time detection of encoded shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6550" to="6559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Centermask: Realtime anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namdar</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9131" to="9140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast interactive object annotation with curve-gcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3496" to="3504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dance: A deep attentive contour model for efficient instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep snake for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijin</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Amodal instance segmentation with kins dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Amodal segmentation based on visible region segmentation and shape prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05598</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12193" to="12202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Polarmask++: Enhanced polar representation for single-shot instance segmentation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Explicit shape encoding for real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fubo</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense reppoints: Representing visual objects with dense point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XXI 16</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Amodal segmentation just like doing a jigsaw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunli</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqin</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07464</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
