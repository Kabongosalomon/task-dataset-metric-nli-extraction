<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DECOUPLED MIXUP FOR DATA-EFFICIENT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DECOUPLED MIXUP FOR DATA-EFFICIENT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint. Work in progress. * Equal contribution ?Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, dynamic mixup methods have improved previous static policies effectively (e.g., linear interpolation) by maximizing salient regions or maintaining the target in mixed samples. The discrepancy is that the generated mixed samples from dynamic policies are more instance discriminative than the static ones, e.g., the foreground objects are decoupled from the background. However, optimizing mixup policies with dynamic methods in input space is an expensive computation compared to static ones. Hence, we are trying to transfer the decoupling mechanism of dynamic methods from the data level to the objective function level and propose the general decoupled mixup (DM) loss. The primary effect is that DM can adaptively focus on discriminative features without losing the original smoothness of the mixup while avoiding heavy computational overhead. As a result, DM enables static mixup methods to achieve comparable or even exceed the performance of dynamic methods. This also leads to an interesting objective design problem for mixup training that we need to focus on both smoothing the decision boundaries and identifying discriminative features. Extensive experiments on supervised and semisupervised learning benchmarks across seven classification datasets validate the effectiveness of DM by equipping it with various mixup methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep Learning has become the bedrock of modern AI for many tasks in machine learning <ref type="bibr" target="#b3">(Bishop, 2006)</ref> such as computer vision <ref type="bibr" target="#b10">(He et al., 2016;</ref><ref type="bibr" target="#b54">2017)</ref>, natural language processing <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>. Using a large number of learnable parameters, deep neural networks (DNNs) can recognize subtle dependencies in large training datasets to be later leveraged to perform accurate predictions on unseen data. However, models might overfit the training set without constraints or enough data <ref type="bibr" target="#b34">(Srivastava et al., 2014)</ref>. To this end, regularization techniques have been deployed to improve generalization <ref type="bibr" target="#b40">(Wan et al., 2013)</ref>, which can be categorized into data-independent or data-dependent ones <ref type="bibr" target="#b8">(Guo et al., 2019)</ref>. Some data-independent strategies, for example, constrain the model by punishing the parameters' norms, such as weight decay . Among datadependent strategies, data augmentations <ref type="bibr" target="#b32">(Shorten &amp; Khoshgoftaar, 2019)</ref> are widely used to increase the diversity of training samples by generating virtual samples.</p><p>Mixup , a data-dependent augmentation technique is proposed to generate virtual samples by a linear combination of data pairs and the corresponding labels with the mixing ratio ? ? [0, 1]. DNNs trained with this technique are typically more generalizable and calibrated <ref type="bibr" target="#b35">(Thulasidasan et al., 2019)</ref>, whose prediction accuracy tends to be consistent with confidence. The main reason is that mixup heuristically smooths the decision boundary to improve the overall robustness by regressing the mixing ratio ? in mixed labels. However, a completely handcrafted mixing policies <ref type="bibr" target="#b37">Uddin et al., 2020;</ref>) (referred to as static methods) may result in a mismatch issue between the mixed labels and the mixed samples, which leads to problems such as instability or slow convergence, namely label mismatch <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, as shown in <ref type="figure">Figure 1</ref>. To mitigate this problem, a line of time-consuming mixup methods are proposed to improve mixing policies to generate object-aware virtual samples by optimizing discriminative regions in the data space <ref type="bibr" target="#b37">(Uddin et al., 2020;</ref>) (referred to as dynamic methods). For example, in <ref type="figure" target="#fig_0">Figure 2</ref> left, the dynamic methods like AutoMix <ref type="bibr" target="#b24">(Liu et al., 2022)</ref> improve mixup accuracy significantly. The basic idea is to decouple the foreground from the background and mix their corresponding features to avoid label mismatch. However, these data-level decoupling methods require a complex deployment and an additional 1.5 times the training time as static methods, which may violate the mixup augmentations' ease of use and lightness. Thus, leaving aside the design of a new mixup policy, a new question raises that can we design a loss function for mixup that takes into account both the smoothness of mixup and the discriminatory of instances without heavy computation? From this perspective, we first transfer the label mismatch issue from the input data to the objective function design level. In other words, as long as we decouple the foreground targets in the objective function, similar effects as dynamic methods can be achieved.</p><p>Motivated by this intuition, we introduce Decoupled Mixup (DM), a mixup objective function for explicitly leveraging the target-relevant information of mixed samples without losing original smoothness. For example, based on the standard cross-entropy loss, an extra decoupled term is introduced to enhance the ability to mine underlying discriminative statistics in the mixed sample by independently computing the predicted probabilities of each mixed class. As a result, DM can further emphasize the contribution of each involved class in mixup and thus boost the efficiency of mixup training (see <ref type="figure" target="#fig_0">Figure 2</ref> right). Our contributions are summarized below:</p><p>? Unlike those static mixup policies that suffer from the label mismatch problem, we propose DM, a mixup objective of mining discriminative features while maintaining smoothness.</p><p>? Our work contributes more broadly to understanding mixup training: it is essential to focus not only on the smoothness by regression of the weight of mixing but also on discrimination by encouraging the network to give a highly confident prediction when the evidence is clear.</p><p>? The proposed DM can be easily generalized to semi-supervised learning by a minor modification. By leveraging the unlabeled data efficiently, it can reduce the conformation bias and significantly improve overall performance.</p><p>? Comprehensive experiments on various tasks verify the effectiveness of DM, e.g., DMbased static mixup policies achieve a comparable or even better performance than dynamic methods without the extra computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>2.1 ? WEIGHTED CROSS-ENTROPY UNDERUTILIZES MIXUP Firstly, we dive into analyzing the generation of mixed samples and the original loss function. Let us define y ? R C as the ground-truth label with C categories. For data point x ? R W?H?C whose embedded representation z is obtained from the model M and the predicted probability p can be calculated through a Softmax function p = ?(z). Given the mixing ratio ? ? [0, 1] and ? related mixup mask H ? R W?H , the mixed sample (x (a,b) , y <ref type="bibr">(a,b)</ref> ) can be generated as x <ref type="bibr">(a,b)</ref> = H x a + (1 ? H) x b , and y (a,b) = ?y a + (1 ? ?)y b , where denotes element-wise product, (x a , y a ) and (x b , y b ) are sampled from a labeled dataset L = {(x a , y a )} n L a=1 . Note that superscripts denote the class index; subscripts indicate the type of data, e.g., x (a,b) represents a mixed sample related to x a and x b ; y i is the value on i-th position. Since the mixup labels are obtained by linear interpolation, the standard CE loss weighted by ?, L M CE = y T (a,b) log ?(z <ref type="bibr">(a,b)</ref> ), is typically used as the optimization objective in the mixup training process, which can be rewritten as:</p><formula xml:id="formula_0">L M CE = ? C i=1 ?I(y i a = 1) log p i (a,b) + (1 ? ?)I(y i b = 1) log p i (a,b) ,<label>(1)</label></formula><p>where I(?) ? {0, 1} is an indicator function that values 1 if and only if the input condition holds. Noticeably, these two items of L M CE are classifying y a and y b according to mixup coefficient ?.</p><p>The effect of ? weighting in standard cross-entropy loss is to smooth the decision space between two classes related in mixed samples. However, the condition for L M CE to be optimal is pa p b = ? 1?? , which means L M CE only focuses on regressing ? value, but not encouraging the model to mine underlying statistics of mixed samples, e.g., small ? value suppress the confidence of predictions even if the mixed features are evident. Therefore, the first challenge is to improve and design a general and efficient objective function for mixup training that maintains the smoothness of mixup and can simultaneously explore the key features related to the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UNRELIABLE MIXUP AMPLIFIES CONFIRMATION BIAS</head><p>If we further consider semi-supervised learning (SSL), combining mixup and pseudo-labeling techniques can effectively improve performances with limited labeled data <ref type="bibr" target="#b1">(Berthelot et al., 2019a)</ref>. For each u a in an unlabeled dataset U = {(u a , v a )} n U a=1 , where v a its corresponding pseudo label, taking MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019b)</ref> as an example, pseudo-label? is generated by computing the average of the model's predicted class distributions across all the K basic augmentations (e.g., image cropping, etc.) of u a :? a = arg max C 1 K K k=1 M (y a |? a ; ?) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True classifier</head><p>False classifier False pseudo label After obtaining the pseudo labels, we can use the same mixup process as before to generate virtual samples to augment the dataset. However, if the reliability of pseudo labels is not considered, the generated samples with false pseudo labels (FPLs) can even mislead the model and aggravate confirmation bias <ref type="bibr" target="#b0">(Arazo et al., 2020)</ref>, as shown in <ref type="figure" target="#fig_1">Figure 3</ref> left. This becomes worse when mixup is employed on FPLs (supported by Sec. 4.3); either side with larger or smaller weights can further reinforce the wrong decision hyperplane and make it irreparable. Yet, if mixup is used only on labeled data, it will not be able to mine deeper into the potential information in a large amount of unlabeled data. Thus, we not only expect DM to improve the training efficiency in the supervised framework as Sec. 2.1 but also want to solve the confirmation bias under semi-supervision, as shown in <ref type="figure" target="#fig_1">Figure 3</ref> right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA-EFFICIENT MIXUP LEARNING</head><p>In data-efficient mixup learning, various scenarios derived from supervised learning (SL) will be considered, including transfer learning (TL) and SSL. The mentioned label mismatch and unreliable mixup are the two main challenges in these scenarios where mixup plays an important role. Both of them will be addressed by decoupled mixup simply and effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DECOUPLED MIXUP</head><p>As stated in the preliminaries 2, the major problem of ? weighted CE (L M CE ) is that, while preserving the ? consistency between x (a,b) and y (a,b) to smooth the predictions but neglect the potential information of mixed samples when the mixed label and sample are mismatching. Proposition 1. Assuming x (a,b) is generated from two different classes, minimizing L M CE is equivalent to regress corresponding ? in the gradient of L M CE :</p><formula xml:id="formula_1">(? z (a,b) L) i = ? ? ? ? ? ? ? ? ? ? ? ?? + exp(z i (a,b) ) c exp(z c (a,b) ) , l = i ?(1 ? ?) + exp(z j (a,b) ) c exp(z c (a,b) ) , l = j exp(z i (a,b) ) c exp(z c (a,b) ) , l = i, j<label>(2)</label></formula><p>As we can see, the predicted probability of x (a,b) is coupled with ? in Equation 2. Such a design deliberately suppresses the prediction confidence for the class with a smaller ?, although it builds a linear correspondence according to ?. In other words, L M CE does not encourage the model to focus on the class with weak features, but it could have been a good hard sample. As consequence, L M CE makes many static mixup methods require a long training time to achieve the desired results <ref type="bibr" target="#b47">Yu et al., 2021)</ref>. Based on previous analysis, a novel mixup objective, decoupled mixup, is raised to learn the comprehensive features of the mixed data adaptively and improve the mixup training efficiency. Before that, we first start with an original Softmax function. For mixed data points z (a,b) generated from a random pair in labeled dataset L, an encoded representation z (a,b) = f ? (x <ref type="bibr">(a,b)</ref> ) is generated by a feature extractor f ? . A mixup categorical probability of i-th class is attained:</p><formula xml:id="formula_2">?(z (a,b) ) i = exp(z i (a,b) ) c exp(z c (a,b) )</formula><p>.</p><p>Empirically, a well-designed dynamic mixup policy that decouples the target object from the background effectively reduces label mismatch and thus improves performance. This motivated us to introduce decoupling to the objective function of mixup. For a mixed sample, we can achieve decoupling by omitting one of the mixed classes and thereby calculating the prediction probability distribution independently. Even if the mixed sample does not contain distinct class characteristics, the model will make relatively high confidence predictions without the competitor class, e.g., another class in the mixup. That is, the model will be encouraged to treat that sample as a hard sample, expecting to mine the underlying feature. And vice versa, if the characteristics are obvious enough, the model's prediction is not limited by the ? value. Therefore, we propose to remove the competitor class in the standard Softmax to achieve decoupled prediction. The score on i-th class is not affected by the j-th class:</p><formula xml:id="formula_3">?(z (a,b) ) i,j = exp(z i (a,b) ) X X X X X exp(z j (a,b) ) + c =j exp(z c (a,b) )</formula><p>.</p><p>By removing the opponent class, the two desired effects are achieved simultaneously: (1) breaking the ? restriction on prediction;</p><p>(2) encouraging more confident prediction on the current class. Compared with Equation 2, the decoupled Softmax makes all items associated with ? become -1, the detailed derivation is given in the Appendix A.1. Proposition 2. With the decoupled Softmax defined above, decoupled mixup cross-entropy L DM (CE) can boost the prediction confidence of the interested classes mutually and escape from the ?constraint:</p><formula xml:id="formula_4">L DM (CE) = ? c i=1 c j=1 y i a y j b log p i (a,b) 1 ? p j (a,b) + log p j (a,b) 1 ? p i (a,b)</formula><p>.</p><p>The proofs of Proposition 1 and 2 are given in Appendix. In this way, the scope and confidence of our target class probability are successfully decoupled from the "background". Further, according to the property of decoupled cross-entropy loss which only pays attention to the current classes, the original linear correspondence between the mixed sample and label could be corrupted. Therefore, to preserve the consistency, L M CE loss is employed in the final Decoupled Mixup (DM) loss. The overall loss function of decoupled mixup can be formulated as follows:</p><formula xml:id="formula_5">L DM = ? y T (a,b) log(?(z (a,b) )) L M CE +? y T [a,b] log(?(z (a,b) ))y [a,b] L DM (CE)</formula><p>.</p><p>where y <ref type="bibr">(a,b)</ref> indicates the mixed label while y <ref type="bibr">[a,b]</ref> is two-hot label encoding, ? is a trade-off factor. Notice that ? is robust and can be set according to the character of mixup methods (see Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ASYMMETRICAL STRATEGY</head><p>Although L DM can improve the learning efficiency in labeled data, how to effectively exploit the unlabeled data and reduce the confirmation bias is still a problem. Recall the confirmation bias problem of SSL: the performance of the student model is restricted by the teacher model when learning from inaccurate pseudo-labels. To strengthen the teacher to provide more accurate predictions, the unlabeled data with larger ? can be used to mixup with the labeled data to form hard samples. Specifically, a larger ? can make the unlabeled data obscure most of the information of the labeled data, but since the label is assured, the whole process of mixup can be treated as hard sample generation. With the addition of these hard samples, the teacher model will have better performance and thus give more reliable pseudo-labels. Formally, given the labeled and unlabeled datasets L and U , AS builds reliable mixed samples between L and U with an asymmetric manner (? &lt; 0.5):</p><formula xml:id="formula_6">x (a,b) = ?x a + (1 ? ?)u b ;? (a,b) = ?y a + (1 ? ?)v b .</formula><p>Due to the uncertainty of the pseudo-label, only the labeled part is retained in L DM (CE) : Binary cross-entropy form of DM. If we treat mixup training as a multi-label classification task (1-vs-all) using mixup binary crossentropy (MBCE) loss <ref type="bibr" target="#b41">(Wightman et al., 2021)</ref> (?(?) denotes Sigmoid), then we can generate the decoupled loss for each class. Proposition 2 demonstrates the decoupled Softmax-based CE can mutually enhance the confidence of predictions for the interested classes and be free from ? limitations. For MBCE, since it is not inherently bound to mutual interference between classes by Softmax, we need to preserve partial consistency and encourage more confident predictions to propose a decoupled mixup binary cross-entropy loss, DM(BCE).</p><formula xml:id="formula_7">L DM (CE) = y T a log ?(z (a,b) ) y b ,<label>where</label></formula><p>To this end, a rescaling function r : ?, t, ? ? ? is designed to achieve this goal. The mixed label is enhanced by r(?): y mix = ? a y a + ? b y b , where ? a and ? b are rescaled. The rescaling function is defined as follows:</p><formula xml:id="formula_8">r(?, t, ?) = ? ? t , 0 ? t, 0 ? ? &lt; 1,<label>(3)</label></formula><p>where ? is the threshold, t is an index to control the convexity. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, there are three situations: (a) when ? = 0, t = 0, the rescaled label is always equal to 1, as two-hot encoding; (b) when ? = 1, t = 1, r(?) is a linear function (vanilla mixup); (c) the rest curves demonstrate t is the parameter that changes the concavity and ? is responsible for truncating.</p><p>Empirical results. In the case of interpolation-based mixup methods (e.g., Mixup, ManifoldMix, etc.) that keep a linear relationship between the mixed label and sample, the decoupled mechanism can be introduced by only adjusting the threshold t. In the case of cutting-based mixing policies (e.g., CutMix, PuzzleMix, etc.) where the mixed samples and labels have a square relationship (generally a convex function), we can approximate the convex function by adjusting ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We adopt two types of top-1 classification accuracy (Acc) metrics (the mean of three trials): (i) the median top-1 Acc of the last 10 epochs <ref type="bibr">(Sohn et al., 2020;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>, and (ii) the best top-1 Acc in all checkpoints. We report the median top-1 Acc for image classification tasks with Mixup variants and the max top-1 Acc for SSL tasks. Popular ConvNets and Transformer-based architectures are used as backbone networks: ResNet variants including ResNet <ref type="bibr" target="#b10">(He et al., 2016)</ref> (R), Wide-ResNet (WRN) <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref>, and ResNeXt-32x4d (RX) <ref type="bibr" target="#b43">(Xie et al., 2017)</ref>, Transformer-based architectures including DeiT  and Swin Transformer (Swin) <ref type="bibr">(Liu et al., 2021)</ref>. Double horizontal line in the tables splits the static and dynamic methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE CLASSIFICATION BENCHMARKS</head><p>This subsection evaluates performance gains of DM on six image classification benchmarks, including CIFAR-10/100 <ref type="bibr" target="#b16">(Krizhevsky et al., 2009</ref>), Tiny-ImageNet <ref type="bibr" target="#b4">(Chrabaszcz et al., 2017)</ref>, <ref type="bibr">ImageNet-1k (Russakovsky et al., 2015)</ref>, CUB-200-2011 (CUB) <ref type="bibr" target="#b39">(Wah et al., 2011)</ref>, FGVC-Aircraft (Aircraft) . There are mainly two types of mixup methods based on their mixing policies: static methods including Mixup , CutMix , ManifoldMix , SaliencyMix (Uddin et al., 2020), FMix , and ResizeMix <ref type="bibr" target="#b29">(Qin et al., 2020)</ref>, and dynamic mixup methods including PuzzleMix , AutoMix <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, and SAMix . For a fair comparison, we use the optimal ? in {0.1, 0.2, 0.5, 0.8, 1.0, 2.0} for all mixup algorithms and follow original hyper-parameters in papers. We adopt the open-source codebase OpenMixup <ref type="bibr" target="#b24">(Li et al., 2022)</ref> for most mixup methods. The detailed training recipes are provided in Appendix A.3. ImageNet and fine-grained classification benchmarks. As shown in <ref type="table" target="#tab_1">Table 2</ref>, 3, and 4, DM(CE) improves over MCE in all mixup algorithms on three training settings: around +0.4% for mixup methods based on ResNet variants using PyTorch-style and RSB A3 settings; around +0.5% and +0.2% for all methods based on DeiT-S and Swin-T using DeiT setting. Meanwhile, <ref type="table" target="#tab_2">Table 3</ref> shows that DM(BCE) noticeably boosts the original MBCE, e.g., +1.21%/+0.68%/+0.79% for RSB/CutMix/ManifoldMix and +0.37%/+0.12% for AutoMix/SAMix. Notice that MBCE(two)     <ref type="bibr">(You et al., 2020)</ref>, and Self-Tuning <ref type="bibr">(Ximei et al., 2021)</ref>. For a fair comparison, we use the same hyper-parameters and augmentations as Self-Tuning: all methods are initialized by PyTorch pre-trained models on ImageNet-1k and trained totally of 27k steps by SGD optimizer with the basic learning rate of 0.001, the momentum of 0.9, and the weight decay of 0.0005. In <ref type="table" target="#tab_5">Table 6</ref>, we adopt DM(CE) and AS for Fine-Tuning, Co-Tuning, and Self-Tuning using Mixup. DM(CE) and AS steadily improve Mixup and the baselines by large margins, e.g., +4.62%?9.19% for 15% labels, +2.02%?5.67% for 30% labels, and +2.09%?3.15% for 50% labels on Cars. This outstanding improvement implies that generating mixed samples efficiently is essential for data-limited scenarios.</p><p>A similar performance will be presented as well in the next SSL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SEMI-SUPERVISED LEARNING BENCHMARKS</head><p>Following <ref type="bibr">(Sohn et al., 2020;</ref><ref type="bibr">Zhang et al., 2021)</ref>, we adopt the most commonly-used CIFAR-10/100 datasets among the famous SSL benchmarks based on WRN-28-2 and WRN-28-8. We mainly evaluate  <ref type="table">Table 7</ref>: Top-1 Acc (%)? of semi-supervised learning on CIFAR-10 (using 250 and 4000 labels) and CIFAR-100 (using 400, 2500, and 10000 labels) based on WRN-28-2 and WRN-28-8, respectively. Notice that DM denotes using DM(CE) and AS, Con denotes various unsupervised consistency losses, Rot denotes the rotation loss in ReMixMatch, and CPL denotes the curriculum labeling in FlexMatch. the proposed DM on popular SSL methods MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019b)</ref> and FixMatch <ref type="bibr">(Sohn et al., 2020)</ref>, and compare with Pesudo-Labeling <ref type="bibr" target="#b18">(Lee et al., 2013)</ref>, ReMixMatch <ref type="bibr" target="#b1">(Berthelot et al., 2019a)</ref>, UDA <ref type="bibr" target="#b42">(Xie et al., 2019)</ref>, and FlexMatch <ref type="bibr">(Zhang et al., 2021)</ref>. For a fair comparison, we use the same hyperparameters and training settings as the original papers and adopt the open-source codebase TorchSSL <ref type="bibr">(Zhang et al., 2021)</ref> for all methods. Concretely, we use an SGD optimizer with a basic learning rate of lr = 0.03 adjusted by Cosine Scheduler, the totally 2 20 steps, the batch size of 64 for labeled data, and the confidence threshold ? = 0.95. <ref type="table">Table 7</ref> shows that adding DM(CE) and AS significantly improves MixMatch and FixMatch: DM(CE) brings 1.81?2.89% gains on CIFAR-10 and 1.27?3.31% gains on CIFAR-100 over MixMatch while bringing 1.78?4.17% gains on CIFAR-100 over FixMatch. Meanwhile, we find that directly applying mixup augmentations to FixMatch brings limited improvements, while FixMatch+DM achieves the best performance in most cases on CIFAR-10/100 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDY</head><p>Since we have demonstrated the effectiveness of DM in the above four subsections and <ref type="figure">Figure 6</ref>, we verify whether DM is robust to hyper-parameters (see full hyper-parameters in Appendix) and the effectiveness of AS in SSL tasks:</p><p>(1) The only hyper-parameter ? in DM(CE) and DM(BCE) can be set according to the types of mixup methods. We grid search ? in {0.01, 0.1, 0.5, 1, 2} on ImageNet-1k. As shown in <ref type="figure" target="#fig_3">Figure 5</ref> left, the static (Mixup and CutMix) and the dynamic methods (PuzzleMix and AutoMix) prefer ? = 0.1 and ? = 1, respectively, which might be because the dynamic variants generate more discriminative and reliable mixed samples than the static methods.</p><p>(2) Hyper-parameters ? and t in DM(BCE) can also be determined by the characters of mixup policies. We grid search ? ? {1, 0.9, 0.8, 0.7} and t ? {2, 1, 0.5, 0.3}. <ref type="figure" target="#fig_3">Figure 5</ref> middle and right show that cutting-based methods (CutMix and AutoMix) prefer ? = 0.8 and t = 1, while the interpolation-based policies (Mixup and ManifoldMix) use ? = 1.0 and t = 0.5.   (3) <ref type="table" target="#tab_7">Table 8</ref> shows the superior of AS(? ? 0.5) in comparison to MCE and AS(? ? 0.5), while using DM(CE) and AS(? ? 0.5) further improves MCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Mixup. As data-dependent augmentation techniques, mixup methods generate new samples by mixing two or more samples and corresponding labels with well-designed mixing policies <ref type="bibr" target="#b24">Liu et al., 2022;</ref>. The pioneering mixing method is Mixup , whose mixed samples are generated by linear interpolation between pairs of samples. After that, cut-based methods are proposed to improve the mixup for localizing important features, especially in the vision field. In other works, authors explore using nonlinear or optimizable interpolation mixup policies, such as PuzzleMix , Co-Mixup <ref type="bibr">(Kim et al., 2021)</ref>, AutoMix <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, and SAMix . Moreover, mixup methods extend to more than two elements <ref type="bibr">(Kim et al., 2021;</ref><ref type="bibr" target="#b5">Dabouei et al., 2021)</ref>, and are utilized in contrastive learning to learn discriminative visual representation <ref type="bibr">(Kalantidis et al., 2020;</ref><ref type="bibr">Lee et al., 2021;</ref><ref type="bibr">Shen et al., 2021;</ref>.</p><p>Transfer Learning. Pseudo-Labeling <ref type="bibr" target="#b18">(Lee et al., 2013</ref>) is a popular semi-supervised learning (SSL) method that utilizes artificial labels converted from teacher model predictions. MixMatch <ref type="bibr" target="#b2">(Berthelot et al., 2019b)</ref> and ReMixMatch <ref type="bibr" target="#b1">(Berthelot et al., 2019a)</ref> apply mixup on labeled and unlabeled data to enhance the diversity of the dataset. More accurate pseudo-labeling relies on data augmentation techniques to introduce consistency regularization, e.g., UDA <ref type="bibr" target="#b42">(Xie et al., 2019)</ref> and FixMatch <ref type="bibr">(Sohn et al., 2020)</ref> employ weak and strong augmentations to improve the consistency. Furthermore, CoMatch <ref type="bibr" target="#b20">(Li et al., 2021a)</ref> unifies consistency regularization, entropy minimization, and graph-based contrastive learning to mitigate confirmation bias. FlexMatch <ref type="bibr">(Zhang et al., 2021)</ref> improves FixMatch by applying curriculum learning for dynamically updating confidence threshold class-wisely. Finetuning a pre-trained model on labeled datasets is a widely adopted form of transfer learning (TL) in various applications. Previously, <ref type="bibr" target="#b7">(Donahue et al., 2014;</ref><ref type="bibr" target="#b28">Oquab et al., 2014)</ref> show that transferring pre-trained AlexNet features to downstream tasks outperforms hand-crafted features. More recently, Self-Tuning <ref type="bibr">(Ximei et al., 2021)</ref> introduced contrastive learning into TL to tackle confirmation bias and model shift issues in a one-stage framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we introduce Decoupled Mixup (DM), a new objective function for considering both smoothing the decision boundaries and mining discriminative features. We demonstrate in both supervised and semi-supervised scenarios with various tasks that mixup algorithms equipped with DM can have a considerable performance gain. The proposed DM helps early static mixup methods (e.g., MixUp and CutMix) achieve a comparable or better performance than the computational expensive dynamic mixup policies. Most impotently, DM raises a question worthy of researching: is it necessary to design very complex mixup policies to achieve expected results? Based on our analysis and experiments, we argue that the loss function of mixup classification is a new question worthy of studying. Although DM alleviates the label mismatch issue from a different perspective, its effect is obtained by adding regular terms. We believe that there exists a more elegant and graceful form of loss function than DM to explore the potential power of mixup essentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In this Appendix, we provide proofs of the proposition 1 ( ?A.1) and proposition 2 ( ?A.2) and implementation details ( ?A.3).</p><p>A.1 PROOF OF PROPOSITION 1 Proposition 1. Assuming x (a,b) is generated from two different classes, minimizing L M CE is equivalent to regress corresponding ? in the gradient of L M CE :</p><formula xml:id="formula_9">(? z (a,b) L M CE ) i = ? ? ? ? ? ? ? ? ? ? ? ?? + exp(z i (a,b) ) c exp(z c (a,b) ) , l = i ?(1 ? ?) + exp(z j (a,b) ) c exp(z c (a,b) ) , l = j exp(z i (a,b) ) c exp(z c (a,b) ) , l = i, j<label>(4)</label></formula><p>Proof. For the mixed sample (x (a,b) , y (a,b) ), z (a,b) is derived from a feature extractor f ? (i.e z (a,b) = f ? (x (a,b) )). According to the definition of the mixup cross-entropy loss L M CE , we have: .</p><formula xml:id="formula_10">? z(a,b) L M CE l = ?L M CE ?z l (a,b) = ? ? ?z l (a,b) y T (a,b) log ?(z (a,b) ) = ? C i=1 y i (a,b) ? ?z l (a,b) log( exp(z i (a,b) ) C j=1 exp(z j (a,b) ) ) = ? C i=1 y i<label>(</label></formula><p>Proof. For the mixed sample (x (a,b) , y (a,b) ), z (a,b) is derived from a feature extractor f ? (i.e z (a,b)=f ? <ref type="figure">(x (a,b)</ref> ) ). According to the definition of the mixup cross-entropy loss L DM (CE) , we have: </p><formula xml:id="formula_11">L</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>inconsistency between mixed labels and sampleFigure 1: Illustration of the problem of label mismatch. The red mixed labels are the ground truth. Experimental overviews of the label mismatch issue. Compared with static policies like Mixup and CutMix, the dynamic method AutoMix significantly reduces the difficulty of mixup classification and alleviates the label mismatch problem by providing more reliable mixed samples, but also brings a large computational overhead. Left: Top-1 and Top-2 accuracy of mixed data on ImageNet-1k with 100 epochs. Prediction is counted as correct if the Top-1 prediction belongs to {y a , y b }; prediction is counted as correct if the Top-2 predictions are equal to {y a , y b }. Right: Taking Mixup as an example, decouple mixup cross-entropy (DMCE) significantly improves training efficiency by alleviating the label mismatch issue from the perspective of designing a loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Left: unconstrained mixup misleads the model by generating bad samples. Right: the decision bound can be correctly guided by the DM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>y a and y b are one-hot labels. AS could be regarded as a special case of DM that only decouples the hard samples with ground-truth labels. Replacing L DM (CE) withL DM (CE) can leverage the hard samples in mixed samples between L and U while alleviating the problem of confirmation bias. Rescaled label of different ? value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Ablation of hyper-parameters on ImageNet-1k based on ResNet-34. Left: analyzing the balancing weight ? in DM(CE); Middle: analyzing ? in DM(BCE) when t is fixed to 1 and 0.5; Right: analyzing t in DM(BCE) when ? is fixed to 1 and 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 6: Top-1 Acc of mixed samples on ImageNet-1k validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 2 .</head><label>2</label><figDesc>With the decoupled Softmax defined above, decoupled mixup cross-entropy L DM (CE) can boost the prediction confidence of the interested classes mutually and escape from the ?-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>)</head><label></label><figDesc>DM (CE) = y T [a,b] log H(Z (a,b) ) y [a,b] y T a log H(Z (a,b) ) y b + y T b log H(Z (a,b) ) y a where p (a,b) = ?(z (a,b) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Top-1 Acc (%)? of small-scale image classification on CIFAR-100 and Tiny-ImageNet datasets based on ResNet variants.Table 1shows small-scale classification results, the proposed DM(CE) significantly improves MCE based on various mixup algorithms: it meets our expectation that DM(CE) brings more performance gains for most static mixup variants, e.g., +1.32%/0.86%/0.69% on CIFAR-100 and +1.21%/1.34% on Tiny for Mixup, except for ResizeMix. It might be because ResizeMix tries to preserve the full target information of the two mixing classes. Meanwhile, DM(CE) still enhances dynamic mixup methods even though these algorithms have achieved high performance: DM(CE) brings 0.23%?0.36% on CIFAR-100 for the previous state-ofthe-art PuzzleMix and brings 0.21%?0.27% on Tiny for the current best method SAMix.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tiny-ImageNet</cell><cell></cell></row><row><cell></cell><cell></cell><cell>R-18</cell><cell cols="2">RX-50</cell><cell cols="2">WRN-28-8</cell><cell></cell><cell>R-18</cell><cell cols="2">RX-50</cell></row><row><cell>Methods</cell><cell cols="10">MCE DM(CE) MCE DM(CE) MCE DM(CE) MCE DM(CE) MCE DM(CE)</cell></row><row><cell>Mixup</cell><cell>79.12</cell><cell>80.44</cell><cell>82.10</cell><cell>82.96</cell><cell>82.82</cell><cell>83.51</cell><cell>63.86</cell><cell>65.07</cell><cell>66.36</cell><cell>67.70</cell></row><row><cell>CutMix</cell><cell>78.17</cell><cell>79.39</cell><cell>81.67</cell><cell>82.39</cell><cell>84.45</cell><cell>84.63</cell><cell>65.53</cell><cell>66.45</cell><cell>66.47</cell><cell>67.46</cell></row><row><cell cols="2">ManifoldMix 80.35</cell><cell>80.95</cell><cell>82.88</cell><cell>83.15</cell><cell>83.24</cell><cell>83.42</cell><cell>64.15</cell><cell>65.45</cell><cell>67.30</cell><cell>68.48</cell></row><row><cell>FMix</cell><cell>79.69</cell><cell>80.00</cell><cell>81.90</cell><cell>82.74</cell><cell>84.21</cell><cell>84.28</cell><cell>63.47</cell><cell>65.34</cell><cell>65.08</cell><cell>66.96</cell></row><row><cell>ResizeMix</cell><cell>80.01</cell><cell>80.06</cell><cell>81.82</cell><cell>82.96</cell><cell>84.87</cell><cell>84.72</cell><cell>63.74</cell><cell>64.33</cell><cell>65.87</cell><cell>68.56</cell></row><row><cell>SaliencyMix</cell><cell>79.12</cell><cell>79.28</cell><cell>81.53</cell><cell>82.61</cell><cell>84.35</cell><cell>84.41</cell><cell>64.60</cell><cell>66.56</cell><cell>66.55</cell><cell>67.52</cell></row><row><cell>PuzzleMix</cell><cell>81.13</cell><cell>81.34</cell><cell>82.85</cell><cell>82.97</cell><cell>85.02</cell><cell>85.25</cell><cell>65.81</cell><cell>66.52</cell><cell>67.83</cell><cell>68.04</cell></row><row><cell>AutoMix</cell><cell>82.04</cell><cell>82.32</cell><cell>83.64</cell><cell>83.94</cell><cell>85.18</cell><cell>85.38</cell><cell>67.33</cell><cell>68.18</cell><cell>70.72</cell><cell>71.56</cell></row><row><cell>SAMix</cell><cell>82.30</cell><cell>82.40</cell><cell>84.42</cell><cell>84.53</cell><cell>85.50</cell><cell>85.59</cell><cell>68.89</cell><cell>69.16</cell><cell>72.18</cell><cell>72.39</cell></row><row><cell cols="5">Small-scale classification benchmarks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-1 Acc (%)? of image classification on ImageNet-1k with ResNet variants using PyTorchstyle training recipe. 70.20 73.97 74.26 77.12 77.41 CutMix 68.95 69.26 73.58 73.88 77.07 77.32 ManifoldMix 69.98 70.33 73.98 74.25 77.01 77.30 FMix 69.96 70.26 74.08 74.34 77.19 77.38 ResizeMix 69.50 69.90 73.88 74.00 77.42 77.65 SaliencyMix 69.16 69.57 73.56 73.92 77.14 77.42 PuzzleMix 70.12 70.32 74.26 74.51 77.54 77.71 AutoMix 70.51 70.64 74.52 74.77 77.91 78.15 SAMix 70.85 70.90 74.96 75.10 78.11 78.36</figDesc><table><row><cell></cell><cell>R-18</cell><cell></cell><cell>R-34</cell><cell></cell><cell>R-50</cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">MCE MD(CE) MCE MD(CE) MCE MD(CE)</cell></row><row><cell>Vanilla</cell><cell>70.04</cell><cell>-</cell><cell>73.85</cell><cell>-</cell><cell>76.83</cell><cell>-</cell></row><row><cell>Mixup</cell><cell>69.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-1 Acc (%)? of image classification on ImageNet-1k based on ResNet-50 using RSB A3 training recipe.</figDesc><table><row><cell>Methods</cell><cell cols="3">MCE MD(CE) MBCE MBCE MD(BCE)</cell></row><row><cell></cell><cell></cell><cell>(one) (two)</cell><cell>(one)</cell></row><row><cell>RSB</cell><cell>76.49 77.72</cell><cell>78.08 76.95</cell><cell>78.43</cell></row><row><cell>Mixup</cell><cell>76.01 76.69</cell><cell>77.66 77.42</cell><cell>78.28</cell></row><row><cell>CutMix</cell><cell>76.47 77.22</cell><cell>77.62 67.54</cell><cell>78.21</cell></row><row><cell cols="2">ManifoldMix 76.14 76.93</cell><cell>77.78 67.78</cell><cell>78.20</cell></row><row><cell>FMix</cell><cell>76.09 76.87</cell><cell>77.76 73.44</cell><cell>78.11</cell></row><row><cell>ResizeMix</cell><cell>76.90 77.21</cell><cell>77.85 77.30</cell><cell>78.32</cell></row><row><cell cols="2">SaliencyMix 76.85 77.25</cell><cell>77.93 72.74</cell><cell>78.24</cell></row><row><cell>PuzzleMix</cell><cell>77.27 77.60</cell><cell>78.02 77.19</cell><cell>78.15</cell></row><row><cell>AutoMix</cell><cell>77.45 77.82</cell><cell>78.33 77.46</cell><cell>78.62</cell></row><row><cell>SAMix</cell><cell>78.33 78.45</cell><cell>78.64 77.58</cell><cell>78.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>ResizeMix 79.93 80.03 80.94 81.01 SaliencyMix 79.32 79.86 80.68 80.83 PuzzleMix 79.84 80.25 81.03 81.16 AutoMix 80.78 80.91 81.80 81.92 SAMix 80.94 81.12 81.87 81.97</figDesc><table><row><cell cols="3">: Top-1 Acc (%)? of classifi-</cell></row><row><cell cols="3">cation on ImageNet-1k with ViTs.</cell></row><row><cell></cell><cell>DeiT-S</cell><cell>Swin-T</cell></row><row><cell>Methods</cell><cell cols="2">MCE MD(CE) MCE MD(CE)</cell></row><row><cell>DeiT</cell><cell cols="2">79.80 80.37 81.28 81.49</cell></row><row><cell>Mixup</cell><cell cols="2">79.65 80.04 80.71 80.97</cell></row><row><cell>CutMix</cell><cell cols="2">79.78 80.20 80.83 81.05</cell></row><row><cell>FMix</cell><cell cols="2">79.41 79.89 80.37 80.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>MCE DM(CE) MCE DM(CE) MCE DM(CE) Mixup 78.39 79.90 84.58 85.04 79.52 82.66 85.18 86.68 CutMix 78.40 78.76 85.68 85.97 78.84 81.64 84.55 85.75 ManifoldMix 79.76 79.92 86.38 86.42 80.68 82.57 86.60 86.92 FMix 77.28 80.10 84.06 84.85 79.36 80.44 84.85 85.04 ResizeMix 78.50 79.58 84.77 84.92 78.10 79.54 84.08 84.51 SaliencyMix 77.95 78.28 83.29 84.51 80.02 81.31 84.31 85.07 AutoMix 79.87 81.08 86.56 86.74 81.37 82.18 86.69 86.82 SAMix 81.11 81.27 86.83 86.95 82.15 83.68 86.80 87.22 denotes using two-hot encoding for corresponding mixing classes, which yield worse performance than MBCE, and DM(BCE) adjusts the labels for the mixing classes by Equation 3. It verifies the necessity of MD(BCE) in the case of using MBCE. Table 5 shows that DM(CE) noticeably boosts the original MCE for eight popular mixup variants, especially bringing 0.53%?3.14% gains on Aircraft based on ResNet-18.</figDesc><table><row><cell cols="5">: Top-1 Acc (%)? of fine-grained image classification on</cell></row><row><cell cols="5">CUB-200 and FGVC-Aircrafts datasets with ResNet variants.</cell></row><row><cell>Datasets</cell><cell>CUB-200</cell><cell></cell><cell cols="2">FGVC-Aircrafts</cell></row><row><cell></cell><cell>R-18</cell><cell>RX-50</cell><cell>R-18</cell><cell>RX-50</cell></row><row><cell>Methods</cell><cell>MCE DM(CE)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Tuning 45.25?0.12 59.68?0.21 70.12?0.29 39.57?0.20 57.46?0.12 67.93?0.28 36.77?0.12 60.63?0.18 75.10?0.21 +DM 50.04?0.17 61.39?0.24 71.87?0.23 43.15?0.22 61.02?0.15 70.38?0.18 41.30?0.16 62.65?0.21 77.19?0.19 BSS 47.74?0.23 63.38?0.29 72.56?0.17 40.41?0.12 59.23?0.31 69.19?0.13 40.57?0.12 64.13?0.18 76.78?0.21 Co-Tuning 52.58?0.53 66.47?0.17 74.64?0.36 44.09?0.67 61.65?0.32 72.73?0.08 46.02?0.18 69.09?0.10 80.66?0.25 +DM 54.96?0.65 68.25?0.21 75.72?0.37 49.27?0.83 65.60?0.41 74.89?0.17 51.78?0.34 74.15?0.29 83.02?0.26 Self-Tuning 64.17?0.47 75.13?0.35 80.22?0.36 64.11?0.32 76.03?0.25 81.22?0.29 72.50?0.45 83.58?0.28 88.11?0.29 +Mixup 62.38?0.32 74.65?0.24 81.46?0.27 59.38?0.31 74.65?0.26 81.46?0.27 70.31?0.27 83.63?0.23 88.66?0.21 +DM 73.06?0.38 79.50?0.35 82.64?0.24 67.57?0.27 80.71?0.25 84.82?0.26 81.69?0.23 89.22?0.21 91.26?0.19</figDesc><table><row><cell cols="10">: Top-1 Acc (%)? of transfer learning on various TL benchmarks using only 15%, 30% and</cell></row><row><cell cols="3">50% labels based on ResNet-50.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CUB-200</cell><cell></cell><cell></cell><cell>FGVC-Aircraft</cell><cell></cell><cell></cell><cell>Stanford-Cars</cell><cell></cell></row><row><cell>Methods</cell><cell>15%</cell><cell>30%</cell><cell>50%</cell><cell>15%</cell><cell>30%</cell><cell>50%</cell><cell>15%</cell><cell>30%</cell><cell>50%</cell></row><row><cell>Fine-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>02?0.09 95.81?0.01 60.06?1.62 73.51?0.20 78.10?0.15 FixMatch+Mixup CE+Con+MCE 95.05?0.23 95.83?0.19 50.61?0.73 72.16?0.18 78.75?0.14 FixMatch+DM CE+Con+DM 95.23?0.09 95.87?0.11 59.75?0.95 74.12?0.23 79.58?0.17</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-10</cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Methods</cell><cell>Losses</cell><cell>250</cell><cell>4000</cell><cell>400</cell><cell>2500</cell><cell>10000</cell></row><row><cell>Pseudo-Labeling</cell><cell>CE</cell><cell cols="5">53.51?2.20 84.92?0.19 12.55?0.85 42.26?0.28 63.45?0.24</cell></row><row><cell>MixMatch</cell><cell>CE+Con</cell><cell cols="5">86.37?0.59 93.34?0.26 32.41?0.66 60.24?0.48 72.22?0.29</cell></row><row><cell>ReMixMatch</cell><cell>CE+Con+Rot</cell><cell cols="5">93.70?0.05 95.16?0.01 57.15?1.05 73.87?0.35 79.08?0.27</cell></row><row><cell>MixMatch+DM</cell><cell>CE+Con+DM</cell><cell cols="5">89.16?0.71 95.15?0.68 35.72?0.53 62.51?0.37 74.70?0.28</cell></row><row><cell>UDA</cell><cell>CE+Con</cell><cell cols="5">94.84?0.06 95.71?0.07 53.61?1.59 72.27?0.21 77.51?0.23</cell></row><row><cell>FixMatch</cell><cell>CE+Con</cell><cell cols="5">95.14?0.05 95.79?0.08 53.58?0.82 71.97?0.16 77.80?0.12</cell></row><row><cell>FlexMatch</cell><cell>CE+Con+CPL</cell><cell>95.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation of the proposed asymmetric strategy (AS) and DM(CE) of semi-supervised transfer learning on CUB-200 based on ResNet-18.</figDesc><table><row><cell>Methods</cell><cell>15%</cell><cell>30%</cell><cell>50%</cell></row><row><cell>Self-Tuning</cell><cell cols="3">57.82 69.12 73.59</cell></row><row><cell>+MCE</cell><cell cols="3">63.36 72.81 75.73</cell></row><row><cell>+MCE+AS(? ? 0.5)</cell><cell cols="3">59.04 69.67 74.89</cell></row><row><cell>+MCE+AS(? ? 0.5)</cell><cell cols="3">62.97 72.46 75.40</cell></row><row><cell cols="4">+DM(CE)+AS(? ? 0.5) 66.17 74.25 77.68</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supermix: Supervising the mixing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13794" to="13803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Pr?gel-Bennett Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5275" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-mixup: Saliency guided joint mixup with supermodular diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">I-mix: A domain-agnostic strategy for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Comatch: Semi-supervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Boosting discriminative visual representation learning with scenario-agnostic mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15454</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Openmixup: Open mixup toolbox and benchmark for visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://github.com/Westlake-AI/openmixup" />
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automix: Unveiling the power of mixup for stronger classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11101</idno>
		<title level="m">Resizemix: Mixing data with preserved object information and true labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Un-mix: Rethinking image mixtures for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research (JMLR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11001</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Saliencymix: A saliency guided data augmentation strategy for better regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mst</forename><surname>Afm Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wheemyung</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taechoong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Ho</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01791</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-tuning for data-efficient deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ximei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Jinghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mingsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jianmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xinyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Sinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mingsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jianmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Co-tuning for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mixup without hesitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Graphics (ICIG)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">2017) is a rescale version of ImageNet-1k, which has 10,000 training images and 10,000 validation images of 200 classes in 64?64 resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Dataset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<idno>Small scale classification benchmarks: CIFAR-10/100</idno>
	</analytic>
	<monogr>
		<title level="m">) Large scale classification benchmarks: ImageNet-1k</title>
		<imprint>
			<publisher>FGVC-Aircraft</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tiny-ImageNet</note>
	<note>2011) contains 11,788 images from 200 wild bird species for fine-grained classification. 000 images of 100 classes of aircrafts. Standford-Cars</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename></persName>
		</author>
		<title level="m">Training settings on ImageNet-1k. Tab. A1 shows the full training settings of PyTorch, DeiT, and RSB A3 on ImageNet-1k. Following</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>we replace the step learning rate decay by Cosine Scheduler (Loshchilov &amp; Hutter, 2016) for better performances</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">2022), where we reproduce most comparison methods. Notice that static methods denote Mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hyper-parameter settings. We follow the basic hyper-parameter settings (e.g., ?) for mixup variants in OpenMixup</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ResizeMix (Qin et al., 2020), and dynamic methods denote PuzzleMix. AutoMix (Liu et al., 2022</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">We set the hyperparameters of DM(CE) as follows: For CIFAR-100 and ImageNet-1k, static methods use ? = 0.1 and dynamic methods use ? = 1. For Tiny-ImageNet and fine-grained datasets, static methods use ? = 1 based on ResNet-18 while ? = 0.1 based on ResNeXt-50; dynamic methods use ? = 1. As for the hyper-parameters of DM(BCE) on ImageNet-1k, cutting-based methods use t = 1 and ? = 0.8, while interpolation-based methods use t = 0.5 and ? = 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Samix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Similarly, interpolation-based methods denote Mixup and ManifoldMix while cutting-based methods denote the rest mixup variants mensioned above</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Note that we use ? = 0.2 and ? = 2 for the static and dynamic methods when use the proposed DM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
