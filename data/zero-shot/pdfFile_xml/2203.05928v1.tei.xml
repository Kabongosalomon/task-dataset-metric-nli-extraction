<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TFCNet: Temporal Fully Connected Networks for static unbiased temporal reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">TFCNet: Temporal Fully Connected Networks for static unbiased temporal reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal Reasoning is one important functionality for vision intelligence. In computer vision research community, temporal reasoning is usually studied in the form of video classification, for which many state-of-the-art Neural Network structures and dataset benchmarks are proposed in recent years, especially 3D CNNs and Kinetics. However, some recent works found that current video classification benchmarks contain strong biases towards static features, thus cannot accurately reflect the temporal modeling ability. New video classification benchmarks aiming to eliminate static biases are proposed, with experiments on these new benchmarks showing that the current clip-based 3D CNNs are outperformed by RNN structures and recent video transformers.</p><p>In this paper, we find that 3D CNNs and their efficient depthwise variants, when video-level sampling strategy is used, are actually able to beat RNNs and recent vision transformers by significant margins on static-unbiased temporal reasoning benchmarks. Further, we propose Temporal Fully Connected Block (TFC Block), an efficient and effective component, which approximates fully connected layers along temporal dimension to obtain video-level receptive field, enhancing the spatiotemporal reasoning ability. With TFC blocks inserted into Video-level 3D CNNs (V3D), our proposed TFCNets establish new state-of-theart results on synthetic temporal reasoning benchmark, CATER, and real world static-unbiased dataset, Diving48, surpassing all previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal Reasoning in videos requires the computer vision models to understand complicated spatiotemporal patterns and reason among frames, which is often studied in the form of video classification. Many state-of-the-art spatiotemporal learning models have been proposed in recent years, especially 3D CNNs <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b21">Wang et al., 2018a;</ref><ref type="bibr" target="#b5">Feichtenhofer et al., 2018)</ref>, which have achieved state-of-the-art accuracy on mainstream benchmarks, Kinetics-400, UCF-101 etc <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b17">Soomro et al., 2012)</ref>. However, some recent works <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020;</ref><ref type="bibr" target="#b12">Li et al., 2018)</ref> demonstrate that many mainstream benchmarks have strong biases towards static representations, for example, scene and objects, which means that many categories can be recognized by considering only spatial features, without considering temporal features. In fact, there is no problem with that for video action recognition, since many data in the wild inevitably lead to biases. However, for the purpose of benchmarking the model's ability for temporal reasoning, such 2D spatial biases should be controlled or eliminated. According to , if a benchmark contains representation biases, a model which learns better such biases leads to higher accuracy on this benchmark. Thus if a video benchmark contains strong static biases and temporal biases, it is ambiguous to explain the performance of models on this benchmark.</p><p>In <ref type="figure">Figure 1</ref>, we show cases of static-biased benchmarks and static-unbiased benchmarks, from static-biased video classification benchmark Kinetics-400 <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017)</ref>, synthetic static-unbiased object permanence benchmark CATER <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref>, and real world static-unbiased temporal reasoning benchmark Div-ing48 . In the first column, even with one frame, it is still easy to recognize that the label should be "springboard diving" since the static features are enough to discriminate the label. However, for Diving48, which contains 48 fine-grained diving action labels, it is impossible to distinguish the label with one frame. Also, for CATER snitch localization task, there is a golden snitch appearing in the first frame, which can later be contained, moved and uncontained by other objects in the video. The task asks the model to reason where is the golden snitch in the last frame. Please note that the snitch can be visible, contained or occluded in the last frame, which requires the model to reason the whole video and even state-of-the-art tracking models fail <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref>. The properties of such static-unbiased benchmarks <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020;</ref><ref type="bibr" target="#b12">Li et al., 2018)</ref> are so different from previous staticbiased benchmarks <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b17">Soomro</ref>  et al., 2012) that it has been shown many state-of-the-art clip-level 3D CNNs <ref type="bibr" target="#b5">(Feichtenhofer et al., 2018)</ref> fail on these benchmarks <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020;</ref><ref type="bibr" target="#b27">Zhou et al., 2021)</ref> and also are surpassed by long range models, for example LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> and video transformers <ref type="bibr" target="#b0">(Bertasius et al., 2021)</ref>. Thus it is an important direction to explore the properties of 3D CNNs on these static-unbiased video benchmarks.</p><p>Our contributions in this paper are from three aspects. First, we found that when video-level sampling strategy is used, 3D CNNs and our proposed efficient depthwise variants , which we term V3D, surpass all LSTM and video transformers by significant margins in terms of temporal reasoning capabilities. Second, we propose novel Temporal Fully Connected Operations (TFC Operations), which efficiently obtain global temporal receptive field with few extra parameters and cheap computation. Finally, we integrate the proposed TFC Operations with V3D to form TFCNets, a new type of video-level temporal reasoning models, which achieve new state-of-the-art results on static-unbiased temporal reasoning benchmarks, CATER and Diving48.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Classification Models In deep learning era, there are mainly 3 types of network structure for video classification. First, Two-stream convolutional networks were originally proposed by <ref type="bibr" target="#b16">(Simonyan &amp; Zisserman, 2014)</ref>, where one stream is used for learning from RGB images, and the other one for optical flow, with late fusion yielding the final prediction. Although the accuracy of Two-stream networks is high, the main limitation of such structures is that the computation of optical flow is highly expensive. Second, various 3D CNNs have been proposed <ref type="bibr" target="#b18">(Tran et al., 2015;</ref><ref type="bibr" target="#b1">Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b21">Wang et al., 2018a;</ref><ref type="bibr" target="#b5">Feichtenhofer et al., 2018)</ref>, where 3D convolutions or temporal convolutions plus spatial convolutions are applied directly on video sequence to model spatiotemporal features. It is noteworthy that most of 3D CNNs are clip-based methods, which only sample a random clip of the whole video during training. Third, long-term modeling frameworks have been developed for capturing long range temporal structures. Ng et al <ref type="bibr" target="#b14">(Ng et al., 2015;</ref><ref type="bibr" target="#b3">Donahue et al., 2015)</ref> process video sequences with CNNs and RNNs, where CNNs are used for frame-level feature extraction and RNNs are used for aggregating video-level temporal information. Temporal Segment Networks (TSN) <ref type="bibr" target="#b20">(Wang et al., 2016)</ref>, originally designed for 2D CNNs, have been proposed to model videolevel spatiotemporal information with video-level sampling strategy, which sample several frames from the whole video during training. The frames are fed to the same 2D CNN backbone, which predicts a confidence score for each frame. The output scores are averaged to generate final video-level prediction. Recently, V4D <ref type="bibr" target="#b26">(Zhang et al., 2020)</ref> improves TSN by utilizing 4D convolutions to model short term and long term spatiotemporal features with 3D CNNs. TimeSformer <ref type="bibr" target="#b0">(Bertasius et al., 2021)</ref> transfers recent vision transformer <ref type="bibr" target="#b4">(Dosovitskiy et al., 2021)</ref> structures to video domain, surpassing previous clip-level 3D CNNs on various benchmarks.</p><p>Biases in Spatiotemporal Reasoning Benchmarks Biases are hard to be eliminated in machine learning yet can be controlled. For temporal reasoning, Li et al  proposed metrics to explicitly calculate the representation biases towards static features in video classification benchmarks. They found that many video benchmarks <ref type="bibr" target="#b1">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b17">Soomro et al., 2012)</ref> lead to high static biases, which makes these benchmarks not suitable for testing the temporal reasoning models. Thus they proposed a new benchmark Diving48, which controls the static biases by using similar foreground and background in all videos thus forcing the model to reason fine-grained temporal structures. CATER also controls the static biases by using simple foreground geometric objects and the same background, requiring the model to understand the whole video to infer which grid the snitch localizes in the last frame. However, we are not using Something-Something dataset in this paper, which is usually considered a temporalbiased benchmark in <ref type="bibr" target="#b26">Zhang et al., 2020)</ref>. We found that static-unbiased and temporal-biased are not the same concept since temporal-biased benchmark can also bias towards static features at the same time. We found Something-Something <ref type="bibr" target="#b8">(Goyal et al., 2017b)</ref> contains strong static biases. Although the benchmark hides the detailed 30408 unique objects categories by a unified term "something", these uncontrolled various foreground entities and backgrounds lead to ambiguity in terms of comparing temporal reasoning capabilities of different models. Something-Else <ref type="bibr" target="#b13">(Materzynska et al., 2020)</ref> manually labeled all the foreground objects in something-something and improved I3D baseline by 5% top1 accuracy with object identity embeddings. These phenomenons indicate that something something is also static-biased thus is not a perfect benchmark for temporal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Fully Connected Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">V3D: a new baseline</head><p>Although 3D CNNs are powerful spatiotemporal models, most of them are clip-based methods, which means that only a clip of the whole video will be used for training. In this paper, we choose to use TSN's video-level sampling strategy with 3D CNN structures, so that both the holistic duration can be covered and video-level spatiotemporal features can be learned. The video level sampling strategy is shown in <ref type="figure">Figure 2</ref>. To be specific, video-level sampling strategy uniformly divides the whole input video into T segments. During training, one frame is randomly chosen from each segment. During Inference, the center frame of each segment is used. The output of video-level sampling strategy is a tensor of C ? T ? H ? W , where C, T, H, W denote input channels, number of frames, height and width of a frame. We term such Video-level 3D CNNs, V3D, for the ease of reference.</p><p>For V3D, we mainly use SlowPath from <ref type="bibr" target="#b5">(Feichtenhofer et al., 2018)</ref> shown in <ref type="table">Table 1</ref> for ablation study, which has been proven to be an effective model on various video classification benchmarks. We further propose V3D Depthwise, where depthwise temporal convolutions are added to 2D TSN ResNet structures. The depthwise convolution is utilized in the residual form <ref type="bibr" target="#b9">(He et al., 2016)</ref> with Residual Depthwise Block (RDW) in <ref type="table">Table 1</ref>, which is shown in Equation <ref type="formula">(1)</ref>. V c denotes the cth channel of input tensor V , O j denotes jth dimension of output tensor, with c = j in our case. W depthwise jc is the temporal depthwise convolution. We extend ResNet18 and ResNet50 to V3D Depthwise and we found that V3D Depthwise are able to obtain similar accuracy with V3D, yet with almost the same parameters with 2D TSN and much fewer FLOPs than V3D.</p><formula xml:id="formula_0">O j = V c + W depthwise jc * V c (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Fully Connected Operation</head><p>Temporal convolution is a local operator, which has to be stacked multiple times to obtain longer temporal receptive field. In order to obtain video-level global temporal recep- tive field at any stage of the network, it is a natural idea to utilize fully connected operators to temporal dimension. Yet, fully connected operations easily lead to overfitting with too many parameters and too much computation, thus they are not trivial to be applied to modern CNNs. In this section, we will start from standard temporal convolution, and extend it to temporal fully connected operations, finally reduce the parameters and computation by approximation and simplification.  <ref type="figure">Figure 2</ref>. We show the overall framework of TFCNets. Video-level sampling strategy is applied to an input video of any length to form a fixed length input tensor. We replace several 3D Residual Blocks with TFC Residual Blocks to transform V3D to TFCNets. We also demonstrate the detailed structures of 3D Residual Blocks and TFC Residual Blocks with convolution kernels and intermediate features.</p><p>where ? Cout j means to concat the jth output channel along the C out dimension and O is the output tensor and O ? R BHW ?Cout?T . In recent 3D CNN structures, there is no degradation in temporal dimension where the stride is 1 and paddings are added. Here we do not show such details in the formula for clarity. The FLOPs of temporal convolution in Equation <ref type="formula">(2)</ref> is B ?C out ?H ?W ?C in ?T ?K.</p><p>Temporal Fully Connected Operation It is a simple idea to extend the temporal kernels W conv ? R Cin?Cout?K to temporal fully connected kernels W connected ? R Cin?Cout?T ?T . Such temporal fully connected kernels increase the temporal modeling length from a local range of K to all frames T , where T is the temporal length of input tensor V , without degradation in temporal dimension. Formally,</p><formula xml:id="formula_1">O = ? Cout j Cin c W connected jc V c<label>(3)</label></formula><p>Also in Equation <ref type="formula" target="#formula_1">(3)</ref>, matrix multiplication is used instead of convolutional operation in Equation <ref type="formula">(2)</ref>. Although gaining global receptive field, the parameters of W connected is T 2 /K times more than standard temporal convolution. Also the FLOPs of W connected operation is B ? C out ? H ? W ? C in ? T ? T , which is T /K times more computation than W conv operation. The parameters and FLOPs make such model easily overfitting and also leads to out of memory issues even on advanced GPUs, which causes fully connected operations rarely used in intermediate stages for modern CNNs.</p><p>Simplified TFC Operation In order to reduce the parameters, we approximate W connected by reducing C in to 1, which means that for each channel of output tensor o j , the same matrix W connected j ? R 1?Cout?T ?T is applied for all input channels,</p><formula xml:id="formula_2">O = ? Cout j Cin c W connected j V c<label>(4)</label></formula><p>Although the parameters are reduced, the FLOPs is not, still being B ? C out ? H ? W ? C in ? T ? T . However, since in Equation (4) W connected j is the same for all channels, we can exchange the order of channel-dimension summation and W connected j , thus,</p><formula xml:id="formula_3">O = ? Cout j W connected j Cin c V c<label>(5)</label></formula><p>Since Cin c V c ? R BHW ?Cin?T , the overall FLOPs become B?C out ?H?W ?T ?T , which decreases the computation cost by C in times. Considering the fact that C in is generally large in modern convolutional neural networks, especially in deeper layers, the computational cost is decreased dramatically. Compared with standard temporal convolution, the FLOPs of equationEquation <ref type="formula" target="#formula_3">(5)</ref> is T /(C in K) of Equation (2). Since generally C in &gt;&gt; T , our proposed temporal fully connected layer is even much cheaper than standard temporal 1D convolution. In practice, we find that adding a normalization term along the channel dimension in Equation <ref type="formula" target="#formula_3">(5)</ref> would make the model converge faster. The normalization term N (V ) = 1 Cin is simply mean average along the channel dimension, making the training more stable. Thus,</p><formula xml:id="formula_4">O = ? Cout j W connected j 1 C in Cin c V c<label>(6)</label></formula><p>We will use TFC Operation to refer the Simplified TFC Operation for simplicity in the following sections, unless mentioned otherwise. We provide the Pytorch implementation of TFC Operation Algorithm 1. We also compare the parameters and computation of various operations in <ref type="table" target="#tab_4">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TFC Residual Block and TFCNet</head><p>With TFC Operation implemented, we change 3D Residual Block to TFC Residual Block, shown in <ref type="figure">Figure 2</ref>. For V3D, the TFC Operation is added to the first temporal convolution of 3D Residual Block in a parallel branch, with element-wise addition to merge the local and global temporal features. Similarly, for V3D Depthwise, we add the TFC Operation in parallel with the first 2D convolution kernel of the Residual Block. We turn every other 3D Residual Blocks in res3 and res4 to TFC Residual Blocks. For 2D TSN baseline, we add TFC Operations to the first 2D kernel of every Residual Blocks in res4 and res5. We term these baseline networks with TFC Blocks "TFCNets".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison with previous methods with global receptive field</head><p>We notice that there are some previous operations also able to obtain global receptive field. Two typical such methods are SE Block and Non-local Block. Here we will discuss and compare our proposed TFC layer with these methods. SE Block Hu et al  proposed SE Block for 2D image classification. We can simply extend SE Block to its 3D version, which is able to model spatiotemporal features. The spatiotemporal SE block adopts 3D global average pooling on the whole input tensor V ? R B?Cin?T ?H?W to squeeze the T, H, W dimensions into one scalar, in which way the global view is obtained. Two fully-connected layers and ReLU activations are further adopted to generate the channel-wise attention maps, followed by expanding the B ? C in attention maps to B ? C in ? T ? H ? W and element-wise multiplication with the input tensor V .</p><p>Compared with our TFC Operation, SE Block obtains global receptive field by squeezing spatiotemporal dimensions into one scalar, which is efficient, yet completely loses finegrained spatiotemporal structures. Also it has to be used in each Residual Block according to the authors to obtain best accuracy, which leads to more overall parameters than TFCNet. We will show in our experiments that 3D SE Block actually hurts the performance of 3D CNNs on long-range video reasoning. Instead, our TFC Operation does not make any dimension degradation, neither spatial dimensions nor temporal dimension.</p><p>Non-local Blocks Wang et al <ref type="bibr" target="#b23">(Wang et al., 2018b)</ref>proposed Nonlocal Blocks to model global spatiotemporal features, which can be considered one typical implementation of self-attention methods. Nonlocal blocks generates attention maps of T HW ? T HW by applying matrix multiplication on two tensors of size T HW ? C/2. The computation cost of Nonlocal Blocks or self attention methods are composed of two parts, the embedded layers and matrix multiplication. We notice that the embedded layers have already lead to more computation than our proposed TFC Operation. If spatiotemporal attention is calculated, the computation cost of Nonlocal Blocks becomes hundreds times of our TFC Operation and Nonlocal Blocks cannot be trained with the settings of TFCNets due to out of memory issue. Although we use linear scaling rule <ref type="bibr" target="#b7">(Goyal et al., 2017a)</ref> to adjust the learning rate, spatiotemporal Nonlocal V3D with 32 frames in temporal dimension cannot converge with too few samples in a batch. It is possible to ease this problem with other techniques, yet this would completely change the settings and lead to unfair comparison. Instead, we use temporal Nonlocal Blocks to compare with our TFC Blocks. We change the input and output channels of four 1x1 convolutions in the temporal Nonlocal Block so that our TFC Operation in a 3D Residual Block can be directly replaced by the temporal Nonlocal Block.</p><formula xml:id="formula_5">Operation Parameter FLOPs Equation (2) Cout ? Cin ? K B ? Cout ? H ? W ? T ? Cin ? K Equation (3) Cout ? Cin ? T ? T B ? Cout ? H ? W ? T ? Cin ? T Equation (6) (ours) Cout ? T ? T B ? Cout ? H ? W ? T ? T SE Operation Cin ? Cin ? 1/8 B ? Cin ? Cin ? 1/8 Nonlocal Operation Cin ? Cin/2 ? 4 B ? Cin/2 ? H ? W ? T ? Cin ? 4 + B ? Cin ? T ? H ? W ? T ? H ? W Temporal Nonlocal Operation Cin ? Cin/4 ? 3 + Cin/4 ? Cin/4 B ? Cin/4 ? H ? W ? T ? Cin ? 3.25 + B ? Cin/2 ? T ? T ? H ? W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Limitation</head><p>Our proposed TFC Operation comes with one inevitable disadvantage, which is the fixed length of temporal dimension. This means that the number of input frames during training must be identical with the frames during inference. Yet this is not a problem for classification, which is the task of this paper, since the video-level sampling strategy perfectly solves this issue by its variant sampling intervals. We will show in the experiment section that TFCNets achieve new state-of-the-art accuracy on Diving48 benchmark, whose input videos are of different length. However, for downstream tasks which use TFCNets to extract features, it remains to be an open problem, which we will explore in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We use two static unbiased temporal reasoning benchmarks to demonstrate the strong temporal reasoning ability of TFC-Net. CATER <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020</ref>) is a synthetic dataset, where videos use the same background with simple geometry objects moving in the foreground. The most challenging task in CATER is the snitch localization, shown in <ref type="figure">Figure 1</ref>, which requires the temporal model understanding object permanence. There are 3850 videos for training and 1650 videos for validation. There are 36 classes, corresponding to the 6 ? 6 grid. The videos are all of 300 frames. We also validate TFCNets on real world temporal reasoning benchmark, Diving48 . There are 48 fine-grained categories with 16k videos for training, 2k for testing. Since there is no official validation set, the testing set is used for validation by all previous methods. We follow the same criterion for fair comparison. The video lengths are variant, with 158 frames on average, min 24 frames and max 822 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">implementation details</head><p>We use pre-trained weights from ImageNet-1k <ref type="bibr" target="#b2">(Deng et al., 2009</ref>) to initialize the model. For TSN, V3D baselines and our proposed TFCNets, we adopt video-level sampling strategies where we uniformly sample the same number of frames covering the whole video for both training and testing. We first resize each frame to 320 ? 240. The only data augmentation we used are simple random cropping of 224 ? 224 and random crop. During testing, we use center cropping to crop a region of 224 ? 224 at the center of each frame. We keep the training and inference pipeline simple and effective, without any tricks.</p><p>In order to demonstrate the importance of video-level sampling strategy, we also experiment with the 3D CNN and TFCNet structures with clip-level sampling strategy. For the clip-level sampling, following <ref type="bibr" target="#b5">(Feichtenhofer et al., 2018)</ref>, we first resize each frame to 320 ? 256. During training, we randomly select a clip of 64 frames from each video where we uniformly sample 32 frames in this clip with a fixed stride of 2. During Testing we use spatial fully convolutional testing by following <ref type="bibr" target="#b23">(Wang et al., 2018b;</ref><ref type="bibr" target="#b24">Yue et al., 2018;</ref><ref type="bibr" target="#b5">Feichtenhofer et al., 2018)</ref>. We sample 10 clips evenly from the full-length video, and crop 256 ? 256 regions to spatially cover the whole frame for each clip.</p><p>We utilize a SGD optimizer with an initial learning rate of 0.01, weight decay is set to 10 ?5 with a momentum of 0.9. The batch size is 8 on each GPU, with 8 GPUs the total batch size is 64. The learning rate drops by 10 at epoch 35, 65, and the model is trained for 90 epochs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CATER</head><p>We thoroughly explore the properties of our proposed TFC-Net by conducting ablation studies on CATER snitch localization <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref>. Besides Top1 accuracy, L1 Loss is also used for the metrics following <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of TFCNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Backbone total frames extra annotation/data top1 top5 L1 Loss params GFLOPs Tracking <ref type="bibr" target="#b28">(Zhu et al., 2018)</ref> SiamRPN all frames -33.9 -2.4 --R3D <ref type="bibr" target="#b23">(Wang et al., 2018b)</ref> ResNet50 64 ? 10 ? 3 -57.4 78.4 1.4 --R3D+LSTM <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref> ResNet50 64 ? 10 ? 1 -60.2 81.8 1.2 --R3D+NL <ref type="bibr" target="#b23">(Wang et al., 2018b)</ref> ResNet50 32 ? 10 ? 3 -26.7 68.9 2.6 --R3D+NL+LSTM <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref> ResNet50 32 ? 10 ? 1 -46.2 69.9 1.5 --Hopper <ref type="bibr" target="#b27">(Zhou et al., 2021)</ref> Transformer all frames LA-CATER <ref type="bibr" target="#b15">(Shamsian et al., 2020)</ref> 73.2 93.8 0.85 --OPNet <ref type="bibr" target="#b15">(Shamsian et al., 2020)</ref> OPNet all frames LA-CATER <ref type="bibr" target="#b15">(Shamsian et al., 2020)</ref> 74.8 -0.54 --V4D <ref type="bibr" target="#b26">(Zhang et al., 2020)</ref> ResNet50 <ref type="formula">8</ref>  V3D ResNet50, please note that our V3D ResNet50 baseline has already surpassed all previously published state-of-theart results on CATER <ref type="bibr" target="#b6">(Girdhar &amp; Ramanan, 2020)</ref>, which indicates that V3D is a very strong baseline for static-unbiased learning. Our TFC Blocks further improve V3D by 1.6%, enabling global temporal receptive field in the intermediate stages with only 1M extra parameters and 0.3GFLOPs more computation. Our TFC V3D Depthwise ResNet50 fur-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Backbone segment ? crop top1 top5 parameters GFLOPs SlowFast <ref type="bibr" target="#b5">(Feichtenhofer et al., 2018)</ref> ResNet101 128 ? 3 77.6 -53.7M 213x3 TimeSformer-L <ref type="bibr" target="#b0">(Bertasius et al., 2021)</ref> TimeSformer 96 ? 3 81.0 -121.4M 2380x3 TQN <ref type="bibr">(Zhang et al., 2021)</ref> S3D+Feature Bank all frames 81. video-level vs clip-level Most previous 3D CNNs are clipbased models, which means that during training only a short clip from the whole video is randomly sampled. For longrange spatiotemporal reasoning tasks, e.g. CATER, where the spatiotemporal patterns change from the first frame to the last frame, it is obvious that such local sampling strategy will not work well. We compare clip-level sampling strategy against video-level ones to demonstrate the importance of video-level learning. Shown in <ref type="table">Table 3b</ref>, for both V3D baselines and our proposed TFC V3D, video-level sampling strategy leads to consistent improvements of about 54% over the clip-level sampling strategy. Also the standard clip-level methods use much more clips and higher resolution (224 to 256), leading to 39 times more computation cost than video-level counterparts.</p><p>Besides, <ref type="table">Table 3b</ref> also proves that our proposed TFC Block is a general module that not only improves video-level methods, but also is able to lead to 2.1% improvement against 3D baseline even in the clip-level learning setting. Finally, we also compare the video-level sampling strategy against the recently proposed V4D <ref type="bibr" target="#b26">(Zhang et al., 2020)</ref>, which adopts hybrid sampling strategy that is locally clip-level and globally video-level. We demonstrate that video-level sampling strategy still outperforms V4D sampling strategy by more than 30% absolutely. However, our experiment shows that V4D leads to more than 20% absolute improvement over clip-level 3D CNNs, which is consistent with the conclusion of V4D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different temporal length</head><p>We compare the improvement of TFC Blocks with different input frames for training and inference. Please note that the parameters of Temporal Fully Connected Operations are quadratic to the temporal length, thus the parameters of TFC V3D with different input frames are slightly different, shown in <ref type="table">Table 3c</ref>. We found that when the input frames are rather sparse, for example, 8 frames, adding TFC Block causes decrease in top1 accuracy. With more frames used for training, the advantage of TFCNet becomes more significant. We infer that this phenomenon is due to the limited temporal receptive field of V3D. When the temporal length is only 8 frames, the receptive field of V3D is enough to learn temporal interactions. However, with input frame increased to 32, it becomes harder for V3D to learn global spatiotemporal representation. Our proposed TFC Blocks make it possible to obtain global temporal receptive field at intermediate stages for V3D, thus lead to higher accuracy.</p><p>Comparison against SE Block and Nonlocal Block SE Block and Nonlocal Block are another two typical structures to obtain global receptive field in modern neural networks. We compare our proposed TFC Block with SE Block and Nonlocal Block from the aspects of accuracy, parameters and GFLOPs. We extend the original 2D SE Block to 3D spatiotemporal SE Block and we follow  to insert one SE Block to every Residual Bottleneck for best performance. For Nonlocal Blocks, we found the original spatiotemporal Nonlocal Blocks <ref type="bibr" target="#b23">(Wang et al., 2018b)</ref> lead to out of memory issues with our V3D structures, thus we experiment with temporal Nonlocal Blocks, replacing the TFC Operation branches in 3D Residual Blocks for fair comparison. For original Nonlocal Networks <ref type="bibr" target="#b23">(Wang et al., 2018b)</ref>, we also compare their results with TFCNets on CATER in the following section.</p><p>In <ref type="table">Table 3d</ref>, we show that compared to baseline V3D ResNet50, SE V3D ResNet50 actually causes decrease in accuracy. We believe this is because SE Block applies spatiotemporal global average pooling to obtain global receptive field, with the side effect of destroying fine-grained spatiotemporal structures. This phenomenon indicates the importance of keeping the dimensions of space and time for the intermediate features.</p><p>Also, we found that Nonlocal Blocks do increase the accuracy of V3D network. However, Nonlocal V3D leads to 27GFLOPs more computation cost than V3D, compared with our TFC V3D with only 0.3GFLOPs more computation. Also, our TFC V3D has the highest accuracy among these modules of global receptive field and is the cheapest in terms of parameters.</p><p>These experiments also prove that the accuracy gain of TFCNets is NOT simply due to adding more parameters.</p><p>Compare with State-of-the-art We compare our proposed TFCNets with previous state-of-the-art results reported on this dataset, including various CNNs, CNN+RNN, and recent vision transformers. Our TFCNets establish new stateof-the-art results on this benchmark, surpassing all previous methods, including the ones utilizing external training data or annotations <ref type="bibr" target="#b15">(Shamsian et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Diving48</head><p>We also test the temporal reasoning ability of TFCNet on real world dataset, Diving48. The top1 accuracy of TFCNet on Diving48 V2 is shown in <ref type="table">Table 5</ref>. With the fewest parameters and FLOPs, our TFCNets achieve new state-of-the-art top1 accuracy of 88.3%. Compared with recent video transformers, our TFCNets are 7.3% higher in terms of accuracy. For Diving48 V1, since there are many videos mislabeled, it does not accurately reflect the temporal reasoning ability of different models. Here we report the results of TFC-Net on Diving48 V1 only for completeness. Compared to the best published result 44.7% top1 accuracy from Corr-Net ResNet101 , our TFCNet ResNet50 achieves 56.3% top1 accuracy with 11.6% absolute increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we study static unbiased temporal reasoning in videos and propose Temporal Fully Connected Networks to learn video-level spatiotemporal features. We propose very efficient and effective TFC Operations, combined with V3D and our proposed depthwise variants. TFCNets achieve new state-of-the-art results on object permanence dataset CATER and real world temporal reasoning benchmark Diving48.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2203.05928v1 [cs.CV] 11 Mar 2022</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">witcherofresearch@gmail.com</cell></row><row><cell></cell><cell></cell><cell cols="3">the golden snitch</cell></row><row><cell></cell><cell></cell><cell cols="3">where is the golden</cell></row><row><cell></cell><cell></cell><cell cols="3">snitch in the 6x6 grid?</cell></row><row><cell></cell><cell></cell><cell cols="3">groundtruth: 26</cell></row><row><cell>groundtruth:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>springboard diving</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>groundtruth: ['Inward', '25som',</cell><cell>30</cell><cell>26</cell><cell>35</cell></row><row><cell></cell><cell>'NoTwis', 'PIKE']</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>6</cell><cell></cell><cell>11</cell></row><row><cell></cell><cell></cell><cell cols="3">0 1 2 3 4 5</cell></row><row><cell>Kinetics 400</cell><cell>Diving48</cell><cell>CATER</cell><cell></cell></row><row><cell cols="5">Figure 1. comparison of static-biased Kinetics-400 and static-</cell></row><row><cell cols="4">unbiased video benchmarks Diving48 and CATER.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>BxCxTxHxW BxCxTxHxW BxCxTxHxW BxC/4xTxHxW TFC Operation Temporal Conv Temporal Conv 3D Residual Block TFC Residual Block</head><label></label><figDesc>Temporal Convolution Formally, in modern 3D CNNs (which are mixed forms of 2D convolutions and 1D temporal convolutions which are demonstrated inTable 1), a tensor of an input video is of size V input ? R B?Cin?T ?H?W , where B is batch size, C in is number of input channel, T, H, W represent temporal length, spatial height, width respectively. For a standard 1D temporal convolution operation, firstly, the input tensor should be converted to V ? R BHW ?Cin?T by permutation. Although it can be implemented directly with 3D convolution with a kernel of 3?1?1, we follow the standard 1D temporal convolution notations. Given a temporal 1D convolution kernel W conv ? R Cin?Cout?K , where C in is the number of input channels, C out is the number of output channels, K is the length of temporal kernel, * being</figDesc><table><row><cell></cell><cell>Video-level Sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x1x1,C,C/4</cell><cell>3x1x1,C,C/4</cell><cell>TxT,1,C/4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BxC/4xTxHxW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Global</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Avg</cell><cell>1x3x3,C/4,C/4</cell><cell>1x3x3,C/4,C/4</cell></row><row><cell></cell><cell>3D</cell><cell>TFC</cell><cell>3D</cell><cell>3D</cell><cell>Pooling</cell><cell></cell></row><row><cell></cell><cell>Residual Block</cell><cell>Residual Block</cell><cell>Residual Block</cell><cell>Residual Block</cell><cell>FC</cell><cell>BN</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1x1x1,C/4,C</cell><cell>1x1x1,C/4,C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell></cell><cell cols="2">TFCNet</cell><cell></cell><cell></cell><cell>BxCxTxHxW</cell></row><row><cell>video</cell><cell>3x32x224x224</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">convolution operation, a standard temporal convolution can</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>be denoted:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cin</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O = ? Cout j</cell><cell>W conv jc</cell><cell>*  V c</cell><cell>(2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>c</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 Pseudocode of Temporal Fully Connected Operation in a PyTorch-like style.</figDesc><table><row><cell>import torch</cell></row><row><cell>class TFC(torch.nn.Module):</cell></row><row><cell>def __init__(self, out_channels=1024,time=32):</cell></row><row><cell>super(TFC, self).__init__()</cell></row><row><cell>#temporal fully connected layer</cell></row><row><cell>self.fc=torch.nn.Linear(time,time * out_channels,</cell></row><row><cell>bias=False)</cell></row><row><cell>self.c_out=out_channels</cell></row><row><cell>def forward(self,input):</cell></row><row><cell>b,c_in,t,h,w=input.shape</cell></row><row><cell>#permute and view the spatiotemporal tensor</cell></row><row><cell>#to a temporal tensor</cell></row><row><cell>x=input.permute(0,3,4,1,2).contiguous().view(b * h</cell></row><row><cell>* w,c_in,t)</cell></row><row><cell>#mean average along input channel dimension</cell></row><row><cell>x=torch.mean(x,dim=1)</cell></row><row><cell>x=x.view(b * h * w,t)</cell></row><row><cell>x=self.fc(x)</cell></row><row><cell>#permute and view the temporal tensor</cell></row><row><cell>#to a spatiotemporal tensor</cell></row><row><cell>x=x.view(b,h,w,self.c_out,t)</cell></row><row><cell>x=x.permute(0,3,4,1,2).contiguous()</cell></row><row><cell>#b,c_out,t,h,w</cell></row><row><cell>return x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Temporal Convolution, TFC Operation and other methods with global temporal receptive field.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>In order to compare the structures fairly, all of the input are of size 3 ? 32 ? 224 ? 224, where 3 is the RGB channel, 32 is the number of frames, 224 ? 224 is the center crop of each frame.</figDesc><table><row><cell>model</cell><cell cols="4">backbone input T ? H ? W top1</cell><cell>gain</cell><cell cols="4">top5 L1 Loss parameters GFLOPs</cell></row><row><cell>TSN</cell><cell>ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell>34.1</cell><cell>-</cell><cell cols="2">57.0</cell><cell>2.33</cell><cell>23.58M</cell><cell>131.5</cell></row><row><cell>TFC TSN</cell><cell>ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell cols="4">73.6 +39.5% 92.3</cell><cell>0.68</cell><cell>26.65M</cell><cell>132.1</cell></row><row><cell>V3D</cell><cell>ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell>77.9</cell><cell>-</cell><cell cols="2">94.0</cell><cell>0.54</cell><cell>31.63M</cell><cell>167.7</cell></row><row><cell>TFC V3D</cell><cell>ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell>79.5</cell><cell>+1.6%</cell><cell cols="2">94.0</cell><cell>0.50</cell><cell>32.68M</cell><cell>168.0</cell></row><row><cell>V3D Depthwise</cell><cell>ResNet18</cell><cell cols="2">32 ? 224 ? 224</cell><cell>76.1</cell><cell>-</cell><cell cols="2">93.5</cell><cell>0.57</cell><cell>11.19M</cell><cell>58.3</cell></row><row><cell cols="2">TFC V3D Depthwise ResNet18</cell><cell cols="2">32 ? 224 ? 224</cell><cell>77.4</cell><cell>+1.3%</cell><cell cols="2">94.3</cell><cell>0.52</cell><cell>11.97M</cell><cell>58.6</cell></row><row><cell>V3D Depthwise</cell><cell>ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell>76.6</cell><cell>-</cell><cell cols="2">94.6</cell><cell>0.51</cell><cell>23.59M</cell><cell>131.6</cell></row><row><cell cols="2">TFC V3D Depthwise ResNet50</cell><cell cols="2">32 ? 224 ? 224</cell><cell>79.7</cell><cell>+3.1%</cell><cell cols="2">95.5</cell><cell>0.47</cell><cell>24.64M</cell><cell>132.0</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) Effectiveness of TFCNet.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell></cell><cell cols="8">sampling strategy top1 top5 frame ? segment ? crops parameters GFLOPs</cell></row><row><cell cols="2">V3D ResNet50</cell><cell>clip</cell><cell></cell><cell cols="2">23.1 46.4</cell><cell cols="3">32 ? 10 ? 3</cell><cell>31.63M</cell><cell>6571.1</cell></row><row><cell cols="2">V3D ResNet50</cell><cell cols="2">video</cell><cell cols="2">77.9 94.0</cell><cell cols="3">1 ? 32 ? 1</cell><cell>31.63M</cell><cell>167.7</cell></row><row><cell cols="2">V4D ResNet50(Zhang et al., 2020)</cell><cell cols="2">clip-video</cell><cell cols="2">43.4 63.1</cell><cell></cell><cell cols="2">8 ? 4 ? 1</cell><cell>36.42M</cell><cell>373.6</cell></row><row><cell cols="2">V4D ResNet50(Zhang et al., 2020)</cell><cell cols="2">clip-video</cell><cell cols="2">47.2 72.1</cell><cell cols="3">8 ? 10 ? 3</cell><cell>36.42M</cell><cell>934.0</cell></row><row><cell cols="2">TFC V3D ResNet50</cell><cell>clip</cell><cell></cell><cell cols="2">25.2 54.0</cell><cell cols="3">32 ? 10 ? 3</cell><cell>32.68M</cell><cell>6582.9</cell></row><row><cell cols="2">TFC V3D ResNet50</cell><cell cols="2">video</cell><cell cols="2">79.5 94.0</cell><cell cols="3">1 ? 32 ? 1</cell><cell>32.68M</cell><cell>168.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(b) video-level sampling.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>model</cell><cell cols="8">training and inference segment top1 paramters GFLOPs</cell></row><row><cell cols="2">V3D ResNet50</cell><cell></cell><cell>8</cell><cell></cell><cell cols="2">55.2</cell><cell cols="2">31.63M</cell><cell>41.9</cell></row><row><cell cols="2">TFC V3D ResNet50</cell><cell></cell><cell>8</cell><cell></cell><cell cols="2">54.6</cell><cell cols="2">31.70M</cell><cell>41.9</cell></row><row><cell cols="2">V3D ResNet50</cell><cell></cell><cell>16</cell><cell></cell><cell cols="2">69.7</cell><cell cols="2">31.63M</cell><cell>83.8</cell></row><row><cell cols="2">TFC V3D ResNet50</cell><cell></cell><cell>16</cell><cell></cell><cell cols="2">70.2</cell><cell cols="2">31.90M</cell><cell>83.9</cell></row><row><cell cols="2">V3D ResNet50</cell><cell></cell><cell>32</cell><cell></cell><cell cols="2">77.9</cell><cell cols="2">31.63M</cell><cell>167.7</cell></row><row><cell cols="2">TFC V3D ResNet50</cell><cell></cell><cell>32</cell><cell></cell><cell cols="2">79.5</cell><cell cols="2">32.68M</cell><cell>168.0</cell></row><row><cell></cell><cell></cell><cell cols="4">(c) Different input length</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>model</cell><cell></cell><cell cols="2">input size</cell><cell cols="5">top1 paramters GFLOPs</cell></row><row><cell></cell><cell cols="2">V3D ResNet50</cell><cell cols="3">32 ? 224 ? 224 77.9</cell><cell cols="2">31.63M</cell><cell cols="2">167.7</cell></row><row><cell></cell><cell cols="2">SE V3D ResNet50</cell><cell cols="3">32 ? 224 ? 224 77.0</cell><cell cols="2">34.14M</cell><cell cols="2">167.9</cell></row><row><cell cols="6">Non-local V3D ResNet50 32 ? 224 ? 224 78.6</cell><cell cols="2">34.62M</cell><cell cols="2">195.4</cell></row><row><cell></cell><cell cols="2">TFC V3D ResNet50</cell><cell cols="3">32 ? 224 ? 224 79.5</cell><cell cols="2">32.68M</cell><cell cols="2">168.0</cell></row><row><cell></cell><cell cols="7">(d) compare against SE Network and Nonlocal Network</cell><cell></cell></row><row><cell></cell><cell cols="6">Table 3. Ablations on CATER snitch localization.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">We compare against 4 types of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">video-level baselines, Temporal Segment Networks, V3D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ResNet50, V3D Depthwise ResNet18 and V3D Depthwise</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ResNet50. We show in Table 3a that TFCNet consistently improve vari-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ous backbones with very few extra parameters and GFLOPs.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">For TSN ResNet50, TFCNet improves top1 accuracy of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">TSN by 39.5% with 3.1M more parameters and 0.6GFLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">more computation, proving the importance of learning long-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">range temporal interaction for spatiotemporal reasoning. For</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>.</head><label></label><figDesc>Comparison with state-of-the-art on CATER snitch localization.</figDesc><table><row><cell></cell><cell>? 10 ? 3</cell><cell>-</cell><cell>47.2 72.1</cell><cell>-</cell><cell>36.42M 934.0</cell></row><row><cell>V3D</cell><cell>ResNet50 1 ? 32 ? 1</cell><cell>-</cell><cell cols="3">77.9 94.0 0.54 31.63M 167.7</cell></row><row><cell>TFC V3D (ours)</cell><cell>ResNet50 1 ? 32 ? 1</cell><cell>-</cell><cell cols="3">79.5 94.0 0.50 32.68M 168.0</cell></row><row><cell>TFC V3D Depthwise (ours)</cell><cell>ResNet50 1 ? 32 ? 1</cell><cell>-</cell><cell cols="3">79.7 95.5 0.47 24.64M 132.0</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cater: A diagnostic dataset for compositional actions &amp; temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJgzt2VKPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>training imagenet in 1 hour. CoRR, abs/1706.02677</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning object permanence from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamsian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kleinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Appearanceand-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal query networks for fine-grained video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gputa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">V4d: 4d convolutional neural networks for videolevel representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJeLopEYDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multihop transformer for spatiotemporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopper</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MaZFq7bJif7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
