<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Demoireing with Learnable Bandpass Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
							<email>zhengbolun1024@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
							<email>shanxin.yuan@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
							<email>gregory.slabaugh@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
							<email>ales.leonardis@huawei.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Demoireing with Learnable Bandpass Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image demoireing is a multi-faceted image restoration task involving both texture and color restoration. In this paper, we propose a novel multiscale bandpass convolutional neural network (MBCNN) to address this problem. As an end-to-end solution, MBCNN respectively solves the two sub-problems. For texture restoration, we propose a learnable bandpass filter (LBF) to learn the frequency prior for moire texture removal. For color restoration, we propose a two-step tone mapping strategy, which first applies a global tone mapping to correct for a global color shift, and then performs local fine tuning of the color per pixel. Through an ablation study, we demonstrate the effectiveness of the different components of MBCNN. Experimental results on two public datasets show that our method outperforms state-ofthe-art methods by a large margin (more than 2dB in terms of PSNR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiscale bandpass CNN</head><p>We propose a Multi-scale Bandpass CNN (MBCNN) to do image demoireing, i.e., to recover the underlying clean image from the moire image. Our model works in three scales and has three different types of blocks, which are moire texture removal block (MTRB), global tone mapping block (GTMB), and local tone mapping block (LTMB). The details of each block are described in Sec. 3.2 and Sec. 3.3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital screens are ubiquitous in modern daily life. We have TV screens at home, laptop/desktop screens in the office, and large LED screens in public spaces. It is becoming common practice to take pictures of these screens to quickly save information. Sometimes taking a photo is the only practical way to save information. Unfortunately, a common side effect is that moire patterns can appear, degrading the image quality of the photo. Moire patterns appear when two repetitive patterns interfere with each other. In the case of taking pictures of screens, the cameras color filter array (CFA) interferes with the screen's subpixel layout.</p><p>Unlike other image restoration problems, including denoising <ref type="bibr" target="#b43">[44]</ref>, demosaicing <ref type="bibr" target="#b8">[9]</ref>, color constancy <ref type="bibr" target="#b0">[1]</ref>, sharpening <ref type="bibr" target="#b27">[28]</ref>, etc., much less attention has been paid to image demoireing, which is to recover the underlying clean image from an image contaminated by moire patterns. Only very recently, a few attempts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> have been made to address image demoireing. However, the problem remains to a large extent an unsolved problem, due to the large vari- ation of moire patterns in terms of frequencies, shapes, colors, etc.</p><p>Recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12]</ref> tried to remove moire patterns of different frequency bands through multi-scale design. DMCNN <ref type="bibr" target="#b30">[31]</ref> proposed to deal with moire patterns with a multi-scale CNN with multi-resolution branches and summed up the outputs from different scales to obtain a final output. MDDM <ref type="bibr" target="#b2">[3]</ref> improved DMCNN by introducing an adaptive instance normalization <ref type="bibr" target="#b16">[17]</ref> based on a dynamic feature encoder. DCNN <ref type="bibr" target="#b23">[24]</ref> proposed a coarse-to-fine structure to remove moire patterns from two scales. The coarse scale result was upsampled and concatenated with the fine scale input for further residual learning. MopNet <ref type="bibr" target="#b11">[12]</ref> used a multi-scale feature aggregation sub-module to address the complex frequency, and two other sub-modules to address edges and pre-defined moire types. Our model also adopts a multi-scale design with three branches for three different scales. Among different scales, our model adopts a gradual upsampling strategy to smoothly increase the resolution.</p><p>Generally, none of the existing methods tried to model the moire patterns explicitly. In our model, we explicitly model the moire patterns by learning the frequency prior of moire patterns and respectively restore the moire image from texture and color. Our contributions are as follows.</p><p>? We introduce a unified framework namely multi-scale bandpass CNN (MBCNN) for image demoireing. The network performs both texture restoration and color restoration within the same model.</p><p>? We propose a learnable bandpass filter (LBF) for efficient moire texture removal. The LBF introduces a learnable bandpass to learn the frequency prior, which could precisely separate moire texture from normal image texture.</p><p>? Our method includes global/local tone mapping for accurate color restoration. The global tone mapping learns the global color shift from moire images to clean images, while the local tone mapping is to make a local fine-grained color restoration.</p><p>? We also propose an advanced Sobel loss (ASL) to learn the structural high-frequency information. With the ASL, we develop a multi-scale supervision to remove moire patterns in three scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image demoireing requires both texture and color restoration, rendering it a complex challenge. In this section, we make a brief introduction of several CNN-based methods in related tasks, where deep learning has made significant impact.</p><p>Image restoration. Dong et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> were the first to propose end-to-end convolutional neural networks for image super-resolution and compression artifact reduction. Subsequent research <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45]</ref> further improved these models by increasing the network depth, introducing skip connections <ref type="bibr" target="#b25">[26]</ref> and residual learning. Much deeper networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47]</ref> were then introduced. DRCN <ref type="bibr" target="#b20">[21]</ref> proposed recursive learning for parameter sharing. Tai et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> introduced a recursive residual learning and proposed a memory block. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> replaced the recursive connection in the memory block by a dense connection <ref type="bibr" target="#b15">[16]</ref>. Moreover, several studies focused on multiscale CNNs inspired by high-level computer vision methods. Mao et al. <ref type="bibr" target="#b5">[6]</ref> proposed a skip connection-based multiscale autoencoder. Cavigelli et al. <ref type="bibr" target="#b1">[2]</ref> introduced a multisupervised network for compression artifact reduction.</p><p>Frequency domain learning. Several studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b48">49]</ref> focus on frequency domain. Liu et al. <ref type="bibr" target="#b24">[25]</ref> introduced the discrete wavelet transform and its inverse to replace conventional upscaling and downscaling operations for image restoration. Guo et al. <ref type="bibr" target="#b10">[11]</ref> introduced convolution-based window sampling, Discrete Cosine Transform (DCT) and inverse DCT (IDCT) to construct a DCT-domain learning network. Zheng et al. <ref type="bibr" target="#b48">[49]</ref> introduced implicit DCT to extend the DCT-domain learning to color image compression artifact reduction.</p><p>Color restoration. Image dehazing and image enhancement are two classic color restoration problems. Eilertsen et al. <ref type="bibr" target="#b6">[7]</ref> proposed a Gamma correction based loss function and trained a U-Net <ref type="bibr" target="#b28">[29]</ref> based CNN for high dynamic range (HDR) image reconstruction. Gharbi et al. <ref type="bibr" target="#b9">[10]</ref> proposed HDRNet to learn local piece-wise linear tone mapping. Inspired by the guided filter <ref type="bibr" target="#b12">[13]</ref>, Wu et al. <ref type="bibr" target="#b35">[36]</ref> proposed an end-to-end trainable guided filter for image enhancement. Ren et al. <ref type="bibr" target="#b26">[27]</ref> grouped a hazy image and several pre-enhanced images together as input, and proposed a symmetric autoencoder to learn a gated fusion for image dehazing. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed a densely connected pyramid CNN for image dehazing. Remarkably, few of these color restoration methods introduce residual connection in their solutions.</p><p>Image demoireing. Recently, several end-to-end image demoireing solutions have been proposed. Sun et al. <ref type="bibr" target="#b30">[31]</ref> first introduced a CNN for image demoireing (DMCNN) and created an ImageNet <ref type="bibr" target="#b29">[30]</ref>-based moire dataset for training and testing. Cheng et al. <ref type="bibr" target="#b2">[3]</ref> improved DMCNN by introducing an adaptive instance normalization <ref type="bibr" target="#b16">[17]</ref> based dynamic feature encoder. He et al. <ref type="bibr" target="#b11">[12]</ref> introduced additional moire attribute labels based on shape, color, and frequency for more precise moire pattern removal. None of the existing methods modeled the moire patterns explicitly. We treat the image demoireing problem as moire texture removal and color restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>A moire image captured by a digital camera can be modeled as:</p><formula xml:id="formula_0">I moire = ?(I clean ) + N moire<label>(1)</label></formula><p>where I clean is the clean image displayed on the screen, N moire is the introduced moire texture, and ? is the color degradation caused by the screen and the camera sensor. I clean can be then expressed as:</p><formula xml:id="formula_1">I clean = ? ?1 (I moire ? N moire )<label>(2)</label></formula><p>where ? ?1 is the inverse function of ?, which is known as the tone mapping function in the image processing field. Modeled in this way, the image demoireing task can be divided into two steps, i.e., moire texture removal and tone mapping. The architecture of MBCNN is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The input image I with the shape of h ? w ? c is first reversibly downsampled into four subimages? with the shape of h 2 ? w 2 ? 4c. With the tensor? as input, the following network consists of three branches, each to recover the moire image in a specific scale. Following Eq. 2, each branch sequentially executes the moire texture removal and tone mapping, and finally outputs an up-scaled image to be fused in the finer scale branch. In branch I and II, after fusing the feature of current branch and the output of the coarser scale branch, additional GTMB and MTRB are stacked to remove the texture and color errors caused by the scale change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Moire texture removal</head><p>Moire patterns exhibit considerable variation in shape, frequency, color, etc. Some examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where the moire patterns have different characteristics. The moire texture can be written as:</p><formula xml:id="formula_2">N moire = i j N si fij<label>(3)</label></formula><p>where N si fij denotes the moire texture component of scale s i and frequency f ij . Following this formulation, we can first estimate the components of moire texture at different scales and frequencies, and then reconstruct the moire texture based on all the estimated components.</p><p>Block-DCT is an effective way for handling frequency related problems. Assuming that the frequency spectrum in block-DCT domain of each N si fij is F S si fij , then Eq. 3 can be rewritten as</p><formula xml:id="formula_3">N moire = i j D ?1 (F S si fij ) = D ?1 ( i j F S si fij )<label>(4)</label></formula><p>where D ?1 denotes the block-IDCT function. Given a color image patch P , we denote the moire texture of each color channel as N c P , c ? {R, G, B}. Then the representation of the moire texture N P is</p><formula xml:id="formula_4">C(N P ) = c?{R,G,B} C(N c P )<label>(5)</label></formula><p>where C denotes a learnable convolution. Based on Eq. 4, Eq. 5 can be rewritten as</p><formula xml:id="formula_5">C(N P ) = c?{R,G,B} C(D ?1 ( i j F S si fij )) c = i C(D ?1 ( c?{R,G,B} j F S si fij c )) = i C(D ?1 ( c?{R,G,B} F S si c ))<label>(6)</label></formula><p>where F S si c is the combined frequency spectrum of channel c with the scale of s i . Here, we define the c?{R,G,B} F S si c as the implicit frequency spectrum (IFS) denoted as ? si . Now, we can have</p><formula xml:id="formula_6">C(N P ) = i C(D ?1 (? si ))<label>(7)</label></formula><p>Learnable Bandpass Filter. Inspired by the implicit DCT <ref type="bibr" target="#b48">[49]</ref>, we can directly estimate ? si with a deep CNN block. Since the transforms presented in Eq. 7 are all linear, they can be modeled by a simple convolution layer. As the frequency spectrum of moire texture is always regular, we can use a bandpass filter to amplify certain frequencies and diminish others. However, it's difficult to get the frequency spectrum prior modeling the moire texture, because there would be several frequencies in different scales and they can also affect each other. To solve this problem, we propose a learnable bandpass filter (LBF) to learn the prior from moire images. LBF introduces a learnable weights for each frequency, which can be expressed as</p><formula xml:id="formula_7">C(N P ) = i C(D ?1 (? si ? ? si ))<label>(8)</label></formula><p>where ? si denotes the learnable weights of DCT domain frequencies for the scale s i . Assuming the size of block-IDCT is p ? p, then the corresponding DCT domain frequency spectrum totally has p 2 frequencies, so the size of ? si is p 2 . All parameters of ? si are initialized to be 1 and constrained to be non-negative, the passbands are learned from the image data during training. D ?1 can be implemented by a predefined 1 ? 1 convolution layer, whose weights are fixed as the IDCT matrix.</p><p>CNN Structure. Following Eq. 8, we can respectively remove moire texture from different scales. For each specific scale, we propose a moire texture removal block (MTRB), see <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Assuming the input of the MTRB is x M T RB in , a dense block is first used for feature extraction, which is denoted as F deep . Then a 3 ? 3 convolution layer estimates the IFS ? from F deep . The dense block has K densely connected <ref type="bibr" target="#b15">[16]</ref> 3 ? 3 n D -channel dilated convolution <ref type="bibr" target="#b39">[40]</ref> with ReLU activation (Conv ReLU ) layers. We adopt dilated convolution rather than normal convolution to enlarge the receptive field of the dense block to produce F deep , so that the p 2 sized ? can be easily estimated from the F deep . After estimating ?, the learnable weight ? and the block-IDCT layer D ?1 , a convolution layer C M 2 is added as indicated in Eq. 8.</p><p>Considering that the D ?1 might lead to large local output and produce excessive gradient, we stacked a Feature Scale Layer (FSL) to linearly constrain the output of C M 2 . Finally, we introduce the residual connection <ref type="bibr" target="#b13">[14]</ref> to remove the moire texture in convolution domain. Thus, the final output of MTRB x M T RB out can be obtained by</p><formula xml:id="formula_8">x M T RB out = x M T RB in + S(C M 2 (D ?1 (? ? ?)))<label>(9)</label></formula><p>where S denotes the FSL.</p><p>Directly multiplying ? and ? will consume large amount of calculations. Instead, we reshape ? to the size of 1 ? 1 ? p ? p, and multiply it with the convolution kernel of D ?1 layer, then the ? is directly sent to D ?1 layer. In this way, the product ? ? ? can be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tone mapping</head><p>The RGB color space is an extremely large space containing 256 3 colors, making it difficult to do point-wise tone mapping. Observing that there are color shifts between the moire and clean images, we propose a two-step tone mapping strategy with two types of tone mapping blocks: Global Tone Mapping Block (GTMB) and Local Tone Mapping Block (LTMB). Global tone mapping block. The GTMB is proposed to learn the global color shift, see <ref type="figure" target="#fig_3">Figure 4</ref> for the detailed structure. Given the input x GT M B in , we first extract a global feature F through a 3?3 Conv ReLU layer with the stride of 2 and a global average pooling (GAP) layer. Then, to extract a deep global feature ?, we stack two fully connected (FC) layers with ReLU activation (FR 1 , FR 2 ) and a FC layer without ReLU activation (FC). Besides, we use an 1? 1 Conv ReLU layer extracts the local feature F local from</p><formula xml:id="formula_9">Layer CR G1 CR G2 CR G3 FR 1 FR 2 FC Stride 2 ? 2 1 ? 1 1 ? 1 - - - Kernel 3 ? 3 1 ? 1 1 ? 1 - - - Output Ch. n G ? 2 n G ? 2 n G n G ? 8 n G ? 4 n G ? 2</formula><formula xml:id="formula_10">x GT M B in . The output of GTMB x GT M B out</formula><p>can be obtained as</p><formula xml:id="formula_11">x GT M B out = CR G3 (? ? F local )<label>(10)</label></formula><p>Assuming the CR G3 outputs a n G -channel tensor, <ref type="table" target="#tab_0">Table 1</ref> lists the attributions of all learnable layers in GTMB. GTMB vs. Channel Attention. The attention mechanism has proven to be effective in many tasks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, and several channel attention blocks have been proposed <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b14">15]</ref>. Our GTMB can be view as a channel attention block. However, GTMB is different from existing channel attention blocks in several aspects. First, existing channel attention blocks are always activated by a Sigmoid unit, while there are no such constraints for the ? in GTMB. Second, channel attention is directly applied on the input of the existing channel attention blocks, while the ? in GTMB is applied on the local feature F local . Finally, existing channel attention blocks are aimed at making an adaptive channel-wise feature re-calibration; the goal of GTMB is to make a global color shift and avoid the irregular and inhomogeneous local color artifacts (more analysis are described in Sec. 4.3.1).</p><p>Local tone mapping block. The LTMB is developed to fit a local fine-grained tone mapping function. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the structure of LTMB is similar to MTRB. LTMB first takes a similar dense block in MTRB to extract the deep feature Then, the output of LTMB is obtained by</p><formula xml:id="formula_12">x LT M B out = CR L (F LT M B deep )<label>(11)</label></formula><p>where CR L is a 1?1 convolution, and x LT M B out has the same shape with x LT M B in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>In this paper, we use the L1 loss as the base loss function, as it has been proven <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48</ref>] that L1 loss is more effective than L2 loss for image restoration tasks. However, the L1 loss itself is not enough as it is a point-wise loss that cannot provide structural information, while moire patterns are structural artifact. We propose an Advanced Sobel Loss (ASL) to solve this problem. The proposed ASL can be expressed as</p><formula xml:id="formula_13">ASL(?, Z) = 1 N Sobel * (Z) ? Sobel * (?)<label>(12)</label></formula><p>where Z denotes the groundtruth,? denotes the output of CNN, and Sobel * denotes the advanced Sobel filtering. <ref type="figure" target="#fig_6">Figure 6</ref> illustrates the details of ASL. Compared to classic Sobel filters ( <ref type="figure" target="#fig_6">Figure 6(a)</ref>), the advanced Sobel filters provide two additional filters of 45 ? directions ( <ref type="figure" target="#fig_6">Figure 6(b)</ref>), which could provide richer structure information. We combine ASL and L1 loss as the final loss function, which can be expressed as,</p><formula xml:id="formula_14">Loss(?, Z) = L1(?, Z) + ? ? ASL(?, Z)<label>(13)</label></formula><p>where L1 denotes the L1 loss, ASL denotes the ASL, and ? is a hyper-parameter to balance the L1 loss and ASL. When training MBCNN, we adopt the multi-supervising strategy that supervising the outputs from all branches, which can be expressed as,</p><formula xml:id="formula_15">loss = Loss(? s1 , Z s1 ) + Loss(? s2 , Z s2 ) + Loss(? s3 , Z s3 )<label>(14)</label></formula><p>where s 1 , s 2 , and s 3 indicate branch 1, 2, and 3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We have conducted extensive ablation studies and outperformed state-of-the-art by large margins on two public datasets: LCDMoire <ref type="bibr" target="#b40">[41]</ref> and TIP2018 <ref type="bibr" target="#b30">[31]</ref> The LCDMoie dataset consists of 10,200 synthetically generated image pairs with 10,000 training images, 100 validation images and 100 testing images. The TIP2018 dataset consists of real photographs constructed by photographing images of the ImageNet <ref type="bibr" target="#b29">[30]</ref> dataset displayed on computer screens with various combinations of different camera and screen hardware. It has 150,000 real clean and moire image pairs, split into 135,000 training images and 15,000 testing images. Both LCDMoire and TIP2018 datasets are used to do comparison with state-of-the-art methods. LCDMoire dataset is also used for ablation study. The ablation study is conducted on the validation set, as the test dataset's ground truth is not available. Please note: the validation dataset is completely independent and not used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>For the MBCNN model, we adopt the following settings, with c = 3, n G = 128, n D = 64, K = 5. Adam <ref type="bibr" target="#b21">[22]</ref> is used as our training optimizer. The learning rate is initialized to be 10 ?4 . The validation was conducted after every training epoch. If the decrease in the validation loss was lower than 0.001 dB for four consecutive epochs, the learning rate was halved. When the learning rate became lower than 10 ?6 , the training procedure was completed. For LCDMoire dataset, we 128 ? 128 patches were randomly cropped from the images, with the batch size set to 16. When the 128 ? 128 patch trained model converged, we re-grouped the training data into 256 ? 256 patches for fine-tuning the model. This time, the learning rate was set to 10 ?5 , the batch size was set to 4. Training a MBCNN roughly takes 40 hours with a NVidia RTX2080Ti GPU. For TIP2018 dataset, we follow <ref type="bibr" target="#b30">[31]</ref> and set the patch size as 256 ? 256 through out the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To verify the effectiveness of each component in our model, we conduct extensive ablation studies, including evaluation of MTRB vs. GTMB and LTMB, learnable bandpass filter, and loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">MTRB vs. GTMB and LTMB</head><p>As described in previous sections, the MTRB is designed for removing moire texture, GTMB and LTMB are designed for color restoration. We investigate the effect of the MTRB using a trained MBCNN, and visualize the experimental results in <ref type="figure">Figure.</ref> 7. Due to the residual connection in MTRB, we can separate the effect of MTRB from the two tone mapping blocks by forcing the learned scale in the feature scaling layer to be zero. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, without MTRBs, the degraded color can still be well restored, and some of very high frequency moire texture can also be well removed. However many high frequency image details are lost, and the low-frequency moire texture largely remains. The result is mainly caused by two reasons. First, because 3 ? 3 convolutions are used in GTMB and LTMB, the CNN has certain denoising and local smoothing capabilities. Second, although the proposed tone mapping blocks do have a great ability to restore color, the major contribution to moire texture removal is made by MTRBs. This experiment demonstrates that the MTRBs have strong capability to do moire texture removing, while the GTMBs and LTMBs are good at restoring colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Learnable bandpass filter</head><p>In this section, we investigate the contribution of LBF and explain the reasons why we choose the relevant settings.  passband (LP). We applied the settings described in Section 4.1, and respectively removed the DDT and LP from the MTRBs to conduct the investigation. We removed the entire DDT by replacing it by a 1 ? 1 convolution layer to keep the output shape unchanged. In this case, the MTRB degenerates to a residual dense block (RDB). We removed the LP by keeping the entire DDT, but forcing all parameters in the passbands to be 1, which will not be updated during training phase. We denote the networks constructed without LP or DDT as MBCNN-nLP and MBCNN-nDDT, respectively. We tested the performance of these three models on the validation set of LCDMoire. As shown in <ref type="table">Table 2</ref>, MBCNN-nLP introduces the DDT which could provide a structural learning path and explicitly ensure the internal receptive field (block-IDCT size), and finally leads to a slight improvement of 0.18dB from MBCNN-nDDT. MBCNN introduces the learnable bandpass to learn the frequency prior of the moire texture and leads a significant improvement of 0.95 dB from MBCNN-nLP. Some demoireing results produced by these three models are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The LBFs enable the MBCNN to better sense the moire texture and recover more accurate details from moire images.  Block-IDCT size p. p is a very important parameter for DDT. With a larger p, the LBF can learn a more accurate and more complete frequency prior. We denoted the MBCNN constructed with the block-IDCT size of p as MBCNN-p. We respectively validated the performance of MBCNNs constructed with p = 6, 8, 10, 12. p = 8 is found to be the best for moire texture removal. As shown in Table 3, larger p doesn't always lead to a better result. There are two reasons for this observation. First, enlarging p increases the complexity and difficulty of the frequency prior learning. Second, the receptive field provided by the front dense block cannot support a p that is too large. We visualize the learned passbands in the LBFs from an MBCNN-8 model in <ref type="figure" target="#fig_9">Figure 9</ref>. The LBFs perform band suppression mainly at the beginning of the branches. The LBFs at the end of the branches are primarily avoiding over-smoothing caused by concatenating the output from the upper scale. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Study of the loss function</head><p>In this subsection, we investigate the contribution from the loss functions. To demonstrate the effectiveness of the proposed ASL, we compare it with several related and wellknown loss functions, including Sobel loss, Laplace loss, SSIM loss <ref type="bibr" target="#b47">[48]</ref> and perceptual loss basing on pre-trained Vgg16 network <ref type="bibr" target="#b17">[18]</ref>. Generally, all loss function are loaded through the multi-supervising strategy stated in Eq. 14 and finally measured by an MAE function. To balance the outputs of these losses and L1 loss, we assigned different ? (in Eq, 13) to different losses. As shown in <ref type="table">Table 4</ref>, the structural high frequency loss provided by the Sobel loss leads to a significant improvement of 1.81dB, and the additional two directional filters from ASL further improve the performance of 0.40dB. Though Laplace loss is also a high frequency descriptor, because it has a much higher weight on the center pixel than the neighbouring pixels, it behaves similar to the L1 loss. Besides, the SSIM loss and perceptual loss also can improve the performance. The SSIM loss behaves similar to Laplace loss, while the perceptual loss is the second best loss function which is only 0.21 dB inferior to ASL. Generally, our ASL is an simple and effective loss function for image demoireing task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with prior work</head><p>In this subsection, we compare the proposed method with several most related prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison on LCDMoire dataset</head><p>We first compare with the participating methods in the AIM19 image demoireing challenge <ref type="bibr" target="#b41">[42]</ref>. The results on the validation set (again, independent and not used in training) is shown in <ref type="table" target="#tab_5">Table 5</ref>. Since the ground-truth of the LCDMoire testing set is not released, we provide the performance on the LCDMoire validation set. We also compared with several methods that did not participate in the challenge, including CAS-CNN <ref type="bibr" target="#b1">[2]</ref>, MWCNN <ref type="bibr" target="#b24">[25]</ref>, DM-CNN <ref type="bibr" target="#b30">[31]</ref>. The result and average running time per image are shown in <ref type="table" target="#tab_6">Table 6</ref>. Because we have demonstrated the superiority of the ASL, we trained the methods (CAS-CNN, MWCNN, DMCNN) with L1 loss plus ASL. Limited by the global residual connection, MWCNN fails to solve the image demoireing problem, while CAS-CNN achieves a very close performance to DMCNN. The proposed MBCNN method clearly outperforms these other methods, with a significant performance gain of +7.88dB/+0.075 PSNR than CAS-CNN. From the visualized results shown in <ref type="figure" target="#fig_0">Figure 10</ref>, our MBCNN accurately removes moire texture and restores most image details.</p><p>However, since MBCNN consumes considerable parameters compared to several compared methods, we propose a light version of MBCNN (MBCNN-light) by setting n G = <ref type="figure" target="#fig_0">Figure 11</ref>. Qualitative comparison on TIP2018 dataset.   64, n D = 32, while keeping other settings unchanged. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the fewer parameters leads to a performance reduction of ?1.46 dB/?0.028 from MBCNN. Nevertheless, MBCNN-light still outperforms other participating methods even in this reduced form of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Recently, several studies have reported that the geometric self-ensemble could reasonably enhance the performance in the final testing phase. We adopted this strategy during testing time by rotating the input image by 90 ? , 180 ? and 270 ? to generate three augmented input images, and calculating the mean image of the original output and three augmented outputs (rotated back) as the final output. We denoted this self-ensemble MBCNN as MBCNN + . Perhaps surprisingly, this strategy leads to a dramatic reduction in performance. We speculate that because the moire texture is a strongly direction-aware artifact, changing the direction would mislead the network to make an inaccurate restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison on TIP2018 dataset</head><p>Since some related work is evaluated on the TIP2018 dataset, we further evaluated our MBCNN on the TIP2018 dataset to compare with several related methods including DnCNN <ref type="bibr" target="#b43">[44]</ref>, VDSR <ref type="bibr" target="#b19">[20]</ref>, EDSR <ref type="bibr" target="#b22">[23]</ref>, UNet <ref type="bibr" target="#b28">[29]</ref>, DM-CNN <ref type="bibr" target="#b30">[31]</ref>, MopNet <ref type="bibr" target="#b11">[12]</ref>. As shown in <ref type="table" target="#tab_7">Table 7</ref>, our pro-posed MBCNN beats the second best method by +2.28 dB, in terms of PSNR, and achieved the second best SSIM result which is only 0.002 lower than the best. Moreover, the visualized results shown in <ref type="figure" target="#fig_0">Figure 11</ref> also demonstrates the proposed method outperformed other compared methods.</p><p>More qualitative examples are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a multiscale bandpass CNN (MBCNN) for image demoireing, and significantly outperform state-of-the-art methods by more than 2dB in terms of PSNR. A learnable bandpass filter (LBF) is proposed to learn the frequency prior. Our model has two steps: moire texture removal and tone mapping. A LBF-based residual CNN block is used for moire texture removal, and another two CNN blocks for global and local tone mappings. An ablation study was conducted to show the importance of the components in the network. We have also clarified the the effect of the block-IDCT size in the LBF, and demonstrated that the block-IDCT size of 8 is the best for the image demoireing task. Experiments on two public datasets show that our model outperformed state-of-the-art methods by large margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Moire texture of different scales, frequencies, and colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of our multi-scale bandpass CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The structure of moire texture removal block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The structure of global tone mapping block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The structure of local tone mapping block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>LT M B deep from the input of LTMB x LT M B in .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Details of advanced Sobel loss. (a) Classic Sobel filters. (b) Two additional filters for advanced Sobel filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Demoireing results produced by MBCNN with and without MTRB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Demoireing results produced by MBCNN-nDDT, MBCNN-nLP and MBCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>The learned frequency domain priors from the LBFs in different MTRBs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Demoireing results on the validation set of LCDMoire produced by proposed methods and other prior mehods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table /><note>Attributions of learnable layers in GTMB.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of MBCNNs with different p values.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison of MBCNN models and the top 7 participating methods in the AIM19 demoireing challenge.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">IPCV IITM</cell><cell>PCALab</cell><cell>IAIR</cell><cell cols="2">XMU-VIPLab KU-CVIP MoePhoto</cell><cell>Islab-zju</cell><cell>MBCNN</cell></row><row><cell cols="7">PSNR/SSIM 32.23/0.96 32,39.0.97 35.27/0.97</cell><cell>39.21/0.99</cell><cell>40.17/0.98 41.91/0.99 42.90/0.99 44.04/0.9948</cell></row><row><cell>Model</cell><cell cols="7">CAS-CNN MWCNN DMCNN MBCNN MBCNN-light MBCNN +</cell></row><row><cell>PSNR</cell><cell>36.16</cell><cell>28.93</cell><cell>35.48</cell><cell>44.04</cell><cell>42.81</cell><cell>33.65</cell></row><row><cell>SSIM</cell><cell>0.9873</cell><cell>0.9698</cell><cell>0.9785</cell><cell>0.9948</cell><cell>0.9940</cell><cell>0.9859</cell></row><row><cell>Time(s)</cell><cell>0.14</cell><cell>0.14</cell><cell>0.10</cell><cell>0.25</cell><cell>0.12</cell><cell>1.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance comparison of MBCNN models and other prior work on the validation set of LCDMoire.</figDesc><table><row><cell></cell><cell cols="6">DnCNN VDSR EDSR UNet DMCNN MopNet MBCNN</cell></row><row><cell>PSNR</cell><cell>24.54</cell><cell>24.68</cell><cell>26.82 26.49</cell><cell>26.77</cell><cell>27.75</cell><cell>30.03</cell></row><row><cell>SSIM</cell><cell>0.834</cell><cell>0.837</cell><cell>0.853 0.864</cell><cell>0.871</cell><cell>0.895</cell><cell>0.893</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison of MBCNN models and other related works on TIP2018 dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast fourier color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-scale dynamic feature encoding network for image demoir?ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep representations using convolutional auto-encoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian-Feng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Zhu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Liao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hdr image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Moir? pattern removal with multi-scale feature enhancing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siggraph Asia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep bilateral learning for realtime image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mop moire patterns using mopnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Bin He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeplyrecursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Demoireing of camera-captured screen images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated fusion network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Raisr: rapid and accurate image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Moire photo restoration using multiresolution convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compression artifacts removal using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of WSCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image superresolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mem-Net: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep multi-view enhancement hashing for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3d room layout estimation from a single rgb image. TMM, 2020</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stat: spatial-temporal attention mechanism for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Aim 2019 challenge on image demoreing: dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ales Leonardis, and etc. Aim 2019 challenge on image demoreing: methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanxin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TCI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Implicit dual-domain convolutional network for robust color image compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
