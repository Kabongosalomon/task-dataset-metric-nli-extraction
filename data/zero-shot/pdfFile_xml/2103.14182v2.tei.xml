<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Vision and Image Understanding Self-Attentive 3D Human Pose and Shape Estimation from Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">eBay Inc</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Amazon, Oakland</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of California at Merced</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Vision and Image Understanding Self-Attentive 3D Human Pose and Shape Estimation from Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the task of estimating 3D human pose and shape from videos. While existing frame-based approaches have made significant progress, these methods are independently applied to each image, thereby often leading to inconsistent predictions. In this work, we present a video-based learning algorithm for 3D human pose and shape estimation. The key insights of our method are two-fold. First, to address the inconsistent temporal prediction issue, we exploit temporal information in videos and propose a self-attention module that jointly considers short-range and long-range dependencies across frames, resulting in temporally coherent estimations. Second, we model human motion with a forecasting module that allows the transition between adjacent frames to be smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M datasets. Extensive experimental results show that our algorithm performs favorably against the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose and shape estimation <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b4">Bogo et al., 2016)</ref> is an active research topic in computer vision and computer graphics that finds numerous applications <ref type="bibr" target="#b36">Liu et al., 2019)</ref>. The inherent under-constrained nature where multiple 3D meshes can explain the same 2D projection makes this problem very challenging. While frame-based methods <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b4">Bogo et al., 2016)</ref> and video-based approaches <ref type="bibr" target="#b27">(Kocabas et al., 2020;</ref><ref type="bibr" target="#b33">Lee et al., 2018b;</ref><ref type="bibr" target="#b49">Rayat Imtiaz Hossain and Little, 2018;</ref><ref type="bibr" target="#b24">Kanazawa et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2019b)</ref> have been developed to recover human pose in the literature, numerous issues remain to be addressed. First, existing approaches employ recurrent neural networks (RNNs) to model temporal information for consistent predictions. However, it is difficult to train RNNs to capture long-range dependencies <ref type="bibr" target="#b52">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b45">Pascanu et al., 2013)</ref>. On the other hand, one recent approach employing RNNs does not consistently render smooth predictions across frames <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref>. Second, as most real-world datasets do not contain <ref type="figure">Fig. 1</ref>: 3D human pose and shape estimation. The results are generated by our method without prior information of camera or manual initialization. The embedded video can be viewed using Adobe Acrobat. ground-truth camera parameter annotations, existing methods typically reproject the predicted 3D joints onto the 2D space using the estimated camera parameters, followed by a loss enforced between the reprojected 2D joints and the corresponding ground-truth 2D joints. Nevertheless, such regularization terms are still insufficient to account for complex scenes. Third, existing methods <ref type="bibr" target="#b27">(Kocabas et al., 2020;</ref><ref type="bibr" target="#b24">Kanazawa et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2019b;</ref><ref type="bibr" target="#b23">Kanazawa et al., 2018)</ref> do not perform well for humans under heavy occlusion or out-of-view, as there is no explicit constraint enforced on the invisible regions.</p><p>In this paper, we propose the Self-attentive Pose and Shape Network (SPS-Net) for 3D human pose and shape estimation from videos. Our key insights are two-fold. First, motivated by the attention models in neural machine translation <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref> and image generation <ref type="bibr" target="#b60">(Zhang et al., 2019a</ref>) tasks, we develop a self-attention module to exploit temporal cues in videos for coherent predictions. For each input frame, our selfattention module derives a visual representation by observing past and future frames and predicting the associated attention weights. Second, motivated by the autoregressive models in human motion prediction <ref type="bibr" target="#b24">(Kanazawa et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2019b)</ref>, we develop a forecasting module that leverages visual cues from human motion to encourage our model to generate temporally smooth predictions. By jointly considering both features, our SPS-Net is able to estimate accurate and temporally coherent human pose and shape (see <ref type="figure">Figure 1</ref>).</p><p>In addition to coherent and smooth predictions, we also address the issues with ground-truth camera parameters and heavy occlusion for robust 3D human pose and shape estimation from videos. To account for images without ground-truth camera parameter annotations, we exploit the property that the camera parameters for the overlapped frames of two segments from the same video should be the same. We enforce this constraint with a camera parameter consistency loss. Furthermore, we address the occlusion and out-of-view issues by masking out some regions of the video frames. Our core idea is to leverage the predictions of the original video frames to supervise those of the synthesized occluded or partially visible data, making our model more robust to the occlusion and out-of-view issues. We demonstrate the effectiveness of the proposed SPS-Net on three standard benchmarks, including the 3DPW <ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>, and Hu-man3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> datasets.</p><p>Our main contributions can be summarized as follows:</p><p>? We present a video-based learning algorithm for 3D human pose and shape estimation. ? We propose a camera parameter consistency loss that provides additional supervisory signals for model training, resulting in more accurate camera parameter predictions. ? Our model learns to predict plausible estimations when occlusion or out-of-view occurs in a self-supervised fashion. ? Extensive evaluations on three challenging benchmarks demonstrate that our method achieves the state-of-the-art performance against existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D human pose and shape estimation. Existing methods for 3D human pose and shape estimation can be broadly categorized as frame-based and video-based. Frame-based methods typically use an off-the-shelf keypoint detector (e.g., Deep-Cut <ref type="bibr" target="#b48">(Pishchulin et al., 2016)</ref>) to fit the SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> body model <ref type="bibr" target="#b4">(Bogo et al., 2016)</ref>, leverage silhouettes and keypoints for model fitting <ref type="bibr" target="#b31">(Lassner et al., 2017)</ref>, or directly regress the parameters for the SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> body model from pixels using neural networks <ref type="bibr" target="#b29">(Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b23">Kanazawa et al., 2018;</ref><ref type="bibr" target="#b30">Kolotouros et al., 2019b)</ref>. While these frame-based approaches are able to recover 3D poses from a single image, independently applying these algorithms to each video frame often leads to temporally inconsistent predictions. Video-based methods, on the other hand, usually adopt RNN-based models to generate temporally coherent predictions. These approaches either focus on estimating the human body of the current frame <ref type="bibr" target="#b1">(Arnab et al., 2019;</ref><ref type="bibr" target="#b51">Sun et al., 2019;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref> or predicting the past and future motions <ref type="bibr" target="#b24">(Kanazawa et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2019b)</ref>. Our algorithm differs from these video-based methods in three aspects. First, in contrast to adopting RNN-based models, we develop a self-attention module to aggregate temporal information and a forecasting module to model human motion for predicting temporally coherent estimations. Second, we enforce a consistency loss on the prediction of camera parameters to regularize model learning. Third, we address the occlusion and out-of-view issues with a self-supervised learning scheme to generate plausible human pose and shape predictions. Attention models. Attention models have been shown effective in neural machine translation <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref> and image generation problems <ref type="bibr" target="#b60">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b44">Parmar et al., 2018)</ref>. For machine translation, employing self-attention models <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref> helps capture short-range and longrange correlations between tokens in the sentence for improving the translation quality. In image generation, the Image Transformer <ref type="bibr" target="#b44">(Parmar et al., 2018)</ref> and SAGAN <ref type="bibr" target="#b60">(Zhang et al., 2019a)</ref> show that leveraging self-attention mechanisms facilitates the models to generate realistic images. In 3D human pose and shape estimation, the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method adopts a self-attention scheme in the discriminator for feature aggregation, allowing the discriminator to better distinguish the motions of attended video frames between the real sequences and generated ones.</p><p>We adopt self-attention modules in both the SPS-Net and discriminator. Our method differs from the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> in that our self-attention module aims to derive a representation for each frame that contains temporal information by jointly considering short-range and long-range dependencies across video frames, whereas the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method aims to derive a single representation for the entire pose sequence. Future human pose predictions. Predicting future poses from videos has been studied by a few approaches in the literature. Existing algorithms estimate 2D poses from pixels <ref type="bibr" target="#b13">(Denton and Birodkar, 2017;</ref><ref type="bibr" target="#b15">Finn et al., 2016)</ref>, optical flow <ref type="bibr" target="#b54">(Walker et al., 2016)</ref>, or 2D poses <ref type="bibr" target="#b55">(Walker et al., 2017)</ref>, or predict 3D outputs based on 3D inputs <ref type="bibr" target="#b6">(Butepage et al., 2017;</ref><ref type="bibr" target="#b16">Fragkiadaki et al., 2015;</ref><ref type="bibr" target="#b22">Jain et al., 2016;</ref><ref type="bibr" target="#b35">Li et al., 2018;</ref><ref type="bibr" target="#b53">Villegas et al., 2018)</ref>. Other approaches learn 3D pose prediction from 2D inputs <ref type="bibr" target="#b61">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b24">Kanazawa et al., 2019)</ref>.</p><p>Similar to the HMMR <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> and PHD <ref type="bibr" target="#b61">(Zhang et al., 2019b</ref>) methods, we leverage visual cues from human motion to predict temporally smooth predictions. Our method differs from them in that our self-attention module helps capture short-range and long-range dependencies across video frames in the input video, while the 1D convolution in the temporal encoder and autoregressive module of these methods does not have such ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Overview of the Self-attentive Pose and Shape Network (SPS-Net). Our SPS-Net is composed of four main components: a feature encoder E (highlighted in blue), a self-attention module A (highlighted in green), a forecasting module F (highlighted in orange), and three parameter regressors R shape , R pose , and R camera (highlighted in yellow). The feature encoder extracts features from input video frames. The encoded features are then passed to the self-attention module to produce latent representations that contain temporal information of past and future frames and to the forecasting module to predict the features of the next time step. The latent representations and the predicted features of the same time step are forwarded to the feature fusion module for feature aggregation. Finally, the fused representations are passed to three parameter regressors to predict the corresponding shape, pose, and camera parameters, respectively.</p><p>Consistency constraints for visual learning. Exploiting consistency constraints to regularize model learning has been shown effective in numerous applications, including semantic matching <ref type="bibr" target="#b64">(Zhou et al., 2015)</ref>, optical flow estimation <ref type="bibr" target="#b42">(Meister et al., 2018)</ref>, depth prediction <ref type="bibr" target="#b18">(Gordon et al., 2019)</ref>, and imageto-image translation <ref type="bibr" target="#b65">(Zhu et al., 2017;</ref><ref type="bibr" target="#b32">Lee et al., 2018a;</ref><ref type="bibr" target="#b20">Huang et al., 2018)</ref>. Other methods exploit consistency constraints across multiple network outputs, including depth and optical flow estimation <ref type="bibr" target="#b66">(Zou et al., 2018)</ref>, joint semantic matching and object co-segmentation <ref type="bibr" target="#b10">(Chen et al., 2020)</ref>, ego-motion <ref type="bibr" target="#b63">(Zhou et al., 2017)</ref>, and domain adaptation <ref type="bibr" target="#b9">(Chen et al., 2019b)</ref>. In our work, we show that enforcing consistency constraints on the prediction of camera parameters for the overlapped video frames of two segments from the same video results in performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>In this section, we first provide an overview of our approach. Next, we describe the details of the self-attention and forecasting modules, followed by formulating the proposed camera parameter consistency loss. We then motivate the self-supervised learning scheme for addressing the occlusion and out-of-view issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithmic overview</head><p>Given an input video V = {I i } N i=1 of length N containing a single person, our goal is to learn a model that recovers the 3D human body of each frame. We present the Self-attentive Pose and Shape Network (SPS-Net), comprising four components: 1) feature encoder E, 2) self-attention module A, 3) forecasting module F, and 4) three parameter regressors R shape , R pose , and R camera .</p><p>As shown in <ref type="figure">Figure 2</ref>, we first apply the encoder E to each frame I i ? V to extract the feature f i = E(I i ) ? R d , where d denotes the number of channels of the feature f i . Next, the selfattention module A takes all the encoded features { f i } N i=1 as input and outputs the corresponding latent representations</p><formula xml:id="formula_0">{h i } N i=1 ,</formula><p>where h i ? R d denotes the latent representation for I i , containing temporal information of past and future frames. The forecasting module F takes each encoded feature f i as input and forecasts the feature of the next time step</p><formula xml:id="formula_1">f i+1 = F( f i ) ? R d . The latent representations {h i } N i=1 and the predicted features { f i+1 } N i=1</formula><p>of the same time step (e.g., h i and f i ) are passed to a feature fusion module to derive the fused representations</p><formula xml:id="formula_2">{F i } N i=1 , where F i ? R d</formula><p>contains both global temporal and local motion information. The pose parameter regressor R pose takes each fused representation F i as input and renders the pose parameters ? i for each frame I i , where ? i = R pose (F i ) ? R 72 . The shape parameter regressor R shape , on the other hand, takes all the fused representations {F i } N i=1 as input and regresses the shape parameters ? ? R 10 of the input video V.</p><p>3D human body representation. Similar to the state-of-theart methods <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, we adopt the SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> body model to describe the human body using a 3D mesh representation. The SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> model is described by the pose ? ? R 72 and shape ? ? R 10 parameters. The pose parameters ? contain the global body rotation and the relative 3D rotation of 23 joints in axis-angle format. The shape parameters ? are parameterized by the first 10 linear coefficients of a PCA shape space. We use a gender-neutral shape model as in previous work <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>. The differentiable SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> body model takes the pose ? and shape ? parameters as input and outputs a triangular mesh M(?, ?) ? R 6890?3 consisting of 6, 890 mesh vertices by shaping a template body mesh based on forward kinematics. The 3D keypoints X ? R k?3 of k body joints can be obtained by applying a pre-trained linear regressor W to the 3D mesh M(?, ?), and is defined as</p><formula xml:id="formula_3">X = W M(?, ?).</formula><p>Camera model. Similar to existing approaches <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, we use a weak-perspective camera model in this work. By estimat- <ref type="figure">Fig. 3</ref>: Overview of the self-attention module A. Our self-attention module A is composed of an attention network Q and an attention network K. Given a sequence of input features, the self-attention module first predicts the attention vector q and the attention vector k for each frame. Next, we compute the inner product between each attention vector q and all attention vectors {k j } N j=1 , followed by normalizing the weights using a softmax function. The input features are first fused using the associated weights and then summed with the skipped input features to derive the final latent representations as output.</p><p>ing the camera parameters {s, R, t} using the regressor R camera , where s ? R denotes the scale, R ? R 3?3 is the global rotation in axis-angle format, and t ? R 2 denotes the translation, the 2D projection x ? R k?2 of the 3D keypoints X can be obtained by x = s?(RX(?, ?)) + t, where ? is an orthographic projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-attention module</head><p>Given a sequence of features { f i } N i=1 encoded by the encoder E, our goal is to leverage temporal cues in the input video to provide more information that helps regularize the estimation of human pose and shape. Existing methods exploit temporal information by resorting to an RNN-based model, e.g., GRU <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> or LSTM <ref type="bibr" target="#b33">(Lee et al., 2018b;</ref><ref type="bibr" target="#b49">Rayat Imtiaz Hossain and Little, 2018)</ref>. However, training RNN-based models is difficult to capture long-range dependencies <ref type="bibr" target="#b52">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b45">Pascanu et al., 2013)</ref>.</p><p>Motivated by the attention models <ref type="bibr" target="#b52">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b60">Zhang et al., 2019a;</ref><ref type="bibr" target="#b44">Parmar et al., 2018)</ref> which have been shown effective to jointly capture short-range and long-range dependencies while being more parallelizable to train <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref>, we develop a self-attention module to learn latent representations h that jointly observe past and future video frames for producing temporally consistent pose and shape predictions. In this work, we aim to exploit the idea that the occluded frames can benefit from the information of the nonoccluded frames, while the non-occluded frames do not have to depend on the information from the occluded frames (i.e., anti-symmetric attention of humans that can be occluded and un-occluded between frames in either direction). To achieve this, we have an attention network Q and an attention network K in our self-attention module A.</p><p>As shown in <ref type="figure">Figure 3</ref>, for each feature f i , we first apply the attention network Q and the attention network K to encode an attention vector q i = Q( f i ) ? R d and an attention vector k i = K( f i ) ? R d , respectively. To consider the dependency between two input frames I i and I j , we compute the inner product between the attention vector q i of frame I i and the attention vector k j of frame I j , i.e., w j i = q i ? k j ? R. To derive the latent representation h i for frame I i , we first apply a softmax layer to all the weights {w l i } N l=1 computed between the attention vector q i of frame I i and all attention vectors {k l } N l=1 for normalization to derive the attention weights. The attention weights {a l i } N l=1 are computed by</p><formula xml:id="formula_4">a l i = exp(w l i ) N n=1 exp(w n i )</formula><p>.</p><p>(1)</p><p>We then apply a weighted sum layer to sum over all input features { f l } N l=1 with the associated attention weights {a l i } N l=1 . In addition, we add a residual connection <ref type="bibr" target="#b19">(He et al., 2016)</ref> to pass the input feature f i to the output of the self-attention module. Specifically, the latent representation h i is described by</p><formula xml:id="formula_5">h i = f i + N l=1 a l i ? f l .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Forecasting module</head><p>In addition to considering global temporal information as in the self-attention module A, we exploit visual cues from human motion to encourage our model to generate temporally smooth predictions. Motivated by methods that focus on tackling human motion prediction <ref type="bibr" target="#b24">(Kanazawa et al., 2019;</ref><ref type="bibr" target="#b61">Zhang et al., 2019b)</ref>, we develop a forecasting module F that takes each encoded feature f i as input and forecasts the feature of the next time step f i+1 . As the feature of the next time step is available (given by the encoder), we train the forecasting module F in a self-supervised fashion with a feature regression loss:</p><formula xml:id="formula_6">L feature = N?1 i=1 f i+1 ? f i+1 2 .</formula><p>(3)</p><p>We note that since the feature of the next time step of f N is not available, we do not compute the feature regression loss on f N+1 . The feature regression loss L feature allows the forecasting module F to forecast the feature of the next time step for each input feature by exploiting visual cues from human motion that provide more temporal context for generating temporally smooth predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D human pose and shape estimation</head><p>To jointly consider the latent representations {h i } N i=1 that contain global temporal information and the predicted features</p><formula xml:id="formula_7">{ f i+1 } N?1 i=1</formula><p>that contain local motion information for predicting the parameters for 3D human pose and shape estimation, we have a feature fusion module that fuses</p><formula xml:id="formula_8">{h i } N i=1 and { f i+1 } N?1 i=1 at the same time step to derive the fused representations {F i } N i=1</formula><p>. We note that since our encoder E is pre-trained on single-image pose and shape estimation task and fixed during training as in prior work <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, the feature f i encoded by the encoder E is static and does not contain motion information. Therefore, we use the predicted feature f i from the forecasting module F that contains motion information for feature fusion.</p><p>As shown in <ref type="figure">Figure 4</ref>, our feature fusion module is composed of a fully connected (FC) layer, followed by a softmax layer. Given a latent representation h i and a predicted feature f i , we first apply the FC layer to each input feature to predict a weight. The predicted weights are then normalized using a softmax layer. The two input features are then fused by</p><formula xml:id="formula_9">F i = a h i ?h i +a f i ? f i ? R d . We note that since f 1 is not available, we define F 1 = h 1 .</formula><p>Next, we pass all the fused features {F i } N i=1 to the shape R shape , pose R pose , and camera R camera parameter regressors to predict the corresponding parameters, respectively. Similar to one prior work <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref>, we adopt an iterative error feedback scheme to regress the parameters. To train the proposed SPS-Net, we impose a SMPL parameter regression loss L SMPL on the estimated pose {? i } N i=1 and shape? parameters, a 3D joint loss L 3D joint on the predicted 3D joints {X i } N i=1 , and a 2D joint loss L 2D joint on the reprojected 2D joints {x i } N i=1 <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>. Specifically, the SMPL parameter regression loss L SMPL , the 3D joint loss L 3D joint , and the 2D joint loss L 2D joint are defined as</p><formula xml:id="formula_10">L SMPL = ? ?? 2 + N i=1 ? i ?? i 2 , L 3D joint = N i=1 X i ?X i 2 , L 2D joint = N i=1 x i ?x i 2 .<label>(4)</label></formula><p>Mask loss. Since the ground-truth pose</p><formula xml:id="formula_11">{? i } N i=1 , shape ?, and 3D joint {X i } N i=1</formula><p>annotations are usually not available, using the 2D joint loss L 2D joint alone is insufficient to train the SPS-Net as there are numerous 3D meshes that can explain the same 2D projection. To address this issue, we exploit the idea that the reprojection of the 3D mesh using the estimated camera parameters should be consistent with the segmentation mask obtained by directly segmenting the human from the input video frame. We leverage an off-the-shelf instance segmentation model <ref type="bibr" target="#b5">(Bolya et al., 2019)</ref> to compile a pseudo ground-truth segmentation</p><formula xml:id="formula_12">? ! ! ! " Softmax ! ! " ! " ! ! " ! " FC FC # ! ? ! $ ! " ! " ? Fig. 4:</formula><p>Overview of the feature fusion module. Our feature fusion module consists of a shared fully connected layer and a softmax layer. We first apply the FC layer to each input feature to predict a weight. We then apply a softmax layer to normalize the predicted weights. The input features are combined with the normalized weights to generate F i . mask m pseudo i for each input video frame I i . 1 Then, we use the pseudo ground-truth segmentation mask to supervise the reprojection of the 3D mesh with a mask loss:</p><formula xml:id="formula_13">L mask = ? N i=1 m pseudo i log(m proj i ),<label>(5)</label></formula><p>where m proj i denotes the reprojection of the 3D mesh using the estimated camera parameters. Camera parameter consistency loss. Since there are no ground-truth camera parameter annotations for most datasets, existing methods <ref type="bibr" target="#b27">(Kocabas et al., 2020;</ref><ref type="bibr" target="#b23">Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a)</ref> regularize the estimation of camera parameters via reprojecting the detected 3D keypoints onto 2D space and enforcing a 2D joint loss L 2D joint between the reprojected 2D joints and the corresponding ground-truth 2D joints. This weaker form of supervision, however, is still underconstrained. To address the absence of ground-truth camera parameter annotations, we exploit the idea that the overlapped video frames in different sequence segments from the same video should have the same camera parameter predictions. Given two input sequence segments S 1 = {I S 1 i } k i=n and S 2 = {I S 2 i } k+1 i=n+1 from the same video V, the overlapped frames are {I i } k i=n+1 . We enforce the camera parameter predictions of the overlapped frames {I i } k i=n+1 to be the same in these two input sequence segments S 1 and S 2 . To achieve this, we propose a camera parameter consistency loss L camera which is defined as</p><formula xml:id="formula_14">L camera = k i=n+1 R camera (F S 1 i ) ? R camera (F S 2 i ) 2 ,<label>(6)</label></formula><p>where F S 1 i ? R d and F S 2 i ? R d are the fused feature of frame I S 1 i and frame I S 2 i , respectively. Incorporating such consistency loss during training not only regularizes the prediction of camera parameters but also provides more supervisory signals to facilitate model training. Adversarial loss. In addition to the aforementioned loss functions, we also adopt an adversarial learning scheme that aims to encourage our method to recover a sequence of 3D meshes with realistic motions <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref>. Similar to the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method, we adopt the AMASS <ref type="bibr" target="#b38">(Mahmood et al., 2019)</ref> dataset and employ a discriminator D that takes as input a sequence of pose parameters with the associated shape parameters? = [? 1 , ...,? N ,?] estimated by the SPS-Net (treated as a fake example) and a sequence of those ? = [? 1 , ..., ? N , ?] sampled from the AMASS <ref type="bibr" target="#b38">(Mahmood et al., 2019)</ref> dataset (treated as a real example), and aims to distinguish whether the input sequences are realistic or not.</p><p>As shown in <ref type="figure">Figure 5</ref>, our discriminator D is composed of a self-attention module A D and a classifier C D . We first concatenate the estimated shape parameters? with each of the estimated pose parameters {? i } N i=1 to form the joint representations</p><formula xml:id="formula_15">{? i } N i=1 , where? i = [?,? i ] ? R 82 .</formula><p>We then pass all joint rep-</p><formula xml:id="formula_16">resentations {? i } N i=1 to the self-attention module A D to derive the latent representations {? i } N i=1 , where? i ? R 82 is the la- tent representation of? i . To derive the motion representation M of?, we average all the latent representations {? i } N i=1 , i.e., M = 1 N N i=1? i ? R 82 .</formula><p>The motion representation M ? R 82 of ? can be derived similarly. The classifier C D takes the motion representationsM and M as input and distinguishes whether the input motion representations are realistic or not. Specifically, we have an adversarial loss L adv which is defined as</p><formula xml:id="formula_17">L adv = E ??p ? [ D(?) ? 1 2 ] + E? ?p? [ D(?) 2 ].<label>(7)</label></formula><p>Leveraging the unpaired data from the AMASS <ref type="bibr" target="#b38">(Mahmood et al., 2019)</ref> dataset serves as a weak supervision to encourage the SPS-Net to recover a sequence of 3D meshes with realistic motions. We note that our discriminator D is different from that of the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method in two aspects. First, our discriminator has a self-attention module, while the discriminator of the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method has two GRU layers. Second, we use self-attention to derive a representation for each frame that contains temporal information by jointly considering short-range and long-range dependencies across video frames, whereas the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method leverages self-attention to derive a single representation for the entire pose sequence. Self-supervised occlusion handling. While the aforementioned loss functions regularize the learning of the SPS-Net, the 2D and 3D joint losses and the mask loss are only enforced on the visible keypoints and regions of the human body. That is, there is no explicit constraint imposed on the invisible keypoints and regions. We develop a self-supervised learning scheme to allow our model to produce plausible predictions in order to account for the occlusion and out-of-view scenarios. For each input frame I i , we first synthesize the occluded version I i by randomly masking out some regions. We then leverage the predictions of the original frames to supervise those of the synthesized occluded or partially visible frames and develop a self-supervised parameter regression loss L param to exploit this property with <ref type="figure">Fig. 5</ref>: Overview of the discriminator D. Our discriminator D consists of a self-attention module A D and a classifier C D . Given a sequence of pose parameters {? i } N i=1 and the associated shape parameters ?, we first derive the latent representations {H i } N i=1 using the self-attention module A D . We then average all the latent representations {H i } N i=1 to derive the motion representation M. The classifier C D takes the motion representation M as input and distinguishes whether the input motion representation M is realistic or not.</p><formula xml:id="formula_18">L param = ? ?? 2 + N i=1 ? i ?? i 2 + N i=1 R camera (F i ) ? R camera (F i ) 2 . (8) Self-Attention ? ! " # ! = [ , ! ] Average $ Real or Fake Classifier " = [ , " ] # = [ , # ] ? ? ! ?</formula><p>By simulating the occlusion and out-of-view scenes, our model is able to predict plausible shape, pose, and camera parameters from the occluded or partially visible frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first describe the implementation details. Next, we describe the datasets for model training and testing, followed by the evaluation metrics. We then present the quantitative and visual comparisons to existing methods as well as the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>We implement our model using PyTorch <ref type="bibr">(Paszke et al., 2019)</ref>. Same as prior work <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, we adopt the ResNet-50 <ref type="bibr" target="#b19">(He et al., 2016)</ref> pre-trained on single-image pose and shape estimation task <ref type="bibr" target="#b23">(Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a)</ref> to serve as our encoder E. Our encoder E is fixed and outputs a 2, 048dimensional feature for each frame, i.e., f i ? R 2048 . We set the length of the input sequence to 32 with a batch size of 16. Both the attention network Q and the attention network K in the self-attention module A consist of 2 fully connected layers, each of which has a hidden size of 2, 048, followed by a LeakyReLU layer. As for the forecasting module F, unlike prior methods <ref type="bibr" target="#b61">(Zhang et al., 2019b;</ref><ref type="bibr" target="#b24">Kanazawa et al., 2019</ref>) that use 1D convolution layers, our forecasting module F is composed of 2 fully connected layers, each of which has a hidden size of 2, 048, followed by a LeakyReLU layer. Both the attention network Q and the attention network K in the self-attention module A D also consist of 2 fully connected layers, each of which has a hidden size of 82, followed by a LeakyReLU layer. The classifier C D in the discriminator D is composed of a fully connected layer, followed by a sigmoid function. The input and output dimensions of the classifier C D are 82 and 1, respectively. Similar to the HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref>, the SMPL <ref type="bibr" target="#b37">(Loper et al., 2015)</ref> parameter regressor {R pose , R shape } is composed of 2 fully connected layers with a hidden size of 1, 024. The shape R shape , pose R pose , and camera R camera parameter regressors are initialized from the pre-trained weights of the HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref> approach. The weights of the self-attention module A, the forecasting module F, the feature fusion module, and the discriminator D are randomly initialized. We use the ADAM (Kingma and Ba, 2014) optimizer for training. The learning rates for the SPS-Net and the discriminator D are set to 5 ? 10 ?5 and 1 ? 10 ?4 , respectively. Following the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020</ref>) method, we set the hyperparameters for the loss functions as follows: ? ? =0.06, ? ? =60, ? 3D joint =300, ? 2D joint =300, and ? adv =2. For the other hyperparameters, we set ? feature =1, ? mask =300, ? camera =0.1, ? ? param =0.06, ? ? param =60, and ? camera param =0.1. We train our model on a single NVIDIA V100 GPU with 32GB memory for 120 epochs. For each epoch, there are 500 iterations. Camera parameter consistency loss L camera . To compute the camera parameter consistency loss L camera , in each iteration we sample two consecutive sequence segments by shifting the starting index for data sampling by 1. Assuming that the starting index for data sampling is n, we first sample a sequence segment S 1 = {I i } n+31 i=n . We then shift the starting index for data sampling by 1 and sample another sequence segment S 2 = {I i } n+32 i=n+1 . Given these two sequence segments S 1 and S 2 , the overlapped video frames are {I i } n+31 i=n+1 . We enforce the camera parameter predictions of the overlapped video frames {I i } n+31 i=n+1 to be the same in these two sequence segments with a camera parameter consistency loss. Self-supervised occlusion handling. Since the ground-truth 2D joint annotations are available, for each training image, we randomly sample 3 to 5 keypoints. For each keypoint, we randomly sample a width offset between 25 and 50 pixels and a height offset between 25 and 50 pixels to determine the region to be masked out for synthesizing the occluded training data. The shape, pose, and camera parameter predictions of the occluded training data are supervised by those of the original training data. We note that for frames with ground-truth pose parameter annotations, the self-supervised parameter regression loss L param can be computed against the ground truth. However, in our training set, only the MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref> and Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> datasets contain ground-truth pose parameter annotations. For ease of implementation, we choose to compute the loss against the predictions of the original frames. The formulation of the selfsupervised parameter regression loss L param is applicable to all training data, with or without ground truth. Multi-person tracking. To recover human body from videos that contain multiple person instances, we first leverage a multiperson tracker to detect and track each person instance. We then apply our SPS-Net to each person tracking result to estimate the 3D human pose and shape. The multi-person tracker is composed of an object detector and an object tracker. We adopt the YOLOv4 <ref type="bibr" target="#b3">(Bochkovskiy et al., 2020)</ref> as the object detector and the SORT <ref type="bibr" target="#b2">(Bewley et al., 2016)</ref> as the object tracker. The multi-person tracker first applies the YOLOv4 <ref type="bibr" target="#b3">(Bochkovskiy et al., 2020)</ref> detector to each video frame to detect each person instance. Then the person detection results are passed to the SORT <ref type="bibr" target="#b2">(Bewley et al., 2016)</ref> method to associate the detected person instances in the current frame to the existing ones. Specifically, the SORT <ref type="bibr" target="#b2">(Bewley et al., 2016)</ref> first predicts the bounding box in the current frame for each existing person. Then, we compute the intersection over union (IoU) between the detected bounding boxes and the predicted bounding boxes. By using the Hungarian algorithm with a minimum IoU threshold, we can assign each detected person instance to an existing one or consider the detected person instance a new one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental settings</head><p>We describe the datasets and the evaluation metrics below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Datasets</head><p>Similar to the state-of-the-art human pose and shape estimation methods <ref type="bibr" target="#b23">(Kanazawa et al., 2018</ref><ref type="bibr" target="#b24">(Kanazawa et al., , 2019</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, we adopt a number of datasets that contain either 2D or 3D ground-truth annotations for training. Specifically, we use the PennAction <ref type="bibr" target="#b62">(Zhang et al., 2013)</ref>, InstaVariety <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref>, PoseTrack <ref type="bibr" target="#b0">(Andriluka et al., 2018)</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>, and Hu-man3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> datasets for training. Same as the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> method, we use the Kinetics-400 <ref type="bibr" target="#b25">(Kay et al., 2017)</ref> dataset to complement the missing parts of the InstaVariety <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> dataset. We evaluate our method on the 3DPW <ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>, and Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> datasets. The details of each dataset are described below. <ref type="bibr">3DPW (von Marcard et al., 2018)</ref>. The 3DPW dataset is an in-the-wild 3D dataset, containing 60 videos of several in-thewild and indoor activities. The training, validation, and test sets are composed of 24, 12, and 24 video sequences, respectively. We evaluate our method on the 3DPW test set. MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>. The MPI-INF-3DHP dataset consists of multi-view videos captured in indoor environments. The training set contains 8 subjects, each of which has 16 videos. Following existing approaches <ref type="bibr" target="#b29">(Kolotouros et al., 2019a;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>, we use the training set for model training and evaluate our SPS-Net on the test set. Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref>. The Human3.6M dataset is composed of 15 sequences of several people performing different actions. This dataset is collected in an indoor and controlled environment. The training set contains 1.5 million images, each of which has 3D ground-truth annotations. Same as the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020</ref>) method, we train our model on 5 subjects (i.e., S1, S5, S6, S7, and S8) and evaluate our method on the remaining 2 subjects (i.e., S9 and S11). PennAction <ref type="bibr" target="#b62">(Zhang et al., 2013)</ref>. The PennAction dataset is composed of 2, 326 videos of 15 actions. Each video is annotated with 2D keypoints. We use this dataset for training. InstaVariety <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref>. The InstaVariety dataset is composed of videos of 24-hour long collected from Instagram. Each video is annotated with 2D joints obtained by using the OpenPose <ref type="bibr" target="#b7">(Cao et al., 2019)</ref> and Detect and Track <ref type="bibr" target="#b17">(Girdhar et al., 2018)</ref> methods. We adopt this dataset for training. PoseTrack <ref type="bibr" target="#b0">(Andriluka et al., 2018)</ref>. The PoseTrack dataset consists of 1, 337 videos. The training set is composed of 792 videos. The validation set contains 170 videos. The test set comprises 375 videos. Each video is annotated with 15 keypoints. We use the training set for model training.  <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref> dataset. (Right) Results on the Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> dataset. The bold and underlined numbers indicate the top two results, respectively. The "-" indicates the result is not available.  <ref type="bibr" target="#b41">(Mehta et al., 2017b)</ref> 9.81M ------72.5 --EpipolarPose <ref type="bibr" target="#b28">(Kocabas et al., 2019)</ref> 34.28M ------77.5 --TCN <ref type="bibr">(Cheng et al., 2020)</ref> -------84.1 --RepNet <ref type="bibr" target="#b56">(Wandt and Rosenhahn, 2019)</ref> 10.03M -----97.8 82.5 --CMR <ref type="bibr" target="#b30">(Kolotouros et al., 2019b)</ref> 46.31M 70.2 ------50.1 -STRAPS <ref type="bibr" target="#b50">(Sengupta et al., 2020)</ref> 12.48M 66.8 ------55.4 -NBF <ref type="bibr" target="#b43">(Omran et al., 2018)</ref> 68.11M 90.7 ------59.9 -ExPose <ref type="bibr" target="#b12">(Choutas et al., 2020)</ref> 47.22M 60.7 93.4 -------HUND <ref type="bibr" target="#b59">(Zanfir et al., 2020)</ref> -56.5 87.7 -----53.0 72.0 HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref> 26  <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> methods. Our method is capable of estimating shapes that cover human bodies well and predicting more accurate poses for limbs in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Evaluation metrics</head><p>We use the procrustes aligned mean per joint position error (PA-MPJPE), mean per joint position error (MPJPE), percentage of correct keypoints (PCK) <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>, per vertex error (PVE), and mean acceleration error of every joint in mm/s 2 <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance evaluation and comparisons</head><p>We compare the performance of our SPS-Net with existing frame-based methods <ref type="bibr" target="#b8">Chen et al., 2019a;</ref><ref type="bibr" target="#b28">Kocabas et al., 2019;</ref><ref type="bibr" target="#b41">Mehta et al., 2017b;</ref><ref type="bibr">Cheng et al., 2020;</ref><ref type="bibr" target="#b56">Wandt and Rosenhahn, 2019;</ref><ref type="bibr" target="#b30">Kolotouros et al., 2019b;</ref><ref type="bibr" target="#b50">Sengupta et al., 2020;</ref><ref type="bibr" target="#b43">Omran et al., 2018;</ref><ref type="bibr" target="#b12">Choutas et al., 2020;</ref><ref type="bibr" target="#b59">Zanfir et al., 2020;</ref><ref type="bibr" target="#b23">Kanazawa et al., 2018;</ref><ref type="bibr" target="#b29">Kolotouros et al., 2019a)</ref> and video-based approaches <ref type="bibr" target="#b24">(Kanazawa et al., 2019;</ref><ref type="bibr" target="#b1">Arnab et al., 2019;</ref><ref type="bibr" target="#b14">Doersch and Zisserman, 2019;</ref><ref type="bibr" target="#b51">Sun et al., 2019;</ref><ref type="bibr" target="#b27">Kocabas et al., 2020)</ref>. <ref type="table" target="#tab_0">Table 1</ref> presents the quantitative results on the 3DPW <ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref>, MPI-INF-3DHP <ref type="bibr" target="#b40">(Mehta et al., 2017a)</ref>, and Human3.6M <ref type="bibr" target="#b21">(Ionescu et al., 2013)</ref> datasets.</p><p>Experimental results on all three datasets show that our method performs favorably against existing frame-based and video-based approaches on the PA-MPJPE, MPJPE, PVE, and PCK evaluation metrics. However, the acceleration error of our method is inferior to that of the HMMR <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> approach. The reason for the inferior performance is that the goal of the HMMR <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> method lies in predicting past and future motions given a single image. While we have a forecasting module F that predicts the feature of the future frame based on past information, we do not aim to optimize the performance on the human motion prediction task but instead focus on learning to estimate 3D human pose and shape of the current frame. On the other hand, as noted by the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref>, the HMMR <ref type="bibr" target="#b24">(Kanazawa et al., 2019)</ref> method applies smoothing to the predictions, leading to overly smooth pose predictions at the expense of sacrificing the accuracy of pose and shape estimation. <ref type="figure">Fig. 7</ref>: Visual results of occlusion handling. We present visual results on the CrowdPose dataset . Our results demonstrate the ability of the SPS-Net to recover plausible human bodies for the occluded person instances.</p><p>In addition to quantitative comparisons, we present 1) visual comparisons with the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> and SPIN <ref type="bibr" target="#b29">(Kolotouros et al., 2019a)</ref>   <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> and SPIN <ref type="bibr" target="#b29">(Kolotouros et al., 2019a)</ref>. We observe that our model recovers bodies that well cover humans and estimates more accurate poses for limbs in particular. Visual results of occlusion handling. <ref type="figure">Figure 7</ref> presents example visual results of occlusion handling on the CrowdPose dataset . We observe that our model is able to recover plausible human bodies for the occluded person instances, demonstrating the robustness of our SPS-Net. Visual results of different viewpoints. We visualize human bodies recovered by our SPS-Net from different viewpoints in <ref type="figure" target="#fig_1">Figure 8</ref>. Our results show that our method estimates accurate rotation parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>Loss functions. To analyze the effectiveness of each loss function, we conduct an ablation study by removing one loss function at a time. Specifically, we analyze how much performance gain each loss function contributes. <ref type="table" target="#tab_3">Table 2</ref> shows the results on the 3DPW <ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref> test set.</p><p>Without the camera parameter consistency loss L camera , there is no explicit constraint imposed on the prediction of camera parameters, leading to performance drops of 1.7 in PA-MPJPE and 3.5 in PVE. When removing the mask loss L mask , our model does not have any constraints to regularize the 3D mesh. Performance drops of 5.9 in PA-MPJPE and 7.0 in PVE occur. Without the self-supervised parameter regression loss L param , our model does not learn to produce plausible predictions when the occlusion or out-of-view issues occur, resulting in performance drops of 5.4 in PA-MPJPE and 4.6 in PVE. When removing the adversarial loss L adv , our model does not learn to render 3D meshes that have realistic motions. Performance drops on all three evaluation metrics occur, which also concur with the findings in the HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref> and VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref>. <ref type="figure">Figure 9</ref> presents two visual comparisons with the variant methods of our SPS-Net (i.e., Ours w/o L camera and Ours w/o L mask ). Our visual results show that both the camera parameter consistency loss L camera and the mask loss L mask allow our model to predict more accurate pose and shape estimates.</p><p>The ablation study on loss functions shows that all four losses are crucial to the SPS-Net. Self-attention and forecasting modules. We conduct an ablation study to analyze the contribution of the self-attention module A and the forecasting module F in the SPS-Net. Specifi-    <ref type="figure">Figure 12</ref> shows visual comparisons with the variants of our SPS-Net (i.e., Ours w/o Self-Attention A and Ours w/o Forecasting F) on the CrowdPose dataset . Our visual results show that without either the self-attention module A or the forecasting module F, the degraded model cannot recover accurate poses. Self-attention module vs. GRU. To analyze the effectiveness of employing different temporal modules, we conduct an ablation study by swapping the self-attention module A in the SPS-Net with a two-layer GRU module as in the VIBE <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref> model, i.e., comparing the performance between the "Ours (Self-Attention)" method and the "Ours (GRU)" approach. <ref type="table" target="#tab_5">Table 4</ref> presents the results on the 3DPW (von Marcard et al., 2018) test set. We observe that employing the self-attention module results in performance improvement over adopting the GRU on all three evaluation metrics. Input sequence length. We conduct an ablation study to analyze the effect of the input sequence length.    <ref type="bibr" target="#b56">(Wandt and Rosenhahn, 2019)</ref> Titan X -10 CMR <ref type="bibr" target="#b30">(Kolotouros et al., 2019b)</ref> RTX 2080Ti -3.3 STRAPS <ref type="bibr" target="#b50">(Sengupta et al., 2020)</ref> RTX 2080Ti 120 0.25 NBF <ref type="bibr" target="#b43">(Omran et al., 2018)</ref> V100 18 -ExPose <ref type="bibr" target="#b12">(Choutas et al., 2020)</ref> Quadro P5000 -0.16 HUND <ref type="bibr" target="#b59">(Zanfir et al., 2020)</ref> P100 72 0.055 HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref> Titan <ref type="formula">1080Ti</ref>  Our results show that the performance on all three metrics improves as the input sequence length increases. When the input sequence length increases from 32 (the default setting in our experiments) to 48, our results can be further improved. However, due to GPU memory constraints, we are not able to experiment with longer input sequence lengths. Sensitivity analysis. To analyze the sensitivity of the SPS-Net with respect to the hyperparameters, we perform a sensitivity analysis on the hyperparameters ? mask and ? camera . We report the PA-MPJPE results on the 3DPW <ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref> test set. <ref type="figure" target="#fig_2">Figure 10</ref> presents the experimental results. We observe that when the hyperparameter is set to 0 (i.e., the corresponding loss function is removed), our SPS-Net suffers from performance drops. When the hyperparameters are set within a suitable range (i.e., around 300 for ? mask and around 0.1 for ? camera ), the performance of our SPS-Net is improved, demonstrating the effectiveness of the corresponding loss func-tion. When the hyperparameters are set to large values (e.g., 1 ? 10 4 for ? mask and 1 ? 10 3 for ? camera ), our model training will be dominated by optimizing the corresponding loss, leading to performance drops.</p><p>The sensitivity analysis of hyperparameters shows that when each hyperparameter is set within a suitable range, the performance of our method is improved and remains stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Run-time analysis</head><p>We report the model training time in hours, the inference time for processing an image in seconds, and the GPU platform used by each method in <ref type="table" target="#tab_8">Table 6</ref>. First, the training time of our method is shorter than that of the HMR <ref type="bibr" target="#b23">(Kanazawa et al., 2018)</ref>, STRAPS <ref type="bibr" target="#b50">(Sengupta et al., 2020)</ref>, and HUND <ref type="bibr" target="#b59">(Zanfir et al., 2020)</ref>. Second, the inference time of our method is comparable to that of the VIBE method <ref type="bibr" target="#b27">(Kocabas et al., 2020)</ref>, and shorter than that of the ExPose <ref type="bibr" target="#b12">(Choutas et al., 2020)</ref> and STRAPS <ref type="bibr" target="#b50">(Sengupta et al., 2020)</ref> approaches. Third, our method performs favorably against existing frame-based and video-based approaches on all three datasets as shown in Table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Failure modes</head><p>We present the failure cases of our method in <ref type="figure">Figure 11</ref>. As our SPS-Net assumes that the input video frames contain a single person, if missing detection happens, our method will not be able to perform human pose and shape estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose the SPS-Net for estimating 3D human pose and shape from videos. The main contributions of this work lie <ref type="figure">Fig. 11</ref>: Failure cases. If missing detection happens, our SPS-Net will not be able to predict 3D human body. in the design of the self-attention module that captures shortrange and long-range dependencies across video frames and the forecasting module that allows our model to exploit visual cues from human motion for producing temporally coherent predictions. To address the absence of ground-truth camera parameter annotations, we propose a camera parameter consistency loss that not only regularizes the learning of camera parameter prediction but also provides additional supervisory signals to facilitate model training. We develop a self-supervised learning scheme that explicitly models the occlusion and out-of-view scenarios by masking out some regions in the video frames. By leveraging the predictions of the original video frames to supervise those of the synthesized occluded or partially visible data, our model learns to predict plausible estimations. Extensive experimental results on three challenging datasets show that our SPS-Net performs favorably against the state-of-the-art 3D human pose and shape estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours w/o Self-Attention A</head><p>Ours w/o Forecasting F Ours <ref type="figure">Fig. 12</ref>: Visual comparisons with our variant methods. We present visual comparisons with the Ours w/o Self-Attention A and Ours w/o Forecasting F methods on the CrowdPose dataset .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>methods, 2) visual results of occlusion handling, and 3) visual results of different viewpoints. Visual comparisons with the VIBE and SPIN methods. Figure 6 shows two visual comparisons with the VIBE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results of 3D human pose and shape estimation. We visualize the 3D human body from different viewpoints recovered by our SPS-Net on the 3DPW (von Marcard et al., 2018) test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 10 :</head><label>10</label><figDesc>Sensitivity analysis of hyperparameters. We report the PA-MPJPE results of our method on the 3DPW (von Marcard et al., 2018) dataset. Experimental results show that the performance of our SPS-Net is stable when the hyperparameters are set within a suitable range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the 3DPW (von Marcard et al., 2018) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of 3D human pose and shape estimation. We present the experimental results with comparisons to existing methods. (Left) Results on the 3DPW (von Marcard et al., 2018) dataset. (Middle) Results on the MPI-INF-3DHP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Visual comparisons. We present two visual comparisons with the SPIN<ref type="bibr" target="#b29">(Kolotouros et al., 2019a)</ref> and VIBE</figDesc><table><row><cell></cell><cell></cell><cell>.98M</cell><cell>76.7</cell><cell>130.0</cell><cell>-</cell><cell>37.4</cell><cell>89.8</cell><cell>124.2</cell><cell>72.9</cell><cell>56.8</cell><cell>88.0</cell></row><row><cell></cell><cell>SPIN (Kolotouros et al., 2019a)</cell><cell>26.98M</cell><cell>59.2</cell><cell>96.9</cell><cell>116.4</cell><cell>29.8</cell><cell>67.5</cell><cell>105.2</cell><cell>76.4</cell><cell>41.1</cell><cell>-</cell></row><row><cell>Video based</cell><cell>Temporal 3D Kinetics (Arnab et al., 2019) Motion to the Rescue (Doersch and Zisserman, 2019) DSD-SATN (Sun et al., 2019) HMMR (Kanazawa et al., 2019) VIBE (Kocabas et al., 2020)</cell><cell>---29.76M 48.30M</cell><cell>72.2 74.7 69.5 72.6 56.5</cell><cell>---116.5 93.5</cell><cell>---139.3 113.4</cell><cell>---15.2 27.1</cell><cell>----63.4</cell><cell>----97.7</cell><cell>----89.0</cell><cell>--42.4 56.9 41.5</cell><cell>--59.1 -65.9</cell></row><row><cell></cell><cell>Ours</cell><cell>51.43M</cell><cell>50.4</cell><cell>85.8</cell><cell>100.6</cell><cell>22.1</cell><cell>60.7</cell><cell>94.3</cell><cell>90.1</cell><cell>38.7</cell><cell>58.9</cell></row><row><cell>SPIN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VIBE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on loss functions. We report the experimental results on the 3DPW (von Marcard et al., 2018) test set. The bold and underlined numbers indicate the top two results, respectively.</figDesc><table><row><cell>Method</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell></row><row><cell>Ours</cell><cell>50.4</cell><cell>85.8</cell><cell>100.6</cell></row><row><cell>Ours w/o L camera</cell><cell>52.1</cell><cell>88.2</cell><cell>104.1</cell></row><row><cell>Ours w/o L mask</cell><cell>56.3</cell><cell>90.0</cell><cell>107.6</cell></row><row><cell>Ours w/o L param</cell><cell>55.8</cell><cell>89.4</cell><cell>105.2</cell></row><row><cell>Ours w/o L adv</cell><cell>56.2</cell><cell>93.4</cell><cell>112.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the self-attention and forecasting modules. We report the experimental results on the 3DPW (von Marcard et al., 2018) test set. The bold and underlined numbers indicate the top two results, respectively.</figDesc><table><row><cell>Method</cell><cell>Number of parameters</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell><cell>Acceleration Error ?</cell></row><row><cell>Ours</cell><cell>51.43M</cell><cell>50.4</cell><cell>85.8</cell><cell>100.6</cell><cell>22.1</cell></row><row><cell>Ours w/o Forecasting F</cell><cell>47.23M</cell><cell>54.2</cell><cell>91.9</cell><cell>104.3</cell><cell>23.3</cell></row><row><cell>Ours w/o Self-Attention A</cell><cell>34.64M</cell><cell>57.6</cell><cell>96.6</cell><cell>104.7</cell><cell>22.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on different temporal modules. We report the experimental results on the 3DPW<ref type="bibr" target="#b39">(von Marcard et al., 2018)</ref> test set. The bold and underlined numbers indicate the top two results, respectively.Table 3shows the results on the 3DPW (von Marcard et al., 2018) test set. Without either the forecasting module F or the self-attention module A, the degraded method suffers from significant performance loss in all metrics. When both modules are jointly utilized, our model achieves the best results, demonstrating the complementary importance of these two components.</figDesc><table><row><cell>Method</cell><cell>Number of parameters</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell></row><row><cell>Ours (Self-Attention)</cell><cell>51.43M</cell><cell>50.4</cell><cell>85.8</cell><cell>100.6</cell></row><row><cell>Ours (GRU)</cell><cell>50.88M</cell><cell>52.8</cell><cell>87.7</cell><cell>103.2</cell></row><row><cell cols="5">cally, we show the contribution of each component by disabling</cell></row><row><cell cols="2">(removing) one at a time.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the input sequence length. We present the experimental results on the 3DPW (von Marcard et al., 2018) test set. The bold and underlined numbers indicate the top two results, respectively.</figDesc><table><row><cell>Input sequence length</cell><cell>PA-MPJPE ?</cell><cell>MPJPE ?</cell><cell>PVE ?</cell></row><row><cell>8</cell><cell>55.3</cell><cell>92.4</cell><cell>110.8</cell></row><row><cell>16</cell><cell>53.1</cell><cell>87.6</cell><cell>105.5</cell></row><row><cell>32</cell><cell>50.4</cell><cell>85.8</cell><cell>100.6</cell></row><row><cell>48</cell><cell>50.2</cell><cell>85.1</cell><cell>100.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Visual comparisons with our variant methods. (Left) Visual comparisons with the Ours w/o L camera method. (Right) Visual comparisons with the Ours w/o L mask approach.</figDesc><table><row><cell>presents</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Run time analysis. We report the GPU platform, the model training time in hours, and the inference time for processing an image in seconds. The "-" indicates the result is not available.</figDesc><table><row><cell>Method</cell><cell>Platform</cell><cell>Training</cell><cell>Inference</cell></row><row><cell>Yang et al. (Yang et al., 2018)</cell><cell>Titan X</cell><cell>-</cell><cell>1.1</cell></row><row><cell>Mehta et al. (Mehta et al., 2017b)</cell><cell>Titan X</cell><cell>-</cell><cell>3.3</cell></row><row><cell>RepNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We note that while other existing instance segmentation models can also be used for compiling segmentation masks, we leave the discussion of adopting different instance segmentation models as future work.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simple online and realtime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Yolact: real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Show, match and segment: Joint weakly supervised learning of semantic matching and object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">2020. 3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Detect-andtrack: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Auto-conditioned recurrent networks for extended complex human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neural rendering and reenactment of human actor videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image transformer</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>in: ICML</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Synthetic training for accurate 3d human pose and shape estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Mo 2 cap 2: Real-time mobile 3d motion capture with a cap-mounted fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TVCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Neural descent for visual 3d human pose and shape. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Predicting 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">From actemes to action: A stronglysupervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
