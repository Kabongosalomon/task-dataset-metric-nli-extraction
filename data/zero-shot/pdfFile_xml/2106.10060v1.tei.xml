<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning of Generalized Game Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintan</forename><surname>Trivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Digital Games</orgName>
								<orgName type="institution">University of Malta Msida</orgName>
								<address>
									<country key="MT">Malta</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Liapis</surname></persName>
							<email>antonios.liapis@um.edu.mt</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Digital Games</orgName>
								<orgName type="institution">University of Malta Msida</orgName>
								<address>
									<country key="MT">Malta</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
							<email>georgios.yannakakis@um.edu.mt</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Digital Games</orgName>
								<orgName type="institution">University of Malta Msida</orgName>
								<address>
									<country key="MT">Malta</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning of Generalized Game Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-computer vision in games</term>
					<term>generalized represen- tations</term>
					<term>contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representing games through their pixels offers a promising approach for building general-purpose and versatile game models. While games are not merely images, neural network models trained on game pixels often capture differences of the visual style of the image rather than the content of the game. As a result, such models cannot generalize well even within similar games of the same genre. In this paper we build on recent advances in contrastive learning and showcase its benefits for representation learning in games. Learning to contrast images of games not only classifies games in a more efficient manner; it also yields models that separate games in a more meaningful fashion by ignoring the visual style and focusing, instead, on their content. Our results in a large dataset of sports video games containing 100k images across 175 games and 10 game genres suggest that contrastive learning is better suited for learning generalized game representations compared to conventional supervised learning. The findings of this study bring us closer to universal visual encoders for games that can be reused across previously unseen games without requiring retraining or fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The use of pixels to represent games is gradually dominating the field of artificial intelligence (AI) in games <ref type="bibr" target="#b0">[1]</ref> with applications that vary from gameplaying agents <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, and game content generation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> all the way to player affect modeling <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Deep learning methods-predominately variants of convolutional neural networks (ConvNets)-process the RGB pixels of the game and convert them into a compressed representation that approximates the internal state of the game world. While computer vision methods appear to offer certain capacities when it comes to their general use across games, they come with certain limitations including the computational cost of training and their poor reusability across games.</p><p>A common approach for reducing computation effort and improving on generality is to use and fine-tune pretrained models such as ResNet <ref type="bibr" target="#b8">[9]</ref> that are trained on large datasets such as ImageNet <ref type="bibr" target="#b9">[10]</ref>. While such models can detect many common everyday real-world objects, they still require finetuning when applied to games as they are far from ideal replicas of the real-world. Importantly, games are not merely image representations; gameplay images contain both functional This project has received funding from the EU's Horizon 2020 programme under grant agreement No 951911. properties associated with the game genre (e.g. corridors, tracks and platforms that define movement constraints) and aesthetic elements unique to each game (e.g. the various art styles available in match-3 tile games). As a visual example of this issue, <ref type="figure" target="#fig_0">Fig. 1</ref> displays games with similar content (i.e. same game genre) but with different visual style. The representations obtained from these game images with a pre-trained ResNet-50 model can be visualized on a 2D plane using t-distributed stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b10">[11]</ref>. We observe that the 2D embeddings of representations of 3 different Soccer, FPS and Racing games form their own separate "clusters" in the Euclidean space. This phenomenon, named domain gap <ref type="bibr" target="#b11">[12]</ref>, occurs due to the visual styling differences in each game, leading to shifts in the distributions. It is therefore expected that any AI algorithm that builds on representations obtained from a pre-trained model will only operate well on the particular game it is trained on, and will require substantial fine-tuning (retraining or transfer learning) in order to be of use on other games. This lack of generalizability and reusability makes pixel-based deep learning impractical across different games even if they belong to the same genre.</p><p>To tackle the above-mentioned challenges in this paper we introduce contrastive learning <ref type="bibr" target="#b12">[13]</ref> as a novel way to approach the domain gap challenge in games. Our hypothesis is that by contrasting pixel-based representations-instead of merely classifying them-we can fine-tune pre-trained Con-vNet models that better capture the underlying content of the game rather than its style. We test our hypothesis on a new dataset, namely Sports10, featuring 100k images of 175 different games across 10 sports game genres. By training ConvNets on this dataset via fully supervised and contrastive learning techniques, we show that the latter is better suited for not only achieving higher genre-classification accuracy, but more importantly, for attaining better generalization capacity. Findings suggest that contrastive learning yields more general pixel-based representations of games by focusing more on the content of the game while demonstrating better invariance to the visual styling differences in the provided images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REPRESENTATION LEARNING IN GAMES</head><p>Recent research in machine learning has often developed intelligent systems that use pixel information as the (only) input. In games, pixel representations have been used for contentbased retrieval for moments <ref type="bibr" target="#b13">[14]</ref> and to predict players' affective states <ref type="bibr" target="#b6">[7]</ref>; however, the most prominent application is Deep Reinforcement Learning. Mnih et al. <ref type="bibr" target="#b1">[2]</ref> introduced one of the first game-playing agents for Atari games that learns control-policy directly from pixels using ConvNets. This work was extended by Kempka et al. <ref type="bibr" target="#b14">[15]</ref> to play Doom (id Software, 1993) using screen buffer and depth information processed by ConvNets in the ViZDoom platform. Ha and Schimdhuber <ref type="bibr" target="#b2">[3]</ref> presented a recurrent model that uses convolutional auto-encoders with temporal memory to create a world model of the game. Since auto-encoders are designed to reconstruct the input from its encoding, this world model can be seen as a compressed representation of the game that encodes both content and style information of the environment.</p><p>A number of approaches attempt to encode only the content information so that it makes the subsequent policy-learning task easier. Srinivas et al. <ref type="bibr" target="#b15">[16]</ref>, for instance, combined policy learning along with representation learning in a unified framework which yields representations that contain only the content information of the game, which is a better resemblance of a game's internal state. Another direction focusing on the separation of content from style is to derive style-invariant representations of the game environment using data augmentation techniques-such as color shift, gray-scale conversion, etc.that produce different styles of the same image <ref type="bibr" target="#b16">[17]</ref>. Such learning frameworks encourage the convolutional encoder to ignore style-related information of the game that is present in the screen pixels and focus more on the content. The scope of generalization in such approaches, however, still remains limited to the game environment that the visual encoder is being trained on. As a result, these methods are still susceptible to the domain gap problem described earlier.</p><p>When it comes to video games, Luo et al. <ref type="bibr" target="#b17">[18]</ref> show how to use transfer learning to train ConvNets for extracting game events, but they do not focus on generalization. Khameneh and Guzdial <ref type="bibr" target="#b18">[19]</ref> try to tackle generalization, but their method operates on game events extracted from internal state rather than pixels. To the best of our knowledge there have been no attempts to specifically tackle the domain gap challenge in computer vision for video games. Motivated by this knowledge gap and inspired by recent trends in computer vision <ref type="bibr" target="#b19">[20]</ref> we test the capacity of contrastive learning <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref> to train visual encoders for video games and evaluate the extent to which it can mitigate domain gap problems by learning representations that can generalize over different games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERALIZATION</head><p>While the broader definition of generalization in AI may be rather subjective and open-ended <ref type="bibr" target="#b21">[22]</ref>, in this section we provide its formal definition for our work and restrict its scope in terms of representation learning in video games. We then discuss how to quantitatively measure it in order to evaluate the performance of our generalization models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definition</head><p>In this paper we define generalization as the ability of a trained ConvNet model to process pixels of a game screen and extract a meaningful representation of the game's content without being affected by the graphic styling of the game. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes generalisation in the form of a causality graph <ref type="bibr" target="#b19">[20]</ref> showing the style-invariance requirement for a generalized game representation. Under this causality framework, only the content of the game defines the game genre while the style only affects the rendered image of the game. Different video games belonging to the same genre can be thought of as having the same content but varying style. Thus, a model that generalizes well should be able to extract representations from any game within the same genre without exhibiting a domain gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Measuring Generalization</head><p>We propose to measure generalization in terms of the mitigation of domain gap across different styles of games of the same genre. This can be measured in the representation space, which is a d-dimensional Euclidean Space where d is the size of the latent representation. In this formulation, better generalization means the representations of different games of the same genre form a compact cluster and are well-separated from representation clusters of other genres. Hence, we choose the Silhouette Score <ref type="bibr" target="#b22">[23]</ref> as a metric for evaluating the quality of representation clusters.</p><p>Assume that a given dataset D contains images X belonging to a set of game genres Y = {y 1 , y 2 , ..., y n }. Let's denote the i th image of this dataset as X i ? X having genre label y i ? Y. The pixel-representation extracted from X i is denoted by x i where |x i | = d. Let's denote the average intracluster Euclidean distance (cluster compactness) of this image representation within its own genre as a(x i ). Likewise, let's denote the average inter-cluster distance (separation from other clusters) to the nearest cluster as b(x i ). Then, the Silhouette Coefficient s(X i ) of this image is defined as:</p><formula xml:id="formula_0">s(X i ) = b(x i ) ? a(x i ) max{a(x i ), b(x i )}<label>(1)</label></formula><p>with the assumption that the set of images belong to more than one game genre (|Y| &gt; 1). The combined Silhouette Score S(D) for the entire set of images can be defined as the average silhouette coefficient over all points i in D. This S(D) lies within [?1, 1] with higher values indicating more compact and better separated clusters. If we group visually different games of the same genre and assign them the same cluster label, we expect the silhouette score of this group of images would quantify domain gap issues present in the dataset. Based on the properties of S(D), we argue that it is a good measurement of generalization capability across different games.</p><p>In addition to the silhouette score, we use t-SNE <ref type="bibr" target="#b10">[11]</ref> for qualitative analysis by visualizing the representation space, which offers an approximate 2-dimensional projection of the d-dimensional representations learned by the visual encoder. Note that this technique is merely used in our work for its intuitive visualization of what the ConvNet models have learned, rather than evaluation or comparison of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SPORTS10: A DATASET OF SPORTS GAME GENRES</head><p>In order to learn style-invariant features of games from pixels, we created a new game genres dataset 1 for our experiments. It contains 100, 000 gameplay images of 175 different games across 10 game genres that are extracted from publicly available sports video game titles. Each of the 10 genres-American Football, Basketball, Bike Racing, Car Racing, Fighting, Hockey, Soccer, <ref type="table" target="#tab_0">Table Tennis</ref>, Tennis and Volleyball-contain exactly 10, 000 hand-curated images of the game-play sequence, ensuring the removal of all menu, transitions or cutscenes of the game. <ref type="figure" target="#fig_2">Fig. 3</ref> shows a glimpse of the variety of games that are a part of this dataset, divided into our interpretation of three visual styling categories: retro (arcade-style, 1990s and earlier), modern (roughly 2000s) and photoreal (roughly late 2010s). The genre and styling-wise breakdown of total games in our dataset is given in <ref type="table" target="#tab_0">Table I</ref>.</p><p>Note that we limit the scope of study to only include games that are grounded in reality in terms of both gameplay as well as visual appearance. This means that fictional or fantasy game genres have not been included in our dataset. The reason for this is two-fold. First, we use pre-trained models in our experiments that are trained on real-world data. Second, it is more difficult to define a common game genre when a fictional/fantasy game contains unique gameplay elements and visual tropes not found in any other games. Any game within these genres that satisfies this grounded-inreality criterion, however, can be considered within the scope of generalization. In future studies the scope of generalization can be expanded to cover additional game genres of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GAME REPRESENTATION LEARNING</head><p>This section describes the training procedure of the ConvNet visual encoder to obtain generalized representations. Since we are trying to learn general-purpose representations that can be used for a number of core AI applications in games <ref type="bibr" target="#b0">[1]</ref> including game-playing, game content generation, affect modeling, etc., we need to formulate our training method with a proxy learning task which acts as a general-purpose learning framework. Based on the analysis carried out by Mitrovic et al.</p><p>[20], we formulate the task of learning game representations from pixels as an image classification problem.</p><p>Let's denote our game genres dataset as D = {(X i , y i )} for all pairs where X i is an RGB image of height h and width w drawn from the set X ? R h?w?3 . y i ? Y is the game genre label for X i belonging to a set of different game genres. Thus, our image classification problem can now be defined as learning a function f : X ? Y. In our experiments, we are using ConvNets to estimate the function f by iterating over the training dataset D. Thus, the function f can be seen as a composite function f = c ? r where r : X ? R d is estimated by the visual encoder comprising of convolutional layers, R d is the d-dimensional representation space learnt by the encoder and c : R d ? Y is the classifier comprising of fully-connected layers giving the output class prediction. After investigating various architectures and baseline models, we select the ResNet-50 <ref type="bibr" target="#b8">[9]</ref> architecture as our visual encoder for all experiments reported in this paper due to its reasonable size (? 25 ? 10 6 learnable parameters) to performance ratio. Moreover, we initialize the learnable parameters of this model with the weights learnt from pre-training it on the ImageNet dataset (available at keras.io). Lastly, the classifier contains two fully-connected layers with a dropout rate of 0.2 and the last layer employs the softmax-activated cross-entropy loss function for learning class probabilities. In the following subsections, we first present the different data pre-processing techniques employed to prepare the input images for feeding into the neural networks. Then, we lay out the two different training approaches-Fully Supervised Learning and Supervised Contrastive Learning-that we use for learning the functions r and c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Pre-Processing</head><p>Before feeding the images from our dataset to the ResNet encoder for training, we perform multiple data-preprocessing steps. First, we resize the images to h = 224 and w = 224 regardless of the original image dimensions. We settle upon this size for two reasons: (a) the version of pre-trained ResNet model used has been originally trained on this image size, and (b) this also happens to be the ideal image size we can fit onto our GPU hardware (8GB VRAM) for training.</p><p>Then, we split our dataset into training set D T and valida-</p><formula xml:id="formula_1">tion set D V such that D T ? D V = ? and D T ? D V = D.</formula><p>Instead of naively dividing all the images in D into the two sets based on their genre, we algorithmically select D T and D V such that the games selected for the training set do not overlap with those in the validation set. This ensures that the models are tested on games that are not encountered during training, enabling us to evaluate the out-of-distribution generalization <ref type="bibr" target="#b23">[24]</ref> performance of our models, i.e., on new/unseen games. Moreover, this algorithm aims for roughly 75%-25% split and tries to pick equal ratios of games across the visual styling categories, so that the balance of retro, modern, and photoreal games is maintained in both the training and validation sets.</p><p>Next, we perform image data augmentation using various techniques studied in <ref type="bibr" target="#b24">[25]</ref> such as horizontal flipping, zoom, brightness, height/width re-scaling and rotation, shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. These augmentations are re-applied with a different probability each time an image is loaded for training across multiple training epochs. This ensures that the input batches given to the neural network during each epoch are slightly different, making the training process more robust and limiting over-fitting on the training images. We shall denote this step as the function aug : R h?w?3 ? R h?w?3 and X i = aug(X i ) which means X i is a randomly augmented version of image X i . Note that this function is only applied to the images in D T and not in D V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fully Supervised Learning</head><p>The most popular method for training ConvNets for image classification tasks is standard supervised learning. This method trains both the encoder r and classifier c during the same training iteration. We sample batches of size b = 64 from D T and for all the images X i in this batch, we obtain its augmentation X i = aug(X i ). Then, we compute the representation of this augmented image using the ResNet-50 encoder, given by x i = r(X i ). The class probabilities predicted by the classifier are given by ? i = c(x i ) where the probabilities are normalized with softmax activation over n = 10 classes of our dataset. The network is trained to minimize the categorical cross-entropy loss, defined as:</p><formula xml:id="formula_2">L ce = ? ??i y i log(? i )<label>(2)</label></formula><p>where y i is the true label of input image X i . L ce is used to calculate the gradients using the Adam optimizer <ref type="bibr" target="#b25">[26]</ref> to update the parameters of both the encoder and the classifier networks. After hyper-parameter tuning, we settle on using a decaying learning-rate initialized at 0.001, a batch size of 64 and |D T |/10 training steps per iteration for this experiment. The results obtained from this learning method after 10 training epochs are presented in Section VI. The accuracy metrics are calculated based on the class predictions; the silhouette scores and t-SNE embeddings are calculated using the representation x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supervised Contrastive Learning</head><p>In this section, we introduce contrastive learning as an alternate method to train the ConvNet model. Contrastive learning operates with both labelled <ref type="bibr" target="#b20">[21]</ref> and unlabelled <ref type="bibr" target="#b12">[13]</ref> data; since our dataset contains game genre labels we proceed with the supervised variant and compare it to standard supervised learning in order to test and highlight the impact of this training framework in improving generalization in (gamebased) computer vision tasks.</p><p>The contrastive learning framework involves training the functions r and c in two steps using two separate loss functions, unlike the previous method where only one loss function (Eq. (2)) is employed for both. The first pre-training step for the encoder uses a pairwise loss function, namely the contrastive loss <ref type="figure" target="#fig_2">(Eq. (3)</ref>). In this step, we take the representation x i and use a projection network to map it onto a lower-dimensional embedding space which is a hyperspherical manifold of unit radius, as explained in <ref type="figure" target="#fig_4">Fig. 5</ref>. Let's denote this as p : R d ? R 128 so that z i = p(x i ) gives us the embedding of image X i . The function p is also learnt by a fully connected neural network layer, chosen to be of size 128 in our experiments. Then, the supervised contrastive loss can be calculated on a given batch of images as:</p><formula xml:id="formula_3">L con = z i ? z j 2 2 , when y i = y j max(0, m ? z i ? z j 2 ) 2 , when y i = y j<label>(3)</label></formula><p>where m is the margin hyper-parameter and its value is set to m = 1.0 in our experiments. Eq. (3) is the max margin variant of contrastive loss as proposed by Hadsell et al. <ref type="bibr" target="#b26">[27]</ref>. In principle, this function pulls representations of same-class labels closer together on the hyperspherical manifold and pushes apart those that belong to different class labels, as explained in <ref type="figure" target="#fig_4">Fig. 5</ref> using the terms positive pair and negative pair. This arranges the representations so that images of the same label form a compact cluster and the clusters formed by images of different labels are as well separated from this cluster as possible. Hence, the representations learnt by the encoder under this framework are expected to be better organized in the representation space compared to the fully supervised approach. Preliminary experiments tested other variants of the contrastive loss function such as supervised NT-Xent <ref type="bibr" target="#b20">[21]</ref>, triplet <ref type="bibr" target="#b27">[28]</ref> and multi-class n-pairs <ref type="bibr" target="#b28">[29]</ref> but none performed as well as the max margin loss.</p><p>In the second step of this framework, we train the classifier c on the learned representations using the cross-entropy loss (Eq.</p><p>(2)). The projection network p is discarded at this point and the weights of the encoder r are set to be non-trainable since the representations are already well-organized in the Euclidean space due to the pre-training step. At this point, the task of learning c becomes trivial for the classifier. The accuracy and silhouette metrics are calculated for this method similar to the fully supervised method as described in Section V-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>In this section, we first present an objective comparison of the two training methods in terms of classification accuracy and silhouette metrics. Then, we dive into comparing the representation spaces learned by each method and explain its importance towards evaluating generalization. <ref type="table" target="#tab_0">Table II</ref> shows the comparison of classification accuracy results between the two training methods in terms of mean and 95% confidence interval over 5 runs with random seed initialization. We observe that while the fully supervised method achieves higher accuracy on the training data, contrastive learning achieves significantly higher accuracy (p &lt; 0.05) on the validation data. However, this is only a marginal improvement and overall both approaches seem to be able to learn the image classification problem. Additional experiments with different training/validation splits showed similar trends in training and testing accuracies. Merely observing the average accuracy values obtained, there does not seem to be a major advantage of using supervised contrastive learning over the conventional method. Findings appear more interesting and relevant, however, when one compares the representation spaces themselves, instead of their classification capacities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Analysis</head><p>At the start of the training process, we use the pre-trained ResNet model with ImageNet weights. The average silhouette score for this model is ?0.03, which means that the different game genres have poor clustering and the representations of all games are not arranged well in the Euclidean space. We observe, however, that fine-tuning this model with fully supervised training improves this average silhouette score to around 0.22, offering an improvement in the clustering of representations by a significant margin. Fine-tuning the representation via supervised contrastive training offers a much larger (and significant) improvement that reaches silhouette scores of 0.56, on average. This indicates that the clusters of game genres obtained via contrastive learning are much more compact and well separated compared to standard supervised learning. Based on our findings in the Sports10 dataset we conclude that supervised contrastive learning is better suited (compared to fully supervised learning) for solving domain gap problems, thereby making it the preferred method towards generalization in pixel-based game representations.</p><p>Lastly, we want to look into the genre-wise classification accuracy for our best performing model. <ref type="figure" target="#fig_5">Fig. 6</ref> gives us the confusion matrix showcasing the overlap between true label and the predicted label. Based on this matrix, it seems that the two genres Volleyball and Basketball are tougher to classify relative to other genres. Basketball games sometimes get misclassified as Fighting and Bike Racing while Volleyball games are often confused as Basketball or Hockey. This happens when the representations of these genres lie closer to the clusters of other genres than their own, leading to misclassification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Analysis</head><p>In addition to the quantitative results above, we present an alternative qualitative analysis by directly visualizing the representation space and seeing the layout of the clusters for all game genres. <ref type="figure" target="#fig_6">Fig. 7</ref> visualizes the three representation spaces: (a) ImageNet Representations without fine-tuning; (b) Fine-tuning via Fully Supervised Learning; (c) Fine-tuning via Contrastive Learning, using t-SNE on the validation dataset. Note that none of the games in the validation set were seen during training, so these results give a fair indication of how these models will perform on unseen games of these 10 genres. Notably, we observe that the t-SNE analysis is in alignment with the silhouette scores obtained. Without training (see <ref type="figure" target="#fig_6">Fig.  7a</ref>) all 10 game genres are poorly clustered; their clustering improves slightly when fully supervised learning is employed (see <ref type="figure" target="#fig_6">Fig. 7b</ref>) due to fine-tuning. The representations in this space appear to form separate clusters for each genre which are distant to other clusters. Within each cluster, however, we can observe islands of sub-clusters formed by the different games present under that genre label. The observation of <ref type="figure" target="#fig_6">Fig. 7b</ref> indicated that there still exist domain gaps within the genres, albeit to a lesser extent compared to the ImageNet pretrained model. The t-SNE embeddings obtained via contrastive learning (see <ref type="figure" target="#fig_6">Fig. 7c</ref>) showcase the best-formed clusters, while addressing both inter-genre separability and intra-genre compactness. Such a representation space can only be expected to generalize well over any game of these genres and the applications built on top of one game are expected to be easily transferable to other games of that genre.</p><p>VII. APPLICATIONS OF GENERAL REPRESENTATIONS This paper applies representation learning to a broad set of games that follow real-world patterns. Results indicate that contrastive learning can capture the genre-specific visual content while filtering out stylistic differences between games. This opens up a number of interesting directions for applications that benefit from general pixel-based representations.</p><p>An obvious application for generalized pixel-based representations is game playing agents. For example, the current learned representation can be used with the Google Research Football Environment <ref type="bibr" target="#b29">[30]</ref> to initialize the visual encoder of an imitation-or reinforcement-learning agent. With most genre-specific information (such as pattern of football pitch, goalposts) already present in these representations, fine-tuning the task-specific visual information (such as position of players, ball) or learning the control-policy becomes much more sample efficient compared to starting from scratch.</p><p>Procedural content generation (PCG) tends to use gamespecific representations (e.g. tilesets) to produce content (e.g. levels). Therefore, pixel-based general representations are more suited for evaluating the content rather than for explicitly generating it. As an evaluation mechanism, general representations can be very beneficial to e.g. evaluating the typicality <ref type="bibr" target="#b30">[31]</ref> of a generated game compared to similar examples of its genre. Moreover, the general representations can serve for coherence evaluation when combining generators of multiple facets (e.g. visuals, level structures, rules) in order to assess whether the resulting game fits the patterns of a specific genre. This would allow orchestration of game content <ref type="bibr" target="#b31">[32]</ref> not only to avoid nonsensical combinations but also to identify the genre of the generated game and potentially create games of different genres within one run.</p><p>General representations also seem ideally suited for PCG via Machine Learning <ref type="bibr" target="#b32">[33]</ref>. Guzdial et al. <ref type="bibr" target="#b33">[34]</ref> investigate how gameplay videos can be mined for level patterns. Contrastive learning can augment this line of research by detecting general level patterns across multiple games of the same genre and thus generate content for any game of this genre. It should be noted that the tested sports dataset has fairly uniform levels per genre (e.g. the same football pitch in all football games) and thus the level patterns are less critical in terms of content detected. Future work should explore to which extent contrastive learning can be used to detect level patterns in more mechanically diverse games within e.g. the platformer genre.</p><p>Another important application is player modeling <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Work on identifying highlights in gameplay videos <ref type="bibr" target="#b35">[36]</ref> could benefit from concise, genre-specific representations for e.g. classification purposes. Moreover, such general representations can be used to model affective responses based on game footage alone, extending current work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> that trains custom ConvNets for each game. As larger and more mixed-genre datasets for players' affect become available <ref type="bibr" target="#b36">[37]</ref>, applying contrastive learning for pre-processing to remove stylistic differences will be crucial.</p><p>Finally, the general representations can be utilized to learn game dynamics with pixel-based forward models. GameGAN <ref type="bibr" target="#b4">[5]</ref> is a good example of learning a physics engine of Pac-Man (Namco 1980) that predicts future game states at pixel-level based on user inputs. Such forward models could be shared across different games of the same genre if they are built using generalized representations. This can massively reduce the workload of creating new game state representations (as well as gameplaying agents) for genres that already have a neural engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this study we introduced the use of contrastive learning as a method for yielding general game representations with the aim of improving their reusability across different games without requiring re-training. We test our hypothesis that contrastive learning is beneficial for pixel-based representation learning on a new dataset, named Sports10, that contains 100k labelled images from 175 games across 10 sports genres. Our experimental results suggest that contastive learning outperforms conventional supervised learning in classifying game genres. More importantly for the purposes of this paper, the representations learned via contrastive learning yield more compact clusters of game representations belonging to the same genre which are, in turn, better separated from clusters of other genres. It appears that learning through contrasting game images leads to fewer domain gap issues compared to the representations learned under conventional supervised learning. The methods and results of the paper form a basis for further research on areas of game AI <ref type="bibr" target="#b0">[1]</ref>: from gameplaying agents and game world models, all the way to pixel-based procedural content generation and player modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Examples of the domain gap problem observed when comparing different games of the same genre. The scatterplots (right) highlight the intra-genre domain discrepancies with the help of t-SNE visualization of Imagenet-ResNet50 feature vectors on screenshots of various games (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The causality framework for generalization in games. Solid lines represent a causal dependency and dashed lines represent flow of data. For a given game belonging to one of the genres (y 1..n ), the graph showcases the relationship of style (S) and content (C) of that game's image (X) with its learned representation and the predicted genre category (?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>A subset of the Sports10 Dataset showcasing the variation in graphic styling of different games across each of the 10 sports genres selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of the different image augmentation techniques used with associated random probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Supervised Contrastive Learning Framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Confusion Matrix in terms of validation accuracy (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Two dimensional projections of t-SNE embeddings calculated on a random sample taken from the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Summary of total games per genre in the dataset and their distribution across the different visual styling categories.</figDesc><table><row><cell>Game Genre</cell><cell>Retro</cell><cell>Modern</cell><cell cols="2">Photoreal Total</cell></row><row><cell>American Football</cell><cell>2</cell><cell>11</cell><cell>6</cell><cell>19</cell></row><row><cell>Basketball</cell><cell>3</cell><cell>12</cell><cell>3</cell><cell>18</cell></row><row><cell>Bike Racing</cell><cell>8</cell><cell>7</cell><cell>4</cell><cell>19</cell></row><row><cell>Car Racing</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>15</cell></row><row><cell>Fighting</cell><cell>3</cell><cell>11</cell><cell>9</cell><cell>23</cell></row><row><cell>Hockey</cell><cell>9</cell><cell>7</cell><cell>1</cell><cell>17</cell></row><row><cell>Soccer</cell><cell>7</cell><cell>8</cell><cell>2</cell><cell>17</cell></row><row><cell>Table Tennis</cell><cell>3</cell><cell>10</cell><cell>5</cell><cell>18</cell></row><row><cell>Tennis</cell><cell>6</cell><cell>4</cell><cell>2</cell><cell>12</cell></row><row><cell>Volleyball</cell><cell>6</cell><cell>9</cell><cell>2</cell><cell>17</cell></row><row><cell>Total</cell><cell>52</cell><cell>84</cell><cell>39</cell><cell>175</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Fine-tuning the pre-trained models on the Sports10 dataset after 10 epochs. We present average values across 5 runs and corresponding 95% confidence intervals. ? 0.08 90.41 ? 1.53 0.22 ? 0.01 Supervised Contrastive 91.83 ? 0.39 93.42 ? 0.70 0.56 ? 0.01</figDesc><table><row><cell>Learning Method</cell><cell>Training Accuracy</cell><cell>Validation Accuracy</cell><cell>Silhouette Score</cell></row><row><cell>Pre-Trained (ImageNet)</cell><cell>-</cell><cell>-</cell><cell>-0.03 ? 0.01</cell></row><row><cell>Fully Supervised</cell><cell>99.64</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at github.com/ChintanTrivedi/contrastive-game-representations</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Artificial intelligence and games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">World models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10122</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Agent57: Outperforming the atari human benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13350</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to simulate dynamic environments with gamegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for procedural content generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From pixels to affect: A study on games and player experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pixels and sounds of emotion: General-purpose representations of arousal in games</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conf. on computer vision and pattern recognition</title>
		<meeting>of the IEEE conf. on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crawling, indexing, and retrieving moments in videogames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FDG</title>
		<meeting>of FDG</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ja?kowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CIG</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforcement learning with augmented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 34th conf. on Neural Information Processing Systems</title>
		<meeting>of the 34th conf. on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Player experience extraction from gameplay video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entity embedding as game representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Khameneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Experimental AI in Games workshop</title>
		<meeting>of the Experimental AI in Games workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07922</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Out of distribution generalization in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02667</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 30th Intl. conf. on Neural Information Processing Systems</title>
		<meeting>of the 30th Intl. conf. on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raichuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stanczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zajac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11180</idno>
		<title level="m">Google research football: A novel reinforcement learning environment</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Some empirical criteria for attributing creativity to a computer program</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Orchestrating game generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Preuss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bidarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Games</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Procedural content generation via machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Summerville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Snodgrass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holmg?rd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Isaksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nealen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Games</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward game level generation from gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guzdial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FDG Workshop on PCG</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Player modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Spronck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Loiacono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial and Computational Intelligence in Games. Dagstuhl Follow-Ups</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep unsupervised multi-view detection of video game stream highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Foundations of Digital Games</title>
		<meeting>of the Foundations of Digital Games</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Melhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liapis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Yannakakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02643</idno>
		<title level="m">The Affect Game AnnotatIoN (AGAIN) dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
