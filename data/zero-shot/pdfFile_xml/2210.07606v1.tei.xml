<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Heterophily For Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Luan</surname></persName>
							<email>sitao.luan@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
							<email>chenqing.hua@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qincheng</forename><surname>Lu</surname></persName>
							<email>qincheng.lu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zhu</surname></persName>
							<email>jiaqi.zhu@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
							<email>mingde.zhao@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Zhang</surname></persName>
							<email>shuyuan.zhang@mail</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
							<email>chang@cs</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mila</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Heterophily For Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using graph structures based on the relational inductive bias (homophily assumption). While GNNs have been commonly believed to outperform NNs in real-world tasks, recent work has identified a non-trivial set of datasets where their performance compared to NNs is not satisfactory. Heterophily has been considered the main cause of this empirical observation and numerous works have been put forward to address it. In this paper, we first revisit the widely used homophily metrics and point out that their consideration of only graph-label consistency is a shortcoming. Then, we study heterophily from the perspective of post-aggregation node similarity and define new homophily metrics, which are potentially advantageous compared to existing ones. Based on this investigation, we prove that some harmful cases of heterophily can be effectively addressed by local diversification operation. Then, we propose the Adaptive Channel Mixing (ACM), a framework to adaptively exploit aggregation, diversification and identity channels node-wisely to extract richer localized information for diverse node heterophily situations. ACM is more powerful than the commonly used uni-channel framework for node classification tasks on heterophilic graphs and is easy to be implemented in baseline GNN layers. When evaluated on 10 benchmark node classification tasks, ACM-augmented baselines consistently achieve significant performance gain, exceeding state-of-theart GNNs on most tasks without incurring significant computational burden.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Networks (NNs) <ref type="bibr" target="#b21">[22]</ref> have revolutionized many machine learning areas, including image recognition <ref type="bibr" target="#b20">[21]</ref>, speech recognition <ref type="bibr" target="#b12">[13]</ref> and natural language processing <ref type="bibr" target="#b1">[2]</ref>, due to their effectiveness in learning latent representations from Euclidean data. Recent research has shifted focus on non-Euclidean data <ref type="bibr" target="#b5">[6]</ref>, e.g., relational data or graphs. Combining graph signal processing and convolutional neural networks <ref type="bibr" target="#b22">[23]</ref>, numerous Graph Neural Network (GNN) architectures have been proposed <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, which empirically outperform traditional NNs on graph-based machine learning tasks such as node classification, graph classification, link prediction and graph generation, etc.GNNs are built on the homophily assumption <ref type="bibr" target="#b33">[34]</ref>: connected nodes tend to share similar attributes with each other <ref type="bibr" target="#b13">[14]</ref>, which offers additional information besides node features. This relational inductive bias <ref type="bibr" target="#b2">[3]</ref> is believed to be a key factor leading to GNNs' superior performance over NNs' in many tasks.</p><p>However, growing empirical evidence suggests that GNNs are not always advantageous compared to traditional NNs. In some cases, even simple Multi-Layer Perceptrons (MLPs) can outperform GNNs by a large margin on relational data <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8]</ref>. An important reason for this is believed to be the heterophily problem: the homophily assumption does not always hold, so connected nodes may in fact have different attributes. Heterophily has received lots of attention recently and an increasing number of models have been put forward to address this problem <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper, we first show that by only considering graph-label consistency, existing homophily metrics 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2210.07606v1 <ref type="bibr">[cs.</ref>LG] 14 Oct 2022 are not able to describe the effect of some cases of heterophily on aggregation-based GNNs. We propose a post-aggregation node similarity matrix, and based on it, we derive new homophily metrics, whose advantages are illustrated on synthetic graphs (Sec. 3). Then, we prove that diversification operation can help to address some harmful cases of heterophily (Sec. 4). Based on this, we propose the Adaptive Channel Mixing (ACM) GNN framework which augments uni-channel baseline GNNs, allowing them to exploit aggregation, diversification and identity channels adaptively, node-wisely and locally in each layer. ACM significantly boosts the performance of 3 uni-channel baseline GNNs by 2.04% ? 27.5% for node classification tasks on 7 widely used benchmark heterophilic graphs, exceeding SOTA models (Sec. 6) on all of them. For 3 homophilic graphs, ACM-augmented GNNs can perform at least as well as the uni-channel baselines and are competitive compared with SOTA.</p><p>Contributions 1. To our knowledge, we are the first to analyze heterophily from post-aggregation node similarity perspective. 2. The proposed ACM framework is highly different from adaptive filterbank with multiple channels and existing GNNs for heterophily: 1) the traditional adaptive filterbank channels <ref type="bibr" target="#b38">[39]</ref> uses a scalar weight for each filter and this weight is shared by all nodes. In contrast, ACM provides a mechanism so that different nodes can learn different weights to utilize information from different channels to account for diverse local heterophily; 2) Unlike existing methods that leverage the high-order filters and global property of high-frequency signals <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> which require more computational resources, ACM successfully addresses heterophily by considering only the nodewise local information adaptively. 3. Unlike existing methods that try to facilitate learning filters with high expressive power <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, ACM aims that, when given a filter with certain expressive power, we can extract richer information from additional channels in a certain way to address heterophily. This makes ACM more flexible and easier to be implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we introduce notation and background knowledge. We use bold font for vectors (e.g., v). Suppose we have an undirected connected graph G = (V, E, A), where V is the node set with |V| = N ; E is the edge set without self-loops; A ? R N ?N is the symmetric adjacency matrix with A i,j = 1 if e ij ? E, otherwise A i,j = 0. Let D denote the diagonal degree matrix of G, i.e., D i,i = d i = j A i,j . Let N i denote the neighborhood set of node i, i.e., N i = {j : e ij ? E}. A graph signal is a vector x ? R N defined on V, where x i is associated with node i. We also have a feature matrix X ? R N ?F , whose columns are graph signals and whose i-th row X i,: is a feature vector of node i. We use Z ? R N ?C to denote the label encoding matrix, whose i-th row Z i,: is the one-hot encoding of the label of node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Laplacian, Affinity Matrix and Variants</head><p>The (combinatorial) graph Laplacian is defined as L = D ? A, which is Symmetric Positive Semi-Definite (SPSD) <ref type="bibr" target="#b8">[9]</ref>. Its eigendecomposition is L = U ?U T , where the columns u i of U ? R N ?N are orthonormal eigenvectors, namely the graph Fourier basis, ? = diag(? 1 , . . . , ? N ) with ? 1 ? ? ? ? ? ? N . These eigenvalues are also called frequencies.</p><p>In additional to L, some variants are also commonly used, e.g., the symmetric normalized Laplacian L sym = D ?1/2 LD ?1/2 = I ? D ?1/2 AD ?1/2 and the random walk normalized Laplacian L rw = D ?1 L = I ? D ?1 A. The graph Laplacian and its variants can be considered as high-pass filters for graph signals. The affinity (transition) matrices can be derived from the Laplacians, e.g., A rw = I ? L rw = D ?1 A, A sym = I ? L sym = D ?1/2 AD ?1/2 and are considered to be low-pass filters <ref type="bibr" target="#b32">[33]</ref>. Their eigenvalues satisfy ? i (A rw ) = ? i (A sym ) = 1 ? ? i (L sym ) = 1 ? ? i (L rw ) ? (?1, 1]. Applying the renormalization trick <ref type="bibr" target="#b18">[19]</ref> to affinity and Laplacian matrices respectively leads t? A sym =D ?1/2?D?1/2 andL sym = I ?? sym , where? ? A + I andD ? D + I. The renormalized affinity matrix essentially adds a self-loop to each node in the graph, and is widely used in Graph Convolutional Network (GCN) <ref type="bibr" target="#b18">[19]</ref> as follows:</p><formula xml:id="formula_0">Y = softmax(? sym ReLU(? sym XW 0 ) W 1 )<label>(1)</label></formula><p>where W 0 ? R F ?F1 and W 1 ? R F1?O are learnable parameter matrices. GCNs can be trained by minimizing the following cross entropy loss</p><formula xml:id="formula_1">L = ?trace(Z T log Y )<label>(2)</label></formula><p>where log(?) is a component-wise logarithm operation. The random walk renormalized matrix A rw =D ?1? , which shares the same eigenvalues as? sym , can also be applied in GCN. The corresponding Laplacian is defined asL rw = I ?? rw . The matrix? rw is essentially a random walk matrix and behaves as a mean aggregator that is applied in spatial-based GNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. To bridge spectral and spatial methods, we use? rw in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metrics of Homophily</head><p>The homophily metrics are defined by considering different relations between node labels and graph structures. There are three commonly used homophily metrics: edge homophily <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref>, node homophily <ref type="bibr" target="#b34">[35]</ref> and class homophily <ref type="bibr" target="#b25">[26]</ref> 1 , defined as follows:</p><formula xml:id="formula_2">H edge (G) = {e uv | e uv ? E, Z u,: = Z v,: } |E| , H node (G) = 1 |V| v?V H v node = 1 |V| v?V {u | u ? N v , Z u,: = Z v,: } d v , H class (G) = 1 C ?1 C k=1 h k ? {v | Z v,k = 1} N + , h k = v?V {u | Z v,k = 1, u ? N v , Z u,: = Z v,: } v?{v|Z v,k =1} d v (3) where H v</formula><p>node is the local homophily value for node v; [a] + = max(a, 0); h k is the class-wise homophily metric <ref type="bibr" target="#b25">[26]</ref>. All metrics are in the range of [0, 1]; a value close to 1 corresponds to strong homophily, while a value close to 0 indicates strong heterophily. H edge (G) measures the proportion of edges that connect two nodes in the same class; H node (G) evaluates the average proportion of edge-label consistency of all nodes; H class (G) tries to avoid sensitivity to imbalanced classes, which can make H edge (G) misleadingly large. The above definitions are all based on the linear featureindependent graph-label consistency. The inconsistency relation is implied to have a negative effect to the performance of GNNs. With this in mind, in the following section, we give an example to illustrate the shortcomings of the above metrics and propose new feature-independent metrics that are defined from post-aggregation node similarity perspective, which is novel. Heterophily is widely believed to be harmful for message-passing based GNNs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref> because, intuitively, features of nodes in different classes will be falsely mixed, leading nodes to be indistinguishable <ref type="bibr" target="#b44">[45]</ref>. Nevertheless, it is not always the case, e.g., the bipartite graph 2 shown in <ref type="figure" target="#fig_11">Figure 1</ref> is highly heterophilic according to the existing homophily metrics in equation 3, but after mean aggregation, the nodes in classes 1 and 2 just exchange colors and are still distinguishable <ref type="bibr" target="#b2">3</ref> . This example tells us that, besides graph-label consistency, we need to study the relation between nodes after aggregation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Analysis of Heterophily</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Aggregation Homophily</head><p>To this end, we first define the post-aggregation node similarity matrix as follows:</p><formula xml:id="formula_3">S(?, X) ??X(?X) T ? R N ?N<label>(4)</label></formula><p>where? ? R N ?N denotes a general aggregation operator. S(?, X) is essentially the gram matrix that measures the similarity between each pair of aggregated node features.</p><p>Relationship Between S(?, X) and Gradient of SGC SGC <ref type="bibr" target="#b40">[41]</ref> is one of the most simple but representative GNN models and its output can be written as:</p><formula xml:id="formula_4">Y = softmax(?XW ) = softmax(Y )<label>(5)</label></formula><p>With the loss function in equation 2, after each gradient descent step, we have ?W = ? dL dW , where ? is the learning rate. The update of Y is (see Appendix E for derivation):</p><formula xml:id="formula_5">?Y =?X?W = ??X dL dW ??X dL dW =?XX T?T (Z ? Y ) = S(?, X)(Z ? Y )<label>(6)</label></formula><p>where Z ? Y is the prediction error matrix. The update direction of the prediction for node i is essentially a weighted sum of the prediction error, i.e., ?(Y ) i,: = j?V S(?, X) i,j (Z ? Y ) j,:</p><p>and S(?, X) i,j can be considered as the weights. Intuitively, a high similarity value S(?, X) i,j means node i tends to be updated to the same class as node j. This indicates that S(?, X) is closely related to a single layer GNN model.</p><p>Based on the above definition and observation, we define the aggregation similarity score as follows. Definition 1. The aggregation similarity score is:</p><formula xml:id="formula_6">S agg S(?, X) = 1 |V| v Mean u {S(?, X) v,u |Z u,: = Z v,: } ? Mean u {S(?, X) v,u |Z u,: = Z v,: }<label>(7)</label></formula><p>where Mean u ({?}) takes the average over u of a given multiset of values or variables.</p><p>S agg (S(?, X)) measures the proportion of nodes v ? V as which the average weights on the set of nodes in the same class (including v) is larger than that in other classes. In practice, we observe that in most datasets, we will have S agg (S(?, X)) ? 0.5 4 . To make the metric range in [0,1], like existing metrics, we rescale equation 7 to the following modified aggregation similarity,</p><formula xml:id="formula_7">S M agg S(?, X) = 2S agg S(?, X) ? 1 +<label>(8)</label></formula><p>In order to measure the consistency between labels and graph structures without considering node features and to make a fair comparison with the existing homophily metrics in equation 3, we define the graph (G) aggregation (?) homophily and its modified version 5 as:</p><formula xml:id="formula_8">H agg (G) = S agg S(?, Z) , H M agg (G) = S M agg S(?, Z)<label>(9)</label></formula><p>As the example shown in <ref type="figure" target="#fig_11">Figure 1</ref>, when? =? rw , it is easy to see that H agg (G) = H M agg (G) = 1 and other metrics are 0. Thus, this new metric reflects the fact that nodes in classes 1 and 2 are still highly distinguishable after aggregation, while other metrics mentioned before fail to capture such information and misleadingly give value 0. This shows the advantage of H agg (G) and H M agg (G), which additionally exploit information from aggregation operator? and the similarity matrix.</p><p>To comprehensively compare H M agg (G) with the existing metrics on their ability to elucidate the influence of graph structure on GNN performance, we generate synthetic graphs with different homophily levels and evaluate SGC <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on them in the next subsection.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical Evaluation and Comparison on Synthetic Graphs</head><p>In this subsection, we conduct experiments on synthetic graphs generated with different levels of H M edge (G) to assess the output of H M agg (G) in comparison with existing metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generation &amp; Experimental Setup</head><p>We first generated 10 graphs for each of 28 edge homophily levels, from 0.005 to 0.95, for a total of 280 graphs. In every generated graph, we had 5 classes, with 400 nodes in each class. For nodes in each class, we randomly generated 800 intra-class edges and [ 800 Hedge(G) ?800] inter-class edges. The features of nodes in each class are sampled from node features in the corresponding class of 6 base datasets (Cora, CiteSeer, PubMed, Chameleon, Squirrel, Film). Nodes were randomly split into train/validation/test sets, in proportion of 60%/20%/20%. We trained 1-hop SGC (sgc-1) <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on the synthetic graphs <ref type="bibr" target="#b5">6</ref> . For each value of H edge (G), we take the average test accuracy and standard deviation of runs over the 10 generated graphs with that value. For each generated graph, we also calculate H node (G), H class (G) and H M agg (G). Model performance with respect to different homophily values is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Homophily Metrics</head><p>The performance of SGC-1 and GCN is expected to be monotonically increasing if the homophily metric is informative. However, <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Channel Mixing (ACM)</head><p>In prior work <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref>, it has been shown that high-frequency graph signals, which can be extracted by a high-pass filter (HP), is empirically useful for addressing heterophily. In this section, based on the similarity matrix in equation 6, we theoretically prove that a diversification operation, i.e., HP filter, can address some cases of harmful heterophily locally. Besides, a node-wise analysis shows that different nodes may need different filters to process their neighborhood information. Based on the above analysis, in Sec. 4.2 we propose Adaptive Channel Mixing (ACM), a 3-channel architecture which can adaptively exploit local and node-wise information from aggregation, diversification and identity channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Diversification Helps with Harmful Heterophily</head><p>We first consider the example shown in <ref type="figure" target="#fig_3">Figure 3</ref>. From S(?, X), we can see that nodes {1, 3} assign relatively large positive weights to nodes in class 2 after aggregation, which will make nodes {1, 3} hard to be distinguished from nodes in class 2. However, we can still distinguish nodes {1, 3} and {4, 5, 6, 7} by considering their neighborhood differences: nodes {1, 3} are different from most of their neighbors while nodes {4, 5, 6, 7} are similar to most of their neighbors. This indicates that although some nodes become similar after aggregation, they are still distinguishable through their local surrounding dissimilarities.</p><p>This observation leads us to introduce the diversification operation, i.e., HP filter I ?? <ref type="bibr" target="#b10">[11]</ref> to extract information regarding neighborhood differences, thereby addressing harmful heterophily. As S(I ??, X) in <ref type="figure" target="#fig_3">Fig. 3</ref> shows, nodes {1, 3} will assign negative weights to nodes {4, 5, 6, 7} after the diversification operation, i.e., nodes 1,3 treat nodes 4,5,6,7 as negative samples and will move away from them during backpropagation. This example reveals that there are cases in which the diversification operation is helpful to handle heterophily, while the aggregation operation is not. Based on this observation, we first define the diversification distinguishability of a node and the graph diversification distinguishability value, which measures the proportion of nodes for which the diversification operation is potentially helpful. Given S(I ??, X), a node v is diversification distinguishable if the following two conditions are satisfied at the same time,</p><formula xml:id="formula_9">1. Mean u {S(I ??, X) v,u |u ? V ? Z u,: = Z v,: } ? 0; 2. Mean u {S(I ??, X) v,u |u ? V ? Z u,: = Z v,: } ? 0 (10)</formula><p>Then, graph diversification distinguishability value is defined as</p><formula xml:id="formula_10">DD? ,X (G) = 1 |V| {v|v ? V ? v is diversification distinguishable}<label>(11)</label></formula><p>We can see that DD? ,X (G) ? [0, 1]. Based on Def. 2, the effectiveness of diversification in addressing heterophily can be theoretically proved under certain conditions:</p><formula xml:id="formula_11">Theorem 1. (See Appendix G for proof). For C = 2, suppose X = Z,? =? rw .</formula><p>Then for any I ?? rw , all nodes are diversification distinguishable and DD? ,Z (G) = 1.</p><p>With the above results for HP filters, we will now introduce the concept of filterbank which combines both LP (aggregation) and HP (diversification) filters and can potentially handle various local heterophily cases. We then develop ACM framework in the following subsection.  Filterbank For the graph signal x defined on G, a 2-channel linear (analysis) filterbank <ref type="bibr" target="#b10">[11]</ref> 8 includes a pair of filters H LP , H HP , which retain the low-frequency and high-frequency content of x, respectively. Most existing GNNs use a uni-channel filtering architecture <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15]</ref> with either LP or HP channel, which only partially preserves the input information. Unlike the uni-channel architecture, filterbanks with H LP + H HP = I do not lose any information from the input signal, which is called the perfect reconstruction property <ref type="bibr" target="#b10">[11]</ref>. Generally, the Laplacian matrices (L sym , L rw ,L sym ,L rw ) can be regarded as HP filters <ref type="bibr" target="#b10">[11]</ref> and affinity matrices (A sym , A rw ,? sym , A rw ) can be treated as LP filters <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, we extend the concept of filterbank and view MLPs as using the identity (fullpass) filterbank with H LP = I and H HP = 0, which also satisfies H LP + H HP = I + 0 = I.</p><p>Node-wise Channel Mixing for Diverse Local Homophily The example in <ref type="figure" target="#fig_3">Figure 3</ref> also shows that different nodes may need the local information extracted from different channels, e.g., nodes {1, 3} demand information from the HP channel while node 2 only needs information from the LP channel. <ref type="figure" target="#fig_7">Figure 4</ref> reveals that nodes have diverse distributions of node local homophily H v node across different datasets. In order to adaptively leverage the LP, HP and identity channels in GNNs to deal with the diverse local heterophily situations, we will now describe our proposed Adaptive Channel Mixing (ACM) framework.</p><p>Adaptive Channel Mixing (ACM) We will use GCN 9 as an example to introduce the ACM framework in matrix form, but the framework can be combined in a similar manner to many different GNNs. The ACM framework includes the following steps:</p><p>Step 1. Feature Extraction for Each Channel:</p><formula xml:id="formula_12">Option 1: H l L = ReLU H LP H l?1 W l?1 L , H l H = ReLU H HP H l?1 W l?1 H , H l I = ReLU IH l?1 W l?1 I ; Option 2: H l L = H LP ReLU H l?1 W l?1 L , H l H = H HP ReLU H l?1 W l?1 H , H l I = I ReLU H l?1 W l?1 I ; H 0 = X ? R N ?F0 , W l?1 L , W l?1 H , W l?1 I ? R F l?1 ?F l , l = 1, . . . , L; Step 2. Row-wise Feature-based Weight Learning ? l L = Sigmoid H l LW l L ,? l H = Sigmoid H l HW l H ,? l I = Sigmoid H l IW l I ,W l?1 L ,W l?1 H ,W l?1 I ? R F l ?1 ? l L , ? l H , ? l I = Softmax ( ? l L ,? l H ,? l I /T )W l Mix ? R N ?3 , T ? R temperature, W l Mix ? R 3?3 ;</formula><p>Step 3. Node-wise Adaptive Channel Mixing:</p><formula xml:id="formula_13">H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I</formula><p>We will refer to the instantiation which uses option 1 in step 1 as ACM and to the one using option 2 as ACMII. In step 1, ACM(II)-GCN implement different feature extractions for 3 channels using a set of filterbanks. Three filtered components, H l L , H l H , H l I , are obtained. To adaptively exploit information from each channel, ACM(II)-GCN first extract nonlinear information from the filtered signals, then use W l Mix to learn which channel is important for each node, leading to the row-wise weight vectors ? l L , ? l H , ? l I ? R N ?1 whose i-th elements are the weights for node i 10 . These three vectors are then used as weights in defining the updated H l in step 3.</p><p>Complexity The number of learnable parameters in layer l of ACM(II)-GCN is 3F l?1 (F l + 1) + 9, compared to F l?1 F l in GCN. The computation of steps 1-3 takes N F l (8 + 6F l?1 ) + 2F l (nnz(H LP ) + nnz(H HP )) + 18N flops, while the GCN layer takes 2N F l?1 F l + 2F l (nnz(H LP )) flops, where nnz(?) is the number of non-zero elements. An ablation study and a detailed comparison on running time are conducted in Sec. 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations of Diversification</head><p>Like any other method, there exists some cases of harmful heterophily that diversification operation cannot work well. For example, suppose we have an imbalanced dataset where several small clusters with distinctive labels are densely connected to a large cluster. In this case, the surrounding differences of nodes in small clusters are similar, i.e., the neighborhood differences mainly come from their connections to the same large cluster, and this can lead to the diversification operation failing to discriminate them. See Appendix H for a more detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We now discuss relevant work on addressing heterophily in GNNs. <ref type="bibr" target="#b0">[1]</ref> acknowledges the difficulty of learning on graphs with weak homophily and propose MixHop to extract features from multi-hop neighborhoods to get more information. <ref type="bibr" target="#b16">[17]</ref> propose measurements based on feature smoothness and label smoothness that are potentially helpful to guide GNNs when dealing with heterophilic graphs. Geom-GCN <ref type="bibr" target="#b34">[35]</ref> precomputes unsupervised node embeddings and uses the graph structure defined by geometric relationships in the embedding space to define the bi-level aggregation process to handle heterophily. H 2 GCN [45] combines 3 key designs to address heterophily: (1) ego-and neighbor-embedding separation; (2) higher-order neighborhoods; (3) combination of intermediate representations. CPGNN <ref type="bibr" target="#b43">[44]</ref> models label correlations through a compatibility matrix, which is beneficial for heterophilic graphs, and propagates a prior belief estimation into the GNN by using the compatibility matrix. FAGCN <ref type="bibr" target="#b3">[4]</ref> learns edge-level aggregation weights as GAT <ref type="bibr" target="#b39">[40]</ref> but allows the weights to be negative, which enables the network to capture high-frequency components in the graph signals. GPRGNN <ref type="bibr" target="#b7">[8]</ref> uses learnable weights that can be both positive and negative for feature propagation. This allows GPRGNN to adapt to heterophilic graphs and to handle both high-and low-frequency parts of the graph signals (See Appendix J for a more comprehensive comparison between ACM-GNNs, ACMII-GNNs and FAGCN, GPRGNN). BernNet <ref type="bibr" target="#b15">[16]</ref> designs a scheme to learn arbitrary graph spectral filters with Bernstein polynomial to address heterophily. <ref type="bibr" target="#b31">[32]</ref> points out that homophily is not necessary for GNNs and characterizes conditions that GNNs can perform well on heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Evaluation</head><p>In this section, we evaluate the proposed ACM and ACMII framework on real-world datasets (see Appendix D.2 for a performance comparison with basline models on synthetic datasets). We first conduct ablation studies in Sec. 6.1 to validate the effectiveness and efficiency of different components of ACM and ACMII. Then, we compare with state-of-the-art (SOTA) models in Sec. 6.2. The hyperparameter searching range and computing resources are described in Appendix C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study &amp; Efficiency</head><p>We will now investigate the effectiveness and efficiency of adding HP, identity channels and the adaptive mixing mechanism in the proposed framework by performing an ablation study. Specifically, we apply the components of ACM to SGC-1 <ref type="bibr" target="#b40">[41]</ref>  <ref type="bibr" target="#b10">11</ref> and the components of ACM and ACMII to GCN <ref type="bibr" target="#b18">[19]</ref> separately. We run 10 times on each of the 9 benchmark datatsets, Cornell, Wisconsin, Texas, Film, Chameleon, Squirrel, Cora, Citeseer and Pubmed used in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, with the same 60%/20%/20% random splits for train/validation/test used in <ref type="bibr" target="#b7">[8]</ref> and report the average test accuracy as well as the standard deviation. We also record the average running time per epoch (in milliseconds) to compare the computational efficiency. We set the temperature T in equation 4.2 to be 3, which is the number of channels.</p><p>The results in <ref type="table">Table 1</ref> show that on most datasets, the additional HP and identity channels are helpful, even for strong homophily datasets such as Cora, CiteSeer and PubMed. The adaptive mixing mechanism also has an advantage over directly adding the three channels together. This illustrates the necessity of learning to customize the channel usage adaptively for different nodes. The t-SNE visualization in <ref type="figure" target="#fig_9">Figure 5</ref> demonstrates that the high-pass channel(e) and identity channel(f) can extract meaningful patterns, which the low-pass channel(d) is not able to capture. The output of ACM-GCN(c) shows clearer boundaries among classes than GCN(b). The running time is approximately doubled in the ACM and ACMII framework compared to the original models.  <ref type="table">Table 1</ref>: Ablation study on 9 real-world datasets <ref type="bibr" target="#b34">[35]</ref>. Cell with means the component is applied to the baseline model. The best test results are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with Baseline and SOTA Models Datasets &amp; Experimental Setup</head><p>In this section, we evaluate SGC <ref type="bibr" target="#b40">[41]</ref> with 1 hop and 2 hops (SGC-1, SGC-2), GCNII <ref type="bibr" target="#b6">[7]</ref>, GCNII * <ref type="bibr" target="#b6">[7]</ref>, GCN <ref type="bibr" target="#b18">[19]</ref> and snowball networks <ref type="bibr" target="#b28">[29]</ref> with 2 and 3 layers (snowball-2, snowball-3) and combine them with the ACM or ACMII framework 12 . We use? rw as the LP filter and the corresponding HP filter is I ?? rw 13 . Both filters are deterministic. We compare these approaches with several baselines and SOTA GNN models: MLP with 2 layers (MLP-2), GAT <ref type="bibr" target="#b39">[40]</ref>, APPNP <ref type="bibr" target="#b19">[20]</ref>, GPRGNN <ref type="bibr" target="#b7">[8]</ref>, H 2 GCN [45], MixHop <ref type="bibr" target="#b0">[1]</ref>, GCN+JK <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26]</ref>, GAT+JK <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26]</ref>, FAGCN <ref type="bibr" target="#b3">[4]</ref>, GraphSAGE <ref type="bibr" target="#b14">[15]</ref>, Geom-GCN <ref type="bibr" target="#b34">[35]</ref> and BernNet <ref type="bibr" target="#b15">[16]</ref>. In addition to the 9 benchmark datasets used in section 6.1, we further test the above models on a new benchmark dataset, Deezer-Europe <ref type="bibr" target="#b36">[37]</ref>  <ref type="bibr" target="#b13">14</ref> .</p><p>On each dataset used in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we test the models 10 times following the same early stopping strategy, the same 60%/20%/20% random data split <ref type="bibr" target="#b14">15</ref> and Adam <ref type="bibr" target="#b17">[18]</ref> optimizer as used in GPRGNN <ref type="bibr" target="#b7">[8]</ref>. For Deezer-Europe, we test the above models 5 times with the same early stopping strategy, the same fixed splits and Adam used in <ref type="bibr" target="#b25">[26]</ref>.</p><p>Structure information channel and residual connection Besides the filtered features, some recent SOTA models additionally use graph structure information, i.e., MLP ? (A), and residual connection to address heterophily problem, e.g., LINKX <ref type="bibr" target="#b24">[25]</ref> and GloGNN <ref type="bibr" target="#b23">[24]</ref>. MLP ? (A) and residual connection can be directly incorporated into ACM and ACMII framework, which leads us to ACM(II)-GCN+ and ACM(II)-GCN++. See the details of implementation in Appendix B.</p><p>To visualize the performance, in <ref type="figure" target="#fig_10">Fig. 6</ref>, we plot the bar charts of the test accuracy of SOTA models, three selected baselines (GCN, snowball-2, snowball-3), their ACM(II) augmented models, ACM(II)- GCN+ and ACM(II)-GCN++ on the 6 most commonly used benchmark heterophily datasets (See <ref type="table" target="#tab_11">Table 2</ref> in Appendix A.1 for the full results, comparison and ranking). From <ref type="figure" target="#fig_10">Fig. 6</ref>, we can see that (1) after being combined with the ACM or ACMII framework, the performance of the three baseline models is significantly boosted, by 2.04%?27.50% on all the 6 tasks. The ACM and ACMII in fact achieve SOTA performance. (2) On Cornell, Wisconsin, Texas, Chameleon and Squirrel, the augmented baseline models significantly outperform the current SOTA models. Overall, these results suggest that the proposed approach can help GNNs to generalize better on node classification tasks on heterophilic graphs, without adding too much computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Limitations</head><p>We have presented an analysis of existing homophily metrics and proposed new metrics which are more informative in terms of correlating with GNN performance. To our knowledge, this is the first work analyzing heterophily from the perspective of post-aggregation node similarity. The similarity matrix and the new metrics we defined mainly capture linear feature-independent relationships of each node. This might be insufficient when nonlinearity and feature-dependent information is important for classification. In the future, it would be useful to investigate if a similarity matrix could be defined which is capable of capturing nonlinear and feature-dependent relations between aggregated node.</p><p>We have also proposed a multi-channel mixing mechanism which leverages the intuitions gained in the first part of the paper and can be combined with different GNN architectures, enabling adaptive filtering (high-pass, low-pass or identity) at different nodes. Empirically, this approach shows very promising results, improving the performance of the base GNNs with which it is combined and achieving SOTA results at the cost of a reasonable increase in computation time. As discussed in Sec. 4.2, however, the filterbank method cannot properly handle all cases of harmful heterophily, and alternative ideas should be explored as well in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Comparison with SOTA Models on 60%/20%/20% Random Splits</head><p>The main results of the full sets of experiments <ref type="bibr" target="#b15">16</ref> with statistics of datasets are summarized in <ref type="table" target="#tab_11">Table  2</ref>, where we report the mean accuracy (%) and standard deviation. We can see that after applied in ACM or ACMII framework, the performance of baseline models are boosted on almost all tasks and achieve SOTA performance on 9 out of 10 datasets. Especially, ACMII-GCN+ performs the best in terms of average rank (4.40) across all datasets. Overall, It suggests that ACM or ACMII framework can significantly increase the performance of GNNs on node classification tasks on heterophilic graphs and maintain highly competitive performance on homophilic datasets.  <ref type="table" target="#tab_11">Table 2</ref>: Experimental results: average test accuracy ? standard deviation on 10 real-world benchmark datasets. The best results are highlighted in grey and the best baseline results (SOTA in <ref type="figure" target="#fig_10">Figure 6</ref>) are underlined. Results "*" are reported from <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> and results " ? " are from <ref type="bibr" target="#b34">[35]</ref>. NA means the reported results are not available and OOM means out of memory. <ref type="table" target="#tab_4">See table 3 for the results and table 13</ref>     Attention with more complicated design can be found for the node-wise adaptive channel mixing mechanism, but we do not explore this direction deeper in this paper because investigating attention function is not the main contribution of our paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison with SOTA Models on Fixed 48%/32%/20% Splits</head><formula xml:id="formula_14">? l L = ? H l CombW l L ,? l H = ? H l CombW l H ,? l I = ? H l CombW l I ,W l?1 L ,W l?1 H ,W l?1 I ? R 3F l ?1 ? l L , ? l H , ? l I = Softmax ( ? l L ,? l H ,? l I /T )W l Mix ? R N ?3 , T ? R temperature, W l Mix ? R 3?3 ;</formula><p>The performance comparison can be found in table 6. From the results, we do not find significant difference between the frameworks with combined features and raw features. The reason is that the necessary nonlinear information from each channel is combined in ? l L ,? l H ,? l I and W l Mix is enough to learn to mix the combined weights from different channels. The learning of redundant information in the feature extraction step for each channel will not improve the performance. Meanwhile, A disadvantage of the combined feature is that it increases the computational cost. Thus, we decide to use the raw features.     Unlike other baseline GNN models, GCNII and GCNII* are not able to be applied under ACMII framework and we will make an explanation as follows.</p><formula xml:id="formula_15">GCNII: H ( +1) = ? (1 ? ? )?H ( ) + ? H (0) (1 ? ? ) I n + ? W ( ) GCNII*: H ( +1) = ? (1 ? ? )?H ( ) (1 ? ? ) I n + ? W ( ) 1 + +? H (0) (1 ? ? ) I n + ? W ( ) 2</formula><p>From the above formulas of GCNII and GCNII * we cam see that, without major modification, GCNII and GCNII* are hard to be put into ACMII framework. In ACMII framework, before apply?, we first implement a nonlinear feature extractor ?(H W ( ) ). But in GCNII and GCNII*, before multiplying W (or W 1 , W 2 ) to extract features, we need to add another term including H (0) , which are not filtered by?. This makes the order of aggregator? and nonlinear extractor unexchangable and thus, incompatible with ACMII framework. So we did not implement GCNII and GCNII* in ACMII framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation of ACM(II)-GCN+ and ACM(II)-GCN++</head><p>Besides the features extracted by different filters, some recent SOTA models use additional graph structure information explicitly, i.e., MLP ? (A) , to address heterophily problem, e.g., LINKX <ref type="bibr" target="#b24">[25]</ref> and GloGNN <ref type="bibr" target="#b23">[24]</ref> and is found effective on some datasets, e.g., Chameleon, Squirrel. The explicit structure information can be directly incorporated into ACM and ACMII framework, and we have ACM(II)-GCN+ and ACM(II)-GCN++ as follows.</p><p>? ACM-GCN+ and ACMII-GCN+ have an option to include structure information channel (the 4-th channel) in each layer and their differences from ACM-GCN and ACMII-GCN are highlighted in red) as follows,</p><p>Step 1. Feature Extraction for LP, HP, Identity and Structure Information Channel: </p><formula xml:id="formula_16">H l A = ReLU AW l A , W l A ? R N ?F l ,</formula><formula xml:id="formula_17">H l L = LN(H l L ),H l H = LN(H l H ),H l I = LN(H l I ),H l A = LN(H l A ), ? l L = Sigmoid H l LW l L ,? l H = Sigmoid H l HW l H ,? l I = Sigmoid H l IW l I ,? l A = Sigmoid H l AW l A , W l?1 L ,W l?1 H ,W l?1 I ,W l A ? R F l ?1</formula><p>Step 3. Node-wise Adaptive Channel Mixing: Option 1: without structure information</p><formula xml:id="formula_18">? l L , ? l H , ? l I = Softmax ( ? l L ,? l H ,? l I /T )W l Mix ? R N ?3 , T = 3 temperature, W l Mix ? R 3?3 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I Option 2: with structure information ? l L , ? l H , ? l I , ? l A = Softmax ( ? l L ,? l H ,? l I ,? l A /T )W l Mix ? R N ?4 , T = 4 temperature, W l Mix ? R 4?4 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + diag(? l A )H l A</formula><p>? ACM-GCN++ and ACMII-GCN++ have an option to include structure information channel (the 4-th channel) in each layer and residual connection and their differences from ACM-GCN+ and ACMII-GCN+ are highlighted in red) as follows,</p><p>Step 1. Feature Extraction for LP, HP, Identity and Structure Information Channel, Get H X : </p><formula xml:id="formula_19">H X = ReLU (XW X ) ? R F ?F , H l A = ReLU AW l A , W l A ? R N ?F ,</formula><formula xml:id="formula_20">H l L = LN(H l L ),H l H = LN(H l H ),H l I = LN(H l I ),H l A = LN(H l A ), ? l L = Sigmoid H l LW l L ,? l H = Sigmoid H l HW l H ,? l I = Sigmoid H l IW l I ,? l A = Sigmoid H l AW l A , W l?1 L ,W l?1 H ,W l?1 I ,W l A ? R F ?1</formula><p>Step 3. Node-wise Adaptive Channel Mixing: Option 1: without structure information</p><formula xml:id="formula_21">? l L , ? l H , ? l I = Softmax ( ? l L ,? l H ,? l I /T )W l Mix ? R N ?3 , T = 3 temperature, W l Mix ? R 3?3 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + H X Option 2: with structure information ? l L , ? l H , ? l I , ? l A = Softmax ( ? l L ,? l H ,? l I ,? l A /T )W l Mix ? R N ?4 , T = 4 temperature, W l Mix ? R 4?4 ; H l = ReLU diag(? l L )H l L + diag(? l H )H l H + diag(? l I )H l I + diag(? l A )H l A + H X</formula><p>The results of ACM-GCN+, ACMII-GCN+, ACM-GCN++ and ACMII-GCN++ trained on random 60%/20%/20% splits are reported in    <ref type="table">Table 8</ref>: Hyperparameter searching range for ablation study</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hyperparameter Searching Range for GNNs on Real-world Datasets</head><p>See <ref type="table" target="#tab_14">table 9</ref> for the hyperparameter seaching range of baseline GNNs, ACM-GNNs, ACMII-GNNs and several SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Searched Optimal Hyperparameters for Baselines and ACM(II)-GNNs on Real-world Tasks</head><p>See the reported optimal hyperparameters on random 60%/20%/20% splits for baseline GNNs in       For each generated graph, we calculate their H node , H class , H M agg . Then, we reorder the value of the metrics in ascend order for x-axis and plot the corresponding test accuracy.</p><p>Here is a simplified example of how we draw   In order to separate the effects of nonlinearity and graph structure, we compare SGC with 1 hop (sgc-1) with MLP-1 (linear model). For GCN which includes nonlinearity, we use MLP-2 as its corresponding graph-agnostic baseline model. We train the above GNN models, graph-agnostic baseline models and ACM-GNN models on all synthetic datasets and plot the mean test accuracy with standard deviation on each dataset. From <ref type="figure" target="#fig_11">Figure 10</ref> and <ref type="figure" target="#fig_11">Figure 11</ref>, we can see that on each H M agg (G) level, ACM-GNNs will not underperform baseline GNNs and the graph-agnostic models. But when H M agg (G) is small, baseline GNNs will be outperformed by graph-agnostic models by a large margin. This demonstrate that the ACM framework can help GNNs to perform well on harmful graphs while keep competitive on less harmful graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Further Discussion of Aggregation Homophily on Regular Graphs</head><p>We notice that in <ref type="figure" target="#fig_1">Figure 2</ref>(a), the performance of SGC-1 and GCN both have a turning point, i.e., when H edge (G) is smaller than a certain value, the performance will get better instead of getting worse. With some extra restriction on node degree in data generation process, we find that this interesting phenomenon can be theoretically explained by the following proposition 1 based on our proposed similarity matrix which can verify the usefulness of H M agg (G). We first generate regular graphs ,i.e., each node has the same degree, as follows, Generate Synthetic Regular Graphs We first generate 180 graphs in total with 18 edge homophily levels varied from 0.05 to 0.9, each corresponding to 10 graphs. For every generated graph, we have 5 classes with 400 nodes in each class. For each node, we randomly generate 10 intra-class edges and [ 10 Hedge(G) ? 10] inter-class edges. The features of nodes in each class are sampled from node features in the corresponding class of the base dataset. Nodes are randomly split into 60%/20%/20% for train/validation/test. We train 1-hop SGC (sgc-1) <ref type="bibr" target="#b40">[41]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> on synthetic data (see Appendix C.1 for hyperparameter searching range). For each value of H edge (G), we take the average test accuracy and standard deviation of runs over 10 generated graphs. We plot the performance curves in <ref type="figure" target="#fig_1">Figure 12</ref>.</p><p>From <ref type="figure" target="#fig_1">Figure 12</ref> we can see that the turning point is a bit less than 0.2. We derive the following proposition for d-regular graph to explain and predict it. Suppose there are C classes in the graph G and G is a d-regular graph (each node has d neighbors). Given d, edges for each node are i.i.d.generated, such that each edge of any node has probability h to connect with nodes in the same class and probability 1 ? h to connect with nodes in different classes. Let the aggregation operator? =? rw . Then, for nodes v, u 1 and u 2 , where Z u1,: = Z v,: and Z u2,: = Z v,: , we have</p><formula xml:id="formula_22">g(h) ? E S(?, Z) v,u1 ? E S(?, Z) v,u2 = (C ? 1)(hd + 1) ? (1 ? h)d (C ? 1)(d + 1) 2<label>(12)</label></formula><p>and the minimum of g(h) is reached at</p><formula xml:id="formula_23">h = d + 1 ? C Cd = d intra /h + 1 ? C C(d intra /h) ? h = d intra Cd intra + C ? 1</formula><p>where d intra = dh, which is the expected number of neighbors of a node that have the same label as the node.</p><p>The value of g(h) in equation 12 is the expected differences of the similarity values between nodes in the same class as v and nodes in other classes. g(h) is strongly related to the definition of aggregation homophily and its minimum potentially implies the turning point of performance curves. In the synthetic experiments, we have d intra = 10, C = 5 and the minimum of g(h) is reached at h = 5/27 ? 0.1852, which corresponds to the lowest point in the performance curve in <ref type="figure" target="#fig_1">Figure 12</ref>. In other words, the H edge (G) where SGC-1 and GCN perform worst is where g(h) gets the smallest value, instead of the point with the smallest edge homophily value, i.e., H edge (G) = 0. This reveals the advantage of H agg (G) over H edge (G) by taking use of the similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details of Gradient Calculation in equation 6 E.1 Derivation in Matrix Form</head><p>This derivation is similar to <ref type="bibr" target="#b29">[30]</ref>.</p><p>In output layer, we have</p><formula xml:id="formula_24">Y = softmax(?XW ) ? softmax(Y ) = exp(Y )1 C 1 T C ?1 exp(Y ) &gt; 0 L = ?trace(Z T log Y )</formula><p>where 1 C ? R C?1 , (?) ?1 is point-wise inverse function and each element of Y is positive. Then </p><formula xml:id="formula_25">dL = ?trace Z T ((Y ) ?1 dY ) = ?trace Z T (softmax(Y )) ?1 d softmax(Y ) Note that d softmax(Y ) = ? exp(Y )1 C 1 T C ?2 [(exp(Y ) dY )1 C 1 T C ] exp(Y ) + exp(Y )1 C 1 T C ?1 (exp(Y ) dY ) = ? softmax(Y ) exp(Y )1 C 1 T C ?1 [(exp(Y ) dY )1 C 1 T C ] + softmax(Y ) dY = softmax(Y ) ? exp(Y )1 C 1 T C ?1 (exp(Y ) dY )1 C 1 T C + dY Then, dL = ? trace Z T (softmax(Y )) ?1 softmax(Y ) ? exp(Y )1 C 1 T C ?1 (exp(Y ) dY )1 C 1 T C + dY = ? trace Z T ? exp(Y )1 C 1 T C ?1 (exp(Y ) dY )1 C 1 T C + dY = trace Z exp(Y )1 C 1 T C ?1 1 C 1 T C T [exp(Y ) dY ] ? Z T dY = trace exp(Y ) Z exp(Y )1 C 1 T C ?1 1 C 1 T C T dY ? Z T dY = trace exp(Y ) exp(Y )1 C 1 T C ?1 T dY ? Z T dY = trace (softmax(Y ) ? Z) T dY where the 4-th equation holds due to Z exp(Y )1 C 1 T C ?1 1 C 1 T C = exp(Y )1 C 1 T C ?1 . Thus, we have dL dY = softmax(Y ) ? Z = Y ?</formula><formula xml:id="formula_26">dL dW = X T?T dL dY = X T?T (Y ? Z)<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Component-wise Derivation</head><p>DenoteX = XW . We rewrite L as follows:</p><formula xml:id="formula_27">L = ?trace Z T log (exp(Y )1 C 1 T C ) ?1 exp(Y ) = ?trace Z T ? log(exp(Y )1 C 1 T C ) + Y = ?trace Z T Y + trace Z T log exp(Y )1 C 1 T C = ?trace Z T? XW + trace Z T log exp(Y )1 C 1 T C = ?trace Z T? XW + trace 1 T C log (exp(Y )1 C ) 32</formula><p>Expand L component-wisely, we have</p><formula xml:id="formula_28">L = ? N i=1 j?Ni? i,j Z i,:X T j: + N i=1 log ? ? C c=1 exp( j?Ni? i,jXj,c ) ? ? = ? N i=1 log ? ? exp ? ? C c=1 j?Ni? i,j Z i,cXj,c ? ? ? ? + N i=1 log ? ? C c=1 exp ? ? j?Ni? i,jXj,c ? ? ? ? = ? N i=1 log exp C c=1 j?Ni? i,j Z i,cXj,c C c=1 exp( j?Ni? i,jXj,c )</formula><p>Note that C c=1 Z j,c = 1 for any j. Consider the derivation of L overX j ,c :</p><formula xml:id="formula_29">dL dX j ,c = ? N i=1 C c=1 exp( j?Ni? i,jXj,c ) exp C c=1 j?Ni? i,j Z i,cXj,c ? ? ? ? ? ? ? ? ? i,j Z i,c exp C c=1 j?Ni? i,j Z i,cXj,c C c=1 exp( j?Ni? i,jXj,c ) C c=1 exp( j?Ni? i,jXj,c ) 2 ? ? i,j exp C c=1 j?Ni? i,j Z i,cXj,c exp( j?Ni? i,jXj,c ) C c=1 exp( j?Ni? i,jXj,c ) 2 ? ? ? ? ? ? ? = ? N i=1 ? ? ? ? ? ? ? i,j Z i,c C c=1 exp( j?Ni? i,jXj,c ) ? ? i,j exp( j?Ni? i,jXj,c ) C c=1 exp( j?Ni? i,jXj,c ) ? ? ? ? ? ? = ? N i=1 ? ? ? ? ? ?? i,j C c=1,c =c (Z i,c ) exp( j?Ni? i,jXj,c ) + (Z i,c ? 1) exp( j?Ni? i,jXj,c ) C c=1 exp( j?Ni? i,jXj,c ) ? ? ? ? ? ? = ? N i=1? i,j Z i,c P (Y i = c ) + (Z i,c ? 1)P (Y i = c ) = ? N i=1? i,j Z i,c ?P (Y i = c )<label>(14)</label></formula><p>Writing the above in matrix form, we have</p><formula xml:id="formula_30">dL dX =?(Z ? Y ), dL dW = X T?T (Z ? Y ), ?Y ??XX T?T (Z ? Y )<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Proof of Proposition 1</head><p>Proof. According to the given assumptions, for node v, we have? v,k = 1 d+1 , the expected number of intra-class edges is dh (here the self-loop edge introduced by? is not counted based on the definition of edge homophily and data generation process) and inter-class edges is (1 ? h)d. Suppose there are C ? 2 classes. Consider matrix?Z,</p><formula xml:id="formula_31">Then, we have E (?Z) v,c = E k?V? v,k 1 {Z k,: =e T c } = k?V E 1 {Z k,: =e T c } d+1</formula><p>, where 1 is the indicator function.</p><p>When v is in class c, we have</p><formula xml:id="formula_32">k?V E 1 {Z k,: =e T c } d+1 = hd+1 d+1 (hd + 1 = hd intra-class edges + 1 self-loop introduced by?).</formula><p>When v is not in class c, we have</p><formula xml:id="formula_33">k?V E 1 {Z k,: =e T c } d+1 = (1?h)d (C?1)(d+1) ((1 ? h)d</formula><p>inter-class edges uniformly distributed in the other C ? 1 classes).</p><p>For nodes v, u, we have (?Z) v,: , (?Z) u,: ? R C and since elements in? v,k and? u,k are independently generated for all k, k ? V, we have</p><formula xml:id="formula_34">E (?Z) v,c (?Z) u,c = E ( k?V? v,k 1 {Z k,: =e T c } )( k ?V? u,k 1 {Z k ,: =e T c } ) = E ( k?V? v,k 1 {Z k,: =e T c } ) E ( k ?V? u,k 1 {Z k ,: =e T c } )</formula><p>Thus,</p><formula xml:id="formula_35">E S(?, Z) v,u = E &lt; (?Z) v,: , (?Z) u,: &gt; = c E ( k?V? v,k 1 {Z k,: =e T c } ) E ( k ?V? u,k 1 {Z k ,: =e T c } ) = ? ? ? hd+1 d+1 2 + ((1?h)d) 2 (C?1)(d+1) 2 , u, v are in the same class 2(hd+1)(1?h)d (C?1)(d+1) 2 + (C?2)(1?h) 2 d 2 (C?1) 2 (d+1) 2 , u</formula><p>, v are in different classes For nodes u 1 , u 2 , and v, where Z u1,: = Z v,: and Z u2,: = Z v,: ,</p><formula xml:id="formula_36">g(h) ? E S(?, Z) v,u1 ? E S(?, Z) v,u2<label>(16)</label></formula><formula xml:id="formula_37">= (C ? 1) 2 (hd + 1) 2 + (C ? 1) [(1 ? h)d] 2 ? (C ? 1) (2(hd + 1)(1 ? h)d) ? (C ? 2) [(1 ? h)d] 2 (C ? 1) 2 (d + 1) 2 = (C ? 1)(hd + 1) ? (1 ? h)d (C ? 1)(d + 1) 2</formula><p>Setting g(h) = 0, we obtain the optimal h:</p><formula xml:id="formula_38">h = d + 1 ? C Cd<label>(17)</label></formula><p>For the data generation process in the synthetic experiments, we fix d intra , then d = d intra /h, which is a function of h. We change d in equation 17 to d intra /h, leading to</p><formula xml:id="formula_39">h = d intra /h + 1 ? C Cd intra /h<label>(18)</label></formula><p>It is easy to observe that h satisfying equation 18 still makes g(h) = 0, when d in g(h) is replaced by d intra /h. From equation 18 we obtain the optimal h in terms of d intra :</p><formula xml:id="formula_40">h = d intra Cd intra + C ? 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 An Extension of Proposition 1</head><p>Base on the definition of aggregation similarity, we have </p><formula xml:id="formula_41">RV = Mean u {S(?, Z) v,u |Z u,: = Z v,: } ? Mean u {S(?, Z) v,u |Z u,: = Z v,: }</formula><p>Since RV is symmetrically distributed and under the conditions in proposition 1, its expectation is E[RV ] = g(h) as showed in equation 16. Since the minimum of g(h) is 0 and RV is symmetrically distributed, we have P(RV ? 0) ? 0.5 and this can explain why H agg (G) is always greater than 0.5 in many real-world tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proof of Theorem 1</head><p>Proof. Define W c v = (?Z) v,c . Then,</p><formula xml:id="formula_42">W c v = k?V? v,k 1 {Z k,: =e T c } ? [0, 1], C c=1 W c v = 1</formula><p>Note that</p><formula xml:id="formula_43">S(I ??, Z) = (I ??)ZZ T (I ??) T = ZZ T +?ZZ T?T ??ZZ T ? ZZ T?T<label>(19)</label></formula><p>For any node v, let the class v belongs to be denoted by c v . For two nodes v, u, if Z v,: = Z u,: , we have</p><formula xml:id="formula_44">(ZZ T ) v,u = 0 (?ZZ T?T ) v,u = C c=1 W c v W c u (?ZZ T ) v,u = W cu v (ZZ T?T ) v,u = (?ZZ T ) u,v = W cv u</formula><p>Then, from equation 19 it follows that</p><formula xml:id="formula_45">(S(I ??, Z)) v,u = C c=1 W c v W c u ? W cu v ? W cv u When C = 2, S(I ??, Z) v,u = W cu v (W cu u ? 1) + W cv u (W cv v ? 1) ? 0 If Z v,: = Z u,: , i.e., c v = c u , we have (ZZ T ) v,u = 1 (?ZZ T?T ) v,u = C c=1 W c v W c u (?ZZ T ) v,u = W cv v (ZZ T?T ) v,u = (?ZZ T ) u,v = W cu u = W cv u</formula><p>Then, from equation 19 it follows that   <ref type="figure" target="#fig_3">Figure 13</ref>: Example of the case (the area in black box) that HP filter does not work well for harmful heterophily From the black box area of S(I ??, X) in the example in <ref type="figure" target="#fig_3">Figure 13</ref> we can see that nodes in class 1 and 4 assign non-negative weights to each other although there is no edge between them; nodes in class 2 and 3 assign non-negative weights to each other as well. This is because the surrounding differences of class 1 are similar as class 4, so are class 2 and 3. In real-world applications, when nodes in several small clusters connect to a large cluster, the surrounding differences of the nodes in the small clusters will become similar. In such case, HP filter are not able to distinguish the nodes from different small clusters.</p><formula xml:id="formula_46">S(I ??, Z) v,u = 1 + C c=1 W c v W c u ? W cv v ? W cv u = C c=1,c =cv W c v W c u + 1 + W cv v W cv u ? W cv v ? W cv u = C c=1,c =cv W c v W c u + (1 ? W cv v )(1 ? W cv u ) ? 0 Thus, if C = 2, for any v ? V, if Z u,: = Z v,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I The Similarity, Homophily and DD? ,X (G) Metrics and Their Estimations</head><p>Firstly, we would like to clarify that, for each curve in the synthetic experiments, the node features are fixed and we only change the homophily values. But in real-world tasks, different datasets have different features and aggregated features. Thus, to get more instructive information for different datasets and compare them, we need to consider more metrics, e.g. feature-label consistency and aggregated-feature-label consistency. With the similarity score of the features S agg (S(I, X)) and aggregated features S agg S(?, X) listed in <ref type="table" target="#tab_6">Table 15</ref>, our methods open up a new perspective on analyzing and comparing the performance of graph-agnostic models and graph-aware models in real-world tasks. Here are 2 examples.</p><p>Example 1: People observe that GCN (graph-aware model) underperforms MLP-2 (graph-agnostic model) on Cornell, Wisconsin, Texas, Film and people commonly believe that the bad graph structure (low H edge , H node , H class values) is the reason for performance degradation. But based on the high aggregation homophily values, the graph structure inconsistency is not the main cause of the performance degradation. And from <ref type="table" target="#tab_6">Table 15</ref> we can see that the S agg S(?, X) for those 4 datasets are lower than their corresponding S agg (S(I, X)), which implies that it is the aggregated-feature-label inconsistency that causes the performance degradation, i.e. the aggregation step actually decrease the quality of node features rather than making them more distinguishable.</p><p>For the rest 5 datasets Chameleon, Squirrel, Cora, Citeseer, PubMed, we all have S agg S(?, X) larger than S agg (S(I, X)) except PubMed, which means the aggregated features have higher quality than raw features. We can see that the proposed metrics are much more instructive than the existing ones.</p><p>Example 2: According to H edge , H node , H class , the value for Chameleon, and Squirrel are extremely low indicating graph structure are bad for GNNs. But on contrary, GCN outperforms MLP-2 on those 2 datasets. Traditional homophily metrics fail to explain such phenomenon but our method can give an explanation from different angles: For Chameleon, its modified aggregation homophily is not low and its S agg S(?, X) is higher than its S agg (S(I, X)), which means its graph-label consistency together with aggregated-feature-label consistency help the graph-aware model obtain the performance gain; for Squirrel, its modified aggregation homophily is low but its S agg S(?, X)</p><p>is higher than its S agg (S(I, X)), which means although its graph-label consistency is bad, the aggregated-feature-label consistency is the key factor to help the graph-aware model perform better.</p><p>We also need to point out that (modified) aggregation similarity score, S agg S(?, X) and S agg (S(I, X)) are not deciding values because they do not consider the nonlinear structure in the features. In practice, a low score does not tell us the GNN models will definitely perform bad. Hagg(G) 0.9046 ? 0.0282 0.9147 ? 0.0260 0.8596 ? 0.0299 0.8451 ? 0.0041 0.8041 ? 0.0078 0.6788 ? 0.0077 0.9959 ? 0.0011 0.9907 ? 0.0015 0.9724 ? 0.0015 Sagg S(?, X) 0.8266 ? 0.0526 0.8280 ? 0.0351 0.6835 ? 0.0498 0.5345 ? 0.0421 0.8433 ? 0.0070 0.7352 ? 0.0132 0.9487 ? 0.0023 0.9451 ? 0.0038 0.8626 ? 0.0021 Sagg (S(I, X)) 0.9752 ? 0.0174 0.8680 ? 0.0270 0.9661 ? 0.0336 0.5438 ? 0.0184 0.8257 ? 0.0050 0.7472 ? 0.0089 0.9204 ? 0.0044 0.9441 ? 0.0036 0.8835 ? 0.0019 DD? ,X (G) 0.3936 ? 0.0663 0.6073 ? 0.0436 0.4817 ? 0.0762 0.3300 ? 0.0136 0.3329 ? 0.0151 0.3021 ? 0.0101 0.3198 ? 0.0225 0.4424 ? 0.0136 0.1919 ? 0.0046 <ref type="table" target="#tab_6">Table 15</ref>: Additional metrics and their estimations with only training labels (mean ? std) Furthermore, in most real-world applications, not all labels are available to calculate the dataset statistics. Thus, we randomly split the data into 60%/20%/20% for training/validation/test, and only use the training labels for the estimation of the statistics. We repeat each estimation for 10 times and report the mean with standard deviation. The results are shown in table <ref type="bibr" target="#b14">15</ref>.</p><p>Analysis From the reported results we can see that the estimations are accurate and the errors are within the acceptable range, which means the proposed metrics and similarity scores can be accurately estimated with a subset of labels and this is important for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J A Detailed Explanation of the Differences Between ACM(II)-GNNs and GPRGNN, FAGCN</head><p>Differences with GPRGNN [8]:</p><p>? GPRGNN does not feed distinct node-wise feature transformation to different "multi-scale channels"</p><p>We first rewrite GPRGNN as</p><formula xml:id="formula_47">Z = K k=0 ? k H (k) = K k=0 ? k IH (k) = K k=0 diag(? k , ? k , . . . , ? k )H (k) , where H (k) =? sym H (k?1) , H<label>(0)</label></formula><p>i: = f ? (X i: ).</p><p>From the above equation we can see that Z = K k=0 ? k? k sym f ? (X i: ), i.e., the node-wise feature transformation in GPRGNN is only learned by the same ? for all the "multi-scale channels". But in the ACM framework, different channels extract distinct information with different parameters separately.</p><p>? GPRGNN does not have node-wise mixing mechanism.</p><p>There is no node-wise mixing in GPRGNN. The mixing mechanism in GPRGNN is Z = K k=0 diag(? k , ? k , . . . , ? k )H (k) , i.e. for each "multi-scale channel k", all nodes share the same mixing parameter ? k . But in the ACM framework, the node-wise channel mixing can be written as Z = K k=0 diag(? 1 k , ? 2 k , . . . , ? N k )H (k) where K is the number of channels, N is the number of nodes and ? i k , i = 1, . . . , N are the mixing weights that are learned by node i to mix channel k. ACM and ACMII allow GNNs to learn more diverse mixing parameters in diagonal than GPRGNN and thus, have stronger expressive power than GPRGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences with FAGCN [4]:</head><p>? The targets of node-wise operations in ACM (channel mixing) and FAGCN (negative message passing) are different. Instead of using a fixed low-pass filter?, FAGCN tries to learn a more powerful aggregator? based on? by allowing negative message passing. The node-wise operation in FAGCN is similar to GAT <ref type="bibr" target="#b2">[3]</ref> which is trying to modify the node-wise filtering (message passing) process, i.e. for each node i, it assigns different weights ? ij ? [?1, 1] to different neighborhood nodes (equation 7 in FAGCN paper). The goal of this node-wise operation in FAGCN is to learn a new filter during the filtering process node-wisely. But in ACM, the nodewise operation is to mix the filtered information from each channel which is processed by different fixed filters. The targets of two the node-wise operations are actually different things.</p><p>? FAGCN does not learn distinct information from different "channels". FAGCN only uses simple addition to mix information instead of node-wise channel mixing mechanism The learned filter? can be decomposed as? =? 1 +(?? 2 ), where? 1 and ?? 2 represent positive and negative edge (propagation) information, respectively. But FAGCN does not feed distinct information to? 1 and ?? 2 . Moreover, the aggregated? 1 X and "diversified" information (?? 2 )X are simply added together instead of using any node-wise mixing mechanism. In ACM, we learn distinct information separately in each channel with different parameters and add them adaptively and node-wisely instead of just adding them together. In section 6.1, the ablation study has empirically shown that node-wise adaptive channel mixing is better than simple addition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Class 1 Class 2 Figure 1 :</head><label>1121</label><figDesc>#! (G) = 0 H $%"! (G) = 0 H &amp;'()) (G) = 0 agg (G) = 1 agg * (G) = Example of harmless heterophily</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of baseline performance under different homophily metrics.<ref type="bibr" target="#b3">4</ref> See Appendix F.1 for an intuitive explanation under certain conditions.<ref type="bibr" target="#b4">5</ref> In practice, we will only check Hagg(G) when H M agg (G) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example of how diversification can address harmful heterophily</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a)(b)(c) show that the performance curves under H edge (G), H node (G) and H class (G) are U -shaped 7 , while Figure 2(d) reveals a nearly monotonic curve with a little numerical perturbation around 1. This indicates that H M agg (G) provides a better indication of the way in which the graph structure affects the performance of SGC-1 and GCN than existing metrics. (See more discussion on aggregation homophily and theoretical results for regular graphs in Appendix D.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Definition 2 .</head><label>2</label><figDesc>Diversification Distinguishability (DD) based on S(I ??, X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 2</head><label>2</label><figDesc>Filterbank and Adaptive Channel Mixing (ACM) Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>H v node distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>t-SNE visualization of the output layer of ACM-GCN and GCN trained on Squirrel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>? 4.06 % ? 25.22 % Comparison of baseline GNNs (red), ACM-GNNs (green), ACMII-GNNs (blue) with SOTA (magenta line) models on 6 selected datasets. The black lines indicate the standard deviation. The symbol "?" shows the range of performance improvement (%) of ACM-GNNs and ACMII-GNNs over baseline GNNs. See Appendix I for a detailed discussion of the relation between H M agg and GNN performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>A. 6</head><label>6</label><figDesc>H v node Distributions of Different Datasets SeeFigure 7for H v node distributions. We can see that Wisconsin and Texas have high density in low homophily area, Cornell, Chameleon, Squirrel and Film have high density in low and middle homophily area, Cora, CiteSeer and PubMed have high density in high homophily area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :</head><label>7</label><figDesc>H v node distributions of different datasets A.7 Distributions of Learned ? L , ? H , ? I in the Hidden and Output Layers of ACN-GCN SeeFigure 8for the distributions of weights in hidden layers andFigure 9for the distributions of weights in output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>Distributions of the learned ? L , ? H , ? I in the hidden layer of ACM-GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :</head><label>9</label><figDesc>Distributions of the learned ? L , ? H , ? I in the output layer of ACM-GCN B Details of the Implementation B.1 Implementation of ACM-GCMII</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} --SGC-LP+Identity {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} --ACM-SGC-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} -GCN-LP+HP {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 GCN-LP+Identity {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64 ACM-GCN-no adaptive mixing {0.01, 0.05, 0.1} {0, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9} 64</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 2 .</head><label>2</label><figDesc>Suppose we generate 3 graphs with H edge = 0.1, 0.5, 0.9, the test accuracy of GCN on these 3 synthetic graphs are 0.8, 0.5, 0.9. For the generated graphs, we calculate their H M agg , and suppose we get H M agg = 0.7, 0.4, 0.8. Then we will draw the performance of GCN under H M agg with ascend x-axis order [0.4, 0.7, 0.8] and the corresponding reordered y-axis is [0.5, 0.8, 0.9]. Other figures are drawn with the same process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of test accuracy (mean ? std) of MLP-1, SGC-1 and ACM-SGC-1 on synthetic datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of test accuracy (mean ? std) of MLP-2, GCN and ACM-GCN on synthetic datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 12 :</head><label>12</label><figDesc>Synthetic experiments for edge homophily on regular graphs. Proposition 1. (See Appendix F for proof).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>S 1 Meanuv?V 1 MeanuP</head><label>11</label><figDesc>agg S(?, Z) = v Mean u {S(?, Z) v,u |Z u,: = Z v,: } ? Mean u {S(?, Z) v,u |Z u,: = Z v,: } |V| = v?V {S(?,Z)v,u|Zu,:=Zv,:} ?Meanu {S(?,Z)v,u|Zu,: =Zv,:} |V| Then, E S agg S(?, Z) = E ? ? {S(?,Z)v,u|Zu,:=Zv,:} ?Meanu {S(?,Z)v,u|Zu,: =Zv,:} Mean u {S(?, Z) v,u |Z u,: = Z v,: } ? Mean u {S(?, Z) v,u |Z u,: = Z v,: } |V| = P Mean u {S(?, Z) v,u |Z u,: = Z v,: } ? Mean u {S(?, Z) v,u |Z u,: = Z v,: } ? 0 Consider the random variable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>: , we have S(I ??, Z) v,u ? 0; if Z u,: = Z v,: , we have S(I ??, Z) v,u ? 0. Apparently, the two conditions in equation 10 are satisfied. Thus v is diversification distinguishable and DD? ,X (G) = 1. The theorem is proved.H Discussion of the Limitations of Diversification Operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std Acc ? Std ACM-SGC-1 w/70.98 ? 8.39 70.38 ? 2.85 83.28 ? 5.43 25.26 ? 1.18 64.86 ? 1.81 47.62 ? 1.27 85.12 ? 1.64 79.66 ? 0.75 85.5 ? 0.76 12.89 83.28 ? 5.81 91.88 ? 1.61 90.98 ? 2.46 36.76 ? 1.01 65.27 ? 1.9 47.27 ? 1.37 86.8 ? 1.08 80.98 ? 1.68 87.21 ? 0.42 10.44 93.93 ? 3.6 95.25 ? 1.84 93.93 ? 2.54 38.38 ? 1.13 63.83 ? 2.07 46.79 ? 0.75 86.73 ? 1.28 80.57 ? 0.99 87.8 ? 0.58 9.44 88.2 ? 4.39 93.5 ? 2.95 92.95 ? 2.94 37.19 ? 0.87 62.82 ? 1.84 44.94 ? 0.93 85.22 ? 1.35 80.75 ? 1.68 88.11 ? 0.21 11.00 93.77 ? 1.91 93.25 ? 2.92 93.61 ? 1.55 39.33 ? 1.25 63.68 ? 1.62 46.4 ? 1.13 86.63 ? 1.13 80.96 ? 0.93 87.75 ? 0.88 10.00 ACM-GCN w/ 82.46 ? 3.11 75.5 ? 2.92 83.11 ? 3.2 35.51 ? 0.99 64.18 ? 2.62 44.76 ? 1.39 87.78 ? 0.96 81.39 ? 1.23 88.9 ? 0.32 11.44 82.13 ? 2.59 86.62 ? 4.61 89.19 ? 3.04 38.06 ? 1.35 69.21 ? 1.68 57.2 ? 1.01 88.93 ? 1.55 81.96 ? 0.91 90.01 ? 0.8 7.22 94.26 ? 2.23 96.13 ? 2.2 94.1 ? 2.95 41.51 ? 0.99 67.44 ? 2.14 53.97 ? 1.39 88.95 ? 0.9 81.72 ? 1.22 90.88 ? 0.55 4.44 91.64 ? 2 95.37 ? 3.31 95.25 ? 2.37 40.47 ? 1.49 68.93 ? 2.04 54.78 ? 1.27 89.13 ? 1.77 81.96 ? 2.03 91.01 ? 0.7 3.11 94.75 ? 2.62 96.75 ? 1.6 95.08 ? 3.2 41.62 ? 1.15 69.04 ? 1.74 58.02 ? 1.86 88.95 ? 1.3 81.80 ? 1.26 90.69 ? 0.53 2.78 ACMII-GCN w/ 82.46 ? 3.03 91.00 ? 1.75 90.33 ? 2.69 38.39 ? 0.75 67.59 ? 2.14 53.67 ? 1.71 89.13 ? 1.14 81.75 ? 0.85 89.87 ? 0.39 7.44 94.26 ? 2.57 96.00 ? 2.15 94.26 ? 2.96 40.96 ? 1.2 66.35 ? 1.76 50.78 ? 2.07 89.06 ? 1.07 81.86 ? 1.22 90.71 ? 0.67 4.67 91.48 ? 1.43 96.25 ? 2.09 93.77 ? 2.91 40.27 ? 1.07 66.52 ? 2.65 52.9 ? 1.64 88.83 ? 1.16 81.54 ? 0.95 90.6 ? 0.47 6.67 95.9 ? 1.83 96.62 ? 2.44 95.25 ? 3.15 41.84 ? 1.15 68.38 ? 1.36 54.53 ? 2.09 89.00 ? 0.72 81.79 ? 0.95 90.74 ? 0.5 2.78</figDesc><table><row><cell></cell><cell></cell><cell cols="6">Ablation Study on Different Components in ACM-SGC and ACM-GCN (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>Model Components</cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell>Models</cell><cell cols="6">LP HP Identity Mixing Acc ? Comparison of Average Running Time Per Epoch(ms)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2.53</cell><cell>2.83</cell><cell>2.5</cell><cell>3.18</cell><cell>3.48</cell><cell>4.65</cell><cell>3.47</cell><cell>3.43</cell><cell>4.04</cell></row><row><cell></cell><cell></cell><cell>4.01</cell><cell>4.57</cell><cell>4.24</cell><cell>4.55</cell><cell>4.76</cell><cell>5.09</cell><cell>5.39</cell><cell>4.69</cell><cell>4.75</cell></row><row><cell>ACM-SGC-1 w/</cell><cell></cell><cell>3.88 3.31</cell><cell>4.01 3.49</cell><cell>4.04 3.18</cell><cell>4.43 3.7</cell><cell>4.06 3.53</cell><cell>4.5 4.83</cell><cell>4.38 3.92</cell><cell>3.82 3.87</cell><cell>4.16 4.24</cell></row><row><cell></cell><cell></cell><cell>5.53</cell><cell>5.96</cell><cell>5.43</cell><cell>5.21</cell><cell>5.41</cell><cell>6.96</cell><cell>6</cell><cell>5.9</cell><cell>6.04</cell></row><row><cell></cell><cell></cell><cell>3.67</cell><cell>3.74</cell><cell>3.59</cell><cell>4.86</cell><cell>4.96</cell><cell>6.41</cell><cell>4.24</cell><cell>4.18</cell><cell>5.08</cell></row><row><cell></cell><cell></cell><cell>6.63</cell><cell>8.06</cell><cell>7.89</cell><cell>8.11</cell><cell>7.8</cell><cell>9.39</cell><cell>7.82</cell><cell>7.38</cell><cell>8.74</cell></row><row><cell>ACM-GCN w/</cell><cell></cell><cell>5.73 5.16</cell><cell>5.91 5.25</cell><cell>5.93 5.2</cell><cell>6.86 5.93</cell><cell>6.35 5.64</cell><cell>7.15 8.02</cell><cell>7.34 5.73</cell><cell>6.65 5.65</cell><cell>6.8 6.16</cell></row><row><cell></cell><cell></cell><cell>8.25</cell><cell>8.11</cell><cell>7.89</cell><cell>7.97</cell><cell>8.41</cell><cell>11.9</cell><cell>8.84</cell><cell>8.38</cell><cell>8.63</cell></row><row><cell></cell><cell></cell><cell>6.62</cell><cell>7.35</cell><cell>7.39</cell><cell>7.62</cell><cell>7.33</cell><cell>9.69</cell><cell>7.49</cell><cell>7.58</cell><cell>7.97</cell></row><row><cell></cell><cell></cell><cell>6.3</cell><cell>6.05</cell><cell>6.26</cell><cell>6.87</cell><cell>6.44</cell><cell>6.5</cell><cell>6.14</cell><cell>7.21</cell><cell>6.6</cell></row><row><cell>ACMII-GCN w/</cell><cell></cell><cell>5.24</cell><cell>5.27</cell><cell>5.46</cell><cell>5.72</cell><cell>5.65</cell><cell>7.87</cell><cell>5.48</cell><cell>5.65</cell><cell>6.33</cell></row><row><cell></cell><cell></cell><cell>7.59</cell><cell>8.28</cell><cell>8.06</cell><cell>8.85</cell><cell>8</cell><cell>10</cell><cell>8.27</cell><cell>8.5</cell><cell>8.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? 0.70 93.87 ? 3.33 92.26 ? 0.71 38.58 ? 0.25 46.72 ? 0.46 31.28 ? 0.27 66.55 ? 0.72 76.44 ? 0.30 76.25 ? 0.28 86.43 ? 0.13 23.40 GAT 76.00 ? 1.01 71.01 ? 4.66 78.87 ? 0.86 35.98 ? 0.23 63.9 ? 0.46 42.72 ? 0.33 61.09 ? 0.77 76.70 ? 0.42 67.20 ? 0.46 83.28 ? 0.12 26.20 APPNP 91.80 ? 0.63 92.00 ? 3.59 91.18 ? 0.70 38.86 ? 0.24 51.91 ? 0.56 34.77 ? 0.34 67.21 ? 0.56 79.41 ? 0.38 68.59 ? 0.30 85.02 ? 0.09 22.80 GPRGNN 91.36 ? 0.70 93.75 ? 2.37 92.92 ? 0.61 39.30 ? 0.27 67.48 ? 0.40 49.93 ? 0.53 66.90 ? 0.50 79.51? 0.36 67.63 ? 0.38 85.07 ? 0.09 19.20 H2GCN 86.23 ? 4.71 87.5 ? 1.77 85.90 ? 3.53 38.85 ? 1.17 52.30 ? 0.48 30.39 ? 1.22 67.22 ? 0.90 87.52 ? 0.61 79.97 ? 0.69 87.78 ? 0.28 21.80 MixHop 60.33 ? 28.53 77.25 ? 7.80 76.39 ? 7.66 33.13 ? 2.40 36.28 ? 10.22 24.55 ? 2.60 66.80 ? 0.58 65.65 ? 11.31 49.52 ? 13.35 87.04 ? 4.10 28.30 GCN+JK 66.56 ? 13.82 62.50 ? 15.75 80.66 ? 1.91 32.72 ? 2.62 64.68 ? 2.85 53.40 ? 1.90 60.99 ? 0.14 86.90 ? 1.51 73.77 ? 1.85 90.09 ? 0.68 23.40 GAT+JK 74.43 ? 10.24 69.50 ? 3.12 75.41 ? 7.18 35.41 ? 0.97 68.14 ? 1.18 52.28 ? 3.61 59.66 ? 0.92 89.52 ? 0.43 74.49 ? 2.76 89.15 ? 0.87 20.90 FAGCN 88.03 ? 5.6 89.75 ? 6.37 88.85 ? 4.39 31.59 ? 1.37 49.47 ? 2.84 42.24 ? 1.2 66.86 p, 0.53 88.85 ? 1.36 82.37 ? 1.46 89.98 ? 0.54 18.20 BernNet 92.13 ? 1.64 NA 93.12 ? 0.65 41.79 ? 1.01 68.29 ? 1.58 51.35 ? 0.73 NA 88.52 ? 0.95 80.09 ? 0.79 88.48 ? 0.41 14.75 GraphSAGE 71.41 ? 1.24 64.85 ? 5.14 79.03 ? 1.20 36.37 ? 0.21 62.15 ? 0.42 41.26 ? 0.26 OOM 86.58 ? 0.26 78.24 ? 0.30 86.85 ? 0.11 25.78 ? 8.39 70.38 ? 2.85 83.28 ? 5.43 25.26 ? 1.18 64.86 ? 1.81 47.62 ? 1.27 59.73 ? 0.12 85.12 ? 1.64 79.66 ? 0.75 85.5 ? 0.76 24.90 SGC-2 72.62 ? 9.92 74.75 ? 2.89 81.31 ? 3.3 28.81 ? 1.11 62.67 ? 2.41 41.25 ? 1.4 61.56 ? 0.51 85.48 ? 1.48 80.75 ? 1.15 85.36 ? 0.52 25.40 GCNII 89.18 ? 3.96 83.25 ? 2.69 82.46 ? 4.58 40.82 ? 1.79 60.35 ? 2.7 38.81 ? 1.97 66.38 ? 0.45 88.98 ? 1.33 81.58 ? 1.3 89.8 ? 0.3 19.30 GCNII* 90.49 ? 4.45 89.12 ? 3.06 88.52 ? 3.02 41.54 ? 0.99 62.8 ? 2.87 38.31 ? 1.3 66.42 ? 0.56 88.93 ? 1.37 81.83 ? 1.78 89.98 ? 0.52 16.40 GCN 82.46 ? 3.11 75.5 ? 2.92 83.11 ? 3.2 35.51 ? 0.99 64.18 ? 2.62 44.76 ? 1.39 62.23 ? 0.53 87.78 ? 0.96 81.39 ? 1.23 88.9 ? 0.32 20.90 Snowball-2 82.62 ? 2.34 74.88 ? 3.42 83.11 ? 3.2 35.97 ? 0.66 64.99 ? 2.39 47.88 ? 1.23 OOM 88.64 ? 1.15 81.53 ? 1.71 89.04 ? 0.49 19.78 Snowball-3 82.95 ? 2.1 69.5 ? 5.01 83.11 ? 3.2 36.00 ? 1.36 65.49 ? 1.64 48.25 ? 0.94 OOM 89.33 ? 1.3 80.93 ? 1.32 88.8 ? 0.82 19.11 ACM-SGC-1 93.77 ? 1.91 93.25 ? 2.92 93.61 ? 1.55 39.33 ? 1.25 63.68 ? 1.62 46.4 ? 1.13 66.67 ? 0.56 86.63 ? 1.13 80.96 ? 0.93 87.75 ? 0.88 17.00 ACM-SGC-2 93.77 ? 2.17 94.00 ? 2.61 93.44 ? 2.54 40.13 ? 1.21 60.48 ? 1.55 40.91 ? 1.39 66.53 ? 0.57 87.64 ? 0.99 80.93 ? 1.16 88.79 ? 0.5 17.70 ACM-GCNII 92.62 ? 3.13 94.63 ? 2.96 92.46 ? 1.97 41.37 ? 1.37 58.73 ? 2.52 40.9 ? 1.58 66.39 ? 0.56 89.1 ? 1.61 82.28 ? 1.12 90.12 ? 0.4 14.30 ACM-GCNII* 93.44 ? 2.74 94.37 ? 2.81 93.28 ? 2.79 41.27 ? 1.24 61.66 ? 2.29 38.32 ? 1.5 66.6 ? 0.57 89.00 ? 1.35 81.69 ? 1.25 90.18 ? 0.51 14.20 ACM-GCN 94.75 ? 3.8 95.75 ? 2.03 94.92 ? 2.88 41.62 ? 1.15 69.04 ? 1.74 58.02 ? 1.86 67.01 ? 0.38 88.62 ? 1.22 81.68 ? 0.97 90.66 ? 0.47 7.90 ACM-GCN+ 94.92 ? 2.79 96.5 ? 2.08 94.92 ? 2.79 41.79 ? 1.01 76.08 ? 2.13 69.26 ? 1.11 67.4 ? 0.44 89.75 ? 1.16 81.65 ? 1.48 90.46 ? 0.69 4.90 ACM-GCN++ 93.93 ? 1.05 97.5 ? 1.25 96.56 ? 2 41.86 ? 1.48 75.23 ? 1.72 68.56 ? 1.33 67.3 ? 0.48 89.33 ? 0.81 81.83 ? 1.65 90.39 ? 0.33 4.30 ACM-Snowball-2 95.08 ? 3.11 96.38 ? 2.59 95.74 ? 2.22 41.4 ? 1.23 68.51 ? 1.7 55.97 ? 2.03 OOM 88.83 ? 1.49 81.58 ? 1.23 90.81 ? 0.52 7.44 ACM-Snowball-3 94.26 ? 2.57 96.62 ? 1.86 94.75 ? 2.41 41.27 ? 0.8 68.4 ? 2.05 55.73 ? 2.39 OOM 89.59 ? 1.58 81.32 ? 0.97 91.44 ? 0.59 7.22 ACMII-GCN 95.9 ? 1.83 96.62 ? 2.44 95.08 ? 2.07 41.84 ? 1.15 68.38 ? 1.36 54.53 ? 2.09 67.15 ? 0.41 89.00 ? 0.72 81.79 ? 0.95 90.74 ? 0.5 5.90 ACMII-Snowball-2 95.25 ? 1.55 96.63 ? 2.24 95.25 ? 1.55 41.1 ? 0.75 67.83 ? 2.63 53.48 ? 0.6 OOM 88.95 ? 1.04 82.07 ? 1.04 90.56 ? 0.39 7.56 ACMII-Snowball-3 93.61 ? 2.79 97.00 ? 2.63 94.75 ? 3.09 40.31 ? 1.6 67.53 ? 2.83 52.31 ? 1.57 OOM 89.36 ? 1.26 81.56 ? 1.15 91.31 ? 0.6 9.00 ACMII-GCN+ 93.93 ? 3.03 96.75 ? 1.79 95.41 ? 2.82 41.5 ? 1.54 75.51 ? 1.58 69.81 ? 1.11 67.44 ? 0.31 89.18 ? 1.11 81.87 ? 1.38 90.96 ? 0.62 4.4 ACMII-GCN++ 92.62 ? 2.57 97.13 ? 1.68 94.75 ? 2.91 41.66 ? 1.42 75.93 ? 1.71 69.98 ? 1.53 67.5 ? 0.53 89.47 ? 1.08 81.76 ? 1.25 90.63 ? 0.56 5.10</figDesc><table><row><cell></cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Deezer-Europe</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>#nodes</cell><cell>183</cell><cell>251</cell><cell>183</cell><cell>7,600</cell><cell>2,277</cell><cell>5,201</cell><cell>28,281</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell></row><row><cell>#edges</cell><cell>295</cell><cell>499</cell><cell>309</cell><cell>33,544</cell><cell>36,101</cell><cell>217,073</cell><cell>92,752</cell><cell>5,429</cell><cell>4,732</cell><cell>44,338</cell></row><row><cell>#features</cell><cell>1,703</cell><cell>1,703</cell><cell>1,703</cell><cell>931</cell><cell>2,325</cell><cell>2,089</cell><cell>31,241</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell></row><row><cell>#classes</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell>3</cell></row><row><cell>H edge</cell><cell>0.5669</cell><cell>0.4480</cell><cell>0.4106</cell><cell>0.3750</cell><cell>0.2795</cell><cell>0.2416</cell><cell>0.5251</cell><cell>0.8100</cell><cell>0.7362</cell><cell>0.8024</cell></row><row><cell>H node</cell><cell>0.3855</cell><cell>0.1498</cell><cell>0.0968</cell><cell>0.2210</cell><cell>0.2470</cell><cell>0.2156</cell><cell>0.5299</cell><cell>0.8252</cell><cell>0.7175</cell><cell>0.7924</cell></row><row><cell>H class</cell><cell>0.0468</cell><cell>0.0941</cell><cell>0.0013</cell><cell>0.0110</cell><cell>0.0620</cell><cell>0.0254</cell><cell>0.0304</cell><cell>0.7657</cell><cell>0.6270</cell><cell>0.6641</cell></row><row><cell>Data Splits</cell><cell cols="10">60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 60%/20%/20% 50%/25%/25% 60%/20%/20% 60%/20%/20% 60%/20%/20%</cell></row><row><cell>H M agg (G)</cell><cell>0.8032</cell><cell>0.7768</cell><cell>0.694</cell><cell>0.6822</cell><cell>0.61</cell><cell>0.3566</cell><cell>0.5790</cell><cell>0.9904</cell><cell>0.9826</cell><cell>0.9432</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Test Accuracy (%) of State-of-the-art Models, Baseline GNN Models and ACM-GNN models</cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell cols="2">MLP-2 91.30 Geom-GCN* 60.81</cell><cell>64.12</cell><cell>67.57</cell><cell>31.63</cell><cell>60.9</cell><cell>38.14</cell><cell>NA</cell><cell>85.27</cell><cell>77.99</cell><cell>90.05</cell><cell>27.44</cell></row><row><cell>SGC-1</cell><cell>70.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>14 the optimal searched hyperparameters. The results and comparison give us the same conclusion as in Appendix A.1. ? 3.67 64.51 ? 3.66 66.76 ? 2.72 31.59 ? 1.15 60.00 ? 2.81 38.15 ? 0.92 85.35 ? 1.57 78.02 ? 1.15 89.95 ? 0.47 18.22 H2GCN 82.70 ? 5.28 87.65 ? 4.98 84.86 ? 7.23 35.70 ? 1.00 60.11 ? 2.15 36.48 ? 1.86 87.87 ? 1.20 77.11 ? 1.57 89.49 ? 0.38 15.11 GPRGCN 78.11 ? 6.55 82.55 ? 6.23 81.35 ? 5.32 35.16 ? 0.9 62.59 ? 2.04 46.31 ? 2.46 87.95 ? 1.18 77.13 ? 1.67 87.54 ? 0.38 17.67 FAGCN 76.76 ? 5.87 79.61 ? 1.58 76.49 ? 2.87 34.82 ? 1.35 46.07 ? 2.11 30.83 ? 0.69 88.05 ? 1.57 77.07 ? 2.05 88.09 ? 1.38 20.00 GCNII 77.86 ? 3.79 80.39 ? 3.40 77.57 ? 3.83 37.44 ? 1.30 63.86 ? 3.04 38.47 ? 1.58 88.37 ? 1.25 77.33 ? 1.48 90.15 ? 0.43 12.44 MixHop 73.51 ? 6.34 75.88 ? 4.90 77.84 ? 7.73 32.22 ? 2.34 60.50 ? 2.53 43.80 ? 1.48 87.61 ? 0.85 76.26 ?1.33 85.31 ? 0.61 20.78 WRGAT 81.62 ?3.90 86.98 ? 3.78 83.62 ? 5.50 36.53 ? 0.77 65.24 ? 0.87 48.85 ? 0.78 88.20 ? 2.26 76.81 ? 1.89 88.52 ? 0.92 14.33 GGCN 85.68 ? 6.63 86.86 ? 3.29 84.86 ? 4.55 37.54 ? 1.56 71.14 ?1.84 55.17 ? 1.58 87.95 ? 1.05 77.14 ? 1.45 89.15 ? 0.37 10.22 LINKX 77.84 ? 5.81 75.49 ? 5.72 74.60 ? 8.37 36.10 ? 1.55 68.42 ? 1.38 61.81 ? 1.80 84.64 ? 1.13 73.19 ? 0.99 87.86 ? 0.77 18.78 GloGNN 83.51 ? 4.26 87.06 ? 3.53 84.32 ? 4.15 37.35 ? 1.30 69.78 ? 2.42 57.54 ? 1.39 88.31 ? 1.13 77.41 ? 1.65 89.62 ? 0.35 8.78 GloGNN++ 85.95 ? 5.10 88.04 ? 3.22 84.05 ? 4.90 37.70 ? 1.40 71.21 ? 1.84 57.88 ? 1.76 88.33 ? 1.09 77.22 ? 1.78 89.24 ? 0.39 7.33 ACM-SGC-1 82.43 ? 5.44 86.47 ? 3.77 81.89 ? 4.53 35.49 ? 1.06 63.99 ? 1.66 45.00 ? 1.4 86.9 ? 1.38 76.73 ? 1.59 88.49 ? 0.51 17.56 ACM-SGC-2 82.43 ? 5.44 86.47 ? 3.77 81.89 ? 4.53 36.04 ? 0.83 59.21 ? 2.22 40.02 ? 0.96 87.69 ? 1.07 76.59 ? 1.69 89.01 ? 0.6 17.67 Diag-NSD 86.49 ? 7.35 88.63 ? 2.75 85.67 ? 6.95 37.79 ? 1.01 68.68 ? 1.73 54.78 ? 1.81 87.14 ? 1.06 77.14 ? 1.85 89.42 ? 0.43 9.00 O(d)-NSD 84.86 ? 4.71 89.41 ? 4.74 85.95 ? 5.51 37.81 ? 1.15 68.04 ? 1.58 56.34 ? 1.32 86.90 ? 1.13 76.70 ? 1.57 89.49 ? 0.40 10.44 Gen-NSD 85.68 ? 6.51 89.21 ? 3.84 82.97 ? 5.13 37.80 ? 1.22 67.93 ? 1.58 53.17 ? 1.31 87.30 ? 1.15 76.32 ? 1.65 89.33 ? 0.35 ? 6.07 88.43 ? 3.22 87.84 ? 4.4 36.63 ? 0.84 69.14 ? 1.91 55.19 ? 1.49 87.91 ? 0.95 77.32 ? 1.7 90.00 ? 0.52 8.11 ACMII-GCN 85.95 ? 5.64 87.45 ? 3.74 86.76 ? 4.75 36.31 ? 1.2 68.46 ? 1.7 51.8 ? 1.5 88.01 ? 1.08 77.15 ? 1.45 89.89 ? 0.43 9.33 ACM-GCN+ 85.68 ? 4.84 88.43 ? 2.39 88.38 ? 3.64 36.26 ? 1.34 74.47 ? 1.84 66.98 ? 1.71 88.05 ? 0.99 77.67 ? 1.19 89.82 ? 0.41 5.33 ACMII-GCN+ 85.41 ? 5.3 88.04 ? 3.66 88.11 ? 3.24 36.14 ? 1.44 74.56 ? 2.08 67.07 ? 1.65 88.19 ? 1.17 77.2 ? 1.61 89.78 ? 0.49 6.78 ACM-GCN++ 85.68 ? 5.8 88.24 ? 3.16 88.38 ? 3.43 37.31 ? 1.09 74.41 ? 1.49 67.06 ? 1.66 88.11 ? 0.96 77.46 ? 1.65 89.65 ? 0.58 5.33 ACMII-GCN++ 86.49 ? 6.73 88.43 ? 3.66 88.38 ? 3.43 37.09 ? 1.32 74.76 ? 2.2 67.4 ? 2.21 88.25 ? 0.96 77.12 ? 1.58 89.71 ? 0.48 4.78</figDesc><table><row><cell>Datasets/Models</cell><cell>Cornell</cell><cell>Wisconsin</cell><cell>Texas</cell><cell>Film</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell><cell>Average Rank</cell></row><row><cell>Geom-GCN</cell><cell cols="10">60.54 11.67</cell></row><row><cell>NLMLP</cell><cell>84.9 ? 5.7</cell><cell>87.3 ? 4.3</cell><cell>85.4 ? 3.8</cell><cell>37.9 ? 1.3</cell><cell>50.7 ? 2.2</cell><cell>33.7 ? 1.5</cell><cell>76.9 ? 1.8</cell><cell>73.4 ? 1.9</cell><cell>88.2 ? 0.5</cell><cell>16.67</cell></row><row><cell>NLGCN</cell><cell>57.6 ? 5.5</cell><cell>60.2 ? 5.3</cell><cell>65.5 ? 6.6</cell><cell>31.6 ? 1.0</cell><cell>70.1 ? 2.9</cell><cell>59.0 ? 1.2</cell><cell>88.1 ? 1.0</cell><cell>75.2 ? 1.4</cell><cell>89.0 ? 0.5</cell><cell>17.44</cell></row><row><cell>NLGAT</cell><cell>54.7 ? 7.6</cell><cell>56.9 ? 7.3</cell><cell>62.6 ? 7.1</cell><cell>29.5 ? 1.3</cell><cell>65.7 ? 1.4</cell><cell>56.8 ? 2.5</cell><cell>88.5 ? 1.8</cell><cell>76.2 ? 1.6</cell><cell>88.2 ? 0.3</cell><cell>18.56</cell></row><row><cell>ACM-GCN</cell><cell>85.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on fixed splits provided by<ref type="bibr" target="#b34">[35]</ref>: average test accuracy ? standard deviation on 9 real-world benchmark datasets. The best results are highlighted. Results of Geom-GCN, H 2 GCN and GPRGNN, LINX, GloGNN, GloGNN++, Diag-NSD, O(d)-NSD, Gen-NSD, NLMLP, NLGCN and NLGAT are from<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref>; results on the rest models are run by ourselves and the hyperparameter searching range is the same astable 9. A.3 Discussion of Random Walk and Symmetric Renormalized Filters ? 1.83 94.92 ? 2.48 94.1 ? 2.56 Wisconsin 95.75 ? 2.03 96.62 ? 2.44 95.63 ? 2.81 96.25 ? 2.5 Texas 94.92 ? 2.88 95.08 ? 2.07 94.75 ? 2.01 94.59 ? 2.65 Film 41.62 ? 1.15 41.84 ? 1.15 41.58 ? 1.3 41.65 ? 0.6 Chameleon 69.04 ? 1.74 68.38 ? 1.36 67.9 ? 2.76 68.03 ? 1.68 Squirrel 58.02 ? 1.86 54.53 ? 2.09 54.18 ? 1.35 53.68 ? 1.74 Cora 88.62 ? 1.22 89.00 ? 0.72 88.65 ? 1.26 88.19 ? 1.38 Citeseer 81.68 ? 0.97 81.79 ? 0.95 81.84 ? 1.15 81.81 ? 0.86 PubMed 90.66 ? 0.47 90.74 ? 0.5 90.59 ? 0.81 90.54 ? 0.59</figDesc><table><row><cell></cell><cell>RW</cell><cell></cell><cell cols="2">Symmetric</cell></row><row><cell>Datasets/Models</cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell>95.9</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of random walk and symmetric renormalized filters The definitions of the similarity matrix, (modified) aggregation similarity score and diversification distinguishability value can be extended to symmetric normalized Laplacian or other aggregation ? 1.83 93.61 ? 2.37 90.49 ? 2.72 Wisconsin 95.75 ? 2.03 96.62 ? 2.44 95 ? 2.5 97.50 ? 1.25 Texas 94.92 ? 2.88 95.08 ? 2.07 94.92 ? 2.79 94.92 ? 2.79 Film 41.62 ? 1.15 41.84 ? 1.15 40.79 ? 1.01 40.86 ? 1.48 Chameleon 69.04 ? 1.74 68.38 ? 1.36 68.16 ? 1.79 66.78 ? 2.79 Squirrel 58.02 ? 1.86 54.53 ? 2.09 55.35 ? 1.72 52.98 ? 1.66 Cora 88.62 ? 1.22 89.00 ? 0.72 88.41 ? 1.63 88.72 ? 1.5 Citeseer 81.68 ? 0.97 81.79 ? 0.95 81.65 ? 1.48 81.72 ? 1.58 PubMed 90.66 ? 0.47 90.74 ? 0.5 90.46 ? 0.69 90.39 ? 1.33</figDesc><table><row><cell></cell><cell cols="2">With W mix</cell><cell cols="2">Without W mix</cell></row><row><cell>Datasets/Models</cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell>95.9</cell><cell></cell><cell></cell></row></table><note>operations. Yet unfortunately, we cannot extend Theorem 1 at this moment, because we need a condition that the row sum of? is not greater than 1 in the proof. This condition is guaranteed for random walk normalized Laplacian but not for symmetric normalized Laplacian. While in practice, we evaluate our models with symmetric filters and compare them with random walk filters. From table 4 we can see that, there are no big differences between these two filters.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of W mix</figDesc><table /><note>A.4 Ablation Study of W mix From table 5 we can see that ACM(II) with W mix shows superiority in most datasets, although it is not statistically significant on some of them. One possible explanation of the function of W mix is that it could help alleviate the dominance and bias to majority: Suppose in a dataset, most of the nodes need more information from LP channel than HP and identity channels, then W L , W H , W I tend to learn larger ? L than ? H and ? I . For the minority nodes that need more information from HP or identity channels, they are hard to get large ? H or ? I values because W L , W H , W I are biased to the majority. And W mix can help us to learn more diverse alpha values when W L , W H , W I are biased.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? 1.83 95.08 ? 2.64 93.93 ? 3.52 Wisconsin 95.75 ? 2.03 96.62 ? 2.44 96.12 ? 1.31 96 ? 2 Texas 94.92 ? 2.88 95.08 ? 2.07 94.92 ? 2.48 94.59 ? 2.94 Film 41.62 ? 1.15 41.84 ? 1.15 41.62 ? 1.34 41.44 ? 1.18 Chameleon 69.04 ? 1.74 68.38 ? 1.36 68.82 ? 2.18 68.53 ? 3.08 Squirrel 58.02 ? 1.86 54.53 ? 2.09 57.48 ? 1.68 53.28 ? 1.08 Cora 88.62 ? 1.22 89.00 ? 0.72 88.59 ? 1.04 88.75 ? 0.83 Citeseer 81.68 ? 0.97 81.79 ? 0.95 81.9 ? 1.27 81.76 ? 1.05 PubMed 90.66 ? 0.47 90.74 ? 0.5 90.75 ? 0.77 90.58 ? 0.64</figDesc><table><row><cell></cell><cell cols="2">With Raw Features</cell><cell cols="2">With Combined Features</cell></row><row><cell>Datasets/Models</cell><cell>ACM</cell><cell>ACMII</cell><cell>ACM</cell><cell>ACMII</cell></row><row><cell>Cornell</cell><cell>94.75 ? 3.8</cell><cell>95.9</cell><cell></cell><cell></cell></row></table><note>A.5 Learn Weights with Raw Features v.s. Combined Features</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison between raw features and combined features</figDesc><table><row><cell>Construct the combined feature H l Comb = [H l L , H l H , H l I ], Replace the first line in Step 2 by the</cell></row><row><cell>following lines:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>table 2</head><label>2</label><figDesc>Computing Resources For all experiments on synthetic datasets and real-world datasets, we use NVIDIA V100 GPUs with 16/32GB GPU memory, 8-core CPU, 16G Memory. The software implementation is based on PyTorch and PyTorch Geometric<ref type="bibr" target="#b11">[12]</ref>.</figDesc><table><row><cell cols="4">C Hyperparameter Searching Range &amp; Optimal Hyperparameters</cell><cell></cell></row><row><cell cols="3">C.1 Hyperparameter Searching Range for Synthetic Experiments</cell><cell></cell><cell></cell></row><row><cell cols="4">Hyperparameter Searching Range for Synthetic Experiments</cell><cell></cell></row><row><cell>Models\Hyperparameters</cell><cell>lr</cell><cell>weight_decay</cell><cell>dropout</cell><cell>hidden</cell></row><row><cell>MLP-1</cell><cell cols="2">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3 }</cell><cell>-</cell><cell>-</cell></row><row><cell>SGC-1</cell><cell cols="2">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3}</cell><cell>-</cell><cell>-</cell></row><row><cell>ACM-SGC-1</cell><cell cols="3">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9}</cell><cell>-</cell></row><row><cell>MLP-2</cell><cell cols="3">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9}</cell><cell>64</cell></row><row><cell>GCN</cell><cell cols="3">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9}</cell><cell>64</cell></row><row><cell>ACM-GCN</cell><cell cols="3">0.05 {5e-5, 1e-4, 5e-4, 1e-3, 5e-3} { 0.1, 0.3, 0.5, 0.7, 0.9}</cell><cell>64</cell></row></table><note>in Appendix A.1. The results on fixed 48%/32%/20% splits are reported in table 3 in Appendix A.2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Hyperparameter searching range for synthetic experiments C.2 Hyperparameter Searching Range for Ablation Study</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>table 10 ,</head><label>10</label><figDesc></figDesc><table><row><cell>)-GCN++ in</cell></row></table><note>for ACM-GNNs and ACMII-GNNs in table 11 and for ACM(II)-GCN+ and ACM(II</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameter searching range for training on real-world datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Hyperparameters for Baseline GNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>Models\Hyperparameters</cell><cell>lr</cell><cell cols="9">weight_decay dropout hidden # layers Gat heads JK Type lambda alpha_l results</cell><cell>std</cell><cell>average epoch time/average total time</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.98</cell><cell>8.39</cell><cell>2.53ms/0.51s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.05</cell><cell>1.00E-03</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.62</cell><cell>9.92</cell><cell>2.46ms/0.53s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>5.00E-03</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.46</cell><cell>3.11</cell><cell>3.67ms/0.74s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.01</cell><cell>5.00E-03</cell><cell>0.4</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.62</cell><cell>2.34</cell><cell>4.24ms/0.87s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.01</cell><cell>5.00E-03</cell><cell>0.4</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.95</cell><cell>2.1</cell><cell>6.66ms/1.36s</cell></row><row><cell>Cornell</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-03 1.00E-03</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>16 8</cell><cell>--</cell><cell>--</cell><cell>0.5 0.5</cell><cell>0.5 0.5</cell><cell>89.18 90.49</cell><cell>3.96 4.45</cell><cell>25.41ms/8.11s 15.35ms/4.05s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.01</cell><cell>1.00E-04</cell><cell>0.7</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.03</cell><cell>5.6</cell><cell>8.1ms/3.8858s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">60.33 28.53</cell><cell>10.379ms/2.105s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.23</cell><cell>4.71</cell><cell>4.381ms/1.123s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell cols="2">66.56 13.82</cell><cell>5.589ms/1.227s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell cols="2">74.43 10.24</cell><cell>10.725ms/2.478s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.05</cell><cell>5.00E-03</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.38</cell><cell>2.85</cell><cell>2.83ms/0.57s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.75</cell><cell>2.89</cell><cell>2.14ms/0.43s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0.7</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.5</cell><cell>2.92</cell><cell>3.74ms/0.76s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.88</cell><cell>3.42</cell><cell>3.73ms/0.76s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.05</cell><cell>5.00E-04</cell><cell>0.8</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.5</cell><cell>5.01</cell><cell>5.46ms/1.12s</cell></row><row><cell></cell><cell>GCNII</cell><cell>0.01</cell><cell>1.00E-03</cell><cell>0.5</cell><cell>64</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>0.5</cell><cell>83.25</cell><cell>2.69</cell><cell></cell></row><row><cell>Wisconsin</cell><cell>GCNII* FAGCN</cell><cell>0.01 0.05</cell><cell>1.00E-03 1.00E-04</cell><cell>0.5 0</cell><cell>64 32</cell><cell>4 2</cell><cell>--</cell><cell>--</cell><cell>1.5 -</cell><cell>0.3 -</cell><cell>89.12 89.75</cell><cell>3.06 6.37</cell><cell>9.26ms/1.96s 12.9ms/4.6359s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.25</cell><cell>7.80</cell><cell>10.281ms/2.095s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.5</cell><cell>1.77</cell><cell>4.324ms/1.134s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>62.5</cell><cell>15.75</cell><cell>5.117ms/1.049s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>69.5</cell><cell>3.12</cell><cell>10.762ms/2.25s</cell></row><row><cell></cell><cell>APPNP</cell><cell>0.05</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92</cell><cell>3.59</cell><cell>10.303ms/2.104s</cell></row><row><cell></cell><cell>GPRGNN</cell><cell>0.05</cell><cell>0.001</cell><cell>0.5</cell><cell>256</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.75</cell><cell>2.37</cell><cell>11.856ms/2.415s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.05</cell><cell>1.00E-03</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.28</cell><cell>5.43</cell><cell>2.55ms/0.54s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.01</cell><cell>1.00E-03</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.31</cell><cell>3.3</cell><cell>2.61ms/2.53s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.11</cell><cell>3.2</cell><cell>3.59ms/0.73s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.11</cell><cell>3.2</cell><cell>3.98ms/0.82s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0.9</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.11</cell><cell>3.2</cell><cell>5.56ms/1.12s</cell></row><row><cell>Texas</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-04 1.00E-04</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>4 8</cell><cell>--</cell><cell>--</cell><cell>1.5 0.5</cell><cell>0.5 0.5</cell><cell>82.46 88.52</cell><cell>4.58 3.02</cell><cell>15.64ms/3.47s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.01</cell><cell>5.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.85</cell><cell>4.39</cell><cell>8.8ms/6.5252s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.39</cell><cell>7.66</cell><cell>11.099ms/2.329s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.90</cell><cell>3.53</cell><cell>4.197ms/0.95s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>80.66</cell><cell>1.91</cell><cell>5.28ms/1.085s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>75.41</cell><cell>7.18</cell><cell>10.937ms/2.402s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.01</cell><cell>5.00E-06</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.26</cell><cell>1.18</cell><cell>3.18ms/0.70s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.01</cell><cell>5.00E-06</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.81</cell><cell>1.11</cell><cell>2.13ms/0.43s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.51</cell><cell>0.99</cell><cell>4.86ms/0.99s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35.97</cell><cell>0.66</cell><cell>5.59ms/1.14s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0.2</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36</cell><cell>1.36</cell><cell>7.89ms/1.60s</cell></row><row><cell>Film</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-04 1.00E-06</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>8 4</cell><cell>--</cell><cell>--</cell><cell>1.5 1</cell><cell>0.3 0.1</cell><cell>40.82 41.54</cell><cell>1.79 0.99</cell><cell>15.85ms/3.22s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.01</cell><cell>5.00E-05</cell><cell>0.6</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.59</cell><cell>1.37</cell><cell>45.4ms/11.107s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>8</cell><cell>3</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>33.13</cell><cell>2.40</cell><cell>17.651ms/3.566s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>64</cell><cell>1</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>38.85</cell><cell>1.17</cell><cell>8.101ms/1.695s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>8</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>32.72</cell><cell>2.62</cell><cell>8.946ms/1.807s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>4</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>35.41</cell><cell>0.97</cell><cell>20.726ms/4.187s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.1</cell><cell>5.00E-06</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.86</cell><cell>1.81</cell><cell>3.48ms/2.96s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.1</cell><cell>0.00E+00</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.67</cell><cell>2.41</cell><cell>4.43ms/1.12s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.01</cell><cell>1.00E-05</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.18</cell><cell>2.62</cell><cell>4.96ms/1.18s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>1.00E-01</cell><cell>1.00E-05</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.99</cell><cell>2.39</cell><cell>4.96ms/1.00s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.1</cell><cell>5.00E-06</cell><cell>0.9</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.49</cell><cell>1.64</cell><cell>7.44ms/1.50s</cell></row><row><cell>Chameleon</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>5.00E-06 5.00E-04</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>4 4</cell><cell>--</cell><cell>--</cell><cell>0.5 1.5</cell><cell>0.1 0.5</cell><cell>60.35 62.8</cell><cell>2.7 2.87</cell><cell>9.76ms/2.26s 10.40ms/2.17s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.002</cell><cell>1.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.47</cell><cell>2.84</cell><cell>8.4ms/13.8696s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>36.28</cell><cell>10.2</cell><cell>11.372ms/2.297s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>32</cell><cell>1</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>52.3</cell><cell>0.48</cell><cell>4.059ms/0.82s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>8</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>64.68</cell><cell>2.85</cell><cell>5.211ms/1.053s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>68.14</cell><cell>1.18</cell><cell>13.772ms/2.788s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.05</cell><cell>0.00E+00</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.62</cell><cell>1.27</cell><cell>4.65ms/1.44s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.1</cell><cell>0.00E+00</cell><cell>0.9</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.25</cell><cell>1.4</cell><cell>35.06ms/7.81s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.01</cell><cell>5.00E-05</cell><cell>0.7</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.76</cell><cell>1.39</cell><cell>8.41ms/2.50s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>0.00E+00</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.88</cell><cell>1.23</cell><cell>8.96ms/1.92s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.1</cell><cell>0.00E+00</cell><cell>0.8</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.25</cell><cell>0.94</cell><cell>14.00ms/2.90s</cell></row><row><cell>Squirrel</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-04 5.00E-04</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>4 4</cell><cell>--</cell><cell>--</cell><cell>1.5 1.5</cell><cell>0.2 0.3</cell><cell>38.81 38.31</cell><cell>1.97 1.3</cell><cell>13.35ms/2.70s 13.81ms/2.78s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.05</cell><cell>1.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.24</cell><cell>1.2</cell><cell>16ms/6.7961s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.55</cell><cell>2.6</cell><cell>17.634ms/3.562s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>16</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.39</cell><cell>1.22</cell><cell>9.315ms/1.882s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>53.4</cell><cell>1.9</cell><cell>14.321ms/2.905s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>8</cell><cell>2</cell><cell>4</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>52.28</cell><cell>3.61</cell><cell>29.097ms/5.878s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.1</cell><cell>5.00E-06</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.12</cell><cell>1.64</cell><cell>3.47ms/11.55s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.1</cell><cell>1.00E-05</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.48</cell><cell>1.48</cell><cell>2.91ms/6.85s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0.2</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.78</cell><cell>0.96</cell><cell>4.24ms/0.86s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0.1</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.64</cell><cell>1.15</cell><cell>4.65ms/0.94s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.05</cell><cell>1.00E-03</cell><cell>0.6</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.33</cell><cell>1.3</cell><cell>6.41ms/1.32s</cell></row><row><cell>Cora</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-04 5.00E-04</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>16 4</cell><cell>--</cell><cell>--</cell><cell>0.5 0.5</cell><cell>0.2 0.5</cell><cell>88.98 88.93</cell><cell>1.33 1.37</cell><cell>10.16ms/2.24s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.05</cell><cell>5.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.85</cell><cell>1.36</cell><cell>8.4ms/3.3183s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">65.65 11.31</cell><cell>11.177ms/2.278s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>32</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.52</cell><cell>0.61</cell><cell>4.335ms/1.209s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>86.90</cell><cell>1.51</cell><cell>6.656ms/1.346s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>2</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>89.52</cell><cell>0.43</cell><cell>12.91ms/2.608s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.66</cell><cell>0.75</cell><cell>3.43ms/7.30s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.01</cell><cell>5.00E-04</cell><cell>0.9</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.75</cell><cell>1.15</cell><cell>5.33ms/4.40s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0.9</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.39</cell><cell>1.23</cell><cell>4.18ms/0.86s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0.8</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.53</cell><cell>1.71</cell><cell>5.19ms/1.11s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.1</cell><cell>1.00E-03</cell><cell>0.9</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.93</cell><cell>1.32</cell><cell>7.64ms/1.69s</cell></row><row><cell>CiteSeer</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-03 1.00E-03</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>16 16</cell><cell>--</cell><cell>--</cell><cell>0.5 0.5</cell><cell>0.2 0.2</cell><cell>81.58 81.83</cell><cell>1.3 1.78</cell><cell>32.50ms/10.29s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.05</cell><cell>5.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.37</cell><cell>1.46</cell><cell>9.4ms/4.7648s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">49.52 13.35</cell><cell>13.793ms/2.786s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>8</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.97</cell><cell>0.69</cell><cell>5.794ms/3.049s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>73.77</cell><cell>1.85</cell><cell>5.264ms/1.063s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.001</cell><cell>0.001</cell><cell>0.5</cell><cell>8</cell><cell>2</cell><cell>4</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>74.49</cell><cell>2.76</cell><cell>12.326ms/2.49s</cell></row><row><cell></cell><cell>SGC-1</cell><cell>0.05</cell><cell>5.00E-06</cell><cell>0.3</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.75</cell><cell>0.88</cell><cell>6.04ms/2.61s</cell></row><row><cell></cell><cell>SGC-2</cell><cell>0.05</cell><cell>5.00E-05</cell><cell>0.1</cell><cell>64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.79</cell><cell>0.5</cell><cell>8.62ms/3.18s</cell></row><row><cell></cell><cell>GCN</cell><cell>0.1</cell><cell>5.00E-05</cell><cell>0.6</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.9</cell><cell>0.32</cell><cell>5.08ms/1.03s</cell></row><row><cell></cell><cell>Snowball-2</cell><cell>0.1</cell><cell>5.00E-04</cell><cell>0</cell><cell>64</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.04</cell><cell>0.49</cell><cell>5.68ms/1.19s</cell></row><row><cell></cell><cell>Snowball-3</cell><cell>0.1</cell><cell>5.00E-06</cell><cell>0</cell><cell>64</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.8</cell><cell>0.82</cell><cell>8.54ms/1.75s</cell></row><row><cell>PubMed</cell><cell>GCNII GCNII*</cell><cell>0.01 0.01</cell><cell>1.00E-06 1.00E-06</cell><cell>0.5 0.5</cell><cell>64 64</cell><cell>4 4</cell><cell>--</cell><cell>--</cell><cell>0.5 0.5</cell><cell>0.5 0.1</cell><cell>89.8 89.98</cell><cell>0.3 0.52</cell><cell>10.98ms/3.21s 11.47ms/3.24s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.05</cell><cell>5.00E-04</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.98</cell><cell>0.54</cell><cell>14.5ms/6.411s</cell></row><row><cell></cell><cell>Mixhop</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>16</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.04</cell><cell>4.10</cell><cell>17.459ms/3.527s</cell></row><row><cell></cell><cell>H2GCN</cell><cell>0.01</cell><cell>0.001</cell><cell>0</cell><cell>64</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.78</cell><cell>0.28</cell><cell>8.039ms/2.28s</cell></row><row><cell></cell><cell>GCN+JK</cell><cell>0.01</cell><cell>0.001</cell><cell>0.5</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>-</cell><cell>90.09</cell><cell>0.68</cell><cell>12.001ms/2.424s</cell></row><row><cell></cell><cell>GAT+JK</cell><cell>0.1</cell><cell>0.001</cell><cell>0.5</cell><cell>8</cell><cell>2</cell><cell>4</cell><cell>max</cell><cell>-</cell><cell>-</cell><cell>89.15</cell><cell>0.87</cell><cell>20.403ms/4.125s</cell></row><row><cell></cell><cell>FAGCN</cell><cell>0.01</cell><cell>0.0001</cell><cell>0</cell><cell>32</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.86</cell><cell>0.53</cell><cell>41.7ms/20.8362s</cell></row><row><cell>Deezer-Europe</cell><cell>GCNII</cell><cell>0.01</cell><cell>5e-6,1e-5</cell><cell>0.5</cell><cell>64</cell><cell>32</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>0.5</cell><cell>66.38</cell><cell>0.45</cell><cell>126.58ms/63.16s</cell></row><row><cell></cell><cell>GCNII*</cell><cell>0.01</cell><cell>1e-4,1e-3</cell><cell>0.5</cell><cell>64</cell><cell>32</cell><cell>-</cell><cell>-</cell><cell>0.5</cell><cell>0.5</cell><cell>66.42</cell><cell>0.56</cell><cell>134.05ms/66.89s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Gat heads JK Type lambda alpha_l results std average epoch time/average total time</figDesc><table><row><cell>Hyperparameters for ACM-GNNs and ACMII-GNNs</cell></row></table><note>Optimal hyperparameters for baseline models on random 60%/20%/20% splits</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table><row><cell>Datasets</cell><cell>Models\Hyperparameters</cell><cell>lr</cell><cell cols="7">weight_decay dropout hidden with A results std average epoch time/average total time</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0.1</cell><cell>64</cell><cell>Y</cell><cell cols="2">94.92 2.79</cell><cell>16.66ms/3.37s</cell></row><row><cell>Cornell</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.05 0.1</cell><cell>1.00E-02 5.00E-03</cell><cell>0.3 0.4</cell><cell>64 64</cell><cell>Y N</cell><cell cols="2">93.93 1.05 93.93 3.03</cell><cell>12.55ms/2.56s 12.89ms/2.62s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.05</cell><cell>1.00E-02</cell><cell>0.6</cell><cell>64</cell><cell>Y</cell><cell cols="2">92.62 2.57</cell><cell>18.25ms/3.69s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.05</cell><cell>1.00E-02</cell><cell>0.3</cell><cell>64</cell><cell>Y</cell><cell>96.5</cell><cell>2.08</cell><cell>16.54ms/3.35s</cell></row><row><cell>Wisconsin</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.01 0.05</cell><cell>1.00E-02 1.00E-02</cell><cell>0.1 0.1</cell><cell>64 64</cell><cell>N Y</cell><cell cols="2">97.5 96.75 1.79 1.25</cell><cell>12.09ms/2.88s 18.12ms/3.66s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.01</cell><cell>1.00E-02</cell><cell>0.1</cell><cell>64</cell><cell>Y</cell><cell cols="2">97.13 1.68</cell><cell>17.32ms/3.53s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.05</cell><cell>1.00E-03</cell><cell>0.3</cell><cell>64</cell><cell>N</cell><cell cols="2">94.92 2.79</cell><cell>12.05ms/2.44s</cell></row><row><cell>Texas</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.05 0.05</cell><cell>1.00E-02 5.00E-04</cell><cell>0.1 0.2</cell><cell>64 64</cell><cell>Y N</cell><cell cols="2">96.56 95.41 2.82 2</cell><cell>22.63 ms/4.58s 13.20ms/2.67s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.05</cell><cell>5.00E-04</cell><cell>0.1</cell><cell>64</cell><cell>N</cell><cell cols="2">94.75 2.91</cell><cell>12.82ms/2.60s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.01</cell><cell>1.00E-03</cell><cell>0.8</cell><cell>64</cell><cell>N</cell><cell cols="2">41.79 1.01</cell><cell>13.57ms/3.59s</cell></row><row><cell>Film</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.1 0.002</cell><cell>5.00E-05 5.00E-03</cell><cell>0.7 0.9</cell><cell>64 64</cell><cell>N N</cell><cell cols="2">41.86 1.48 41.5 1.54</cell><cell>13.38ms/3.59s 13.76ms/2.77s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.002</cell><cell>5.00E-03</cell><cell>0.9</cell><cell>64</cell><cell>N</cell><cell cols="2">41.66 1.42</cell><cell>13.67ms/2.77s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.002</cell><cell>1.00E-03</cell><cell>0.4</cell><cell>64</cell><cell>Y</cell><cell cols="2">76.08 2.13</cell><cell>18.19ms/8.60s</cell></row><row><cell>Chameleon</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.1 0.1</cell><cell>1.00E-04 5.00E-05</cell><cell>0.7 0.8</cell><cell>64 64</cell><cell>Y Y</cell><cell cols="2">75.23 1.72 75.51 1.58</cell><cell>17.39ms/3.57s 18.69ms/4.17s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.01</cell><cell>1.00E-04</cell><cell>0.8</cell><cell>64</cell><cell>Y</cell><cell cols="2">75.93 1.71</cell><cell>18.70ms/4.53s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.01</cell><cell>1.00E-04</cell><cell>0.6</cell><cell>64</cell><cell>Y</cell><cell cols="2">69.26 1.11</cell><cell>24.71ms/4.97s</cell></row><row><cell>Squirrel</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.01 0.002</cell><cell>1.00E-04 1.00E-03</cell><cell>0.6 0.7</cell><cell>64 64</cell><cell>Y Y</cell><cell cols="2">68.56 1.33 69.81 1.11</cell><cell>21.21ms/4.26s 22.14ms/5.34s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.002</cell><cell>1.00E-04</cell><cell>0.7</cell><cell>64</cell><cell>Y</cell><cell cols="2">69.98 1.53</cell><cell>21.78ms/4.38s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.1</cell><cell>5.00E-03</cell><cell>0.3</cell><cell>64</cell><cell>Y</cell><cell cols="2">89.75 1.16</cell><cell>17.29ms/3.52s</cell></row><row><cell>Cora</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.1 0.05</cell><cell>5.00E-03 5.00E-03</cell><cell>0.5 0.4</cell><cell>64 64</cell><cell>Y Y</cell><cell cols="2">89.33 0.81 89.18 1.11</cell><cell>18.08ms/3.69s 18.21ms/3.69s</cell></row><row><cell></cell><cell>ACMII-GCN++(with xX)</cell><cell>0.1</cell><cell>1.00E-02</cell><cell>0.1</cell><cell>64</cell><cell>Y</cell><cell cols="2">89.47 1.08</cell><cell>18.53ms/3.76s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.1</cell><cell>1.00E-05</cell><cell>0.5</cell><cell>64</cell><cell>N</cell><cell cols="2">81.65 1.48</cell><cell>12.44ms/2.50s</cell></row><row><cell>CiteSeer</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.002 0.05</cell><cell>5.00E-03 5.00E-03</cell><cell>0.8 0.3</cell><cell>64 64</cell><cell>N N</cell><cell cols="2">81.83 1.65 81.87 1.38</cell><cell>14.87ms/15.36s 13.35ms/2.86s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.01</cell><cell>5.00E-04</cell><cell>0.9</cell><cell>64</cell><cell>N</cell><cell cols="2">81.76 1.25</cell><cell>14.04ms/3.88s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.1</cell><cell>1.00E-04</cell><cell>0.1</cell><cell>64</cell><cell>N</cell><cell cols="2">90.46 0.69</cell><cell>15.15ms/3.09s</cell></row><row><cell>PubMed</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.1 0.1</cell><cell>1.00E-04 1.00E-04</cell><cell>0.3 0.1</cell><cell>64 64</cell><cell>N N</cell><cell cols="2">90.39 0.33 90.96 0.62</cell><cell>17.36 ms/3.55s 16.35ms/3.47s</cell></row><row><cell></cell><cell>ACMII-GCN++(with xX)</cell><cell>0.1</cell><cell>1.00E-04</cell><cell>0.3</cell><cell>64</cell><cell>N</cell><cell cols="2">90.63 0.56</cell><cell>16.18ms/3.39s</cell></row><row><cell></cell><cell>ACM-GCN+</cell><cell>0.002</cell><cell>1.00E-06</cell><cell>0.7</cell><cell>64</cell><cell>N</cell><cell>67.4</cell><cell>0.44</cell><cell>281.97ms/140.70s</cell></row><row><cell>Deezer-Europe</cell><cell>ACMII-GCN+ ACM-GCN++(with xX)</cell><cell>0.002 0.002</cell><cell>1.00E-04 1.00E-03</cell><cell>0.8 0.7</cell><cell>64 64</cell><cell>N Y</cell><cell cols="2">67.3 67.44 0.31 0.48</cell><cell>281.48ms/140.46s 332.92ms/166.13s</cell></row><row><cell></cell><cell cols="2">ACMII-GCN++(with xX) 0.002</cell><cell>1.00E-05</cell><cell>0.8</cell><cell>64</cell><cell>N</cell><cell>67.5</cell><cell>0.53</cell><cell>326.09ms/162.72s</cell></row></table><note>Optimal hyperparameters for ACM(II)-GNNs on random 60%/20%/20% splits 25</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table><row><cell>Optimal hyperparameters for ACM(II)-GCN+ and ACM(II)-GCN++ on random</cell></row><row><cell>60%/20%/20% splits</cell></row><row><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc></figDesc><table><row><cell>Optimal hyperparameters for FAGCN and ACM(II)-GNNs on fixed 48%/32%/20% splits</cell></row><row><cell>27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Optimal hyperparameters for ACM(II)-GCN+ and ACM(II)-GCN++ on fixed 48%/32%/20% splitsD Experimental Setup and Further Discussion on Synthetic Graphs D.1 Detailed Description of Data Generation Process? For each node v, we first randomly generate its degree d v .? Given d v , for any h, we sample hd v intra-class edges and (1 ? h)d v inter-class edges.</figDesc><table><row><cell>More specifically in our synthetic experiments, for a given h,</cell></row><row><cell>? we generate node degree d v for nodes in each class from multinomial distribution with</cell></row><row><cell>numpy.random.multinomial(800/h, numpy.ones(400)/400, size=1)[0].</cell></row><row><cell>? For a sampled d v , we generate intra-class edges from (does not include self-loop)</cell></row><row><cell>numpy.random.multinomial(hd v , numpy.ones(399)/399, size=1)[0] and inter-</cell></row><row><cell>class edges from numpy.random.multinomial((1-h) d v , numpy.ones(1600)/1600,</cell></row><row><cell>size=1)[0].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Z</head><label></label><figDesc>For Y and W , we have dY =?XdW and dL = trace dL dY</figDesc><table><row><cell>T</cell><cell>dY</cell><cell>= trace</cell><cell>dL dY</cell><cell>T? X dW = trace</cell><cell>dL dW</cell></row></table><note>T dW To get dL dW we have,</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b25">[26]</ref> did not name this homophily metric. We named it class homophily based on its definition.<ref type="bibr" target="#b1">2</ref> <ref type="bibr" target="#b31">[32]</ref> use the same example but not to demonstrate the deficiency of homophily metrics.<ref type="bibr" target="#b2">3</ref> <ref type="bibr" target="#b7">[8]</ref> also point out the insufficiency of Hnode by examples to show that different graph typologies with the same Hnode(G) can carry different label information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See Appendix C.1 for a description of the hyperparameter searching range and Appendix D for more a detailed description of the data generation process<ref type="bibr" target="#b6">7</ref> A similar J-shaped curve for Hedge(G) is found in<ref type="bibr" target="#b44">[45]</ref>, though using different data generation processes. The authors do not mention the insufficiency of edge homophily.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">In graph signal processing, an additional synthesis filter<ref type="bibr" target="#b10">[11]</ref> is required to form the 2-channel filterbank. But a synthesis filter is not needed in our framework.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">See more variants in Appendix B. 10 See Appendix A.4 and A.5 for more discussion of the components in ACM architecture. 7</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We only test ACM-SGC-1 because SGC-1 does not contain any non-linearity which makes ACM-SGC-1 and ACMII-SGC-1 exactly the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">GCNII and GCNII * are hard to implement with the ACMII framework. See Appendix B for explanation.<ref type="bibr" target="#b12">13</ref> See Appendix A.3 for the comparison of?rw and?sym.<ref type="bibr" target="#b13">14</ref> We choose Deezer-Europe because MLP outperforms GCN on it<ref type="bibr" target="#b25">[26]</ref>.<ref type="bibr" target="#b14">15</ref> Seetable 3in Appendix A.2 for the performance comparison with several SOTA models, e.g., LINKX<ref type="bibr" target="#b24">[25]</ref> and GloGNN<ref type="bibr" target="#b23">[24]</ref>, on the fixed 48%/32%/20% splits provided by<ref type="bibr" target="#b34">[35]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">The splits for all these experiments are random 60%/20%/20% splits for train/valid/test. The open source code we use is from https://github.com/jianhao2016/GPRGNN/blob/ f4aaad6ca28c83d3121338a4c4fe5d162edfa9a2/src/utils.py#L16. See table 3 in Appendix A.2 for the performance comparison with several SOTA models on the fixed 48%/32%/20% splits provided by<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledge</head><p>The authors would like to give very special thanks to William L. Hamilton for valuable discussion and advice. The project was partially supported by DeepMind and NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00797</idno>
		<title level="m">Beyond low-frequency information in graph convolutional networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04579</idno>
		<title level="m">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory. Number 92</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1606.09375</idno>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Graph structured data viewed through a fourier lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Ekambaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Inductive representation learning on large graphs. arXiv, abs/1706.02216</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07308</idno>
		<title level="m">Finding global homophily in graph neural networks when meeting heterophily</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Simple truncated svd based model for node classification on heterophilic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12807</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02174</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training matters: Unlocking potentials of deeper graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08838</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08844</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-Scale Attributed Node Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An adaptive filter-bank equalizer for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1214" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<title level="m">Graph attention networks. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<title level="m">Simplifying graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
