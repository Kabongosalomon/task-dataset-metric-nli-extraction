<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
							<email>xyguo@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<email>ssshi@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stereo-based 3D detection aims at detecting 3D objects from stereo images, which provides a low-cost solution for 3D perception. However, its performance is still inferior compared with LiDAR-based detection algorithms. To detect and localize accurate 3D bounding boxes, LiDARbased detectors encode high-level representations from Li-DAR point clouds, such as accurate object boundaries and surface normal directions. In contrast, high-level features learned by stereo-based detectors are easily affected by the erroneous depth estimation due to the limitation of stereo matching. To solve the problem, we propose LIGA-Stereo (LiDAR Geometry Aware Stereo Detector) to learn stereo-based 3D detectors under the guidance of high-level geometry-aware representations of LiDAR-based detection models. In addition, we found existing voxel-based stereo detectors failed to learn semantic features effectively from indirect 3D supervisions. We attach an auxiliary 2D detection head to provide direct 2D semantic supervisions. Experiment results show that the above two strategies improved the geometric and semantic representation capabilities. Compared with the state-of-the-art stereo detector, our method has improved the 3D detection performance of cars, pedestrians, cyclists by 10.44%, 5.69%, 5.97% mAP respectively on the official KITTI benchmark. The gap between stereo-based and LiDAR-based 3D detectors is further narrowed. The code is available at https: //xy-guo.github.io/liga/. arXiv:2108.08258v1 [cs.CV] 18 Aug 2021 the LiDAR model as "soft" targets, which we found benefits little for training stereo detection networks. The erroneous regression targets would constrain the upper-bound accuracy of bounding box regression. Instead, we force our model to align intermediate features with those of Li-DAR models, which encode high-level geometry representations of the scene. The features from LiDAR models could provide powerful and discriminative high-level geometry-aware features, such as surface normal directions and boundary locations. On the other hand, the LiDAR features can provide extra regularization to alleviate the overfitting problem caused by erroneous stereo predictions.</p><p>Besides learning better geometry features, we further explore how to learn better semantic features for boosting the 3D detection performance. Instead of learning semantic features from indirect 3D supervisions, we propose to attach an auxiliary multi-scale 2D detection head on the 2D semantic features, which could directly guide the learning of 2D semantic features. Our baseline model, Deep Stereo Geometry Network (DSGN) [10], failed to benefit from extra semantic features effectively according to their ablation studies. We argue that the network provides erroneous semantic supervisions from indirect 3D supervisions because of depth estimation errors, while our proposed direct guidance could greatly benefit 3D detection performance from better learning of 2D semantic features. Experiment results show that the performance is further improved, especially for classes with few samples like cyclist.</p><p>The contributions can be summarized as follows. 1) We propose to utilize features from superior LiDAR-based detection models to guide the training of stereo-based 3D detection model. LiDAR features encode compact 3D geometry representations of the scene to guide and regularize the stereo features. 2) By attaching an auxiliary 2D detection head to provide direct 2D supervisions, our model significantly improves the learning efficiency for semantic features, which further improves the recall rate especially for rare categories. 4) On the official KITTI 3D detection benchmark, our proposed method surpasses state-of-the-art models by 10.44%, 5.69%, and 5.97% mAP on the car, pedestrian and cyclist classes respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, LiDAR-based 3D detection <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b61">61]</ref> has achieved increasing performance and stability in autonomous driving and robotics. However, the high cost of LiDAR sensors has limited its applications in low-cost products. Stereo matching <ref type="bibr">[3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35]</ref> is the most common depth sensing technique using only cameras. Compared with LiDAR sensors, stereo cameras are at a  much lower cost and higher resolutions, which makes it a suitable alternative solution for 3D perception. 3D detection from stereo images aims at detecting objects using estimated depth maps <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b65">65]</ref> or implicit 3D geometry representations <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b53">53]</ref>. However, the performance of existing stereo-based 3D detection algorithms is still inferior compared with LiDAR-based algorithms.</p><p>LiDAR-based detection algorithms take raw point cloud as inputs and then encode the 3D geometry information into intermediate and high-level feature representations. To detect and localize accurate 3D bounding boxes, the model must learn robust local features about object boundaries and surface normal directions, which are essential for predicting accurate bounding box size and orientation. The features learned by LiDAR-based detectors provide robust high-level summarization of accurate 3D geometry structures. In comparison, due to the limitation of stereo matching, the inaccurately estimated depth or implicit 3D representation have difficulties in encoding accurate 3D geometry of objects, especially for distant ones. In addition, the target box supervisions only provide object-level supervisions (location, size, and orientation).</p><p>This inspires us to utilize superior LiDAR detection models to guide the training of stereo detection model via imitating the geometry-aware representations encoded by the LiDAR model. Comparing with traditional knowledge distillation <ref type="bibr" target="#b24">[24]</ref> for recognition tasks, we did not take the final erroneous classification and regression predictions from 2. Related Work Stereo Matching. Mayer et al. <ref type="bibr" target="#b35">[35]</ref> proposed the first deep stereo algorithm DispNet, which regresses disparity map from feature-based correlation cost volume. DispNet is then extended using multi-stage refinement <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b31">31]</ref> and auxiliary semantic features <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b60">60]</ref>. State-of-the-art stereo models construct feature-based cost volume by concatenating left-right 2D features for all disparity candidates and then apply 3D aggregation network to predict the disparity probability distribution <ref type="bibr" target="#b26">[26,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b68">68]</ref>. State-of-the-art stereo detection networks <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b53">53]</ref> also estimate depth by similar network structures. Zhang et al. <ref type="bibr" target="#b66">[66]</ref> proposed novel cost volume aggregation strategies to improve the computational efficiency. Yin et al. <ref type="bibr" target="#b64">[64]</ref> accelerated stereo matching by hierarchically estimating local disparity distributions at each scale and composing them together to form the final match density. Xu et al. <ref type="bibr" target="#b55">[55]</ref> proposed a sparse points based intra-scale cost aggregation to alleviate the edge-fattening issue. Cheng et al. <ref type="bibr" target="#b11">[11]</ref> employed neural architecture search (NAS) to automatically search the optimal network structure for stereo matching. LiDAR-based 3D Detection.</p><p>By leveraging the more accurate depth information captured by LiDAR sensors, 3D detection approaches <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b45">45]</ref> with LiDAR point clouds generally achieve better performance than image-based approaches. To learn effective features from irregular and sparse point clouds, most existing approaches adopt the voxelization operation to transfer point clouds to regular grids, where the 3D space is first divided into regular 3D voxels <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b47">47]</ref> or bird-view 2D grids <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b28">28]</ref> to be processed by convolutions for detection head. Yan et al. <ref type="bibr" target="#b58">[58]</ref> proposed to utilize sparse convolutions <ref type="bibr" target="#b17">[17]</ref> for efficient feature learning from sparse voxels. Du et al. <ref type="bibr" target="#b13">[13]</ref> presented a feature imitation strategy to learn better perceptual features from synthesized conceptual scenes. Inspired by them, we propose to imitate the much informative feature maps from LiDAR models for better guidance beyond 3D box annotations. Stereo-based 3D Detection. Stereo-based 3D detection algorithms can be roughly divided into three types: 1) 2D-based methods <ref type="bibr">[7,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b38">38]</ref> first detect 2D bounding box proposals and then regress instancewise 3D boxes. Stereo-RCNN <ref type="bibr" target="#b30">[30]</ref> extended Faster R-CNN <ref type="bibr" target="#b43">[43]</ref> for stereo-inputs to associate left and right images. Disp R-CNN <ref type="bibr" target="#b49">[49]</ref> and ZoomNet <ref type="bibr" target="#b57">[57]</ref> incorporated extra instance segmentation mask and part location map to improve detection quality. However, the final performance is limited by the recall of 2D detection algorithms, and 3D geometry information is not fully utilized.</p><p>2) Pseudo-LiDAR <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b15">15]</ref> based 3D detection first estimate depth maps and then detect 3D bounding boxes using existing LiDAR-based algorithms. Pseudo-LiDAR++ <ref type="bibr" target="#b65">[65]</ref> adapted stereo cost volume to depth cost volume for direct depth estimation. Qian et al. <ref type="bibr" target="#b41">[41]</ref> made Pseudo-LiDAR pipeline end-to-end trainable. However, these models only take geometry information into consideration, which is lack of complementary semantic features.</p><p>3) Volume-based methods construct 3D anchor space <ref type="bibr" target="#b42">[42]</ref> or detect from 3D stereo volume <ref type="bibr">[10,</ref><ref type="bibr" target="#b53">53]</ref>. DSGN <ref type="bibr">[10]</ref> directly constructs differentiable volumetric representation, which encodes implicit 3D geometry structure of the scene, for one-stage stereo-based 3D detection. PLUME <ref type="bibr" target="#b53">[53]</ref> directly constructs geometry volume in 3D space for acceleration. Our work takes DSGN <ref type="bibr">[10]</ref> as the baseline model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Det Head</head><formula xml:id="formula_0">(b) 3 (d) ? ? #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Det Head</head><p>Depth Loss *+ <ref type="figure" target="#fig_6">Figure 2</ref>. The framework of our stereo-based 3D detection algorithm. (a) The stereo matching network which takes a stereo image pair as inputs and outputs a feature volume in 3D space. (b) and (d) share the same structure, which detects 3D objects from dense / sparse 3D feature volumes, respectively. (c) The teacher LiDAR-based detection network <ref type="bibr" target="#b58">[58]</ref>, which processes raw LiDAR point clouds using sparse 3D convolution layers. (e) Our auxiliary 2D detection head for direct semantic supervisions.</p><p>We solve several key problems existed in <ref type="bibr">[10]</ref> and surpass state-of-the-art models by a large margin. Knowledge Distillation. Distillation is first proposed by Hinton et al. <ref type="bibr" target="#b24">[24]</ref> for model compression by supervising student networks with "softened labels" from predictions of large teacher networks. Going further from "softened labels", knowledge from intermediate layers provide richer information from the teacher <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref>. Recently, knowledge distillation has been successfully applied to detection <ref type="bibr">[4,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b14">14]</ref> and semantic segmentation <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">9]</ref>. The knowledge can also be transferred across modalities <ref type="bibr" target="#b19">[19,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b69">69]</ref>. However, the feature imitation from LiDAR-based to stereo-based 3D detectors is first explored in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this work, to improve the performance of our model, we developed two strategies to learn better geometric and semantic features respectively. Due to the limitation of stereo matching, stereo-based detectors are fragile to the erroneous depth estimation, especially for low-texture surfaces, blurry boundaries and occlusion areas. In comparison, the features learned by LiDAR-based detectors provide robust high-level geometry-aware representations (accurate boundaries and surface normal directions). To minimize the gap between LiDAR-based and stereo-based detectors, we propose to utilize LiDAR models to guide the training of stereo-based detectors for better geometric supervision. In addition, we employ auxiliary 2D semantic supervisions to improve the learning efficiency for semantic features.</p><p>In the following section, we will first revisit the baseline model, Deep Stereo Geometry Network (DSGN) <ref type="bibr">[10]</ref>, in Sec. 3.1 to make the paper self-contained. In Sec. 3.2, we describe the proposed LiDAR imitation strategy for encod-ing better geometry representations. In Sec. 3.3, we discuss why the baseline model is of low efficiency for learning semantic features and propose the corresponding solution.</p><p>The training losses are specified in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisit of Deep Stereo Geometry Network</head><p>In this paper, we utilize Deep Stereo Geometry Network (DSGN) <ref type="bibr">[10]</ref> as our baseline model, which directly detects objects using implicit volumetric representations. </p><formula xml:id="formula_1">V st (u, v, w) = concat F L (u, v), F R (u ? f L d(w)s F , v) ,<label>(1)</label></formula><p>in which (u, v) is the current pixel coordinate . w=0, 1, ? ? ? is the candidate depth index, and d(w)=w ? v d +z min is the function to calculate its corresponding depth, where v d is the depth interval, and z min is the minimum depth of the detection area. f , L, and s F are the camera focal length, the baseline of the stereo camera pair, and the stride of the feature map, respectively. After filtering V st with a 3D cost volume aggregation network, we obtain an aggregated stereo volume? st and a depth distribution volume P st . P st (u, v, :) represents the depth probability distribution of pixel (u, v) over all discrete depth levels d(w). Volume in 3D Space.</p><p>In order to convert the feature volume from stereo space to normal 3D space, the 3D detection area is divided into small voxels of the same size. For each voxel, we project its center (x, y, z) back into the stereo space using the feature intrinsics K F to obtain its reprojected pixel coordinate (u, v) and depth index d ?1 (z)=(z?z min )/v d . The volume in the 3D space is then defined as the concatenation of resampled stereo volume and semantic features masked by depth probability,</p><formula xml:id="formula_2">V 3d (x, y, z) = concat V st u, v, d ?1 (z) , F sem (u, v) ? P st u, v, d ?1 (z) ,<label>(2)</label></formula><p>in which V st and F sem provide geometric and semantic features respectively. Note that we ignore the trilinear and bilinear resampling operators for simplicity. Feature in BEV Space and Detection Head. The 3D volume V 3d is then rearranged into 2D bird's eye view (BEV) feature F BEV by merging the channel dimension and the height dimension as <ref type="bibr">[10]</ref>. A 2D aggregation network and detection heads are attached to F BEV to generate the aggregated BEV featureF BEV and predict the final 3D bounding boxes, respectively. The training loss is divided into two parts, depth regression loss and 3D detection loss,</p><formula xml:id="formula_3">L bsl = L depth + L det .<label>(3)</label></formula><p>We have made several modifications to the above baseline to improve both performance and efficiency. Please see details in Sec. 3.4 and Sec. 1 of the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning LiDAR Geometry-aware Representations</head><p>LiDAR-based detection models <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b45">45]</ref> take raw point clouds as inputs, which are then encoded into high-level features (such as accurate boundaries and surface normal directions) for accurate bounding box localization. For stereo-based models, pure depth loss and detection loss could not well make the model learn such features for occlusion areas, non-textured areas and distant objects due to erroneous depth representations.</p><p>Inspired by the above observation, we propose to guide the training of stereo-based detectors using the high-level geometry-aware features from LiDAR models. In this paper, we utilize SECOND <ref type="bibr" target="#b58">[58]</ref> as our LiDAR "teacher". To make the features as consistent as possible between two models, we employ 2D aggregation network and detection head of the same structure, as shown in <ref type="figure" target="#fig_6">Fig. 2</ref> (see details in the supplementary materials).</p><p>Due to the huge differences between the backbone of the LiDAR model and the stereo model, we only focus on how to perform feature imitation for high-level features F im ?{V 3d , F BEV ,F BEV }, which are of the same shape between the two models. Our imitation loss aims at minimizing the feature distances between stereo feature F im ?F im and its corresponding LiDAR feature F lidar im , where g is a single convolution layer with kernel size 1 followed by an optional ReLU layer (depending on whether ReLU is applied to F lidar im ). M f g is the foreground mask, which is 1 inside any ground-truth object box, and 0 otherwise. M sp is the sparse mask of F lidar im , which is 1 for nonempty indices and 1 otherwise. N pos = M f g M sp is the normalization factor. Note that the LiDAR feature F lidar im is normalized by the channel-wise expectation of non-zero absolute values of F lidar im to make sure the scale stability of the L2 loss.</p><formula xml:id="formula_4">L im = Fim?Fim 1 N pos M f g M sp g(F im )? F lidar im E |F lidar im | 2 2 ,<label>(4)</label></formula><p>Experiment results show that the detection performance benefit little from imitating whole feature maps, thus M f g is essential to make the imitation loss focus on foreground objects. We found that F BEV and V 3d provide the most effective supervisions for training our stereo detection network. Please check the ablation studies in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Improving Semantic Features by Direct 2D Supervisions</head><p>From Eq. (2) we can see before resampling semantic feature into 3D space, it is first multiplied by the depth probability from P st (which can be seen as the estimation of 3D occupancy mask. Please see how we supervise P st in Eq. <ref type="formula" target="#formula_6">(6)</ref>). In this way, semantic features are only resampled near the estimated surface (orange dashed lines in <ref type="figure" target="#fig_3">Fig. 3</ref>). However, when there exist large errors in the estimated depth values, the semantic features will be resampled to the wrong positions as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. As a result, the resampled semantic features deviate from the ground-truth positions and are then assigned with negative anchors (red squares in <ref type="figure" target="#fig_3">Fig. 3</ref>), and there is no resampled semantic feature around the positive anchors (green squares in <ref type="figure" target="#fig_3">Fig. 3</ref>). Therefore, the supervision signals for the semantic features are incorrect in this case, which causes the low learning efficiency of semantic features.</p><p>To solve the problem, we add an auxiliary 2D detection head to provide direct supervisions for learning semantic  features. Instead of creating feature pyramids (FPN) using multi-level features like <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b67">67]</ref>, we take only a single feature map F sem to construct feature pyramids as shown in <ref type="figure" target="#fig_6">Fig. 2(e</ref>), which forms an information "bottleneck" to enforce all the semantic features to be encoded into F sem . Five consecutive convolution layers with a stride of 2 are attached to F sem to construct a multi-level feature pyramid, which are then connected to an ATSS <ref type="bibr" target="#b67">[67]</ref> head for 2D detection. Since we find that the dilated convolutions and spatial pyramid pooling (SPP) <ref type="bibr" target="#b20">[20]</ref> have already produced highly semantic features with large receptive fields, we ignore the top-down path of FPN for simplicity. Please see detailed network structures in Sec. 1 of the supplementary materials.</p><p>To ensure the semantic alignment between 2D and 3D features, 2D detection head should predict high scores around re-projected 3D object centers. We made a small modification to the positive sample assignment algorithm of ATSS <ref type="bibr" target="#b67">[67]</ref>. For each ground-truth bounding box g, we select k candidate anchors from each scale if their centers are closest to the re-projected 3D object centers, instead of 2D bounding box centers as in <ref type="bibr" target="#b67">[67]</ref>. The candidate anchors are then filtered by the dynamic IoU threshold as in ATSS <ref type="bibr" target="#b67">[67]</ref> to assign the final positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Modifications to Baseline and Training Losses</head><p>Note that we made several important modifications to DSGN to obtain a faster and more robust baseline model. 1) Decrease the number of channels and layers to reduce memory consumption and computation cost. 2) Utilize SEC-OND <ref type="bibr" target="#b58">[58]</ref>   </p><formula xml:id="formula_5">L = L depth + L det + ? im L im + ? 2d L 2d , L det = L cls +? L1 reg L L1 reg +? IoU reg L IoU reg +? dir L dir ,<label>(5)</label></formula><p>where L cls , L L1 reg , and L dir are the same classification loss, box regression loss, and direction classification loss as in SECOND <ref type="bibr" target="#b58">[58]</ref>. L IoU reg is the average rotated IoU loss <ref type="bibr" target="#b70">[70]</ref> between 3D box predictions and ground-truth bounding boxes. The uni-modal depth loss L depth is formulated as</p><formula xml:id="formula_6">L depth = 1 N gt u,v w ? max 1 ? |d * ? d(w)| v d , 0 log P st (u, v, w) ,<label>(6)</label></formula><p>in which d * is the ground-truth depth. Note that the loss is only applied to the pixels (u, v) with valid LiDAR depths, and N gt denotes the number of valid pixels. Compared with the original L1 loss, the loss in Eq. (6) provides more concentrated supervisions to the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset. We evaluate our method on the challenging KITTI 3D object detection benchmark <ref type="bibr" target="#b16">[16]</ref>. There are 7,481 training and 7,518 testing stereo image pairs with synchronized LiDAR point clouds in the dataset. Following previous papers <ref type="bibr" target="#b46">[46,</ref><ref type="bibr">10]</ref>, the training images are split into training set with 3,712 images and validation set with 3,769 images. There are three object classes in KITTI dataset, car, pedestrian, and cyclist, and each class is divided into three difficulty levels, easy, moderate, and hard. Evaluation Metric. We performed detailed evaluation and analysis for 2D, BEV (bird-view) and 3D detection performance. All the performance results are measured using IoU-based criteria to compute mean averaged precision (mAP) over 40 recall values, except for the results in <ref type="table" target="#tab_1">Table 1</ref>, which are evaluated using the old metric with 11 recall values for fair comparison with previous methods. Network Structure. The main structure of our stereo detector follows that of DSGN <ref type="bibr">[10]</ref>, which consists of 2D feature extraction network (left side of <ref type="figure" target="#fig_6">Fig. 2(a)</ref>), stereo aggregation network (right side of <ref type="figure" target="#fig_6">Fig. 2(a)</ref>), and BEV feature aggregation network ( <ref type="figure" target="#fig_6">Fig. 2(b)</ref>). As described in Sec. 3.4, we made several important modifications to <ref type="bibr">[10]</ref> to reduce computation cost and memory consumption. Compared with <ref type="bibr">[10]</ref>, we reduced the numbers of blocks of conv2 to conv5 from {3, 6, 12, 4} to {3, 4, 6, 3}, which is the same as ResNet-34 <ref type="bibr" target="#b21">[21]</ref>. In addition, we append a small U-Net on the top of the 2D backbone to upsample the SPP feature back into full resolution to provide high-resolution features for stereo matching. The number of channels of the stereo aggregation network is halved from 64 to 32, and the 3D hourglass module for the 3D geometry volume V 3d in <ref type="bibr">[10]</ref> is removed. For the 3D detection head, we follow the open source implementation of SECOND <ref type="bibr" target="#b58">[58]</ref> in OpenPCDet <ref type="bibr" target="#b50">[50]</ref> to replace the original FCOS head in <ref type="bibr">[10]</ref>, which we found having inferior performance. For the LiDAR "teacher", we employ SECOND <ref type="bibr" target="#b58">[58]</ref> because of its simplicity and similarity with our stereo detector. To make the teacher model and the student model as consistent as possible, we modify the stride of the last sparse convolution layer from 2 to 1, to obtain 1/4 downsampled features to match the feature map size of our stereo detector. Please refer to Sec. 1 of the supplementary materials for detailed network structures of our stereo detector and the LiDAR "teacher". Training Details. Our stereo detector is trained using AdamW <ref type="bibr" target="#b34">[34]</ref> optimizer, with ? 1 = 0.9, ? 2 = 0.999. The batch size is fixed to 8. We train the networks with 8 NVIDIA TITAN Xp GPUs, with 1 training sample on each GPU. The model is first trained for 50 epochs using a base learning rate of 0.001, and then for 10 more epochs with a reduced learning rate of 0.0001. The weight decay is set to 0.0001. We apply only horizontal flipping as augmentations. Instead of training individual networks for different classes as <ref type="bibr">[10]</ref>, we employ only a single model to train on all three classes of the KITTI dataset. To improve training stability, the weight normalizers for all the loss terms are averaged across all GPUs to avoid unstable gradients. For the # trick IM 2D pt Car AP 3D-IoU0. <ref type="bibr">7</ref> Car AP 3D-IoU0. <ref type="bibr">5</ref> Ped AP 3D-IoU0. <ref type="bibr">5</ref> Ped AP 3D-IoU0. <ref type="bibr" target="#b25">25</ref> Cyclist AP 3D-IoU0. <ref type="bibr">5</ref> Cyclist AP 3D-IoU0. <ref type="bibr" target="#b25">25</ref>   <ref type="table">Table 4</ref>. Ablation studies on the KITTI validation set. The tricks include full-resolution features for stereo volume construction, auxiliary 3D IoU loss, and the uni-modal depth loss as described in Sec. 3.4. IM denotes imitating geometry-aware representations of LiDAR models in Sec. 3.2. 2D denotes auxiliary direct 2D supervision in Sec. 3.3. pt means ImageNet <ref type="bibr" target="#b12">[12]</ref> pre-trained weights, which helps learn better semantic features. (We only load pre-trained weights for layer conv1-3 due to structure differences). * The results for DSGN <ref type="bibr">[10]</ref> is obtained by evaluating the officially released checkpoints using the new evaluation metrics with 40 recall values for fair comparison.</p><p>hyper-parameters of different losses, ? L1 reg =0.5, ? IoU reg =1.0, ? dir =0.2, ? 2d =1.0, ? im =1.0. The input size is fixed to 320?1248, we crop the upper part of input images which does not contain any object to reduce memory consumption. The overall training time is about 12 hours.</p><p>For the KITTI dataset, the detection area is set to [2m, 59.6m] for the Z (depth) axis, [?30m, 30m] for the X axis, and [?1m, 3m] for the Y axis (in camera coordinate system). The voxel size of V 3d in our stereo model is (0.2m, 0.2m, 0.2m). The input voxel size of the LiDAR detector is set to (0.05m, 0.1m, 0.05m). The spatial resolutions of BEV features of both models are 0.2m?0.2m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>In this section, we provide detailed comparison with state-of-the-art 3D detectors on the KITTI dataset (see <ref type="table" target="#tab_2">Table 2</ref>, 3, 4). Our model is only trained with the KITTI dataset, without any pre-training on Scene Flow datasets.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method surpasses the state-theof-art method CG-Stereo by 8.75%, 9.24%, 9.17% in 3D mAP (IoU=0.7) for easy, moderate and hard difficulty levels of car class. Our model even achieves almost the same performance as our LiDAR teacher (see SECOND (teacher) item in <ref type="table" target="#tab_1">Table 1</ref>) based on 0.5-IoU evaluation metrics. The gap between our model and the LiDAR teacher is only 0.2% mAP for 3D mAP with 0.5 IoU threshold, which proves that stereo-based 3D detection has the potential to replace LiDAR devices as a low-cost solution.</p><p>We also submitted our results to the official evaluation benchmark for evaluating our performance on the test set of the KITTI dataset. For BEV performance, we surpasses PLUME-Middle [53] by 10.48% mAP. For 3D detection performance, we surpasses CDN [15] by 10.44% mAP. Compared with PL++ (SDN+GDC), which is Pseudo-LIDAR++ [65] using 4-beam LiDAR for refinement, we even achieves much better results. Our method significantly reduces the gap between LiDAR-based methods and stereo-based 3D detection methods. For pedestrian and cyclist categories, our method exceeds CG-stereo by 5.69% and 5.97% mAP for 3D detection respectively. Please see Sec. 3 of the supplementary material for visualization results. Computation cost and memory usage. Compared with DSGN <ref type="bibr">[10]</ref>, our model has much less layers and number of channels. As shown in <ref type="table" target="#tab_6">Table 5</ref>, the memory consumption is greatly decreased from 29GB to 10GB for training and 6GB to 4.9GB for testing. For the running time, although we used NVIDIA TITAN Xp, which is expected to be half the speed of NVIDIA V100, our model still decreased half of the running time compared with DSGN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct ablation experiments to validate the contribution of each component of our model on the KITTI validation set. The results are summarized in <ref type="table">Table 4</ref>. The first row SECOND (teacher) is the performance of our LiDAR "teacher". The model (a.) is our reimplementation of DSGN <ref type="bibr">[10]</ref>, but with less channels and using SECOND <ref type="bibr" target="#b58">[58]</ref> detection head (due to memory limitation and simplicity), which achieves superior performance on cars compared with the original paper, but inferior results for pedestrians and cyclists because we train only a single model for all three classes instead of individual models. We applied a series of important tricks to model (a.) to obtain a stronger baseline model (b.), including full-resolution features for stereo volume construction, auxiliary 3D IoU loss for bounding box regression, and the uni-modal depth loss in Eq. <ref type="bibr">(6)</ref>. Please refer to Sec. 2 of the supplementary materials for details. By now, we have obtained a stronger baseline model with 61.35%, 34.11%, 24.04% 3D mAPs on the three classes of the KITTI validation set. In the following parts, we will prove the effectiveness of the proposed Li-DAR feature imitation strategy in Sec. 4.3.1 and the auxiliary direct 2D supervisions in Sec. 4.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation study of LiDAR feature imitation</head><p>Influence of LiDAR feature imitation. We first study whether imitating LiDAR features can benefit 3D detection and which layer is the optimal one. From <ref type="table">Table 6</ref> we can see that each of the three feature layers can provide meaningful geometry-aware guidance, among which V 3d andF BEV produce the most effective guidance. We also tried different combinations of imitation layers but found no further gain. However, when combining V 3d andF BEV as the imitation features, we found the model gives more stable results. As a result, we choose F im ={V 3d ,F BEV } as our final choice. Necessity of foreground mask. We then study the necessity of the foreground mask M f g . If M f g is removed, the imitation loss would be applied for both foreground and background features with an extreme class imbalance ratio. As expected, after removing M f g , there is almost no improvement in our model (w/o M f g ) compared with the baseline model as shown in <ref type="table">Table 6</ref>. Imitation weight. As shown in the last section of <ref type="table">Table 6</ref>, we tested different weights ? im for the LiDAR imitation loss term and found 1.0 is the optimal choice. Therefore, the optimal choice is to imitate foreground features of the 3D volume V 3d and the aggregated BEV fea-turesF BEV . In <ref type="table">Table 4</ref>, by comparing model (b.) and (c.), the feature imitation strategy improves 3D detection performance by 2.9%, 5.2% and 6.9% on car, pedestrian and cyclist classes of the KITTI dataset. Similar conclusions can be drawn by comparing model (d.) and (e.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The effectiveness of direct 2D supervisions</head><p>Due to the inefficiency of the 3D supervision for learning semantic features, we appended an auxiliary 2D detection head to provide direct 2D semantic supervisions for the semantic feature F sem . By comparing the model (d.) and the baseline model (b.) in <ref type="table">Table 4</ref>, the direct 2D supervision improves 3D AP of cars from 62.74% to 63.32%, pedestrians from 33.72% to 34.23%, and cyclists from 26.77% to 30.26%. The category with few data, cyclist, benefits more from this strategy.</p><p>We also evaluated our semantic features by training 2D detection only, but found poor performance on the KITTI dataset, the 2D APs are only 88%, 52% and 36% for cars, pedestrians and cyclists. We owe the poor results to the lack of data and pre-trained weights. With ImageNet pre-trained weights, the 2D APs are improved to 92%, 54% and 41%. To learn more discriminative 2D semantic features, we also performed experiments with ImageNet pre-trained weights to initialize the model. The two proposed strategies can consistently improve 3D detection performance with pretrained weights (see model (f.) and (g.) in <ref type="table">Table 4</ref>). Our final model (model (g.)) is able to learn both high-quality geometric features and semantic features, which further re-  <ref type="table">Table 6</ref>. Ablation studies for the LiDAR imitation loss. The optimal settings are to imitate {V 3d ,FBEV } with foreground mask M f g , and the loss coefficient is set to 1.0.</p><p>duces the gap between LiDAR-based and stereo-based 3D detection algorithms. Note that due to the structure differences between our backbone and ResNet-34, we only load pre-trained weights for the first three blocks from conv1 to conv3. For the detailed 2D detection results, please refer to Sec. 2 of the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose to learn stereo-based 3D detectors under the guidance of high-level geometry-aware LiDAR features and direct semantic supervisions, which successfully improved the geometric and semantic capabilities. Our model surpasses the state-of-the-art algorithms over 10.44% mAP on the official KITTI 3D detection benchmark, which is closing the gap between stereobased and LiDAR-based 3D detection algorithms. However, stereo-based 3D detectors still suffer from occlusions, non-textured area and distant objects. By utilizing more advanced "teachers" and more robust stereo algorithms, we expect this problem to be solved step by step in the future. The main structure of our stereo 3D detector follows that of DSGN <ref type="bibr">[2]</ref> but with much less memory consumption and lower computation cost. The network structure is described in <ref type="table" target="#tab_1">Table 1</ref> in details. 2D Feature Extraction. For the 2D backbone network, we employ a modified version of ResNet-34 <ref type="bibr">[3]</ref> with spatial pyramid pooling (SPP) module and feature upsampling. Compared with <ref type="bibr">[2]</ref>, the numbers of blocks in conv2-5 are reduced from {3, 6, 12, 4} to {3, 4, 6, 3}. The channels of conv2-5 are set to {64, 128, 128, 128}. The SPP module is the same as previous implementations <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>. In addition, we append a small U-Net <ref type="bibr">[5]</ref> on the top of the 2D backbone to upsample the SPP feature back into full resolution to provide high-resolution features for stereo matching. Stereo Aggregation Network. Although the 2D stereo features F l , F r are at full image resolution, we still construct the plane sweep volume (the stereo cost volume) at 1 /4 resolution to save memory. The number of base channels of the stereo aggregation network is set to 32, which is half of the number of channels in <ref type="bibr">[2]</ref>. Space Conversion. The volumetric feature in stereo space V st is converted into 3D space using Eq. (2) in the paper, which is then filtered by a 3D convolution layer and average pooling layer (along Y dimension) to output the volume in 3D space V 3d . The BEV feature is constructed by merging the y dimension and the channel dimension of V 3d and then compressed into 64 channels. BEV Aggregation Network. The structure for the BEV aggregation network follows <ref type="bibr">[2]</ref>, which is a shallow hourglass-like network. 3D Detection Head. For the 3D detection head, we follow the open source implementation of SECOND <ref type="bibr">[7]</ref> in OpenPCDet <ref type="bibr">[6]</ref>. For each class and each (x, z) location in the BEV space, we create two anchors with fixed average size and rotations of 0 and 90 degrees. The default anchor sizes are l a =3.9, w a =1.6, h a =1.56 for cars, l a =0.8, w a =0.6, h a =1.73 for pedestrians, and l a =1.76, w a =0.6, h a =1.73 for cyclists, and their y coordinates are set to y a ={1.78m, 0.6m, 0.6m}, respectively. The training targets are assigned with IoU-based criteria. The matched and unmatched IoU thresholds for the three classes are set to 0.6, 0.5, 0.5 and 0.45, 0.35, 0.35. Anchors with IoU between matched and unmatched thresholds are ignored for training. For each anchor, we output 3-dimensional classification and 7-dimensional regression predictions. The 3D bounding box regression targets are given by the following box encoding functions,</p><formula xml:id="formula_7">x t = x g ? x a d a , y t = y g ? y a h a , z t = z g ? z a d a , w t = log w g w a , l t = log l g l a , h t = log h g h a , ? t = ? g ? ? a ,<label>(1)</label></formula><p>where the subscripts t, a, g denote the encoded regression targets, the anchors, and the ground truth. ? is the yaw direction around the y-axis. The classification loss L cls , the direction classification loss L dir , and the L1 regression loss L L1 reg are the same as SECOND <ref type="bibr">[7]</ref>, and the auxiliary IoU-based regression loss is defined by,</p><formula xml:id="formula_8">L IoU reg = 1 ? IoU 3d (decode(? p , ? a ), ? g )<label>(2)</label></formula><p>where ? p is the regression predictions, ? a is the anchor {x a , y a , z a , w a , l a , h a , ? a }, and ? g is the assigned groundtruth bounding box {x g , y g , z g , w g , l g , h g , ? g }. Similar to the L1 regression loss, the IoU regression loss is only applied to positive samples. Non-maximum suppression (NMS) is applied to the 3D box predictions for each class separately, with the IoU threshold set to 0.25. 2D Detection Head. From the 2D feature extraction part, we have obtained 5-level FPN features spp1-5 from F sem with a sequence of stride-2 convolution layers. The strides of these features are {4, 8, 16, 32, 64}. For each level of the features, we apply a 2D detection head with two branches, classification branch and regression branch, to predict 2D bounding boxes. Following ATSS <ref type="bibr">[8]</ref>, each position is only  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">The Structure of the LiDAR Detector</head><p>The main structure of the LiDAR teacher detector follows that of SECOND <ref type="bibr">[7]</ref> with minor modifications. We utilize the same BEV aggregation network and 3D detection head as the stereo detector. Detailed network structure is described in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">More Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Influence of the trick modifications</head><p>To improve the performance of the baseline model DSGN <ref type="bibr">[2]</ref>, we made several important modifications for both training stability and detection performance. The modifications include adding extra IoU-based regression loss, constructing stereo volume using full-resolution 2D feature, and replacing soft-argmin <ref type="bibr">[1]</ref> with the uni-modal depth supervision loss. The ablation results are shown in <ref type="table" target="#tab_4">Table 3</ref>. The effectiveness of the IoU regression Loss. In previous stereo-based 3D detector implementations, the loss coefficients for regressing locations, orientations, and sizes are usually the same. Although tuning these coefficients has the potential to improve performance, it is trivial and not adaptable. The IoU regression loss <ref type="bibr">[10,</ref><ref type="bibr">4]</ref> instead directly maximizes the 3D IoU between predictions and targets, which can implicitly adapt the regression coefficients. Our results in <ref type="table" target="#tab_4">Table 3</ref> show that the IoU loss can improve about 2.5% mAP for cars.  <ref type="table" target="#tab_6">Table 5</ref>. Ablation studies for 2D detection head. * only loads pretrained weights in conv1, conv2, and conv3.</p><p>Construct stereo volume with high-resolution features.</p><p>In PSMNet <ref type="bibr">[1]</ref> and DSGN <ref type="bibr">[2]</ref>, the stereo volume is constructed using left-right image features of 1 /4 size. However, for stereo detection, high-resolution features are essential to improve the depth estimation precision, especially for distant objects. Inspired by the observation, we append an extra small upsampling network (U-Net) to construct fullresolution features from SPP features. From <ref type="table" target="#tab_4">Table 3</ref>, the U-Net improves the 3D detection performance of moderate samples by 0.6% mAP and hard samples by 2.0% mAP. Depth Supervision Loss. According to <ref type="bibr">[9]</ref>, indirectly learning cost volume by soft-argmin and smooth-L1 loss is prone to overfitting since the cost volume is under constrained. In comparison, directly minimizing Kullback-Leibler divergence between the predicted distribution and the unimodal distribution centered at true disparities provides stronger constraints to the cost volume, which can learn more robust implicit depth features? st . Since the ground-truth distribution is constant, the KL divergence can be simplified as cross entropy loss with soft targets,</p><formula xml:id="formula_9">L depth = 1 N gt u,v w [?p * w log P st (u, v, w)] ,<label>(3)</label></formula><p>where p * w is the ground-truth distribution centered at true disparity d * . Here we investigate several variants of groundtruth distributions, including the bilinearly interpolated distribution (Eq. (6) in the paper), hard-assigned distribution (p w =1 if d(w) is closest to d * ), gaussian distribution (p w ? exp ? </p><p>where k=argmax(P st (u, v, :)) is the depth index with the maximum probability. Local soft-argmin can avoid the influence of the probability values that are far from the peak probability, which can be utilized to evaluate the local ge-ometric accuracy of the implicit stereo embeddings? st . Results in <ref type="table">Table 4</ref> show that ground-truth distributions p * w that are sharper and more concentrated around d * can give better results. The choice of distribution encoding methods is not essential, and hard-assigned distribution can even give better performance than L1 loss. The good choices include hard-assigned distribution, gaussian distribution with ?=0.2, laplacian distribution with ?=0.2, and bilinearly interpolated distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">2D Detection Performance</head><p>We compare our 2D detection branch with ResNet-34 <ref type="bibr">[3]</ref> to confirm that our semantic bottleneck F sem does not constrain the performance of 2D detection. Since our model does not employ max pooling after conv1, we give the results of ResNet-34 without max pooling in the second row of <ref type="table" target="#tab_6">Table 5</ref> for fair comparison. By comparing the results of ResNet-34 w/o Maxpool and Ours (2D only), both models achieve similar performance given ImageNet pretrained weights, which proves that the semantic bottleneck does not constrain the 2D detection performance and has the capa-bility of learning good semantic features. By comparing the models without and with ImageNet pretrained weights, we can see pretrained weights are essential for 2D detection due to the limit of training data in the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visualization Results</head><p>Please see the visualization results in <ref type="figure" target="#fig_1">Fig. 1</ref>. Most of the objects can be detected with high IoU successfully, even for distant objects. We also visualize several failure cases in <ref type="figure" target="#fig_6">Fig. 2</ref>. Most of the failure cases are caused by occlusions and depth estimation errors. Several predictions give large orientation errors, which we believe can be fixed by incorporating predictions of 2D key-points of bounding boxes in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our method utilizes superior geometry-aware features from LiDAR-based 3D detection models to guide the training of stereo-based 3D detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Volume in Stereo Space. Given a left-right image pair (I L , I R ) and their features (F L , F R ), a plane-sweep volume V st is constructed by concatenating left features and corresponding right features for every candidate depth level,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of incorrect semantic supervisions caused by depth estimation errors (drawn in brid's eye view). Green and red squares represent positive and negative anchors, and orange dash lines represent estimated surface with large errors. Values of Pst are large around estimated surfaces, thus semantic features of foreground objects are falsely resampled around the wrong surfaces. Therefore, the supervisions for semantic features are inaccurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 2 () 2 ) 2 P</head><label>222</label><figDesc>d(w)?d * ? , and laplacian distribution (p w ? exp ? |d(w)?d * | ? ). The results are shown in Table 4. To evaluate local depth embeddings, instead of using global soft-argmin [1] to parse depth values from depth distributions, we employ local soft-argmin to predict the final depthst (u, v, w ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Visualization results of the KITTI validation set. The green boxes are ground-truth 3D / BEV bounding boxes. The blue boxes are our predictions. The numbers around the ground-truth BEV boxes are the IoU values of their best predictions. The IoU values will be zero if the corresponding 3D boxes are not detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Failure Cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>SECOND (our teacher) 88.82 78.57 77.40 89.93 87.75 86.67 98.12 90.17 89.64 98.16 90.20 89.71 14.26 13.72 29.22 21.88 18.83 59.51 43.71 37.99 62.46 45.99 41.92 Stereo-RCNN [30] 54.11 36.69 31.07 68.50 48.30 41.47 85.84 66.28 57.24 87.13 74.11 58.Car detection results on the KITTI validation set. The results are evaluated using the original KITTI metric with 11 recall values for fair comparison. If not specified, all results in other tables are evaluated using 40 recall positions. * utilizes 4-beam LiDAR to refine stereo depth estimation.</figDesc><table><row><cell>Sensor LiDAR</cell><cell cols="2">Method MV3D (LiDAR) [8] PL++: P-RCNN + SL* [65] SECOND [58] Point-RCNN [46]</cell><cell cols="3">AP 3D (IoU=0.7) Easy Hard Mod 71.29 62.68 56.56 75.1 63.8 57.4 87.43 76.48 69.10 88.88 78.63 77.38</cell><cell cols="2">AP BEV (IoU=0.7) Easy Mod Hard 86.55 78.10 76.67 88.2 76.9 73.4 89.96 87.07 79.66 ---</cell><cell cols="2">AP 3D (IoU=0.5) Easy Mod Hard ------------</cell><cell>AP BEV (IoU=0.5) Easy Mod Hard ------------</cell></row><row><cell>Stereo</cell><cell cols="2">3DOP [6] TLNet [42] PL: F-PointNet [52] PL++: P-RCNN [65] OC-Stereo [39] Disp-RCNN [49] DSGN [10] CG-Stereo [29] PLUME-Large [53] LIGA-Stereo (Ours)</cell><cell cols="7">6.55 18.15 93 5.07 4.10 12.63 9.49 7.59 46.04 34.63 30.09 55.04 41.25 34.55 59.4 39.8 33.5 72.8 51.8 33.5 89.5 75.5 66.3 89.8 77.6 68.2 67.9 50.1 45.3 82.0 64.0 57.3 89.7 78.6 75.1 89.8 83.8 77.5 64.07 48.34 40.39 77.66 65.95 51.20 89.65 80.03 70.34 90.01 80.63 71.06 64.29 47.73 40.11 77.63 64.38 50.68 90.47 79.76 69.71 90.67 80.45 71.03 72.31 54.27 47.71 83.24 63.91 57.83 ------76.17 57.82 54.63 87.31 68.69 65.80 90.58 87.01 79.76 97.04 88.58 80.34 ---84.7 71.1 65.1 ---91.3 86.6 81.6 84.92 67.06 63.80 89.35 77.26 69.05 97.06 89.97 87.94 97.22 90.27 88.36</cell></row><row><cell></cell><cell>Sensor LiDAR</cell><cell cols="3">Method MV3D [8] SECOND [58] PointPillars [28] Point-RCNN [46] PV-RCNN [45] PL++ (SDN+GDC)* [65] 68.38 Easy 74.97 83.34 82.58 86.96 90.25</cell><cell>Car AP 3D Moderate 63.63 72.55 74.31 75.64 81.43 54.88</cell><cell>Hard 54.00 86.62 Easy 65.82 89.39 68.99 90.07 70.70 92.13 76.82 94.98 49.16 84.61</cell><cell cols="2">Car AP BEV Moderate 78.93 83.77 86.56 87.39 90.65 73.80</cell><cell>Hard 69.80 78.59 82.81 94.00 Easy --82.72 95.92 86.14 98.17 65.59 94.95</cell><cell>Car AP 2D Moderate --91.19 91.90 94.70 85.15</cell><cell>Hard --88.17 87.11 92.04 77.78</cell></row><row><cell></cell><cell>Stereo</cell><cell cols="2">Stereo R-CNN [30] PL: AVOD [52] PL++: P-RCNN [65] ZoomNet [57] OC-Stereo [39] Disp-RCNN [49] DSGN [10] CG-Stereo [29] CDN [15] PLUME-Middle [53] LIGA-Stereo (Ours)</cell><cell>47.58 54.53 61.11 55.98 55.15 68.21 73.50 74.39 74.52 -81.39</cell><cell>30.23 34.05 42.43 38.64 37.60 45.78 52.18 53.58 54.22 -64.66</cell><cell>23.72 61.92 28.25 67.30 36.99 78.31 30.97 72.94 30.25 68.89 37.73 79.76 45.14 82.90 46.50 85.29 46.36 83.32 -83.0 57.22 88.15</cell><cell>41.31 45.00 58.01 54.91 51.47 58.62 65.05 66.44 66.24 66.3 76.78</cell><cell></cell><cell>33.42 93.98 38.40 85.40 51.25 94.46 44.14 94.22 42.97 87.39 47.73 93.45 56.60 95.53 58.95 96.31 57.65 95.85 56.7 -67.40 96.43</cell><cell>85.98 67.79 82.90 83.92 74.60 82.64 86.43 90.38 87.19 -93.82</cell><cell>71.25 58.50 75.45 69.00 62.56 70.45 78.75 82.80 79.43 -86.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Car detection results on the KITTI test set (official KITTI leaderboard). * utilizes 4-beam LiDAR to refine stereo depth estimation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>detection head for 3D detection. 3) Replace the smooth-L1 depth regression loss with the uni-modal depth</figDesc><table><row><cell>Method OC-Stereo [39] DSGN [10] Disp-RCNN [49] CG-Stereo [29] LIGA-Stereo (Ours)</cell><cell>Easy 24.48 20.53 37.12 33.22 40.46</cell><cell>Pedestrian AP 3D Moderate 17.58 15.55 25.80 24.31 30.00</cell><cell>Pedestrian AP BEV Easy Moderate Hard 15.60 29.79 Hard 20.80 18.62 29.40 Easy 14.15 26.61 20.75 18.86 27.76 22.04 40.21 28.34 24.46 40.05 20.95 39.24 29.56 25.87 47.40 27.07 44.71 34.13 30.42 54.44</cell><cell>Cyclist AP 3D Moderate 16.63 18.17 24.40 30.89 36.86</cell><cell>Hard 14.72 16.21 21.12 27.73 32.06</cell><cell>Easy 32.47 31.23 44.19 55.33 58.95</cell><cell>Cyclist AP BEV Moderate 19.23 21.04 27.04 36.25 40.60</cell><cell>Hard 17.11 18.93 23.58 32.17 35.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Pedestrian and cyclist detection results on KITTI test set (official KITTI leaderboard).</figDesc><table><row><cell>loss [64] in Eq. (6) based on Kullback-Leibler divergence. 4) Use a combination of L1 loss and auxiliary rotated 3D IoU loss [70] for better bounding box regression. 5) Attach a small U-Net to the 2D backbone to encode full-resolution feature maps for stereo volume construction. Please check Sec. 1 &amp; 2.1 of the supplementary material for detailed net-work structure and performance analysis for these modifi-cations.</cell></row></table><note>The new overall loss of our model is formulated as</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Easy Mod Hard Easy Mod Hard Easy Mod Hard Easy Mod Hard Easy Mod Hard Easy Mod Hard SECOND [58] (teacher) 91.89 81.08 78.36 99.36 94.35 93.63 72.30 64.79 57.82 86.83 82.03 76.02 85.17 65.81 62.00 89.31 69.41 66.07 DSGN* [10] 72.31 52.29 47.12 94.73 82.33 77.17 36.84 31.42 27.55 67.38 60.39 53.88 35.39 23.16 22.29 46.28 30.58 29.17 a. 76.44 56.73 49.52 96.25 86.92 79.46 24.39 19.10 16.03 73.12 60.43 52.90 29.12 16.27 15.14 38.81 23.36 21.49 b. 82.15 62.74 57.34 98.85 90.21 84.81 41.18 33.72 29.19 76.19 66.04 57.98 45.63 26.77 24.97 51.50 32.39 30.48 c. 84.99 65.67 60.16 98.84 92.47 87.32 47.82 38.89 33.66 82.78 73.65 64.96 57.54 33.70 31.23 74.39 48.44 44.93 d. 84.45 63.32 57.65 96.23 89.94 84.55 40.72 34.23 29.12 76.13 66.36 58.99 52.65 30.26 28.14 58.59 37.96 35.42 e. 85.26 65.64 60.12 98.98 92.50 87.33 43.97 36.95 31.82 77.57 69.14 62.13 54.59 34.07 31.42 65.19 42.96 40.28 f. 81.05 63.07 56.50 98.91 92.22 87.04 40.29 34.13 29.31 77.60 68.54 61.42 42.77 26.13 24.26 52.11 34.63 32.78 g. 86.84 67.71 62.02 98.87 93.21 87.97 45.54 37.80 32.09 81.87 72.18 64.10 60.00 37.31 34.25 79.89 52.17 48.09</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Time and memory consumption comparison. The memory column shows both training and testing memory usage.</figDesc><table><row><cell>Method</cell><cell>Inference Time (s)</cell><cell>GPU</cell><cell cols="2">Memory (GB)</cell><cell>AP 3D (%) IoU 0.7 / 0.5</cell></row><row><cell>DSGN [10] Ours</cell><cell>0.67 0.35</cell><cell>V100 TITAN Xp</cell><cell cols="2">29 / 6.0 10 / 4.9</cell><cell>53.95 / 79.10 68.47 / 92.85</cell></row><row><cell></cell><cell>Imitation Settings</cell><cell cols="2">Easy</cell><cell cols="2">Car AP 3D Moderate</cell><cell>Hard</cell></row><row><cell cols="4">baseline F im = {V 3d } F im = {F BEV } F im = {F BEV } F im = {V 3d ,F BEV } F im = {V 3d , F BEV ,F BEV } 82.95 82.15 84.50 84.60 84.83 84.99 81.13 w/o M f g ? im =0.2 82.61 ? im =0.5 85.14 ? im =1.0 84.99 ? im =2.0 84.47</cell><cell>62.74 65.83 65.16 65.10 65.67 63.49 62.01 64.15 65.44 65.67 65.11</cell><cell>57.34 60.35 58.04 59.42 60.16 58.17 54.83 58.23 59.85 60.16 59.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Supplementary Materials of LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector Xiaoyang Guo Shaoshuai Shi Xiaogang Wang Hongsheng Li CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong {xyguo, ssshi, xgwang, hsli}@ee.cuhk.edu.hk</figDesc><table><row><cell>1. More Implementation Details</cell></row><row><cell>1.1. The Structure of the Stereo Detector</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Detailed network structure of our stereo-based 3D detection network. By default, the convolution layers in the 2D feature extraction module and the 2D head module are followed by batch normalization layers, and the other convolution layers are attached with group normalization layers (the number of groups is set to 32). Detailed network structure of the LiDAR detector.</figDesc><table><row><cell>Output conv1 conv2 conv3 conv4 V 3d F BEV</cell><cell>Input input</cell><cell>Module Config Sparse 3D Convolution Backbone #Channel SpConv (3?3?3) 16 SpConv (3?3?3)?3, s=2 32 SpConv (3?3?3)?3, s=2 64 SpConv (3?3?3)?3, s=1?2?1 64 SpConv (1?1?1) 32 Reshape; Conv (3?3) 32? Ny /4, 64 BEV Aggregation Network &amp; 3D Detection Head Same as the stereo detector, see Table 1</cell><cell>Size 4Nx?2Ny?4Nz 2Nx?Ny?2Nz Nx? Ny /2?Nz Nx? Ny /4?Nz Nx? Ny /4?Nz Nx?Nz</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Ablation studies for the tricks. Gaussian ?=0.2 0.63 / 0.092 50.5 / 29.5 32.3 / 18.2 78.5 / 61.2 / 55.9 Comparison between different depth losses. Depth Err. Med. denotes the average median of depth errors. Foreground (Fg in the Table) metrics are evaluated by averaging object-level results, where boxes with less than 5 ground-truth LiDAR points are ignored. attached with one anchor box. The anchor box sizes for each level are set to {32, 64, 128, 256, 512}. Please refer to the original paper of ATSS [8] for details.</figDesc><table><row><cell>Supervision Smooth L1 L1 Hard-assigned</cell><cell>Err. Med. Fg/All (mm) Fg/All (%) Fg/All (%) &lt; 0.2m &lt; 0.4m 0.64 / 0.13 61.7 / 37.4 40.0 / 22.6 78.9 / 59.7 / 54.0 AP3D (%) 0.61 / 0.10 55.2 / 32.9 35.7 / 20.5 78.3 / 61.3 / 54.2 0.63 / 0.094 50.9 / 29.9 32.8 / 18.7 80.1 / 61.2 / 54.5</cell></row><row><cell>Gaussian ?=0.4 Gaussian ?=0.8 Laplacian ?=0.2 Laplacian ?=0.4 Laplacian ?=0.8 Bilinear (Eq.(6))</cell><cell>0.66 / 0.11 0.70 / 0.13 0.64 / 0.10 0.66 / 0.11 0.68 / 0.13 0.63 / 0.091 51.1 / 29.9 33.1 / 18.7 81.2 / 61.5 / 54.6 55.1 / 32.6 35.4 / 19.7 78.5 / 61.8 / 54.9 59.5 / 37.1 38.4 / 22.0 75.7 / 58.8 / 51.9 52.9 / 31.3 33.8 / 19.3 80.7 / 62.3 / 55.3 55.5 / 33.4 35.6 / 20.3 79.0 / 61.4 / 54.6 58.9 / 36.9 38.2 / 22.2 77.5 / 59.0 / 52.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Asr is all you need: Cross-modal distillation for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2143" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning lightweight pedestrian detector with hierarchical knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1645" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13501</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Associate-3ddet: perceptual-to-conceptual association for 3d point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13329" to="13338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeezed deep 6dof object detection using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heitor</forename><surname>Feliz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Mac?do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sim?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Adriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleber</forename><surname>Teichrieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanchettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wasserstein distances for stereo disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive overhaul of feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Confidence guided stereo 3d object detection with split depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online model distillation for efficient video inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Ravi Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3573" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ida-3d: Instancedepth-aware 3d object detection from stereo vision for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Hao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13015" to="13024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-08-31" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8383" to="8389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5881" to="5890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Edgestereo: A context integrated residual pyramid network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Openpcdet: An opensource toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4933" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><forename type="middle">Urtasun</forename><surname>Plume</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06594</idno>
		<title level="m">Efficient 3d object detection from stereo images</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aanet: Adaptive aggregation network for efficient stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1959" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Training a binary weight object detector by knowledge transfer for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2379" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12557" to="12564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="636" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suihanjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12926" to="12934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Knowledge as priors: Crossmodal knowledge generalization for datasets without superior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6528" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Openpcdet: An opensource toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet,2020.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suihanjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12926" to="12934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
