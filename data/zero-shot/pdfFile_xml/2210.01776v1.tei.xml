<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIFFDOCK: DIFFUSION STEPS, TWISTS, AND TURNS FOR MOLECULAR DOCKING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DIFFDOCK: DIFFUSION STEPS, TWISTS, AND TURNS FOR MOLECULAR DOCKING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the binding structure of a small molecule ligand to a protein-a task known as molecular docking-is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DIFFDOCK, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DIFFDOCK obtains a 38% top-1 success rate (RMSD&lt;2?) on PDB-Bind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, DIFFDOCK has fast inference times and provides confidence estimates with high selective accuracy. * Equal contribution. Correspondance to {gcorso, hstark, bjing}@mit.edu.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The biological functions of proteins can be modulated by small molecule ligands (such as drugs) binding to them. Thus, a crucial task in computational drug design is molecular docking-predicting the position, orientation, and conformation of a ligand when bound to a target protein-from which the effect of the ligand (if any) might be inferred. Traditional approaches for docking <ref type="bibr">[Trott &amp; Olson, 2010;</ref><ref type="bibr" target="#b8">Halgren et al., 2004]</ref> rely on scoring-functions that estimate the correctness of a proposed structure, or pose, and an optimization algorithm that searches for the global maximum of the scoring function. However, since the search space is vast and the landscape of the scoring functions rugged, these methods tend to be too slow and inaccurate, especially for high-throughput workflows.</p><p>Recent works <ref type="bibr">[St?rk et al., 2022;</ref> have developed deep learning models to predict the binding pose in one shot, treating docking as a regression problem. While these methods are much faster than traditional search-based methods, they have yet to demonstrate significant improvements in accuracy. We argue that this may be because the regression-based paradigm corresponds imperfectly with the objectives of molecular docking, which is reflected in the fact that standard accuracy metrics resemble the likelihood of the data under the predictive model rather than a regression loss. We thus frame molecular docking as a generative modeling problem-given a ligand and target protein structure, we learn a distribution over ligand poses.</p><p>We therefore develop DIFFDOCK, a diffusion generative model (DGM) over the space of ligand poses for molecular docking. We define a diffusion process over the degrees of freedom involved in docking: the position of the ligand relative to the protein (locating the binding pocket), its orientation in the pocket, and the torsion angles describing its conformation. DIFFDOCK samples poses by running the learned (reverse) diffusion process, which iteratively transforms an uninformed, noisy prior distribution over ligand poses into the learned model distribution <ref type="figure" target="#fig_0">(Figure 1</ref>). Intuitively, this process can be viewed as the progressive refinement of random poses via updates of their translations, rotations, and torsion angles.</p><p>While DGMs have been applied to other problems in molecular machine learning <ref type="bibr" target="#b11">[Xu et al., 2021;</ref><ref type="bibr" target="#b12">Jing et al., 2022;</ref><ref type="bibr" target="#b10">Hoogeboom et al., 2022]</ref>, existing approaches are ill-suited for molecular docking, where the space of ligand poses is an (m + 6)-dimensional submanifold M ? R 3n , where n and m are, respectively, the number of atoms and torsion angles. To develop DIFFDOCK, we recognize that the docking degrees of freedom define M as the space of poses accessible via a set of allowed ligand pose transformations. We use this idea to map elements in M to the product space of the groups corresponding to those transformations, where a DGM can be developed and trained efficiently.</p><p>As applications of docking models often require only a fixed number of predictions and a confidence score over these, we train a confidence model to provide confidence estimates for the poses sampled from the DGM and to pick out the most likely sample. This two-step process can be viewed as an intermediate approach between brute-force search and one-shot prediction: we retain the ability to consider and compare multiple poses without incurring the difficulties of high-dimensional search.</p><p>Empirically, on the standard blind docking benchmark PDBBind, DIFFDOCK achieves 38% of top-1 predictions with ligand root mean square distance (RMSD) below 2?, nearly doubling the performance of the previous state-of-the-art deep learning model (20%). DIFFDOCK significantly outperforms even state-of-the-art search-based methods (23%), while still being 3 to 12 times faster on GPU. Moreover, it provides an accurate confidence score of its predictions, obtaining 83% RMSD&lt;2? on its most confident third of the previously unseen complexes.</p><p>To summarize, the main contributions of this work are: 1. We frame the molecular docking task as a generative problem and highlight the issues with previous deep learning approaches.</p><p>2. We formulate a novel diffusion process over ligand poses corresponding to the degrees of freedom involved in molecular docking.</p><p>3. We achieve a new state-of-the-art 38% top-1 prediction with RMSD&lt;2? on PDBBind blind docking benchmark, considerably surpassing the previous best search-based (23%) and deep learning methods (20%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>Molecular docking. The molecular docking task is usually divided between known-pocket and blind docking. Known-pocket docking algorithms receive as input the position on the protein where the molecule will bind (the binding pocket) and only have to find the correct orientation and conformation. Blind docking instead does not assume any prior knowledge about the binding pocket; in this work, we will focus on this general setting. Due to the relative rigidity of the protein in the majority of the cases, docking methods typically assume the knowledge of the bound protein structure, and we will follow this assumption <ref type="bibr">[Pagadala et al., 2017]</ref>. Methods are normally evaluated by the percentage of hits, or approximately correct predictions, commonly considered to be those where the ligand RMSD error is below 2? <ref type="bibr" target="#b0">[Alhossary et al., 2015;</ref><ref type="bibr" target="#b9">Hassan et al., 2017;</ref><ref type="bibr" target="#b22">McNutt et al., 2021]</ref>.</p><p>Search-based docking methods. Traditional docking methods <ref type="bibr">[Trott &amp; Olson, 2010;</ref><ref type="bibr" target="#b8">Halgren et al., 2004;</ref><ref type="bibr">Thomsen &amp; Christensen, 2006]</ref> consist of a parameterized physics-based scoring function and a search algorithm. The scoring-function takes in 3D structures and returns an estimate of the quality/likelihood of the given pose, while the search stochastically modifies the ligand pose (position, orientation, and torsion angles) with the goal of finding the global optimum of the scoring function.</p><p>Recently, machine learning has been applied to parameterize the scoring-function <ref type="bibr" target="#b22">[McNutt et al., 2021;</ref><ref type="bibr" target="#b24">M?ndez-Lucio et al., 2021]</ref>. These search-based methods have offered relative improvements when docking to a known pocket but are typically very computationally expensive to run and must still grapple with the very large search space that characterizes blind docking.</p><p>Machine learning for blind docking. Recently, EquiBind <ref type="bibr">[St?rk et al., 2022]</ref> has tried to tackle the blind docking task by directly predicting pocket keypoints on both ligand and protein and aligning them. TANKBind  improved over this by independently predicting a docking pose (in the form of an interatomic distance matrix) for each possible pocket and then ranking them. Although these one-shot or few-shot regression-based prediction methods are orders of magnitude faster, their performance has not yet reached that of traditional search-based methods.</p><p>Diffusion generative models. Let the data distribution be the initial distribution p 0 (x) of a continuous diffusion process described by dx = f (x, t) dt + g(t) dw, where w is the Wiener process. Diffusion generative models (DGMs) 1 model the score 2 ? x log p t (x) of the diffusing data distribution in order to generate data via the reverse diffusion <ref type="bibr">Song et al., 2021]</ref>. In this work, we always take f (x, t) = 0. Several DGMs have been developed for molecular machine learning tasks, including molecule generation <ref type="bibr" target="#b10">[Hoogeboom et al., 2022]</ref>, conformer generation <ref type="bibr" target="#b11">[Xu et al., 2021]</ref>, and protein design <ref type="bibr">[Trippe et al., 2022]</ref>. However, these approaches learn distributions over the full Euclidean space R 3n with 3 coordinates per atom, making them ill-suited for molecular docking where the degrees of freedom are much more restricted.</p><formula xml:id="formula_0">dx = [f (x, t) ? g(t) 2 ? x log p t (x)] + g(t) dw [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DOCKING AS GENERATIVE MODELING</head><p>Although EquiBind and other ML methods have provided strong runtime improvements by avoiding an expensive optimization process over ligand poses, their performance has not yet reached that of search-based methods. As our analysis below argues, this may be caused by the models' uncertainty and the optimization of an objective function that does not correspond to how molecular docking is used and evaluated in practice.</p><p>Molecular docking objective. Molecular docking plays a critical role in drug discovery because the prediction of the 3D structure of a bound protein-ligand complex enables further computational and human expert analyses on the strength and properties of the binding interaction. Therefore, a docked prediction is only useful if its deviation from the true structure does not significantly affect the output of such analyses. Concretely, a prediction is considered acceptable when the distance between the structures (measured in terms of ligand RMSD) is below some small tolerance on the order of the length scale of atomic interactions (a few?ngstr?m). Consequently, the standard evaluation metric used in the field has been the percentage of predictions with a ligand RMSD (to the crystal ligand pose) below some value .</p><p>However, the objective of maximizing the proportion of predictions with RMSD within some tolerance is not differentiable and cannot be used for training with stochastic gradient descent. Instead, maximizing the expected proportion of predictions with RMSD &lt; corresponds to maximizing the likelihood of the true structure under the model's output distribution, in the limit as goes to 0. This observation motivates training a generative model to minimize an upper bound on the negative log-likelihood of the observed structures under the model's distribution. Thus, we view molecular docking as the problem of learning a distribution over ligand poses conditioned on the protein structure and develop a diffusion generative model over this space (Section 4).</p><p>Confidence model. With a trained diffusion model, it is possible to sample an arbitrary number of ligand poses from the posterior distribution according to the model. However, researchers are often interested in seeing only one or a small number of predicted poses and an associated confidence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TANKBind Crystal</head><p>EquiBind DiffDock samples DiffDock top-1 <ref type="figure">Figure 2</ref>: "DIFFDOCK top-1" refers to the sample with the highest confidence. "DIFFDOCK samples" to the other diffusion model samples. Left: Visual diagram of the advantage of generative models over regression models. Given uncertainty in the correct pose (represented by the orange distribution), regression models tend to predict the mean of the distribution, which may lie in a region of low density. Center: when there is a global symmetry in the protein (aleatoric uncertainty), EquiBind places the molecule in the center while DIFFDOCK is able to sample all the true poses. Right: even in the absence of strong aleatoric uncertainty, the epistemic uncertainty causes EquiBind's prediction to have steric clashes and TANKBind's to have many self-intersections. measure 3 for downstream analysis. Thus, we train a confidence model over the poses sampled by the diffusion model and rank them based on its confidence that they are within the error tolerance. The top-ranked ligand pose and the associated confidence are then taken as DIFFDOCK's top-1 prediction and confidence score.</p><p>Problem with regression-based methods. The difficulty with the development of deep learning models for molecular docking lies in the data inherent (aleatoric) uncertainty on the pose (multiple poses could be correct) and the complexity of the task compared with the limited model capacity and data available (epistemic uncertainty). Therefore, given the available co-variate information (only protein structure and ligand identity), any method will exhibit uncertainty about the correct binding pose among many viable alternatives. Any regression-style method that is forced to select a single configuration that minimizes the expected square error would learn to predict the (weighted) mean of such alternatives. In contrast, a generative model with the same co-variate information would instead aim to capture the distribution over the alternatives, populating all/most of the significant modes even if similarly unable to distinguish the correct target. This behavior, illustrated in <ref type="figure">Figure 2</ref>, causes the regression-based models to produce significantly more physically implausible poses than our method. In particular, we observe frequent steric clashes (e.g., 26% of EquiBind's predictions) and self-intersections in EquiBind's and TANKBind's predictions <ref type="bibr">(Figures 5 and 9)</ref>. We found no intersections in DIFFDOCK's predictions. Visualizations and quantitative evidence of these phenomena are in Appendix E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">OVERVIEW</head><p>A ligand pose is an assignment of atomic positions in R 3 , so in principle, we can regard a pose x as an element in R 3n , where n is the number of atoms. However, this encompasses far more degrees of freedom than are relevant in molecular docking. In particular, bond lengths, angles, and small rings in the ligand are essentially rigid, such that the ligand flexibility lies almost entirely in the torsion angles at rotatable bonds. Traditional docking methods, as well as most ML ones, take as input a seed conformation c ? R 3n of the ligand in isolation and change only the relative position and the torsion degrees of freedom in the final bound conformation. <ref type="bibr">4</ref> The space of ligand poses consistent with c is, therefore, an (m + 6)-dimensional submanifold M c ? R 3n , where m is the number of rotatable bonds, and the six additional degrees of freedom come from rototranslations relative to the fixed protein. We follow this paradigm of taking as input a seed conformation c, and formulate molecular docking as learning a probability distribution p c (x | y) over the manifold M c , conditioned on a protein structure y.</p><p>DGMs on submanifolds have been formulated by De Bortoli et al. <ref type="bibr">[2022]</ref> in terms of projecting a diffusion in ambient space onto the submanifold. However, the kernel p(x t | x 0 ) of such a diffusion is not available in closed form and must be sampled numerically with a geodesic random walk, making training very inefficient. We instead define a one-to-one mapping to another, "nicer" manifold where the diffusion kernel can be sampled directly and develop a DGM in that manifold.</p><p>To start, we restate the discussion in the last paragraph as follows:</p><p>Any ligand pose consistent with a seed conformation can be reached by a combination of (1) ligand translations, (2) ligand rotations, and (3) changes to torsion angles.</p><p>This can be viewed as an informal definition of the manifold M c . Simultaneously, it suggests that given a continuous family of ligand pose transformations corresponding to the m + 6 degrees of freedom, a distribution on M c can be lifted to a distribution on the product space of the corresponding groups-which is itself a manifold. We will then show how to sample the diffusion kernel on this product space and train a DGM over it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LIGAND POSE TRANSFORMATIONS</head><p>We associate translations of ligand position with the 3D translation group T(3), rigid rotations of the ligand with the 3D rotation group SO(3), and changes in torsion angles at each rotatable bond with a copy of the 2D rotation group SO(2). More formally, we define operations of each of these groups on a ligand pose c ? R 3n . The translation A tr :</p><formula xml:id="formula_1">T(3) ? R 3n ? R 3n is defined straightforwardly as A tr (r, x) i = x i +r using the isomorphism T(3) ? = R 3 where x i ? R 3 is the position of the ith atom. Similarly, the rotation A rot : SO(3) ? R 3n ? R 3n is defined by A rot (R, x) i = R(x i ?x) +x wher? x = 1 n</formula><p>x i , corresponding to rotations around the (unweighted) center of mass of the ligand.</p><p>Many valid definitions of a change in torsion angles are possible, as the torsion angle around any bond (a i , b i ) can be updated by rotating the a i side, the b i side, or both. However, we can specify changes of torsion angles to be disentangled from rotations or translations. One way of doing so is to identify a central motif in the molecule, such as a ring, and change torsion angles in a way that keeps the motif fixed. However, this special treatment of the central motif introduces an arbitrary asymmetry into the problem and could be difficult for a score model to reason about. Thus, we instead define the operation of elements of SO(2) m such that it causes a minimal perturbation (in an RMSD sense) to the structure: 5 Definition. Let B k,? k (x) ? R 3n be any valid torsion update by ? k around the kth rotatable bond (a k , b k ). We define A tor :</p><formula xml:id="formula_2">SO(2) m ? R 3n ? R 3n such that A tor (?, x) = RMSDAlign(x, (B 1,?1 ? ? ? ? B m,?m )(x)) where ? = (? 1 , . . . ? m ) and</formula><p>RMSDAlign(x, x ) = arg min</p><formula xml:id="formula_3">x ? ?{gx |g?SE(3)} RMSD(x, x ? )<label>(1)</label></formula><p>This means that we apply all the m torsion updates in any order and then perform a global RMSD alignment with the unmodified pose. The definition is motivated by ensuring that the infinitesimal effect of a torsion is orthogonal to any rototranslation, i.e., it induces no linear or angular momentum. These properties can be stated more formally as follows (proof in Appendix A): Proposition 1. Let x(t) := A tor (t?, x) for some ? and where t? = (t? 1 , . . . t? m ). Then the linear and angular momentum are zero:</p><formula xml:id="formula_4">d dtx | t=0 = 0 and i (x i ?x) ? d dt x i | t=0 = 0 wherex = 1 n i x.</formula><p>Now consider the product space 6 P = T 3 ? SO(3) ? SO(2) m and define A :</p><formula xml:id="formula_5">P ? R 3n ? R 3n as A((r, R, ?), x) = A tr (r, A rot (R, A tor (?, x)))<label>(2)</label></formula><p>These definitions collectively provide the sought-after product space corresponding to the docking degrees of freedom. Indeed, for a seed ligand conformation c, we can formally define the space of ligand poses M c = {A(g, c) | g ? P}. This corresponds precisely to the intuitive notion of the space of ligand poses that can be reached by rigid-body motion plus torsion angle flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DIFFUSION ON THE PRODUCT SPACE</head><p>We now proceed to show how the product space can be used to learn a DGM over ligand poses in M c . First, we need a theoretical result (proof in Appendix A): Proposition 2. For a given seed conformation c, the map A(?, c) : P ? M c is a bijection.</p><p>which means that the inverse A ?1 c : M c ? P given by A(g, c) ? g maps ligand poses x ? M c to points on the product space P. We are now ready to develop a diffusion process on P.</p><p>De Bortoli et al. <ref type="bibr">[2022]</ref> established that the DGM framework transfers straightforwardly to Riemannian manifolds with the score and score model as elements of the tangent space and with the geodesic random walk as the reverse SDE solver. Further, the score model can be trained in the standard manner with denoising score matching <ref type="bibr">[Song &amp; Ermon, 2019]</ref>. Thus, to implement a diffusion model on P, it suffices to develop a method for sampling from and computing the score of the diffusion kernel on P. Furthermore, since P is a product manifold, the forward diffusion proceeds independently in each manifold <ref type="bibr">[Rodol? et al., 2019]</ref>, and the tangent space is a direct sum:</p><formula xml:id="formula_6">T g P = T r T 3 ? T R SO(3) ? T ? SO(2) m ? = R 3 ? R 3 ? R m where g = (r, R, ?)</formula><p>. Thus, it suffices to sample from the diffusion kernel and regress against its score in each group independently.</p><p>In all three groups, we define the forward SDE as dx = d? 2 (t)/dt dw where ? 2 = ? 2 tr , ? 2 rot , or ? 2 tor for T(3), SO(3), and SO(2) m respectively and where w is the corresponding Brownian motion. Since T(3) ? = R 3 , the translational case is trivial and involves sampling and computing the score of a standard Gaussian with variance ? 2 (t). The diffusion kernel on SO(3) is given by the IGSO(3) distribution <ref type="bibr" target="#b27">[Nikolayev &amp; Savyolov, 1970;</ref><ref type="bibr" target="#b17">Leach et al., 2022]</ref>, which can be sampled in the axis-angle parameterization by sampling a unit vector? ? so(3) uniformly 7 and random angle ? ? [0, ?] according to</p><formula xml:id="formula_7">p(?) = 1 ? cos ? ? f (?) where f (?) = ? l=0</formula><p>(2l + 1) exp(?l(l + 1)? 2 ) sin((l + 1/2)?) sin(?/2)</p><p>Further, the score of the diffusion kernel is ? ln p t (R | R) = ( d d? log f (?))? ? T R SO(3), where R = R(??)R is the result of applying Euler vector ?? to R. The score computation and sampling can be accomplished efficiently by precomputing the truncated infinite series and interpolating the CDF of p(?), respectively. Finally, the SO(2) m group is diffeomorphic to the torus T m , on which the diffusion kernel is a wrapped normal distribution with variance ? 2 (t). This can be sampled directly, and the score can be precomputed as a truncated infinite series <ref type="bibr" target="#b12">[Jing et al., 2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TRAINING AND INFERENCE</head><p>Diffusion model. Although we have defined the diffusion kernel and score matching objectives on P, we nevertheless develop the training and inference procedures to operate on ligand poses in 3D coordinates directly. Providing the full 3D structure, rather than abstract elements of the product space, to the score model allows it to reason about physical interactions using SE(3) equivariant models, not be dependent on arbitrary definitions of torsion angles <ref type="bibr" target="#b12">[Jing et al., 2022]</ref>, and better generalize to unseen complexes.</p><p>The training and inference procedures technically depend on the choice of seed conformation c used to define the mapping between M c and the product space. However, providing a definite choice of c to the score model introduces an arbitrary inference-time parameter that may affect the final predicted distribution, which is undesirable. In other words, while c defines the manifold of ligand poses, the precise location of c within that manifold should not affect the predicted distribution. Thus, we develop approximate training and inference procedures that remove the dependence on the c; intuitively, these assume that updates to points in the product space P can be applied to ligand poses in M c directly, without referencing the origin conformer c. While these are only an approximation of the theoretically correct procedures, we find that they work well in practice. In Appendix B, we present the training and inference procedures in more detail and further discussion on this point.</p><p>Confidence model. In order to collect training data for the confidence model d(x, y), we run the trained diffusion model to obtain a set of candidate poses for every training example and generate labels by testing whether or not each pose has RMSD below 2?. The confidence model is then trained with cross-entropy loss to correctly predict the binary label for each pose. During inference, the diffusion model is run to generate N poses in parallel, which are passed to the confidence model that ranks them based on its confidence that they have RMSD below 2?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">MODEL ARCHITECTURE</head><p>We construct the score model s(x, y, t) and the confidence model d(x, y) to take as input the current ligand pose x and protein structure y in 3D space. The output of the confidence model is a single scalar that is SE(3)-invariant (with respect to joint rototranslations of x, y) as ligand pose distributions are defined relative to the protein structure, which can have arbitrary location and orientation. On the other hand, the output of the score model must be in the tangent The score model and confidence model have similar architectures based on SE(3)-equivariant convolutional networks over point clouds <ref type="bibr">[Thomas et al., 2018;</ref><ref type="bibr">Geiger et al., 2020]</ref>. The fundamental difference between them is the scale at which they operate: while the score model only considers a coarse-grained representation of the protein with only its ?-carbon atoms, the confidence model has access to the full atomic structure of the protein. This multiscale setup yields improved performance and a significant speed-up w.r.t. doing the whole process at the atomic scale. The architectural components are summarized below and detailed in Appendix C.</p><formula xml:id="formula_9">space T r T 3 ? T R SO(3) ? T ? SO(2) m . The space T r T 3 ? = R 3</formula><p>Structures are represented as heterogeneous geometric graphs formed by ligand atoms, protein residues, and protein atoms (the latter only for the confidence model). Residue nodes receive as initial features language model embeddings trained on protein sequences <ref type="bibr" target="#b19">[Lin et al., 2022]</ref>. Nodes are sparsely connected based on distance cutoffs that depend on the types of nodes being linked and on the diffusion time. Intuitively, nodes are connected with the range of elements that they might be closely interacting with; this range may span widely at the start of the diffusion but is narrow at the end. Convolutional layers simultaneously operate with different sets of weights for different connection types (so 9 sets of weights for the confidence model and 4 for the score at every layer) and generate scalar and vector representations for each node.</p><p>The ligand atom representations after the final interaction layer are then used to produce the different outputs. To produce the two R 3 vectors representing the translational and rotational scores, we convolve the node representations with a tensor product filter placed at the center of mass. For the torsional score, we use a pseudotorque convolution to obtain a scalar at each rotatable bond of the ligand analogously to Jing et al. <ref type="bibr">[2022]</ref>, with the distinction that, since the score model operates on coarse-grained representations, the output is not a pseudoscalar (its parity is neither odd nor even). Finally, the confidence model's output is produced by mean-pooling the ligand atoms' scalar representations followed by a fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Experimental setup. We evaluate our method on the complexes from PDBBind <ref type="bibr" target="#b20">[Liu et al., 2017]</ref>, a large collection of protein-ligand structures collected from PDB <ref type="bibr" target="#b2">[Berman et al., 2003]</ref>, which was used with time-based splits to benchmark many previous works <ref type="bibr">[St?rk et al., 2022;</ref><ref type="bibr">Volkov et al., 2022;</ref>. We compare DIFFDOCK with state-of-the-art search-based methods SMINA <ref type="bibr" target="#b15">[Koes et al., 2013]</ref>, <ref type="bibr">QuickVina-W [Hassan et al., 2017]</ref>, <ref type="bibr">GLIDE [Halgren et al., 2004]</ref>, and GNINA <ref type="bibr" target="#b22">[McNutt et al., 2021]</ref> and the recent deep learning methods EquiBind and TANKBind presented above. Extensive details about the experimental setup, data, baselines, and implementation are in Appendix D.3 and all code is available at https://github.com/gcorso/DiffDock. The repository also contains videos of the reverse diffusion process (images of the same are in <ref type="figure" target="#fig_0">Figure 11</ref>).</p><p>As we are evaluating blind docking, the methods receive two inputs: the ligand with a predicted seed conformation (e.g., from RDKit) and the crystal structure of the protein. Since search-based methods work best when given a starting binding pocket to restrict the search space, we also test the combination of using an ML-based method, such as P2Rank <ref type="bibr" target="#b16">[Kriv?k &amp; Hoksza, 2018]</ref> (also used by TANKBind) or EquiBind to find an initial binding pocket, followed by a search-based method to predict the exact pose in the pocket.</p><p>To evaluate the generated complexes, we compute the heavy-atom RMSD (permutation symmetry corrected) between the predicted and the ground-truth ligand atoms when the protein structures are aligned. All methods except for EquiBind are able to generate multiple structures and rank them. We report the metrics for the highest ranked prediction as the top-1; top-5 refers to selecting the most accurate pose out of the 5 highest ranked predictions, which is a useful metric when multiple predictions are used for downstream tasks. <ref type="table">Table 1</ref>: PDBBind blind docking. All methods receive a small molecule and are tasked to find its binding location, orientation, and conformation. Shown is the percentage of predictions with RMSD &lt; 2? and the median RMSD. The top half contains methods that directly find the pose; the bottom half those that use a pocket prediction method. The last two lines show our method's performance (with standard deviation). In parenthesis we specify the number of poses sampled from the generative model. * indicates that the method runs exclusively on CPU, "-" means not applicable; some cells are empty due to infrastructure constraints. For TANKBind, the runtimes for the top-1 and top-5 predictions are different. Further evaluation details are in Appendix D.3.  Inference runtime. DIFFDOCK holds its superior accuracy while being (on GPU) 3 to 12 times faster than the best search-based method, GNINA <ref type="table">(Table 1)</ref>. This high speed is critical for applications such as high throughput virtual screening for drug candidates or reverse screening for protein targets, where one often searches over a vast number of complexes. As a diffusion model, DIFF-DOCK is inevitably slower than the one-shot deep learning method EQUIBIND, but as shown in <ref type="figure" target="#fig_2">Figure 3</ref>-right and Appendix E.3, it can be significantly sped up without significant loss of accuracy. Selective accuracy of confidence score. As the top-1 results show, DIFFDOCK's confidence model is very accurate in ranking the sampled poses for a given complex and picking the best one. We also investigate the selective accuracy of the confidence model across different complexes by evaluating how DIFFDOCK's accuracy increases if it only makes predictions when the confidence is above a certain threshold, known as selective prediction. In <ref type="figure">Figure 4</ref>, we plot the success rate as we decrease the percentage of complexes for which we make predictions, i.e., increase the confidence threshold. When only making predictions for the top one-third of complexes in terms of model confidence, the success rate improves from 38% to 83%. Additionally, there is a high Spearman correlation of 0.68 between DIFFDOCK's confidence and the negative RMSD. Thus, the confidence score is a good indicator of the quality of DIFFDOCK's top-ranked sampled pose and provides a highly valuable confidence measure for downstream applications.</p><formula xml:id="formula_10">Top-1 RMSD (?) Top-5 RMSD (?) Average Method %&lt;2 Med.<label>%&lt;2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented DIFFDOCK, a diffusion generative model tailored to the task of molecular docking. This represents a paradigm shift from previous deep learning approaches, which use regressionbased frameworks, to a generative modeling approach that is better aligned with the objective of molecular docking. To produce a fast and accurate generative model, we designed a diffusion process over the manifold describing the main degrees of freedom of the task via ligand pose transformations spanning the manifold.</p><p>Empirically, DIFFDOCK outperforms the state-of-the-art by very large margins on PDBBind, has fast inference times, and provides confidence estimates with high selective accuracy. Thus, DIFF-DOCK can offer great value for many existing real-world pipelines and opens up new avenues of research on how to best integrate downstream tasks, such as affinity prediction, into the framework and apply similar ideas to protein-protein and protein-nucleic acid docking. Nataraj S Pagadala, Khajamohiddin Syed, and Jack Tuszynski. Software for molecular docking: a review. Biophysical reviews, 9 <ref type="formula" target="#formula_5">(2)</ref> </p><formula xml:id="formula_11">(x i ?x) ? d dt x i | t=0 = 0 wherex = 1 n i x. Proof. Let x(t) = R(t)(B(t?, x) ?x 0 ) +x 0 + p(t) where B(t?, ?) = B 1,t?1</formula><p>? ? ? ? B m,t?m and R(t), p(t) are the rotation (aroundx 0 :=x <ref type="formula">(0)</ref>) and translation associated with the optimal alignment between B(t?, x) and x. By definition of RMSD, we have</p><formula xml:id="formula_12">||x(t) ? x(0)|| = min R,p ||R(t)(B(t?, x(0)) ?x 0 ) +x 0 + p(t) ? x(0)||<label>(4)</label></formula><p>which, in the limit of t ? 0, becomes</p><formula xml:id="formula_13">d dt x(t) 2 t=0 = min R,p d dt (R(t)(B(t?, x(0)) ?x 0 ) + p(t)) 2 t=0<label>(5)</label></formula><p>The derivative in the LHS of Equation 5 at t = 0 is</p><formula xml:id="formula_14">R (t)(x ?x) + B (t?, x(0)) + p (t)<label>(6)</label></formula><p>and represents the instantaneous velocity of the points x i at t = 0. Denoting r i = x i ?x 0 , our objective is to minimize i ||r i || 2 . Then, if we describe R (t)(x ?x 0 ) = R (t)r by an angular velocity ? and abbreviate B (t?, x(0)) i := b i and p = v, we have</p><formula xml:id="formula_15">i r i 2 = i (bi + ? ? ri + v) ? (bi + ? ? ri + v) = i ||bi|| 2 + 2bi ? (? ? ri) + 2bi ? v + (? ? ri) ? (? ? ri) + 2(? ? ri) ? v + ||v|| 2 = i ||bi|| 2 + 2? ? i (ri ? bi) + 2 i bi ? v + n ||v|| 2 + ? T I(r)?<label>(7)</label></formula><p>where we have used the fact that i r i = 0 and where I(r) = ( i r i ? r i ) I ? i r i r T i is the 3 ? 3 inertia tensor. Then setting gradients with respect to v, ? gives</p><formula xml:id="formula_16">v = ? 1 n i b i and ? = ?I(r) ?1 i r i ? b i (8) Now with r i = b i + ? ? r i + v we evaluate the linear momentum 1 n i r i = 1 n i b i + ? ? i r i + nv = 0<label>(9)</label></formula><p>which is zero by direct substitution of v. Similarly, we evaluate the angular momentum</p><formula xml:id="formula_17">i r i ? r i = i r i ? b i + i r i ? (? ? r i ) + i r i ? v = i r i ? b i + I(r)? = 0<label>(10)</label></formula><p>which is zero by direct substitution of ?. Thus, the linear and angular momentum are zero at t = 0 for arbitrary x(0).</p><p>Note that since we did not use the particular form of B(t?, x) in the above proof, we have shown that RMSD alignment can be used to disentangle rotations and translations from the infinitesimal action of any arbitrary function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF PROPOSITION 2</head><p>Proposition 2. For a given seed conformation c, the map A(?, c) : P ? M c is a bijection.</p><p>Proof. Since we defined M c = {A(g, c) | g ? P}, A(?, c) is automatically surjective. We now show that it is injective. Assume for the sake of contradiction that A(?, c) is not injective, so that there exist elements of the product space g 1 , g 2 ? P with g 1 = g 2 but with</p><formula xml:id="formula_18">A(g 1 , c) = A(g 2 , c) = c . That is, A tr (r 1 , A rot (R 1 , A tor (? 1 , c))) = A tr (r 2 , A rot (R 2 , A tor (? 2 , c)))<label>(11)</label></formula><p>which we abbreviate as c (1) = c (2) . Since only A tr changes the center of mass i c i /n, we have i c</p><p>i /n = i c i /n + r 1 and i c</p><p>i /n = i c i /n + r 2 . However, since c (1) = c (2) , this implies r 1 = r 2 . Next, consider the torsion angles ? 1 = (? </p><formula xml:id="formula_21">? = ? i + ? (1) i mod 2? and ? (2) i ? = ? i + ? (2) i mod 2? for all i = 1, . . . m. However, because ? (1) i = ? (2) i , this means ? (1) i ? = ? (2) i</formula><p>for all i and therefore ? 1 = ? 2 (as elements of SO(2) m ). Now denote c = A tor (? 1 , c) = A tor (? 2 , c) and apply A tr (?r 1 , ?) = A tr (?r 2 , ?) to both sides of Equation 11. We then have</p><formula xml:id="formula_22">A rot (R 1 , c ) = A rot (R 2 , c )<label>(12)</label></formula><p>which further leads to</p><formula xml:id="formula_23">c ?c = R ?1 1 R 2 (c ?c )<label>(13)</label></formula><p>In general, this does not imply that R 1 = R 2 . However, R 1 = R 2 is possible only if c is degenerate, in the sense that all points are collinear along the shared axis of rotation of R 1 , R 2 . However, in practice, conformers never consist of a collinear set of points, so we can safely assume R 1 = R 2 . We now have (r 1 , R 1 , ? 1 ) = (r 2 , R 2 , ? 2 ), or g 1 = g 2 , contradicting our initial assumption. We thus conclude that A(?, c) is injective, completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TRAINING AND INFERENCE</head><p>In this section we present the training and inference procedures of the diffusion generative model. First, however, there are a few subtleties of the generative approach to molecular docking that are worth mentioning. Unlike the standard generative modeling setting where the dataset consists of many samples drawn from the data distribution, each training example (x , y) of protein structure y and ground-truth ligand pose x is the only sample from the corresponding conditional distribution p x (? | y) defined over M x . Thus, the innermost training loop iterates over distinct conditional distributions p x (? | y), along with a single sample from that distribution, rather than over samples from a common data distribution p data (x).</p><p>As discussed in Section 4, during inference, c is the ligand structure generated with a method such as RDKit. However, during training we require M c = M x in order to define a bijection between c ? M x and P. If we take c ? M x , there will be a distribution shift between the manifolds M c considered at training time and those considered at inference time. To circumvent this issue, at training time we predict c with RDKit and replace x with arg min x ? ?Mc RMSD(x , x ? ) using the conformer matching procedure described in Jing et al. <ref type="bibr">[2022]</ref>.</p><p>The above paragraph may be rephrased more intuitively as follows: during inference, the generative model docks a ligand structure generated by RDKit, keeping its non-torsional degrees of freedom (e.g., local structures) fixed. At training time, however, if we train the score model with the local structures of the ground truth pose, this will not correspond to the local structures seen at inference time. Thus, at training time, we replace the ground truth pose by generating a ligand structure with RDKit and aligning it to the ground truth pose while keeping the local structures fixed.</p><p>With these preliminaries, we now continue to the full procedures (Algorithms 1 and 2). The training and inference procedures of a score-based diffusion generative model on a Riemannian manifold consist of (1) sampling and regressing against the score of the diffusion kernel during training; and (2) sampling a geodesic random walk with the score as a drift term during inference <ref type="bibr" target="#b5">[De Bortoli et al., 2022</ref>]. Because we have developed the diffusion process on P but continue to provide the score model with elements in M c ? R 3n , the full training and inference procedures involve repeatedly interconverting between the two spaces using the bijection given by the seed conformation c. </p><formula xml:id="formula_24">? arg min x ? ?Mc RMSD(x , x ? ); Compute (r 0 , R 0 , ? 0 ) ? A ?1 c (x 0 ); Sample t ? Uni([0, 1]); Sample ?r, ?R, ?? from diffusion kernels p tr t (? | 0), p rot t (? | 0), p tor t (? | 0); Set r t ? r 0 + ?r; Set R t ? (?R)R 0 ; Set ? t ? ? 0 + ?? mod 2?; Compute x t ? A((r t , R t , ? t ), c); Predict scores ? ? R 3 , ? ? R 3 , ? ? R m = s(x t , c, y, t) ; Take optimization step on loss L = ||? ? ?p tr t (?r | 0)|| 2 + ||? ? ?p rot t (?R | 0)|| 2 + ||? ? ?p tor t (?? | 0)|| 2</formula><p>Algorithm 2: Inference procedure Input: RDKit prediction c, protein structure y (both centered at origin)</p><formula xml:id="formula_25">Output: Sampled ligand pose x 0 Sample ? N ? Uni(SO(2) m ), R N ? Uni(SO(3)), r N ? N (0, ? 2 tor (T )); Let x N = A((r N , R N , ? N ), c); for n ? N to 1 do</formula><p>Let t = n/N and ?? 2 tr = ? 2 tr (n/N ) ? ? 2 tr ((n ? 1)/N ) and similarly for ?? 2 rot , ?? 2 tor ; Predict scores ? ? R 3 , ? ? R 3 , ? ? R m ? s(x n , c, y, t); Sample z tr , z rot , z tor from N (0, ?? 2 tr ), N (0, ?? 2 rot ), N (0, ?? 2 tor ) respectively; Set r n?1 ? r 0 + ?? 2 tr ? + z tr ; Set R n?1 ? R(?? 2 rot ? + z rot )R n ); Set ? n?1 ? ? n + (?? 2 tor ? + z tor ) mod 2?; Compute x n?1 ? A((r n?1 , R n?1 , ? n?1 ), c); Return x 0 ;</p><p>However, as noted in the main text, the dependence of these procedures on the exact choice of c is potentially problematic, as it suggests that at inference time, the model distribution may be different depending on the orientation and torsion angles of c. Simply removing the dependence of the score model on c is not sufficient since the update steps themselves still occur on P and require a choice of c to be mapped to M c . However, notice that the update steps-in both training and inferenceconsist of (1) sampling the diffusion kernels at the origin; (2) applying these updates to the point on P; and (3) transferring the point on P to M c via A(?, c). Might it instead be possible to apply the updates to 3D ligand poses x ? M c directly?</p><p>It turns out that the notion of applying these steps to ligand poses "directly" corresponds to the formal notion of group action. The operations A tr , A rot , A tor that we have already defined are formally group actions if they satisfy A (?) (g 1 g 2 , x) = A(g 1 , A(g 2 , x)). While true for A tr , A rot , this is not generally true for A tor if we take SO(2) m to be the direct product group; however, the approximation is increasingly good as the magnitude of the torsion angle updates decreases. If we then define P to be the direct product group of its constituent groups, A is a group action of P on M c , as the operations of A tr , A rot , A tor commute and are (under the approximation) individually group actions.</p><p>The implication of A being a group action can be seen as follows. Let ? = g b g ?1 a be the update which brings g a ? P to g b ? P via left multiplication, and let x a , x b be the corresponding ligand poses A(g a , c), A(g b , c). Then</p><formula xml:id="formula_26">x b = A(g b g ?1 a g a , c) = A(?, x a )<label>(14)</label></formula><p>which means that the updates ? can be applied directly to x a using the operation A. The training and inference procedures then become Algorithm 3 and 4 below. The initial conformer c is no longer used, except in the initial steps to define the manifold-to find the closest point to x in training, and to sample x N from the prior over M c in inference.</p><p>Conceptually speaking, this procedure corresponds to "forgetting" the location of the origin element on M c , which is permissible because a change of the origin to some equivalent seed c ? M c merely translates-via right multiplication by A ?1 c (c )-the original and diffused data distributions on P, but does not cause any changes on M c itself. The training and inference routines involve updates-formally left multiplications-to group elements, but as left multiplication on the group corresponds to group actions on M c , the updates can act on M c directly, without referencing the origin c.</p><p>We find that the approximation of A as a group action works quite well in practice and use Algorithms 3 and 4 for all training and experiments discussed in the paper. Of course, disentangling the torsion updates from rotations in a way that makes A tor exactly a group action would justify the procedure further, and we regard this as a possible direction for future work. </p><formula xml:id="formula_27">Let x 0 ? arg min x ? ?Mc RMSD(x , x ? ); Sample t ? Uni([0, 1]); Sample ?r, ?R, ?? from diffusion kernels p tr t (? | 0), p rot t (? | 0), p tor t (? | 0); Compute x t ? A((?r, ?R, ??), x 0 ); Predict scores ? ? R 3 , ? ? R 3 , ? ? R m = s(x t , y, t) ; Take optimization step on loss L = ||? ? ?p tr t (?r | 0)|| 2 + ||? ? ?p rot t (?R | 0)|| 2 + ||? ? ?p tor t (?? | 0)|| 2</formula><p>Algorithm 4: Approximate inference procedure Input: RDKit prediction c, protein structure y (both centered at origin) Output: Sampled ligand pose x 0 Sample ? N ? Uni(SO(2) m ), R N ? Uni(SO(3)), r N ? N (0, ? 2 tor (T )); Let x N = A((r N , R N , ? N ), c); for n ? N to 1 do Let t = n/N and ?? 2 tr = ? 2 tr (n/N ) ? ? 2 tr ((n ? 1)/N ) and similarly for ?? 2 rot , ?? 2 tor ; Predict scores ? ? R 3 , ? ? R 3 , ? ? R m ? s(x n , y, t); Sample z tr , z rot , z tor from N (0, ?? 2 tr ), N (0, ?? 2 rot ), N (0, ?? 2 tor ) respectively; Set ?r ? r 0 + ?? 2 tr ? + z tr ; Set ?R ? R(?? 2 rot ? + z rot ); Set ?? ? ?? 2 tor ? + z tor ; Compute x n?1 ? A((?r, ?R, ??), x n ); Return x 0 ; C ARCHITECTURE DETAILS As summarized in Section 4.5, we use convolutional networks based on tensor products of irreducible representations (irreps) of SO(3) <ref type="bibr">[Thomas et al., 2018]</ref> as architecture for both the score and confidence models. In particular, these are implemented using the e3nn library <ref type="bibr">[Geiger et al., 2020]</ref>. Below, ? w refers to the spherical tensor product of irreps with path weights w, and ? refers to normal vector addition (with possibly padded inputs). Features have multiple channels for each irrep. Both the architectures can be decomposed into three main parts: embedding layer, interaction layers, and output layer. We outline each of them below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 EMBEDDING LAYER</head><p>Geometric heterogeneous graph. Structures are represented as heterogeneous geometric graphs with nodes representing ligand (heavy) atoms, receptor residues (located in the position of the ?carbon atom), and receptor (heavy) atoms (only for the confidence model). Because of the high number of nodes involved, it is necessary for the graph to be sparsely connected for runtime and memory constraints. Moreover, sparsity can act as a useful inductive bias for the model, however, it is critical for the model to find the right pose that nodes that might have a strong interaction in the final pose to be connected during the diffusion process. Therefore, to build the radius graph, we connect nodes using cutoffs that are dependent on the types of nodes they are connecting:</p><p>1. Ligand atoms-ligand atoms, receptor atoms-receptor atoms, and ligand atoms-receptor atoms interactions all use a cutoff of 5?, standard practice for atomic interactions. For the ligand atoms-ligand atoms interactions we also preserve the covalent bonds as separate edges with some initial embedding representing the bond type (single, double, triple and aromatic). For receptor atoms-receptor atoms interactions, we limit at 8 the maximum number of neighbors of each atom. Note that the ligand atoms-receptor atoms only appear in the confidence model where the final structure is already set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Receptor residues-receptor residues use a cutoff of 15? with 24 as the maximum number of neighbors for each residue.</p><p>3. Receptor residues-ligand atoms use a cutoff of 20 + 3 * ? tr? where ? tr represents the current standard deviation of the diffusion translational noise present in each dimension (zero for the confidence model). Intuitively this guarantees that with high probability, any of the ligands and receptors that will be interacting in the final pose the diffusion model converges to are connected in the message passing at every step.</p><p>4. Finally, receptor residues are connected to the receptor atoms that form the corresponding amino-acid.</p><p>Node and edge featurization. For the receptor residues, we use the residue type as a feature as well as a language model embedding obtained from ESM2 <ref type="bibr" target="#b19">[Lin et al., 2022]</ref>. The ligand atoms have the following features: atomic number; chirality; degree; formal charge; implicit valence; the number of connected hydrogens; the number of radical electrons; hybridization type; whether or not it is in an aromatic ring; in how many rings it is; and finally, 6 features for whether or not it is in a ring of size 3, 4, 5, 6, 7, or 8. These are concatenated with sinusoidal embeddings of the diffusion time <ref type="bibr">[Vaswani et al., 2017]</ref> and, in the case of edges, radial basis embeddings of edge length <ref type="bibr">[Sch?tt et al., 2017]</ref>. These scalar features of each node and edge are then transformed with learnable two-layer MLPs (different for each node and edge type) into a set of scalar features that are used as initial representations by the interaction layers.</p><p>Notation Let (V, E) represent the heterogeneous graph, with V = (V , V r ) respectively ligand atoms and receptor residues (receptor atoms V a , present in the confidence model, are for simplicity not included here), and similarly E = (E , E r , E r , E rr ). Let h a be the node embeddings (initially only scalar channels) of node a, e ab the edge embeddings of (a, b), and ?(r ab ) radial basis embeddings of the edge length. Let ? 2 tr , ? 2 rot , and ? 2 tor represent the variance of the diffusion kernel in each of the three components: translational, rotational and torsional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 INTERACTION LAYERS</head><p>At each layer, for every pair of nodes in the graph, we construct messages using tensor products of the current node features with the spherical harmonic representations of the edge vector. The weights of this tensor product are computed based on the edge embeddings and the scalar features-denoted h 0 a -of the outgoing and incoming nodes. The messages are then aggregated at each node and used to update the current node features. For every node a of type t a :</p><formula xml:id="formula_28">h a ? h a ? t?{ ,r} BN (ta,t) 1 |N (t) a | b?N (t) a Y (r ab ) ? ? ab h b with ? ab = ? (ta,t) (e ab , h 0 a , h 0 b )<label>(15)</label></formula><p>Here, t indicates an arbitrary node type, N (t) a = {b | (a, b) ? E tat } the neighbors of a of type t, Y are the spherical harmonics up to = 2, and BN the (equivariant) batch normalisation. The orders of the output are restricted to a maximum of = 1. All learnable weights are contained in ?, a dictionary of MLPs, which uses different sets of weights for different edge types (as an ordered pair so four types for the score model and nine for the confidence) and different rotational orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 OUTPUT LAYER</head><p>The ligand atom representations after the final interaction layer are used in the output layer to produce the required outputs. This is where the score and confidence architecture differ significantly. On one hand, the score model's output is in the tangent space T r T 3 ? T R SO(3) ? T ? SO(2) m . This corresponds to having two SE(3)-equivariant output vectors representing the translational and rotational score predictions and m SE(3)-invariant output scalars representing the torsional score. For each of these, we design final tensor-product convolutions inspired by classical mechanics. On the other hand, the confidence model outputs a single SE(3)-invariant scalar representing the confidence score. Below we detail how each of these outputs is generated.</p><p>Translational and rotational scores. The translational and rotational score intuitively represent, respectively, the linear acceleration of the center of mass of the ligand and the angular acceleration of the rest of the molecule around the center. Considering the ligand as a rigid object and given a set of forces and masses at each ligand, a tensor product convolution between the atoms and the center of mass would be capable of computing the desired quantities. Therefore, for each of the two outputs, we perform a convolution of each of the ligand atoms with the (unweighted) center of mass</p><formula xml:id="formula_29">c. v ? 1 |V | a?V Y (r ca ) ? ?ca h a with ? ca = ?(?(r ca ), h 0 a )<label>(16)</label></formula><p>We restrict the output of v to a single odd and a single even vectors (for each of the two scores).</p><p>Since we are using coarse-grained representations of the protein, the score will neither be even nor odd; therefore, we sum the even and odd vector representations of v. Finally, the magnitude (but not direction) of these vectors is adjusted with an MLP taking as input the current magnitude and the sinusoidal embeddings of the diffusion time. Finally, we (revert the normalization) by multiplying the outputs by 1/? tr for the translational score and by the expected magnitude of a score in SO(3) with diffusion parameter ? rot (precomputed numerically).</p><p>Torsional score. To predict the m SE(3)-invariant scalar describing the torsional score, we use a pseudotorque layer similar to that of Jing et al. <ref type="bibr">[2022]</ref>. This predicts a scalar score ?? for each rotatable bond from the per-node outputs of the atomic convolution layers. For rotatable bond g = (g 0 , g 1 ) and b ? V , let r gb andr gb be the magnitude and direction of the vector connecting the center of bond g and b. We construct a convolutional filter T g for each bond g from the tensor product of the spherical harmonics with a = 2 representation of the bond axisr g : 10</p><formula xml:id="formula_30">T g (r) := Y 2 (r g ) ? Y (r)<label>(17)</label></formula><p>? is the full (i.e., unweighted) tensor product as described in <ref type="bibr" target="#b6">Geiger &amp; Smidt [2022]</ref>, and the second term contains the spherical harmonics up to = 2 (as usual). This filter (which contains orders up to = 3) is then used to convolve with the representations of every neighbor on a radius graph:</p><formula xml:id="formula_31">E ? = {(g, b) | g a rotatable bond, b ? V } e gb = ? (? ) (?(r gb )) ?(g, b) ? E ? h g = 1 |N g | b?Ng T g (r gb ) ? ? gb h b with ? gb = ?(e gb , h 0 b , h 0 g0 + h 0 g1 )<label>(18)</label></formula><p>Here, N g = {b | (g, b) ? E ? } and ? (? ) and ? are MLPs with learnable parameters. Since unlike Jing et al. <ref type="bibr">[2022]</ref>, we use coarse-grained representations the parity also here is neither even nor odd, the irreps in the output are restricted to arrays both even h g and odd h g scalars. Finally, we produce a single scalar prediction for each bond:</p><formula xml:id="formula_32">?? g = ?(h g + h g )<label>(19)</label></formula><p>where ? is a two-layer MLP with tanh nonlinearity and no biases. This is also "denormalized" by multiplying by the expected magnitude of a score in SO(2) with diffusion parameter ? tor .</p><p>Confidence output. The single SE(3)-invariant scalar representing the confidence score output is instead obtained by concatenating the even and odd final scalar representation of each ligand atom, averaging these feature vectors among the different atoms, and finally applying a three layers MLP (with batch normalization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAILS</head><p>In general, all our code is available at https://github.com/gcorso/DiffDock. This includes running the baselines, runtime calculations, training and inference scripts for DIFFDOCK, the PDB files of DIFFDOCK's predictions for all 363 complexes of the test set, and visualization videos of the reverse diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 EXPERIMENTAL SETUP</head><p>Data. We use the molecular complexes in PDBBind <ref type="bibr" target="#b20">[Liu et al., 2017]</ref> that were extracted from the Protein Data Bank (PDB) <ref type="bibr" target="#b2">[Berman et al., 2003]</ref>. We employ the time-split of PDBBind proposed by St?rk et al. <ref type="bibr">[2022]</ref> with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes. This is motivated by the further adoption of the same split  and the critical assessment of PDBBind splits by Volkov et al.</p><p>[2022] who favor temporal splits over artificial splits based on molecular scaffolds or protein sequence/structure similarity. For completeness, we also report the results on protein sequence similarity splits in Appendix E.</p><p>Metrics. To evaluate the generated complexes, we compute the heavy-atom RMSD between the predicted and the crystal ligand atoms when the protein structures are aligned. To account for permutation symmetries in the ligand, we use the symmetry-corrected RMSD of sPyRMSD <ref type="bibr" target="#b23">[Meli &amp; Biggin, 2020]</ref>. For these RMSD values, we report the percentage of predictions that have an RMSD that is less than 2?. We choose 2? since much prior work considers poses with an RMSD less that 2? as "good" or successful <ref type="bibr" target="#b0">[Alhossary et al., 2015;</ref><ref type="bibr" target="#b9">Hassan et al., 2017;</ref><ref type="bibr" target="#b22">McNutt et al., 2021]</ref>. This is a chemically relevant metric, unlike the mean RMSD as detailed in Section 3 since for further downstream analyses such as determining function changes, a prediction is only useful below a certain RMSD error threshold. Less relevant metrics such as the mean RMSD are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 IMPLEMENTATION DETAILS: HYPERPARAMETERS, TRAINING, AND RUNTIME</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MEASUREMENT</head><p>Training Details. We use Adam <ref type="bibr" target="#b14">[Kingma &amp; Ba, 2014]</ref> as optimizer for the diffusion and the confidence model. The diffusion model with which we run inference uses the exponential moving average of the weights during training, and we update the moving average after every optimization step with a decay factor of 0.999. The batch size is 16. We run inference with 20 denoising steps on 500 validation complexes every 5 epochs and use the set of weights with the highest percentage of RMSDs less than 2? as the final diffusion model. We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days). The confidence model is trained on a single 48GB GPU. For inference, only a single GPU is required. Scaling up the model size seems to improve performance and future work could explore whether this trend continues further. For the confidence model uses the validation cross-entropy loss is used for early stopping and training only takes 75 epochs. Code to reproduce all results including running the baselines or to perform docking calculations for new complexes is available at https://github.com/gcorso/DiffDock.</p><p>Hyperparameters. For determining the hyperparameters of DIFFDOCK's score model, we trained smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs. The smaller models were only trained for 250 or 300 epochs, and we used the fraction of predictions with an RMSD below 2? on the validation set to choose the hyperparameters. <ref type="table" target="#tab_3">Table 2</ref> shows the main hyperparameters we tested and the final parameters of the large model we use to obtain our results. We only did little tuning for the minimum and maximum noise levels of the three components of the diffusion. For the translation, the maximum standard deviation is 19?. We also experimented with second-order features for the Tensor Field Network but did not find them to help. The complete set of hyperparameters next to the main ones we describe here can be found in our repository. From the start we have divided the inference schedule in 20 time steps, the effect of using more or less steps for inference is discussed in Appendix E.3. As we found that the large-scale diffusion models overfit the training data on low-levels of noise we stop the diffusion early after 18 steps. At the last diffusion steps no noise is added.</p><p>The confidence model has 4.77 million parameters and the parameters we tried are in <ref type="table" target="#tab_4">Table 3</ref>. We generate 28 different training poses for the confidence model (for which it predicts whether or not they have an RMSD below 2?) with a small score model. The score model used to generate the training samples for the confidence model does not need to be the same one that the model will be applied to at inference time.  Runtime. Similar to all the baselines, the preprocessing times are not included in the reported runtimes. For DIFFDOCK the preprocessing time is negligible compared to the rest of the inference time where multiple reverse diffusion steps are performed. Preprocessing mainly consists of a forward pass of ESM2 to generate the protein language model embeddings, RDKit's conformer generation, and the conversion of the protein into a radius graph. We measured the inference time when running on an RTX A100 40GB GPU when generating 10 samples. The runtimes we report for generating 40 samples and ranking them are extrapolations where we multiply the runtime for 10 samples by 4. In practice, this only gives an upper bound on the runtime with 40 samples, and the actual runtime should be faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 BASELINES: IMPLEMENTATION, USED SCRIPTS, AND RUNTIME DETAILS</head><p>Our scripts to run the baselines are available at https://github.com/gcorso/DiffDock. For obtaining the runtimes of the different methods, we always used 16 CPUs except for GLIDE as explained below. The runtimes do not include any preprocessing time for any of the methods. For instance, the time that it takes to run P2Rank is not included for TANKBind, and P2Rank + SMINA/GNINA since this receptor preparation only needs to be run once when docking many ligands to the same protein. In applications where different receptors are processed (such as reverse screening), the experienced runtimes for TANKBind and P2Rank + SMINA/GNINA will thus be higher.</p><p>We note that for all these baselines we have used the default hyperparameters unless specified differently below. Modifying some of these hyperparameters (for example the scoring method's exhaustiveness) will change the runtime and performance tradeoffs (e.g., if the searching routine is left running for longer then better poses are likely to be found), however, we leave these analyses to future work.</p><p>SMINA <ref type="bibr" target="#b15">[Koes et al., 2013]</ref> improves Autodock Vina with a new scoring-function and user friendliness. The default parameters were used with the exception of setting --num modes 10. To define the search box, we use the automatic box creation option around the receptor with the default buffer of 4? on all 6 sides.</p><p>GNINA <ref type="bibr" target="#b22">[McNutt et al., 2021]</ref> builds on SMINA by additionally using a learned 3D CNN for scoring. The default parameters were used with the exception of setting --num modes 10. To define the search box, we use the automatic box creation option around the receptor with the default buffer of 4? on all 6 sides. <ref type="bibr">-W [Hassan et al., 2017]</ref> extends the speed-optimized QuickVina 2 <ref type="bibr" target="#b0">[Alhossary et al., 2015]</ref> for blind docking. We reuse the numbers from St?rk et al. <ref type="bibr">[2022]</ref> which had used the default parameters except for increasing the exhaustiveness to 64. <ref type="bibr" target="#b8">[Halgren et al., 2004</ref>] is a strong heavily used commercial docking tool. These methods all use biophysics based scoring-functions. We reuse the numbers from St?rk et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QuickVina</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GLIDE</head><p>[2022] since we do not have a license. As explained by <ref type="bibr">St?rk et al. [2022]</ref>, the very high runtime of GLIDE with 1405 seconds per complex is partially explained by the fact that GLIDE only uses a single thread when processing a complex. This fact and the parallelization options of GLIDE are explained here https://www.schrodinger.com/kb/1165. With GLIDE, it is possible to start dataparallel processes that compute the docking results for a different complex in parallel. However, each process also requires a separate software license.</p><p>EquiBind <ref type="bibr">[St?rk et al., 2022]</ref>, we reuse the numbers reported in their paper and generate the predictions that we visualize with their code at https://github.com/HannesStark/EquiBind.</p><p>TANKBind , we use the code associated with the paper at https://github. com/luwei0917/TankBind. The runtimes do not include the runtime of P2Rank or any preprocessing steps. In <ref type="table">Table 1</ref> we report two runtimes (0.72/2.5 sec). The first is the runtime when making only the top-1 prediction and the second is for producing the top-5 predictions. Producing only the top-1 predictions is faster since TANKBind produces distance predictions that need to be converted to coordinates with a gradient descent algorithm and this step only needs to be run once for the top-1 prediction, while it needs to be run 5 times for producing 5 outputs. To obtain our runtimes we run the forward pass of TANKBind on GPU (0.28 seconds) with the default batch size of 5 that is used in their GitHub repository. To compute the time the distances-to-coordinates con-version step takes, we run the file baseline run tankbind parallel.sh in our repository, which parallelizes the computation across 16 processes which we also run on an Intel Xeon Gold 6230 CPU. This way, we obtain 0.44 seconds runtime for the conversion step of the top-1 prediction (averaged over the 363 complexes of the testset).</p><p>P2Rank <ref type="bibr" target="#b16">[Kriv?k &amp; Hoksza, 2018]</ref>, is a tool that predicts multiple binding pockets and ranks them. We use it for running TANKBind and P2Rank + SMINA/GNINA. We download the program from https://github.com/rdk/p2rank and run it with its default parameters.</p><p>EquiBind + SMINA/GNINA <ref type="bibr">[St?rk et al., 2022]</ref>, the bounding box in which GNINA/SMINA searches for binding poses is constructed around the prediction of EquiBind with the --autobox ligand option of GNINA/SMINA. EquiBind is thus used to find the binding pocket and SMINA/GNINA to find the exact final binding pose. We use --autobox add 10 to add an additional 10? on all 6 sides of the bounding box following <ref type="bibr">[St?rk et al., 2022]</ref>.</p><p>P2Rank + SMINA/GNINA. The bounding box in which GNINA/SMINA searches for binding poses is constructed around the pocket center that P2Rank predicts as the most likely binding pocket. P2Rank is thus used to find the binding pocket and SMINA/GNINA to find the exact final binding pose. The diameter of the search box is the diameter of a ligand conformer generated by RDKit with an additional 10? on all 6 sides of the bounding box. Due to the averaging phenomenon of regression-based methods such as TANKBind and EquiBind, they make predictions at the mean of the distribution. If aleatoric uncertainty is present, such as in case of symmetric complexes, this leads to predicting the ligand to be at an un-physical state in the middle of the possible binding pockets as visualized in <ref type="figure" target="#fig_0">Figure 10</ref>. The <ref type="figure">Figure also</ref> illustrates how DIFFDOCK does not suffer from this issue and is able to accurately sample from the modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 PHYSICALLY PLAUSIBLE PREDICTIONS</head><p>In the scenario when epistemic uncertainty about the correct ligand conformation is present, this often results in "squashed-up" predictions of the regression-based methods as visualized in <ref type="figure">Figure 5</ref>.</p><p>If there is uncertainty about the correct conformer, the square error minimizing option is to put all atoms close to the mean.</p><p>These averaging phenomena in the presence of either aleatoric or epistemic uncertainty cause the regression-based methods to often generate steric clashes and self intersections. To investigate this quantitatively, we determine the fraction of test complexes for which the methods exhibit steric clashes. We define a ligand as exhibiting a steric clash if one of its heavy atoms is within 0.4? of a heavy receptor atom. This cutoff is used by protein quality assessment tools and in previous literature <ref type="bibr">[Ramachandran et al., 2011]</ref>. <ref type="table" target="#tab_5">Table 4</ref> shows that DIFFDOCK, as a generative model, produces fewer steric clashes than the regression-based baselines. We generally observe no unphysical predictions from DIFFDOCK unlike the self intersections that, e.g., TANKBind produces ( <ref type="figure">Figure 5</ref>) or its incorrect local structures ( <ref type="figure" target="#fig_6">Figure 6</ref>). This is also visible in the randomly chosen examples of <ref type="figure">Figure 9</ref> and can be examined in our repository, where we provide all predictions of DIFFDOCK for the test set. <ref type="figure">Figure 5</ref>: Ligand self-intersections. TANKBind (blue), EquiBind (cyan), DIFFDOCK (red), and crystal structure (green). Due to the averaging phenomenon that occurs when epistemic uncertainty is present, the regression-based deep learning models tend to produce ligands with atoms that are close together, leading to self-intersections. DIFFDOCK, as a generative model, does not suffer from this averaging phenomenon, and we never found a self-intersection in any of the investigated results of DIFFDOCK. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 FURTHER RESULTS AND METRICS</head><p>In this section, we present further evaluation metrics on the results presented in <ref type="table">Table 1</ref>. In particular, for both top-1 <ref type="table" target="#tab_6">(Table 5</ref>) and top-5 <ref type="table" target="#tab_7">(Table 6)</ref> we report: 25th, 50th and 75th percentiles, the proportion below 2? and below 5? of both ligand RMSD and centroid distance. Moreover, while Volkov et al. <ref type="bibr">[2022]</ref> advocated against artificial protein set splits and for time-based splits, for completeness, in <ref type="table">Table 7</ref> and <ref type="figure" target="#fig_7">Figure 7</ref>, we report the performances of the different methods when evaluated exclusively on the portion of the test set where the UniProt IDs of the proteins are not contained in the data that is seen by DIFFDOCK in its training and validation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 ABLATION STUDIES</head><p>Below we report the performance of our method over different hyperparameter settings. In particular, we highlight the different ways in which it is possible to control the tradeoff between runtime and accuracy in our method. These mainly are: (1) model size, (2) diffusion time, and (3) diffusion samples.</p><p>Model size. The final DIFFDOCK score model has 20.24 million parameters from its 6 convolution layers with 48 scalar and 10 vector features. In <ref type="table">Table 8</ref> we show the results for a smaller score model with 5 convolutions, 24 scalar, and 6 vector features resulting in 3.97 million parameters that can be trained on a single 48GB GPU. The confidence model used is the same for both score models. We find that scaling up the model size helped improve performance which we did as far as possible using four 48GB GPUs for training. Scaling the model size further is a promising avenue for future work.</p><p>Diffusion steps. Another hyperparameter determining the runtime of the method during inference is the number of steps we take during the reverse diffusion. Since these are applied sequentially DIFFDOCK's runtime scales approximately linearly with the number of diffusion steps. In the rest  <ref type="table">Table 7</ref>: PDBBind docking on unseen receptors. Percentage of predictions for which the RMSD to the crystal structure is below 2? and the median RMSD. "*" indicates the method run exclusively on CPU, "-" means not applicable; some cells are empty due to infrastructure constraints. of the paper, we always use 20 steps, but in <ref type="figure">Figure 8</ref> we show how the performance of the model varies with the number of steps. We note that the model reaches nearly the full performance even with just 10 steps, suggesting that the model can be sped up 2x with a small drop in accuracy.</p><p>Diffusion samples. Given a score-based model and a number of steps for the diffusion model, it remains to be determined how many independent samples N to query from the diffusion model and then feed to the confidence model. As expected the more samples the confidence model receives the more likely it is that it will find a pose that it is confident about and, therefore, the higher the performance. The runtime of DIFFDOCK on GPU scales sublinearly until the different samples fit in <ref type="table">Table 8</ref>: Model size comparison. All methods receive a small molecule and are tasked to find its binding location, orientation, and conformation. Shown is the percentage of predictions for which the RMSD to the crystal structure is below 2? and the median RMSD. Top-1 performance Top-5 performance Top-10 performance Number of reverse diffusion steps Fraction with RMSD &lt; 2? <ref type="figure">Figure 8</ref>: Ablation study on the number of reverse diffusion steps. parallel in the model (depends on the protein size and the GPU memory) and approximately linearly for larger sample sizes (however it can be easily parallelized across different GPUs). In <ref type="figure" target="#fig_2">Figure 3</ref> we show how the success rate for the top-1, top-5, and top-10 prediction change as a function of N . For example, for the top-1 prediction, the proportion of the prediction with RMSD below 2? varies between 22% of a random sample of the diffusion model (N = 1) to 38% when the confidence model is allowed to choose between 40 samples. E.4 VISUALIZATIONS <ref type="figure">Figure 9</ref>: Randomly picked examples. The predictions of TANKBind (blue), EquiBind (cyan), GNINA (magenta), DIFFDOCK (red), and crystal structure (green). Shown are the predictions once with the protein and without it below. The complexes were chosen with a random number generator from the test set. TANKBind often produces self intersections (examples at the top-right; middlemiddle; middle-right; bottom-right). DIFFDOCK and GNINA sometimes almost perfectly predict the bound structure (e.g., top-middle). The complexes in reading order are: 6p8y, 6mo8, 6pya, 6t6a, 6e30, 6hld, 6qzh, 6hhg, 6qln. <ref type="figure" target="#fig_0">Figure 10</ref>: Symmetric complexes and multiple modes. EquiBind (cyan), DIFFDOCK highest confidence sample (red), all other DIFFDOCK samples (orange), and the crystal structure (green). We see that, since it is a generative model, DIFFDOCK is able to produce multiple correct modes and to sample around them. Meanwhile, as a regression-based model, EquiBind is only able to predict a structure at the mean of the modes. The complexes are unseen during training. The PDB IDs in reading order: 6agt, 6gdy, 6ckl, 6dz3. <ref type="figure" target="#fig_0">Figure 11</ref>: Reverse Diffusion. Reverse diffusion of a randomly picked complex from the test set. Shown are DIFFDOCK highest confidence sample (red), all other DIFFDOCK samples (orange), and the crystal structure (green). Shown are the 20 steps of the reverse diffusion process (in reading order) of DIFFDOCK for the complex 6oxx. Videos of the reverse diffusion are available at https: //github.com/gcorso/DiffDock/visualizations/README.md.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of DIFFDOCK. Left: The model takes as input the separate ligand and protein structures. Center: Randomly sampled initial poses are denoised via a reverse diffusion over translational, rotational, and torsional degrees of freedom. Right:. The sampled poses are ranked by the confidence model to produce a final prediction and confidence score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>corresponds to translation vectors and T R SO(3) ? = R 3 to rotation (Euler) vectors, both of which are SE(3)-equivariant. Finally, T ? SO(2) m corresponds to scores on SE(3)-invariant quantities (torsion angles). Thus, the score model must predict two SE(3)-equivariant vectors for the ligand as a whole and an SE(3)-invariant scalar at each of the m freely rotatable bonds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 -</head><label>3</label><figDesc>left shows the proportion of RMSDs below an arbitrary threshold with DIFFDOCK exceeding previous methods for almost every possible . 9Figure 3-right plots how the model's performance changes with the number of generative samples. Unlike regression methods like EquiBind, DIFFDOCK is able to provide multiple diverse predictions of different likely poses, as highlighted in the top-5 performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Left: cumulative density histogram of the methods' RMSD. Right: DIFFDOCK's performance as a function of the number of samples from the generative model. "Perfect selection" refers to choosing the sample with the lowest RMSD. Selective accuracy. Percentage of predictions with RMSD below 2? when only making predictions for the portion of the dataset where DIFFDOCK is most confident.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>m ) of c (1) corresponding to some choice of dihedral angles at each rotatable bond. Because A tr and A rot are rigid-body motions, only A tor changes the dihedral angles; in particular, by definition we have ? (1) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3 :</head><label>3</label><figDesc>Approximate training procedure (single epoch) Input: Training pairs {(x , y)}, RDKit predictions {c} foreach c, x , y do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Chemically plausible local structures. TANKBind (blue), EquiBind (cyan), and DIFF-DOCK (red) structures for complex 6g2f. EquiBind (without their correction step) produces very unrealistic local structures and TANKBind, e.g., produces non-planar aromatic rings. DIFFDOCK's local structures are the realistic local structures of RDKit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>PDBBind docking on unseen receptors. Shown is the cumulative density histogram of the methods' RMSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Docking accuracy. DIFFDOCK significantly outperforms all previous methods(Table 1). In particular, DIFFDOCK obtains an impressive 38.2% top-1 success rate (i.e., percentage of predictions with RMSD &lt;2? 8 ) when sampling 40 poses and 35.0% when sampling just 10. This performance vastly surpasses that of state-of-the-art commercial software such as GLIDE (21.8%) and the previous state-of-the-art deep learning method TANKBind (20.4%). The use of ML-based pocket prediction in combination with search-based docking methods improves over the baseline performances, but even the best of these (P2Rank+GNINA) reaches a success rate of only 28.8%.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Med.</cell><cell>Runtime (s)</cell></row><row><cell>QVINAW</cell><cell>20.9</cell><cell>7.7</cell><cell></cell><cell></cell><cell>49*</cell></row><row><cell>GNINA</cell><cell>22.9</cell><cell>7.7</cell><cell>32.9</cell><cell>4.5</cell><cell>127</cell></row><row><cell>SMINA</cell><cell>18.7</cell><cell>7.1</cell><cell>29.3</cell><cell>4.6</cell><cell>126*</cell></row><row><cell>GLIDE</cell><cell>21.8</cell><cell>9.3</cell><cell></cell><cell></cell><cell>1405*</cell></row><row><cell>EQUIBIND</cell><cell>5.5</cell><cell>6.2</cell><cell>-</cell><cell>-</cell><cell>0.04</cell></row><row><cell>TANKBIND</cell><cell>20.4</cell><cell>4.0</cell><cell>24.5</cell><cell>3.4</cell><cell>0.7/2.5</cell></row><row><cell>P2RANK+SMINA</cell><cell>20.4</cell><cell>6.9</cell><cell>33.2</cell><cell>4.4</cell><cell>126*</cell></row><row><cell>P2RANK+GNINA</cell><cell>28.8</cell><cell>5.5</cell><cell>38.3</cell><cell>3.4</cell><cell>127</cell></row><row><cell>EQUIBIND+SMINA</cell><cell>23.2</cell><cell>6.5</cell><cell>38.6</cell><cell>3.4</cell><cell>126*</cell></row><row><cell>EQUIBIND+GNINA</cell><cell>28.8</cell><cell>4.9</cell><cell>39.1</cell><cell>3.1</cell><cell>127</cell></row><row><cell>DIFFDOCK (10)</cell><cell cols="4">35.0?1.4 3.56?0.05 40.7?1.6 2.65?0.10</cell><cell>10</cell></row><row><cell>DIFFDOCK (40)</cell><cell cols="4">38.2?1.0 3.30?0.11 44.7?1.7 2.40?0.12</cell><cell>40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>:91-102, 2017. Srinivas Ramachandran, Pradeep Kota, Feng Ding, and Nikolay V Dokholyan. Automated minimization of steric clashes in protein structures. Proteins, 79(1):261-270, January 2011. Emanuele Rodol?, Zorah L?hner, Alexander M Bronstein, Michael M Bronstein, and Justin Solomon. Functional maps representation on product manifolds. In Computer Graphics Forum, volume 38, pp. 678-689. Wiley Online Library, 2019. Kristof Sch?tt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M?ller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems, 2017. Vignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning on proteins. Advances in Neural Information Processing Systems, 34:25244-25255, 2021. Proposition 1. Let x(t) := A tor (t?, x) for some ? and where t? = (t? 1 , . . . t? m ). Then the linear and angular momentum are zero: d dtx | t=0 = 0 and i</figDesc><table><row><cell>):7946-7958,</cell></row><row><cell>Jun 2022. ISSN 0022-2623.</cell></row><row><cell>Penglei Wang, Shuangjia Zheng, Yize Jiang, Chengtao Li, Junhong Liu, Chang Wen, Atanas Pa-</cell></row><row><cell>tronov, Dahong Qian, Hongming Chen, and Yuedong Yang. Structure-aware multimodal deep</cell></row><row><cell>learning for drug-protein interaction prediction. Journal of chemical information and modeling,</cell></row><row><cell>62(5):1308-1317, 2022.</cell></row></table><note>Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Interna- tional Conference on Learning Representations, 2021. Hannes St?rk, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola. Equibind: Geometric deep learning for drug binding structure prediction. In International Conference on Machine Learning, pp. 20503-20521. PMLR, 2022. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint, 2018. Ren? Thomsen and Mikael H Christensen. Moldock: a new technique for high-accuracy molecular docking. Journal of medicinal chemistry, 49(11):3315-3321, 2006. Brian L Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif- scaffolding problem. arXiv preprint arXiv:2206.04119, 2022. Oleg Trott and Arthur J Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455-461, 2010. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ?ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa- tion processing systems, 2017. Mikhail Volkov, Joseph-Andr? Turk, Nicolas Drizard, Nicolas Martin, Brice Hoffmann, Yann Gaston-Math?, and Didier Rognan. On the frustration to predict binding affinities from protein- ligand structures with deep neural networks. Journal of Medicinal Chemistry, 65(11Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geo- metric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2021.A PROOFSA.1 PROOF OF PROPOSITION 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The hyperparameter options we searched through for DIFFDOCK's score model. This was done with small models before scaling up to a large model. The parameters shown here that impact model size (bottom half of the table) are those of the large model. The final parameters for the large DIFFDOCK model are marked in bold.</figDesc><table><row><cell>PARAMETER</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The hyperparameter options we searched through for DIFFDOCK's confidence model. The final parameters are marked in bold.</figDesc><table><row><cell>PARAMETER</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Steric clashes. Percentage of test complexes for which the predictions of the different methods exhibit steric clashes. Search-based methods never produced steric clashes.</figDesc><table><row><cell></cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>Method</cell><cell cols="2">% steric clashes % steric clashes</cell></row><row><cell>EQUIBIND</cell><cell>26</cell><cell>-</cell></row><row><cell>TANKBIND</cell><cell>6.6</cell><cell>3.6</cell></row><row><cell>DIFFDOCK (10)</cell><cell>2.8</cell><cell>0</cell></row><row><cell>DIFFDOCK (40)</cell><cell>2.2</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Top-1 PDBBind docking.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Ligand RMSD</cell><cell></cell><cell></cell><cell cols="3">Centroid Distance</cell></row><row><cell></cell><cell cols="3">Percentiles ?</cell><cell cols="2">% below threshold ?</cell><cell cols="3">Percentiles ?</cell><cell>% below thresh. ?</cell></row><row><cell>Methods</cell><cell cols="4">25th 50th 75th 5?</cell><cell cols="5">2? 25th 50th 75th 5?</cell><cell>2?</cell></row><row><cell>QVINA-W</cell><cell>2.5</cell><cell>7.7</cell><cell cols="3">23.7 40.2 20.9</cell><cell>0.9</cell><cell>3.7</cell><cell cols="2">22.9 54.6 41.0</cell></row><row><cell>GNINA</cell><cell>2.4</cell><cell>7.7</cell><cell cols="3">17.9 40.8 22.9</cell><cell>0.8</cell><cell>3.7</cell><cell cols="2">23.1 53.6 40.2</cell></row><row><cell>SMINA</cell><cell>3.1</cell><cell>7.1</cell><cell cols="3">17.9 38.0 18.7</cell><cell>1.0</cell><cell>2.6</cell><cell cols="2">16.1 59.8 41.6</cell></row><row><cell>GLIDE (c.)</cell><cell>2.6</cell><cell>9.3</cell><cell cols="3">28.1 33.6 21.8</cell><cell>0.8</cell><cell>5.6</cell><cell cols="2">26.9 48.7 36.1</cell></row><row><cell>EQUIBIND</cell><cell>3.8</cell><cell>6.2</cell><cell cols="2">10.3 39.1</cell><cell>5.5</cell><cell>1.3</cell><cell>2.6</cell><cell>7.4</cell><cell>67.5 40.0</cell></row><row><cell>TANKBIND</cell><cell>2.5</cell><cell>4.0</cell><cell>8.5</cell><cell cols="2">59.0 20.4</cell><cell>0.9</cell><cell>1.8</cell><cell>4.4</cell><cell>77.1 55.1</cell></row><row><cell>P2RANK+SMINA</cell><cell>2.9</cell><cell>6.9</cell><cell cols="3">16.0 43.0 20.4</cell><cell>0.8</cell><cell>2.6</cell><cell cols="2">14.8 60.1 44.1</cell></row><row><cell>P2RANK+GNINA</cell><cell>1.7</cell><cell>5.5</cell><cell cols="3">15.9 47.8 28.8</cell><cell>0.6</cell><cell>2.2</cell><cell cols="2">14.6 60.9 48.3</cell></row><row><cell>EQUIBIND+SMINA</cell><cell>2.4</cell><cell>6.5</cell><cell cols="3">11.2 43.6 23.2</cell><cell>0.7</cell><cell>2.1</cell><cell>7.3</cell><cell>69.3 49.2</cell></row><row><cell>EQUIBIND+GNINA</cell><cell>1.8</cell><cell>4.9</cell><cell>13</cell><cell cols="2">50.3 28.8</cell><cell>0.6</cell><cell>1.9</cell><cell>9.9</cell><cell>66.5 50.8</cell></row><row><cell>DIFFDOCK (10)</cell><cell>1.5</cell><cell>3.6</cell><cell>7.1</cell><cell cols="2">61.7 35.0</cell><cell>0.5</cell><cell>1.2</cell><cell>3.3</cell><cell>80.7 63.1</cell></row><row><cell>DIFFDOCK (40)</cell><cell>1.4</cell><cell>3.3</cell><cell>7.3</cell><cell cols="2">63.2 38.2</cell><cell>0.5</cell><cell>1.2</cell><cell>3.2</cell><cell>80.5 64.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Top-5 PDBBind docking.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Ligand RMSD</cell><cell></cell><cell></cell><cell cols="3">Centroid Distance</cell></row><row><cell></cell><cell cols="3">Percentiles ?</cell><cell cols="2">% below threshold ?</cell><cell cols="3">Percentiles ?</cell><cell>% below thresh. ?</cell></row><row><cell>Methods</cell><cell cols="4">25th 50th 75th 5?</cell><cell cols="5">2? 25th 50th 75th 5?</cell><cell>2?</cell></row><row><cell>GNINA</cell><cell>1.6</cell><cell>4.5</cell><cell cols="3">11.8 52.8 29.3</cell><cell>0.6</cell><cell>2.0</cell><cell>8.2</cell><cell>66.8 49.7</cell></row><row><cell>SMINA</cell><cell>1.7</cell><cell>4.6</cell><cell>9.7</cell><cell cols="2">53.1 29.3</cell><cell>0.6</cell><cell>1.85</cell><cell>6.2</cell><cell>72.9 50.8</cell></row><row><cell>TANKBIND</cell><cell>2.1</cell><cell>3.4</cell><cell>6.1</cell><cell cols="2">67.5 24.5</cell><cell>0.8</cell><cell>1.4</cell><cell>2.9</cell><cell>86.8 62.0</cell></row><row><cell>P2RANK+SMINA</cell><cell>1.5</cell><cell>4.4</cell><cell cols="3">14.1 54.8 33.2</cell><cell>0.6</cell><cell>1.8</cell><cell cols="2">12.3 66.2 53.4</cell></row><row><cell>P2RANK+GNINA</cell><cell>1.4</cell><cell>3.4</cell><cell cols="3">12.5 60.3 38.3</cell><cell>0.5</cell><cell>1.4</cell><cell>9.2</cell><cell>69.3 57.3</cell></row><row><cell>EQUIBIND+SMINA</cell><cell>1.3</cell><cell>3.4</cell><cell>8.1</cell><cell cols="2">60.6 38.6</cell><cell>0.5</cell><cell>1.3</cell><cell>5.1</cell><cell>74.9 58.9</cell></row><row><cell>EQUIBIND+GNINA</cell><cell>1.4</cell><cell>3.1</cell><cell>9.1</cell><cell cols="2">61.7 39.1</cell><cell>0.5</cell><cell>1.1</cell><cell>5.3</cell><cell>73.7 60.1</cell></row><row><cell>DIFFDOCK (10)</cell><cell>1.2</cell><cell>2.7</cell><cell>4.9</cell><cell cols="2">75.1 40.7</cell><cell>0.5</cell><cell>1.0</cell><cell>2.2</cell><cell>87.0 72.3</cell></row><row><cell>DIFFDOCK (40)</cell><cell>1.2</cell><cell>2.4</cell><cell>5.0</cell><cell cols="2">75.5 44.7</cell><cell>0.4</cell><cell>0.9</cell><cell>1.9</cell><cell>88.0 76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>DIFFDOCK (10) 15.7?1.2 6.1?0.4 21.8?1.2 4.2?0.1 10 DIFFDOCK (40) 20.8?2.0 6.2?0.2 28.7?1.4 3.9?0.1 40</figDesc><table><row><cell></cell><cell cols="2">Top-1 RMSD</cell><cell cols="2">Top-5 RMSD</cell><cell>Average</cell></row><row><cell>Method</cell><cell>%&lt;2</cell><cell>Med.</cell><cell>%&lt;2</cell><cell>Med.</cell><cell>Runtime (s)</cell></row><row><cell>QVINAW</cell><cell>15.3</cell><cell>10.3</cell><cell></cell><cell></cell><cell>49*</cell></row><row><cell>GNINA</cell><cell>14.0</cell><cell>13.6</cell><cell>23.0</cell><cell>7.0</cell><cell>127</cell></row><row><cell>SMINA</cell><cell>14.0</cell><cell>8.5</cell><cell>21.7</cell><cell>6.7</cell><cell>126*</cell></row><row><cell>GLIDE</cell><cell>19.6</cell><cell>18.0</cell><cell></cell><cell></cell><cell>1405*</cell></row><row><cell>EQUIBIND</cell><cell>0.7</cell><cell>9.1</cell><cell>-</cell><cell>-</cell><cell>0.04</cell></row><row><cell>TANKBIND</cell><cell>6.3</cell><cell>5.0</cell><cell>11.1</cell><cell>4.4</cell><cell>0.7/2.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Also known as diffusion probabilistic models (DPMs), denoising diffusion probabilistic models (DDPMs), diffusion models, score-based models (SBMs), or score-based generative models (SGMs).2 Not to be confused with the scoring function of traditional docking methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For example, the pLDDT confidence score of AlphaFold2<ref type="bibr" target="#b13">[Jumper et al., 2021]</ref> has had a very significant impact in many applications<ref type="bibr" target="#b26">[Necci et al., 2021;</ref> Bennett et al., 2022].4  RDKit ETKDG is a popular method for predicting the seed conformation. Although the structures may not be predicted perfectly, the errors lie largely in the torsion angles, which are resampled anyways.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Since we do not define or use the composition of elements of SO(2) m , strictly speaking, it is a product space but not a group and can be alternatively thought of as the torus T m with an origin element.6  Since we never compose elements of P, we do not need to define a group structure.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">so(3) is the tangent space of SO(3) at the identity and is the space of Euler (or rotation) vectors, which are equivalent to the axis-angle parameterization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Most commonly used evaluation metric<ref type="bibr" target="#b0">[Alhossary et al., 2015;</ref><ref type="bibr" target="#b9">Hassan et al., 2017;</ref> McNutt et al., 2021]  9  With the exception of very small &lt;1? where GLIDE performs better.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Since the parity of the = 2 spherical harmonic is even, this representation is indifferent to the choice of bond direction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We pay tribute to Octavian-Eugen Ganea (1987, dear colleague, mentor, and friend without whom this work would have never been possible.</p><p>We thank Rachel Wu, Jeremy Wohlwend, Felix Faltings, Jason Yim, Victor Quach, Saro Passaro, Patrick Walters, Michael Heinzinger, Mario Geiger, Michael John Arcidiacono, Noah Getz, and John Yang for valuable feedback and insightful discussions. We thank Wei Lu for his help with running TANKBind. This work was supported by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, the Abdul Latif Jameel Clinic for Machine Learning in Health, the DTRA Discovery of Medical Countermeasures Against New and Emerging (DO-MANE) threats program, the DARPA Accelerated Molecular Discovery program and the Sanofi Computational Antibody Design grant. Bowen Jing acknowledges the support from the Department of Energy Computational Science Graduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast, accurate, and reliable molecular docking with QuickVina 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Alhossary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanus</forename><forename type="middle">Daniel</forename><surname>Handoko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee-Keong</forename><surname>Kwoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2214" to="2216" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving de novo protein binder design with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Coventry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inna</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aza</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dionne</forename><surname>Vafeados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Po</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justas</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyung</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Announcing the worldwide Protein Data Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Henrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Struct Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">980</biblScope>
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer cpi: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4406" to="4414" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Riemannian score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doucet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02763</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09453</idno>
		<title level="m">Euclidean neural networks</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostiantyn</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Lapchevskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tyszkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Batzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes</forename><surname>Uhrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuri</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Sanborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Euclidean neural networks: e3nn, 2020</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas A Halgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leah</forename><forename type="middle">L</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Frye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay L</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein-ligand blind docking using quickvina-w with inter-process spatio-temporal integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nafisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><forename type="middle">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Alhossary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee-Keong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15451</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Equivariant diffusion for molecule generation in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">Garcia</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="8867" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactiongraphnet: A novel and efficient deep graph representation learning framework for accurate protein-ligand interaction predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jike</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ercheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="18209" to="18232" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Torsional diffusion for molecular conformer generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Bowen Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01729</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Augustin??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David Ryan Koes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos J</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camacho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1893" to="1904" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">P2rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radoslav</forename><surname>Kriv?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoksza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models on so (3) for rotational alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><forename type="middle">T</forename><surname>Schmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">G</forename><surname>Degiacomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Willcocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monn: a multiobjective neural network for predicting compound-protein interactions and affinities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangping</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="308" to="322" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models of protein sequences at the scale of evolution enable accurate structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel-Zarandi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<pubPlace>Tom Sercu, Sal Candido, and Alexander Rives</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Forging the basis for developing protein-ligand interaction scoring functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renxiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounts of Chemical Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tankbind: Trigonometry-aware neural networks for drug-protein binding structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gnina 1.0: molecular docking with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishal</forename><surname>Francoeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohide</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Meli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Ragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Ryan</forename><surname>Sunseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">spyrmsd: symmetry-corrected rmsd calculations in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><surname>Meli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Biggin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ehecatl Antonio del Rio-Chanona, and J?rg Kurt Wegner. A geometric deep learning approach to predict binding conformations of bioactive molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>M?ndez-Lucio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mazen</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1033" to="1039" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pignet: a physics-informed deep learning model toward generalized drug-target interaction predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhyun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Zhung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soojung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo Youn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3661" to="3673" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Critical assessment of protein intrinsic disorder prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Necci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio Ce</forename><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tosatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="472" to="481" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Normal distribution on the rotation group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tatjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savyolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Textures and Microstructures</title>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
