<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
							<email>umberto.michieli@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
							<email>zanuttigh@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks suffer from the major limitation of catastrophic forgetting old tasks when learning new ones. In this paper we focus on class incremental continual learning in semantic segmentation, where new categories are made available over time while previous training data is not retained. The proposed continual learning scheme shapes the latent space to reduce forgetting whilst improving the recognition of novel classes. Our framework is driven by three novel components which we also combine on top of existing techniques effortlessly. First, prototypes matching enforces latent space consistency on old classes, constraining the encoder to produce similar latent representation for previously seen classes in the subsequent steps. Second, features sparsification allows to make room in the latent space to accommodate novel classes. Finally, contrastive learning is employed to cluster features according to their semantics while tearing apart those of different classes. Extensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the effectiveness of our approach, significantly outperforming state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a challenging computer vision problem with many real-world applications ranging from robot sensing, to autonomous driving, video surveillance, virtual reality, and many others. For most applications, continuously improving the set of classes to be distinguished is a fundamental requirement. Current state-of-the-art semantic segmentation approaches are typically based on autoencoder structures and on fully convolutional models <ref type="bibr" target="#b37">[38]</ref> that are trained in a single-shot requiring all the dataset to be available at once. Indeed, existing architectures are not designed to incrementally update their inner classification model to accommodate new categories. This issue is wellknown for deep neural networks and it is called catastrophic forgetting <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, as deep architectures fail to update current batch prototype cumulative prototype attractive force (clustering) attractive force (proto. distillation) repulsive force unconstrained latent space span <ref type="figure">Figure 1</ref>. Our continual learning scheme is driven by 3 main components: latent contrastive learning, prototypes matching and features sparsity. Latent representations of old classes are preserved via prototypes matching and clustering, whilst also making room for accommodating new classes via sparsity and repulsive force of contrastive learning. The decoder preserves previous knowledge via output-level distillation. In the figure, bike and cars represent old classes and leave more space to new classes (the dog) thanks to the novel constraints (green dotted ovals versus gray-filled ovals). their parameters for learning new categories while preserving good performance on the old ones.</p><p>Continual learning has been widely studied in image classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref> and object detection <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b33">34]</ref>, while has been tackled only recently in the semantic segmentation field <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>. In this paper, we investigate classincremental continual learning in semantic segmentation. Differently from the majority of previous approaches both in image classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b2">3]</ref> and semantic segmentation <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>, we do not mainly or solely rely on output-level knowledge distillation. In this work, we focus on latent space organization which has been only marginally investigated in the current literature, and we empirically prove it to be complementary to other existing techniques. The main idea is depicted in <ref type="figure">Fig. 1</ref>, where some of the latent space constraints are introduced. First, a prototype matching is devised to enforce features extraction consistency on old classes between the cumulative prototype computed using all previous samples and the current prototype (i.e., the prototype computed on the current batch only). In other words, we force the encoder to produce similar latent representations for previously seen classes in the new steps. Second, a features sparsification constraint makes room in the latent space to accommodate novel classes. To further regularize the latent space, we introduce an attractionrepulsion rule similar in spirit to the recent advancements in contrastive learning. Finally, to enforce the decoder to preserve discriminability on previous categories during classification, we employ a targeted output-level distillation.</p><p>Although continual semantic segmentation has only been faced recently, it already comes with different experimental protocols depending on how the incremental data are considered (see Section 3.1): namely, sequential (new images are labeled with both new and old classes), disjoint (new images are labeled with only new classes, old classes are assigned to the background) and overlapped (new images are labeled with only new classes, images are repeated across training steps with different semantic maps associated to them). In this paper we devise a common framework which allows to tackle all these scenarios and can be applied in combination with previous techniques, which has never been attempted before. We evaluate on standard semantic segmentation datasets, like Pascal VOC2012 <ref type="bibr" target="#b15">[16]</ref> and ADE20K <ref type="bibr">[76]</ref>, in many scenarios.</p><p>Summing up, the main contributions of this work are: 1) We investigate class-incremental learning in semantic segmentation, providing a common framework for different experimental protocols. 2) We explore the latent space organization and we propose complementary techniques with respect to the existing ones. 3) We propose novel knowledge preservation techniques based on prototypes matching, contrastive learning and features sparsity. 4) We benchmark our approach on standard semantic segmentation datasets outperforming state-of-the-art continual learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Continual Learning. Deep learning models are prone to catastrophic forgetting <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48]</ref>, i.e., training a model with new information interferes with previously learned knowledge and typically greatly degrades performance. This phenomenon has been widely studied in image classification task and most of the current techniques fall into the following categories <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref>: regularization approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">73,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>, dynamic architectures [69, <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b34">35]</ref>, parameter isolation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref> and replay-based methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b25">26]</ref>. Regularization-based approaches are by far the most widely employed and mainly come in two flavours, i.e., penalty computing and knowledge distillation <ref type="bibr" target="#b24">[25]</ref>. Penalty computing approaches [73, <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref> protect important weights inside the models to prevent forgetting. Knowledge distillation <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13]</ref> relies on a teacher (old) model transferring or remembering knowledge related to previous tasks to a student model which is trained to learn also additional tasks. Parameter isolation approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref> reserve a subset of weights for a specific task to avoid degradation. Dynamic architectures <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b34">35]</ref> grow new branches for new tasks. Replay-based models exploit stored <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref> or generated <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55]</ref> examples during the learning process of new tasks.</p><p>Continual Semantic Segmentation. Nowadays, deep learning architectures have achieved outstanding results in semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. Current approaches are based on fully convolutional models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">75,</ref><ref type="bibr">72]</ref> and exploit various techniques to cope with multi-scale and spatial dependency. All these approaches, however, require training data and segmentation maps to be available at once (i.e., joint setting) and they experience catastrophic forgetting if new tasks (e.g., new classes to learn) are made available sequentially <ref type="bibr" target="#b41">[42]</ref>. Hence, it emerged the need for continual approaches specifically targeted to solve the semantic segmentation task <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4]</ref>. Earlier works focus on the continual semantic segmentation problem in specific scenarios, e.g., in medical imaging <ref type="bibr" target="#b46">[47]</ref> or remote sensing <ref type="bibr" target="#b57">[58]</ref>, extending standard image-level classification methods. More recently, standard semantic segmentation datasets and targeted methods have been proposed. In <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> an exploration on knowledge distillation techniques is proposed to alleviate forgetting: the authors designed output-level and features-level distillation losses coupled with freezing the encoder's weights. Klingner et al. <ref type="bibr" target="#b32">[33]</ref> extend previous work not requiring old labels during the incremental steps and proposing class importance weighting to emphasize gradients on difficult classes. Cermelli et al. <ref type="bibr" target="#b3">[4]</ref> study the distribution shift of the background class when it incorporates previous and/or future classes (disjoint and overlapped protocols, respectively). Background shift is addressed via unbiased versions of cross entropy and output-level knowledge distillation losses together with an unbiased weight initialization rule for the classifier. Nevertheless, previous works neglect accurate investigation of the latent space in continual learning.</p><p>Latent Space Organization. The analysis of the latent space organization is becoming crucial towards understanding and improvement of classification models <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b48">49]</ref>. Recently, some attention has been devoted to latent regularization in continual image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref>. Besides this, one of the emerging paradigms is constrastive learning applied to visual representations. Dating back to <ref type="bibr" target="#b21">[22]</ref>, these approaches learn representations by contrasting positive against negative pairs and have been recently re-discovered for deep learning. Many works use a memory bank to store the instance class representation vector <ref type="bibr" target="#b66">[67,</ref><ref type="bibr">77,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9]</ref>, while some others explore the usage of in-batch negative samples instead <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">71,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. The contrastive learning objective proposed in this work moves from opposition of positive and negative pairs and also recalls features clustering (if features belong to the same class) and separation (if features belong to different classes), which has been recently applied to adapt semantic segmentation models across domains <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b60">61]</ref>. Prototypes-based regularizing terms gained a great interest and, in particular, have been largely used in the literature of few-shot learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b59">60]</ref>, to learn prototypical representations of each category, and domain adaptation, to enforce orthogonality <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref> or centroid matching <ref type="bibr">[70,</ref><ref type="bibr" target="#b11">12]</ref>. Finally, to minimize the interference among features we drive them to be channel-wise sparse. Only limited attention has been given on sparsity for deep learning architectures <ref type="bibr" target="#b1">[2]</ref>; however, some prior techniques exist for domain adaptation on linear models exploiting sparse codes on a shared dictionary between the domains <ref type="bibr" target="#b53">[54,</ref><ref type="bibr">74]</ref>.</p><p>Our work is the first combining together contrastive learning, sparsity and prototypes matching to regularize latent space for segmenting new categories over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition and Setups</head><p>Before presenting the proposed strategies, we first introduce the semantic segmentation task, which assigns a class to each pixel in an image. We denote the input image space with X ? R H?W ?3 with spatial dimensions H and W , the set of classes (or categories) with C = {c i } C?1 i=0 and the output space with Y ? C H?W (i.e., the segmentation map). Given a training set T = {(x n , y n )} N n=1 , where (x n , y n ) ? X ? Y, we aim at finding a map M from the input space to a pixel-wise class probability vector M : X ? R H?W ?C . Then, the output segmentation mask is computed as? n = arg max c?C M (x n )[h, w, c], where h = 1, .., H, w = 1, ..., W and M (x n )[h, w, c] is the probability for class c in pixel (h, w). Nowadays, M is typically some auto-encoder model made by an encoder E and a decoder D (i.e., M = E ? D). We call F n = E(x n ) the feature map of x n , and y * n the downsampled segmentation map matching the spatial dimensions of F n .</p><p>In the standard supervised setting it is assumed that the training set T is available at once and the model is learned in one shot. In the continual learning scenario, instead, training is achieved over multiple iterations each carrying a novel category to learn and a subset of the training data. More formally, at each learning step k the previous label set C k?1 is expanded with a set of novel classes S k forming a new label set C k = C k?1 ? S k . Additionally, a new training subset T k ? X ? C k is made available and used to update the previous model into a new model M k .</p><p>Step k = 0 consists of a standard supervised training performed with only a subset of training data and classes. As in the standard incremental class learning scenario, we assume the different sets of new classes to be disjoint with the exception of the peculiar background class c 0 , i.e., S i ? S j = {c 0 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Protocols</head><p>Despite being quite a recent field, continual learning in semantic segmentation already comes in different flavors. Sequential: this setup has been proposed in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Each learning step contains a unique set of images, whose pixels belong to classes seen either in the current or in the previous learning steps. At each step, labels for pixels of both old and novel classes are present. Disjoint: this setup has been proposed in <ref type="bibr" target="#b3">[4]</ref>. At each learning step, the unique set of images is identical to the sequential setup. The difference with respect to the sequential setup lies in the set of labels. At each step, only labels for pixels of novel classes are present, while the old ones are labeled as background in the ground truth. Overlapped: this setup moves from the work of <ref type="bibr" target="#b55">[56]</ref> for object detection and has been adapted to semantic segmentation in <ref type="bibr" target="#b3">[4]</ref>. Each training step contains all the images that have at least one pixel of a novel class, with only the novel classes annotated while the rest is set to background. Differently from the other settings, here images may contain pixels of classes that will be learned in future learning steps, but they are labeled as background in the current step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we provide a detailed description of the core modules of the proposed method. Our approach leverages a contrastive learning objective applied over the feature representations, with novel prototypes matching and sparsity constraints. Specifically, features repulsion and attraction based on the semantic classes are enforced by grouping together features of the same class, while simultaneously pushing away those of different categories. We further regularize the distribution of latent representations by the joint application of prototypes matching and sparsity. While prototypes matching seeks for an invariant representation of the features extracted for the old classes, the sparsity objective encourages a lower volume of active feature channels from latent representations (i.e., it concentrates the energy of features on few dimensions) to free up space for new classes.</p><p>An overall scheme of our approach is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>: the training objective is given by the combination of a crossentropy loss (L ce ) with the proposed modules. L ce is the usual cross-entropy loss for all the classes except for the background. The ground truth of the background, indeed, is not directly compared with its probabilities, but with the probability of having either an old class or the background in the current model <ref type="bibr" target="#b3">[4]</ref>. Formally, at step k the background probabilities M (x n )[h, w, c 0 ] are replaced by</p><formula xml:id="formula_0">c?C k?1 M (x n )[h, w, c].</formula><p>The rationale behind this is that the background class could incorporate statistics of previous classes in both the disjoint and overlapped protocols.</p><p>The other components are a prototypes matching target  (L pm ), a contrastive learning objective (L cl ) and a sparsity constraint (L sp ), which will be detailed in the following sections. The training objective is then computed as:</p><formula xml:id="formula_1">L tot = L ce + ? pm ? L pm + ? cl ? L cl + ? sp ? L sp<label>(1)</label></formula><p>where the ? parameters balance the multiple losses and have been tuned using a validation set (see <ref type="bibr">Section 5)</ref>. Our aim is to seek for disentangled latent representations characterized by semantic-driven regularization and to show that this approach can achieve comparable or superior results with respect to standard regularization methods (e.g., output-level knowledge distillation). We further integrate the proposed framework with an output-level knowledge distillation objective <ref type="bibr" target="#b42">[43]</ref> and we show that its effect is highly not overlapping, achieving increased accuracy. The training objective comprising an unbiased output-level distillation module is defined as:</p><formula xml:id="formula_2">L tot = L tot + ? kd ? L kd<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Prototypes Matching</head><p>Prototypes (i.e., class-centroids) are vectors that are representative of each category that appears in the dataset. During training, the features extracted by the encoder contribute in forming the latent prototypical representation of each class. To preserve the geometrical structure of the features of old classes we apply prototypes matching. Current prototypesp c (i.e., computed on the current batch of images) are forced to be placed close to their representation learned from the previous steps p c . We use the Frobenius norm || ? || F as metric distance <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b62">63]</ref>. More formally:</p><formula xml:id="formula_3">L pm = 1 |C k?1 | ||p c ?p c || F c ? C k?1<label>(3)</label></formula><p>The prototypes are computed in-place with a running average updated at each training step with supervision. At training step t with batch B of B images, the prototypes are updated for a generic class c as:</p><formula xml:id="formula_4">p c [t] = 1 Bt B(t?1)p c [t?1]+ xn?B fi?Fn f i 1[y * i = c] |1 [y * n = c] |<label>(4)</label></formula><p>initialized to p c [0] = 0 ?c. f i ? F n is a generic feature vector and y * i the corresponding pixel in y * n , 1 [y * n = c] indicates the pixels in y * n associated to c and | ? | denotes cardinality. We update the prototypes only when we have ground truth labels for that class to avoid incorporating the mutable statistics of the background class: we exclude the background from the incremental steps in the disjoint protocol (as it could contain old classes) and in the overlapped scenario (as it could contain old and future classes).</p><p>For the current batch B of an incremental training stage, the current (or in-batch) prototypesp c [t] are computed as:</p><formula xml:id="formula_5">p c [t] = 1 B xn?B ? ? ? f i ?Fn fi1[y * i =c] |1[y * n =c]| if sequential f i ?Fn fi1[? * i =c] |1[? * n =c]| otherwise<label>(5)</label></formula><p>where? * n (with pixels? * i ) is a pseudo-labeled segmentation map computed from the ground truth data by replacing the background region with the prediction from the previous model, since in the disjoint and overlapped protocols old classes are labeled as background. The difference between (4) and (5) lies in the usage of pseudo-labels: we use them in (5) to compute prototypes for old classes in the current batch since we may not have any label for them, but we avoid to use them in (4), since there is no need to update prototypes computed using the ground truth at previous steps with data from less reliable pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Contrastive Learning</head><p>The second component is similar to recent contrastive learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b58">59]</ref> and clustering <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> approaches to constraint the latent space organization. The underlying idea is to structure the latent space in order to have features of the same category clustered near their prototype and at the same time to force prototypes to be far one from the other. We argue that this organization helps also in continual learning to mitigate forgetting and to facilitate the addition of novel classes, as features are clustered and there is more separation between the clusters. In formal terms, the constraint is defined by a loss L cl made of an attractive term L a cl and a repulsive term L r cl , as follows:</p><formula xml:id="formula_6">L a cl = 1 |c j ? y * n | cj ?y * n fi?Fn || f i ?p cj 1[y * i = c j ]|| F (6) L r cl = 1 |c j ? y * n | cj ?y * n c k ?y * n c k =cj 1 ||p cj ?p c k || F (7)</formula><p>The objective is composed of two terms: L a cl measures how close features are from their respective centroids and L r cl how spaced out prototypes corresponding to different semantic classes are. Hence, the effect provided by the loss minimization is twofold: firstly, feature vectors from the same class are tightened around class feature centroids; secondly, features from separate classes are subject to a repulsive force applied to feature centroids, moving them apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Features Sparsity</head><p>To enforce the regularizing effect brought by contrastive learning, we introduce a further feature-wise objective on the latent space. We propose a sparsity loss to decrease the number of active feature channels of latent vectors. First, to give the same importance to all classes, we normalize each feature vector with respect to the maximum value any of the feature channels for that particular class assumes, i.e.:</p><formula xml:id="formula_7">f i = f i maxg j,l ?gj y * j =y * i g j,l f i , g j ? F n<label>(8)</label></formula><p>We design the sparsity constraint as the ratio between the sum of exponentials and the linear sum of the elements of each feature vector:</p><formula xml:id="formula_8">L sp = 1 |f i ? F n | fi?Fn j exp f i,j jf i,j<label>(9)</label></formula><p>While the contrastive learning objective forces features to lie within tight semantically-consistent well-distanced clusters, the sparsity constraint aims at narrowing down the set of mid-range spurious activations with the aim of letting room for the representation of upcoming classes. In other words, by constraining features of the same classes to be tightly clustered and to be spaced apart from features of other classes and sparse, we can preserve geometrical space (few active channels) and expressiveness (division in well-separated clusters) for the latent representation of future classes. Empirically, we found entropy-based minimization methods in the latent space <ref type="bibr" target="#b61">[62]</ref> to be less reliable for our task. In the Supplementary Material we show some empirical insights on the regularization effects achieved by this constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Output-Level Knowledge Distillation</head><p>The last component of our work is an output-level knowledge distillation which we show to be complementary to the previously introduced strategies. Indeed, we add knowledge distillation on top of all the other components to transfer knowledge from the old model's classifier to the current one. While previous constraints regularize the latent space achieving simultaneously an invariant features extraction with respect to previous steps and an easier addition of novel categories, output-level knowledge distillation directly acts on the classifier, to preserve its discriminative ability regarding old classes. In particular, we start from the preliminary considerations of <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> and we employ the unbiased distillation proposed in <ref type="bibr" target="#b3">[4]</ref> as natural extension to the case in which the background may contain other categories. In this case we avoid to re-normalize the probabilities from the previous step and, instead, we compare the background probability from the previous step with the probability of having either a new class or the background (this accounts for the fact that the background in the previous steps may include samples of the new classes, see <ref type="bibr" target="#b3">[4]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Procedure</head><p>To train and benchmark our approach we resort to two publicly available datasets following <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4]</ref>. The Pascal VOC 2012 <ref type="bibr" target="#b15">[16]</ref> contains 10582 images in the training split and 1449 in the validation split (that we used for testing, as done by all competing works being the test set not publicly available). Each pixel of each image is assigned to one semantic label chosen among 21 different classes (20 plus the background). The ADE20K [76] is a large-scale dataset of 22210 images, 2000 of which form the validation split. The typical benchmark defined in [76] includes 150 classes, representing both stuff (e.g., sky, building) and object classes (e.g., bottle, chair), differently from VOC 2012.</p><p>The proposed strategy is agnostic to the backbone architecture. For the experimental evaluation of all the compared methods we use a standard Deeplab-v3+ <ref type="bibr" target="#b7">[8]</ref> architecture with ResNet-101 <ref type="bibr" target="#b23">[24]</ref> as backbone (differently from <ref type="bibr" target="#b3">[4]</ref> for wider reproducibility) with output stride of 16. The backbone has been initialized using a pre-trained model on Im-ageNet <ref type="bibr" target="#b10">[11]</ref> (see the Supplementary Material for a detailed discussion of the impact of different pre-training strategies). We optimize the network weights following <ref type="bibr" target="#b6">[7]</ref> with SGD and with same learning rate policy, momentum and weight decay. The first learning step involves an initial learning rate of 10 ?2 , which is decreased to 10 ?3 for the following steps as done in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b3">4]</ref>. The learning rate is decreased with a polynomial decay rule with power 0.9. In each learning step we train the models with a batch size of 8 for 30 epochs for Pascal VOC 2012 and a batch size of 4 for 60 epochs for ADE20K. Following <ref type="bibr" target="#b6">[7]</ref>, we crop the images to 512 ? 512 during both training and validation and we apply the same data augmentation (i.e., random scaling the input images of a factor from 0.5 to 2.0 and random left-right flipping during training). In order to set the hyper-parameters of each method, we follow the same continual learning protocol of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>, i.e, we used 20% of the training set as validation and we report the results on the original validation set of the datasets. We use Pytorch to develop and train all the models on a NVIDIA 2080 Ti GPU. The code is available at: https://lttm.dei.unipd.it/paper_data/SDR/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>We evaluate the performance of our method (denoted in the tables with SDR, i.e., Sparse and Disentangled Representations) against some state-of-the-art continual learning frameworks. We report as a lower limit the performance of the na?ve fine-tuning approach (FT), which consists in training the model on the newly available training data with no additional provisions, while the upper limit is given by the offline single-shot training (offline) on the whole dataset T and on all the classes at once. Then, we compare with 3 recent continual semantic segmentation schemes: ILT <ref type="bibr" target="#b41">[42]</ref>, which combines latent and output level knowledge distillation, CIL <ref type="bibr" target="#b32">[33]</ref>, which adds class importance weighting to output-level knowledge distillation, and MiB <ref type="bibr" target="#b3">[4]</ref>, which deals with the background distribution shift and proposes an unbiased weight initialization rule. We also report the results on LwF <ref type="bibr" target="#b35">[36]</ref> (together with its single-headed version LwF-MC <ref type="bibr" target="#b50">[51]</ref>), that according to <ref type="bibr" target="#b3">[4]</ref> is the best performing continual image classification algorithm when adapted to semantic segmentation. For a fair comparison, all the methods have been re-trained with a standard Deeplab-v3+ <ref type="bibr" target="#b7">[8]</ref> architecture with ResNet-101 <ref type="bibr" target="#b23">[24]</ref> as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Pascal VOC2012</head><p>Following previous works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4]</ref>, we design three main experiments adding one class (19-1), five classes at once  and five classes sequentially (15-1) added in alphabetical order. In <ref type="table" target="#tab_1">Table 1</ref> we report comprehensive results on the three experimental protocols defined in Section 3.1. Results are averaged for mIoU of classes in the base step (old), for classes in the incremental steps (new) and for all classes, and are reported at the end of all the incremen-tal steps. For <ref type="bibr" target="#b3">[4]</ref> we also report the original results in their paper (denoted with MiB ?), that uses a different backbone (thus different pre-trained model) and batch size.</p><p>We can appreciate forgetting of previous classes and intransigence in learning new ones even when adding as little as one class (the tv/monitor class is added) in the scenario 19-1. FT always leads to the worst mIoU in terms of old, new and all classes. Incremental methods designed for semantic segmentation allow for a stable improvement across the experimental protocols, in particular MiB, that is specifically targeted to solve the disjoint and the overlapped scenarios, while CIL and ILT encounter difficulties in the overlapped scenario. Also LwF allows for a good improvement while its single-headed version has lower performance in this scenario. Our method (SDR) significantly outperforms all the competitors in the disjoint and overlapped scenarios (with a gap of more than 3% against the best competing approach in the disjoint setup), while in the sequential setup the gap is smaller. Further adding on top of our method the MiB framework (i.e., unbiased cross entropy, knowledge distillation and classifier initialization), which we regard as the current state-of-the-art approach for class incremental semantic segmentation, the results increase on all the scenarios, showing that proposed techniques are complementary with respect to previous schemes.</p><p>When moving to the addition of 5 classes at once (i.e., potted plant, sheep, sofa, train, tv/monitor) we immediately notice an overall increased drop of performance of all compared methods, especially in disjoint and overlapped protocols, due to the increased domain shift occurring when adding more classes at once with very variable content. In this and in the following scenario, indeed, we are adding to the model classes belonging to different macroscopic groups, according to <ref type="bibr" target="#b15">[16]</ref>, which are responsible for a variegate distribution: three indoor classes (potted plant, sofa and tv/monitor), one animal class (sheep) and one vehicle class (train). All compared methods obtain a relevant improvement with respect to FT but are always surpassed by SDR, which in particular outrun the best competing method (MiB) by more than 20% in the disjoint scenario.</p><p>In the final scenario we add the last 5 classes sequentially in 5 consecutive learning steps. This approach leads to the largest accuracy drop being the model exposed to a reiterated addition of single classes, which are also coming from different semantic contexts. In the sequential scenario LwF and MiB (which is designed for background shift) show poor final accuracy. ILT and CIL, instead, show results comparable to our approach. In the disjoint and in the overlapped scenarios all the methods heavily suffer from the semantic shift undergone by the background class: LwF (both versions) and ILT have poor performance in these scenarios, while CIL is able to achieve some improvement only in the disjoint scenario. The best competitor is again MiB that  is able to obtain a mIoU of 33% and 36.7% in the disjoint and overlapped scenarios respectively. Our approach (SDR) is able to significantly increase the final mIoU in both scenarios to 48.1% and 39.2%; it achieves a remarkable result especially in the disjoint scenario thanks to the novel features-level constraints which help the model to maintain accuracy on old classes while learning new ones.</p><p>Visual results for each scenario in the disjoint protocol are shown in the first three rows of <ref type="figure" target="#fig_1">Fig. 3</ref>, where our method is compared against all the competitors consistently obtain-ing better segmentation maps. For example, our method does not mislead the bus windows with tv/monitor instances in row 1 differently from several competitors (which are more biased toward predicting the novel class), and it is the only one able to distinguish the bus in row 2 and the car in row 3 from the similar-looking train class. Here, train is added in the incremental step causing catastrophic forgetting of similar classes in competing approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">ADE20K</head><p>Following <ref type="bibr" target="#b3">[4]</ref> we split the dataset into disjoint image sets with the only constraint that a minimum number of images (i.e., 50) have labeled pixels on C k . Classes are ordered according to <ref type="bibr">[76]</ref>. In this comparison we report the same competing methods of Section 6.1. The scenarios we consider are the addition of the last 50 classes at once (100-50), of the last 50 classes 10 at a time (100-10) and of the last 100 classes in 2 steps of 50 classes each . The results are summarized in <ref type="table">Table 2</ref>, where we can appreciate that the proposed approach outperforms competitors in every scenario, in particular with a larger gain when multiple incremental steps are performed. When adding 50 classes at a time LwF-MC and CIL achieve low results and are outperformed by the other competitors (i.e., LwF, ILT and MiB), which in turn are always consistently surpassed by our framework. In the scenario 100-10, instead, all competing approaches (except for MiB) are unable to provide useful outputs leading to extremely low results, while our method stands out from competitors outperforming also MiB by a good margin. Visual results for each scenario are reported as last rows of <ref type="figure" target="#fig_1">Fig. 3</ref>, which confirm our considerations showing how SDR produces less noisy predictions and does not overestimate the background as some competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>To evaluate the effect of each component, we report an ablation analysis in <ref type="table">Table 3</ref> on the Pascal dataset in the challenging 15-1 scenario. As already noticed, FT leads to a We observe that also the contrastive loss brings a significant contribution if applied alone improving the mIoU of 13.5%. Introducing standard output-level knowledge distillation on top increases the accuracy on old classes mainly, and its unbiased version prevents forgetting even more accounting for the background shift across the incremental learning steps. Finally, we show that two of the proposed approaches (namely, sparsity and contrastive learning) may be beneficial also for the more general case of standard (i.e., non incremental) semantic segmentation. Hence, we conduct some additional experiments on Pascal VOC2012 and ADE20K, reported in <ref type="table" target="#tab_3">Table 4</ref>, showing the clear benefit of the two components in this setup. On both datasets the outcome is consistent, gaining 0.9% and 1% respectively, even starting from an architecture (i.e., Deeplab-v3+) which is already state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we presented some latent representation shaping techniques to prevent forgetting in continual semantic segmentation. In particular, the proposed constraints on the latent space regularize the learning process reducing forgetting whilst simultaneously improving the recognition of novel classes. A prototypes matching constraint enforces latent space consistency on old classes, a features sparsification objective reduces the number of active channels limiting cross-talk between features of different classes, and contrastive learning clusters features according to their semantic while tearing apart those of different classes. Our evaluation shows the effectiveness of the proposed techniques, which can also be seamlessly applied in combination of previous methods (e.g., knowledge distillation). Future research will exploit the proposed techniques in different tasks, such as standard semantic segmentation and classincremental open-set domain adaptation, and explore the combination of our approach with output-level techniques. In this document we present some additional material to better motivate our method and we conduct some supplementary experiments. More in detail, we start by discussing some of the design choices that led to the models of the losses and constraints presented in the main paper in Section 8. Then, Section 9 shows some additional ablation studies. Finally, many additional qualitative and quantitative results for both the Pascal VOC2012 and the ADE20K datasets are presented in Sections 10, 11 and 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Design Choices</head><p>In this section we present some additional discussion and results motivating the design choices behind the various modules exploited in our work. Prototypes Matching enforces latent space consistency on old classes, forcing the encoder to produce similar latent representation for previously seen classes in the subsequent steps. The target is achieved by considering the Euclidean distance in the latent space (see Section 4.1 of the paper). Although different distance metrics could have been used in principle (e.g., cosine distance <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45]</ref>) we found that a simple Euclidean distance was easier to understand and very computationally efficient results similar to more complex schemes. Contrastive Learning aims at clustering features according to their semantics while tearing apart those of different classes (see Section 4.2 of the paper): we implement it as an attractive force between latent representations with their prototypical representation, against a repulsive one between prototypes of different semantic categories. This attractionrepulsion rule is enforced again using an Euclidean distance metric. Knowledge Distillation is employed to constraint the decoder to preserve previous knowledge at the output-level and it is implemented as a standard cross-entropy on the output softmax probabilities between old model and current model predictions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref> (see Section 4.4 of the paper). Sparsity: We think that the most peculiar constraint is represented by the sparsity objective. However, the underlying concept is simple: applying some latent-level sparsification we allow the model to retain enough discriminative power to accommodate the upcoming representations of novel classes without cross-talk with previous ones (see <ref type="bibr">Section 4.3)</ref>. Here, a wide range of possibilities could be considered to address the aforementioned task and one may wonder why the sparsity constraint was designed as it is presented in the main paper. In this document, we present an empirically-driven ablation study to understand the behavior of our constraints. Common sparsity losses are the L0 or L1 norms of feature vectors; however, we show that they achieve lower accuracy. In this work, we define the sparsity objective as the ratio between a stretching function (i.e., the sum of exponentials) and a linear function (i.e., the sum) applied to the feature vectors which were previously normalized with respect to the maximum value that is assumed by any of the feature channels for that particular class. In some extreme cases, the model of Eq. (9) could lead to degenerate solutions, however we argue that these do not happen in practice on a model learning compact representations. We checked to avoid the zero division in the practical implementation, while the all-ones case is degenerate in the sense that energy cannot be re-distributed in any way since all channels are already onset to the maximum value and, furthermore, this configuration would not be informative for the decoder.</p><p>First, we observe that the normalization with respect to the class-wise maximum value is an important step of the algorithm, as it gives the same importance to each class. We believe that such normalization is fair (sometimes, features of few particular classes may just be on average more active than features of other classes); however, we can think of getting rid of it and normalizing with other strategies, e.g., with respect to:</p><p>? the maximum value for each feature (norm max);</p><p>? the overall maximum value (norm max overall);</p><p>? the L2 norm of each feature (norm L2).</p><p>In such cases, Eq. (8) would become respectively:</p><formula xml:id="formula_9">f i = f i max fi,j ?fi f i,j f i ? F n (S1) f i = f i max g j,l ?gj g j,l f i , g j ? F n (S2) f i = f i ||f i || 2 f i ? F n (S3)</formula><p>Furthermore, in principle any stretching function could be applied in spite of the sum of exponentials over the linear sum. For instance, the sum of squares (power 2) or sum of the cubic powers (power 3) could be used as stretching functions: i.e., formulating Eq. (9) respectively as:</p><formula xml:id="formula_10">L sp = 1 |f i ? F n | fi?Fn jf 2 i,j jf i,j (S4) L sp = 1 |f i ? F n | fi?Fn jf 3 i,j jf i,j .</formula><p>(S5) Finally, following the success of recent works exploiting entropy minimization <ref type="bibr" target="#b61">[62]</ref> techniques, an alternative strategy could be to minimize the entropy of the latent representations opportunely preceded by L1 or softmax normalization of each feature vector in order to obtain a probability distribution over the channels. More formally: <ref type="table" target="#tab_1">Table S1</ref> shows the performance of the aforementioned approaches in the 19-1 and 15-1 disjoint scenarios on Pascal VOC2012. Different normalization rules bring to consistently lower results, proving the efficacy of using class guidance during normalization. Also, different stretching functions are found to be less adequate for our purpose reducing the final mIoU of about 2% to 4%. Finally, entropy minimization techniques obtain competitive and comparable results in the 15 ? 1 scenario, while they experience a drop of about 2?3% of mIoU when only one class is added. Finally, in <ref type="figure">Figure S1</ref> we show the effect of the proposed module on the feature vectors. The plot shows the feature distribution of the fine-tuning (FT) approach against the addition of the sparsity regularizing constraint. We can appreciate how adding L sp reduced spurious activations in the range 0.3?0.85, that correspond to uncertain and unreliable predictions. </p><formula xml:id="formula_11">f i = f i ||f i || 1 f i ? F n (S6) f i = exp (f i ) j exp (f i,j ) f i ? F n (S7) L sp = 1 |f i ? F n | fi?Fn j ?f i,j ? log f i,j (S8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Additional Ablation Studies</head><p>In this section, we report a couple of additional ablation studies concerning the dataset size and the pre-training.</p><p>Random Split. Looking at <ref type="table" target="#tab_1">Table 1</ref> of the main paper, we see that in some cases, especially on the 15-1 setup, the proposed method is still far from the offline reference. An interesting question is whether this is due to the difficulty of handling new classes or if, more fundamentally, it is due to an inherent difficulty to train a network using only a small subset of the data at each step. To answer this, we split the dataset equally in 5 parts (each part containing all classes, thus removing the complexity of learning new classes) and then we trained the network sequentially on each of this parts. We obtained 69.9% of mIoU against 75.4% of the joint training, 5.6% of the FT (disjoint) and 48.1% of SDR (disjoint). The difference with respect to joint training is relatively small, and it could be due to sub-optimal network weights estimation (samples are taken from the 5 parts accessed subsequently, instead of the full dataset); on the other side, the difference with respect to FT is very large proving that handling unseen classes is the key issue and the proposed latent constraints aim at addressing it.</p><p>Considerations on Pre-Training. The results reported in the main paper have been obtained initializing the weights of the backbone ResNet-101 approach on the Im-ageNet dataset. This is the standard setup in continual semantic segmentation approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33]</ref>. Additional considerations have been already addressed in <ref type="bibr" target="#b42">[43]</ref>, where it has been shown that pre-training on a segmentation benchmark could boost the accuracy; nonetheless, the ranking of the proposed strategies is mainly maintained.</p><p>On the other hand, even ImageNet contains visual sam-ples for many of the elements present in the Pascal dataset (for classification task instead of segmentation), potentially limiting the magnitude of decay on old tasks, and likely raising accuracies for new concepts that are not necessarily completely new to the encoder. Here, we show how the network performs without such a strong prior on the latent representations. The results are strongly affected by the fact that datasets for in-the-wild segmentation are often too small to reliably train complex deep networks from scratch. We trained on VOC2012 without pre-training and we achieved a low mIoU of 24.4% when training for 30 epochs, as we do in the main paper, and 40.9%, when training for 120 epochs (about 30 hours of computation). In continual learning, the final mIoU are also lower (as the starting point is much lower), but the improvements achieved by our approach and the ranking of the various methods are preserved, for instance in VOC2012 15-1 disjoint the accuracy of SDR (13.5%) is still significantly above FT (4.1%) and MiB (10.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Additional Qualitative Results</head><p>Many qualitative experimental results are reported for all the different scenarios, experimental protocols (i.e., sequential, disjoint and overlapped) and datasets.</p><p>Pascal VOC2012. The results for this dataset are reported in <ref type="figure" target="#fig_0">Figures S2, S3</ref> and S4 respectively for sequential, disjoint and overlapped protocols. In each figure, 3 images for each scenario (i.e., 19-1, 15-5 and 15-1) are depicted. We compare our method with na?ve fine tuning and the competitors, i.e., LwF <ref type="bibr" target="#b35">[36]</ref>, ILT <ref type="bibr" target="#b41">[42]</ref>, CIL <ref type="bibr" target="#b32">[33]</ref> and MiB <ref type="bibr" target="#b3">[4]</ref>. The images show how our approach alleviates forgetting and at the same time accommodates new classes to learn. On the other side, the fine-tuning and the compared approaches often deviate (i.e., are biased) in predicting novel classes being added or the special background class.</p><p>ADE20K. We report several visual results in <ref type="figure">Figure S5</ref> also for this dataset. In particular, we show 3 images for each scenario (i.e., 100-50, 100-10, 50-50). Again, we can appreciate how our method largely outperforms compared approaches in all scenarios better capturing the details of the shapes of the objects (e.g, in rows 1-4) and not degenerating into an overestimation of the background (e.g., in the 100-10 scenario). In particular, we notice how compared approaches have big difficulties in handling multiple additions of multiple classes (they struggle in tackling catastrophic forgetting in the 100-10 scenario), while our method can achieve reasonably good output segmentation maps also in the most challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Qualitative Results Across Incremental Steps</head><p>In this section we analyze the performance across the various incremental steps, comparing our method with the top performing competitor (i.e., MiB <ref type="bibr" target="#b3">[4]</ref>).</p><p>Pascal VOC2012. The results on two sample scenes from this dataset are reported in <ref type="figure">Figure S6</ref> for the disjoint 15-1 experimental protocol, where an initial training stage over 15 classes is followed by 5 incremental learning steps each carrying one class to be learned. In the first row our method shows quite robust results across the different learning steps, being able to preserve content semantics. MiB, instead, is able to avoid catastrophic forgetting for one incremental step but it degenerates after introducing the sheep class (step 2), which is predicted in spite of person probably due to the confusion of the arms and legs (caused also by their similar color). Latent representations got even more damaged across subsequent steps, while our approach (SDR) can reduce the interference on latent representations of old classes. Similar considerations also holds for the second set of images, although forgetting is less evident in this scenario: SDR achieves superior performance thanks to correct spatial localization and latent disentanglement.</p><p>ADE20K. For this dataset we consider two distinct scenarios: i.e., 5 incremental steps each adding 10 categories to the model (100-10) in <ref type="figure">Figure S7</ref>, and 2 incremental steps each adding 50 classes to the model  in <ref type="figure">Figure S8</ref>.</p><p>The first scenario is definitely the most challenging one as the model need to adapt 5 times to discover new (and possibly unrelated) classes. Nevertheless, we can appreciate that our model obtain quite robust results across the various steps in the 2 sample scenes shown in <ref type="figure">Figure S7</ref>, while MiB suffers more from catastrophic forgetting previous knowledge. In the first sample scene our approach shows a small gradual degradation across the multiple steps, while MiB firstly completely looses the wall on the background in step 2, then the curtain in step 3 and finally also the hand basin in step 4. Similarly, in the second scene our approach maintains very good results across all the steps, while MiB at the third step misleads the sky on the background.</p><p>In <ref type="figure">Figure S8</ref> we consider the case in which only two incremental steps with 50 classes each are performed. It can be appreciated how in the first step the predicted segmentation maps are quite precise according to both our approach and MiB, but, in both examples, MiB produces a less precise map after the second incremental step. More in detail, we remark some differences: our model can identify the tree (green) in the first image, that MiB only partially captures in the first step and completely misses it in the second. Similarly, SDR preserves the walls (gray) in the second image that MiB misleads in the second step. Again, the latent space regularization helps in preserving previous classes representations and in accommodating new classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Quantitative Results: per-Class Accuracy</head><p>We also analyze per-class accuracy for all compared methods in some scenarios. We report the results of perclass IoU and per-class pixel accuracy (PA) on the disjoint 19-1 <ref type="table">(Tables S2 and S3)</ref>,   <ref type="table" target="#tab_3">(Tables S4 and S5</ref>) and 15-1 <ref type="table" target="#tab_9">(Tables S6 and S7</ref>) scenarios on the VOC2012 dataset.</p><p>Even when adding as little as 1 class (scenario 19-1 in <ref type="table">Tables S2 and S3</ref>) we appreciate how FT and LwF-MC are generally able to learn the new class to some extent but they catastrophically forget previous classes resulting in a poor final mIoU. This performance drop is typically due to a biased prediction toward the new class (high per-class PA for that class but low IoU). The other competing approaches and our proposal, instead, are more balanced across the various classes and can greatly alleviate forgetting (with performance gains distributed across the classes) when learning the new class, thus resulting in higher mIoUs.</p><p>Analyzing the per-class IoUs on the 15-5 case in Tables S4 and S5 we can appreciate how FT is completely unable to preserve knowledge about previous classes which are heavily forgotten. The competitors can better preserve knowledge related to previous classes while learning new classes but our approach shows superior results in both retaining old classes knowledge and in learning new ones.</p><p>The last 15-1 scenario is shown in Tables S6 and S7. Here we can confirm most of the previous considerations; our method outperforms all the competitors proving its scalability when multiple incremental steps are made. From the per-class pixel accuracy we can observe that most of competing approaches are biased toward the prediction of the very few last classes added to the model, thus reducing the IoU for the other classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDR (ours)</head><p>MiB <ref type="bibr" target="#b3">[4]</ref> SDR (ours)</p><p>MiB <ref type="bibr" target="#b3">[4]</ref> RGB label step 1 step 2 step 3 step 4 step 5 <ref type="figure">Figure S7</ref>. Qualitative results on sample scenes in experimental protocol 100-10 on ADE20K during the various incremental steps (best viewed in colors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDR (ours)</head><p>MiB <ref type="bibr" target="#b3">[4]</ref> SDR (ours)</p><p>MiB <ref type="bibr" target="#b3">[4]</ref> RGB label step 1 step 2 <ref type="figure">Figure S8</ref>. Qualitative results on sample scenes in experimental protocol 50-50 on ADE20K during the various incremental steps (best viewed in colors).  <ref type="table">aero  bike  bird  boat  bottle  bus  car  cat  chair  cow  din. table  dog  horse  mbike  person  plant  sheep  sofa  train  tv  old new all  FT  95.3</ref>       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed approach, with an old class (cat) and a new class (car). Latent representations of old classes are preserved over time via prototypes matching and clustering, whilst also making room for accommodating new classes via sparsity and repulsive force in contrastive learning. The decoder is constrained to act as in previous steps on previous classes via output-level distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of competing approaches in different scenarios on disjoint VOC 2012 and ADE20K (best viewed in colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ce + sp Figure S1. Histogram of occurrences of feature activations. Features are normalized with respect to the class-wise maximum value over all feature channels (Eq. (8)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>27.5 0.0 1.6 15.4 11.5 0.0 4.1 0.5 0.0 0.0 0.0 0.0 0.2 0.2 0.0 72.0 90.0 77.2 89.7 80.7 8.4 33.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) 86.2 47.1 34.2 69.1 37.9 61.3 67.2 72.5 81.1 17.9 51.3 40.8 72.9 67.6 68.5 70.8 8.3 4.8 2.7 24.5 24.2 59.2 12.9 48.1 SDR+MiB 86.9 32.0 29.8 76.0 42.8 60.7 67.4 64.7 85.8 19.2 50.3 39.4 75.1 73.0 69.3 78.2 3.4 2.7 11.5 34.0 20.1 59.4 14.3 48.7 offline 92.5 89.9 39.2 87.6 65.2 77.3 91.1 88.5 92.9 34.8 84.0 53.7 88.9 85.0 85.1 84.9 60.0 79.7 47.0 82.2 73.5 77.5 68.5 75.4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>mIoU on multiple incremental scenarios and protocols on VOC2012. Best in bold, runner-up underlined. ?: results from<ref type="bibr" target="#b3">[4]</ref>.old  new all old new all old new all old new all old new all old new all old new all old new all FT 63.4 21.2 61.4 35.2 13.2 34.2 34.7 14.9 33.8 62.0 38.1 56.3 8.4 33.5 14.4 12.5 36.9 18.3 49.0 17.8 41.6 5.8 4.9 5.6 4.9 3.2 4.5 LwF [36] 67.2 26.4 65.3 65.8 28.3 64.0 62.6 23.4 60.8 68.0 43.0 62.1 39.7 33.3 38.2 67.0 41.8 61.0 33.7 13.7 29.0 26.2 15.1 23.6 24.0 15.0 21.9 LwF-MC [51] 49.2 0.9 46.9 38.5 1.0 36.7 37.1 2.3 35.4 70.6 19.5 58.4 41.5 25.4 37.6 59.8 22.6 51.0 12.1 1.9 9.7 6.9 2.1 5.7 6.9 2.3 5.8 ILT [42] 64.3 22.7 62.3 66.9 23.4 64.8 50.2 29.2 49.2 71.3 47.8 65.7 31.5 25.1 30.0 69.0 46.4 63.6 49.2 30.3 48.3 6.7 1.2 5.4 5.7 1.0 4.6 CIL [33] 64.1 22.8 62.1 62.6 18.1 60.5 35.1 13.8 34.0 63.8 39.8 58.1 42.6 35.0 40.8 14.9 37.3 20.2 52.4 22.3 45.2 33,3 15.9 29.31.9 66.5 67.0 26.0 65.1 69.6 23.8 67.4 73.0 44.4 66.1 47.5 34.1 44.3 73.1 44.5 66.3 35.7 11.0 29.8 39.0 15.0 33.3 44.5 11.7 36.7 SDR (ours) 68.4 35.3 66.8 69.9 37.3 68.4 69.1 32.6 67.4 73.6 46.7 67.2 73.5 47.3 67.2 75.4 52.6 69.9 58.5 10.1 47.0 59.2 12.9 48.1 44.7 21.8 39.2 SDR + MiB 70.6 24.8 68.5 70.8 31.4 68.9 71.3 23.4 69.0 74.6 43.8 67.3 74.6 44.1 67.3 76.3 50.2 70.1 58.1 11.8 47.1 59.4 14.3 48.7 47.3 14.7 39.5 offline 75.5 73.5 75.4 75.5 73.5 75.4 75.5 73.5 75.4 77.5 68.5 75.4 77.5 68.5 75.4 77.5 68.5 75.4 77.5 68.5 75.4 77.5 68.5 75.4 77.5 68.5 75.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>19-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15-5</cell><cell>15-1</cell></row><row><cell></cell><cell cols="3">sequential</cell><cell>disjoint</cell><cell>overlapped</cell><cell cols="3">sequential</cell><cell>disjoint</cell><cell>overlapped</cell><cell>sequential</cell><cell>disjoint</cell><cell>overlapped</cell></row><row><cell>Method</cell><cell cols="9">old new all 1 6.3 4.5 5.9</cell></row><row><cell>MiB ?[4]</cell><cell>-</cell><cell>-</cell><cell cols="4">-69.6 25.6 67.4 70.2 22.1 67.8 -</cell><cell>-</cell><cell cols="2">-71.8 43.3 64.7 75.5 49.4 69.0 -</cell><cell>-</cell><cell>-46.2 12.9 37.9 35.1 13.5 29.7</cell></row><row><cell cols="2">MiB [4] 68.2 19-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RGB</cell><cell></cell><cell></cell><cell>label</cell><cell>FT</cell><cell cols="2">LwF [32]</cell><cell></cell><cell cols="2">ILT [42]</cell><cell>CIL [33]</cell><cell>MiB [4]</cell><cell>SDR (ours)</cell><cell>offline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>mIoU over multiple incremental scenarios on disjoint setup of ADE20K. Best in bold, runner-up underlined. 100-50 100-10 50-50 Method old new all old new all old new all FT 0.0 22.5 7.5 0.0 2.5 0.8 13.9 12.0 12.6 LwF [36] 25.0 23.9 24.6 5.4 5.6 5.5 32.2 22.9 26.0 24.7 33.3 21.0 5.3 15.8 39.1 22.6 28.1 SDR (ours) 37.4 24.8 33.2 28.9 7.4 21.7 40.9 23.8 29.5 SDR+MiB 37.5 25.5 33.5 28.9 11.7 23.2 42.9 25.4 31.3 offline 43.9 27.2 38.3 43.9 27.2 38.3 50.9 32.1 38.3 Ablation on disjoint VOC2012 15-1 in terms of mIoU.</figDesc><table><row><cell cols="2">LwF-MC [51] 8.6 0.0 5.8 0.0 0.9 0.3 2.8 0.5 1.2</cell></row><row><cell>ILT [42]</cell><cell>27.2 21.7 25.4 0.0 0.2 0.8 41.9 21.1 28.0</cell></row><row><cell>CIL [33]</cell><cell>0.0 22.5 7.5 0.0 2.0 0.6 14.0 11.9 12.6</cell></row><row><cell>MiB [4]</cell><cell>37.6 new all</cell></row><row><cell></cell><cell>5.8 4.9 5.6</cell></row><row><cell></cell><cell>30.0 11.0 25.4</cell></row><row><cell></cell><cell>18.7 9.0 16.4</cell></row><row><cell></cell><cell>40.4 12.9 33.9</cell></row><row><cell></cell><cell>41.0 13.2 34.4</cell></row><row><cell></cell><cell>50.0 15.9 41.9</cell></row><row><cell></cell><cell>59.2 12.9 48.1</cell></row></table><note>L ce L pm L sp L cl L kd L kd old</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Standard (non-incremental) semantic segmentation.L ce L sp L cl mIoU VOC2012 mIoU ADE20K</figDesc><table><row><cell>75.4</cell><cell>38.3</cell></row><row><cell>75.8</cell><cell>38.7</cell></row><row><cell>75.8</cell><cell>38.8</cell></row><row><cell>76.3</cell><cell>39.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S1 .</head><label>S1</label><figDesc>Comparison of different Lsp in terms of mIoU in the disjoint scenarios 19-1 and 15-1 on Pascal VOC2012 dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU 19?1 mIoU 15?1</cell></row><row><cell>L0</cell><cell>66.7</cell><cell>46.3</cell></row><row><cell>L1</cell><cell>65.9</cell><cell>45.4</cell></row><row><cell>norm max</cell><cell>67.4</cell><cell>47.8</cell></row><row><cell>norm max overall</cell><cell>67.5</cell><cell>45.6</cell></row><row><cell>norm L2</cell><cell>64.8</cell><cell>44.3</cell></row><row><cell>power 2</cell><cell>66.3</cell><cell>44.2</cell></row><row><cell>power 3</cell><cell>66.6</cell><cell>45.3</cell></row><row><cell>entropy (L1)</cell><cell>65.3</cell><cell>48.0</cell></row><row><cell>entropy (softmax)</cell><cell>66.0</cell><cell>48.0</cell></row><row><cell>ours</cell><cell>68.4</cell><cell>48.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Qualitative results on sample scenes in different scenarios (19-1, 15-5 and 15-1) on Pascal VOC 2012 of the proposed method and of competing approaches in the sequential setup (best viewed in colors). Qualitative results on sample scenes in different scenarios (19-1, 15-5 and 15-1) on Pascal VOC 2012 of the proposed method and of competing approaches in the disjoint setup (best viewed in colors). Qualitative results on sample scenes in different scenarios (19-1, 15-5 and 15-1) on Pascal VOC 2012 of the proposed method and of competing approaches in the overlapped setup (best viewed in colors). Qualitative results on sample scenes in different scenarios (100-50, 100-10 and 50-50) on ADE20K of the proposed method and of competing approaches (best viewed in colors). Qualitative results on sample scenes in the disjoint experimental protocol 15-1 on Pascal VOC 2012 during the various incremental steps (best viewed in colors).</figDesc><table><row><cell>background aeroplane bicycle din. table dog horse RGB label FT Figure S2. background aeroplane bicycle 19-1 19-1 19-1 15-5 15-5 15-5 15-1 15-1 15-1 din. table dog horse 19-1 19-1 19-1 15-5 15-5 15-5 15-1 15-1 15-1 RGB label FT din. table dog horse Figure S3. background aeroplane bicycle 19-1 19-1 19-1 15-5 15-5 15-5 15-1 15-1 15-1 RGB label FT 100-50 Figure S4. 100-50 100-50 100-10 100-10 100-10 50-50 50-50 50-50 RGB label FT din. table dog horse SDR (ours) MiB [4] SDR (ours) MiB [4] RGB label step 1 (p. plant) step 2 (sheep) bird boat bottle mbike person pplant sheep sofa train bus car cat LwF [32] ILT [42] CIL [33] MiB [4] SDR (ours) chair tv bird boat bottle bus car cat chair mbike person pplant sheep sofa train tv LwF [32] ILT [42] CIL [33] MiB [4] SDR (ours) bird boat bottle bus car cat chair mbike person pplant sheep sofa train tv LwF [32] ILT [42] CIL [33] MiB [4] SDR (ours) LwF [32] ILT [42] CIL [33] MiB [4] SDR (ours) bird boat bottle bus car cat chair mbike person pplant sheep sofa train tv step 3 (sofa) step 4 (train) step 5 (tv) cow unlabeled offline cow unlabeled offline cow unlabeled offline offline cow unlabeled Figure S5. background aeroplane bicycle Figure S6.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 .Table S3 .</head><label>S2S3</label><figDesc>Per-class IoU of compared methods in disjoint experimental protocol on scenario 19-1 of Pascal VOC 2012. 75.4 31.1 71.7 50.8 66.0 81.6 79.0 87.9 32.1 66.9 49.9 84.1 66.2 77.3 79.4 51.8 68.5 42.1 65.8 28.3 65.8 28.3 64.0 LwF-MC [51] 78.6 63.6 0.4 61.2 10.6 35.2 52.8 35.1 75.5 0.4 63.9 1.5 75.5 67.8 32.6 13.1 13.0 63.4 0.7 25.9 1.0 38.5 1.0 36.7 ILT [42] 87.7 79.5 31.6 77.4 54.5 66.5 70.9 79.0 90.4 31.4 66.5 52.9 85.1 67.7 78.1 82.0 56.0 67.3 41.4 72.3 23.4 66.9 23.4 64.8 CIL [33] 85.3 71.4 33.6 75.2 56.5 59.3 45.8 67.2 85.9 27.6 62.7 46.9 85.2 67.9 75.2 83.7 47.4 67.0 42.3 66.0 18.1 62.6 18.Per-class pixel accuracy of compared methods in disjoint experimental protocol on scenario 19-1 of Pascal VOC 2012. .2 74.9 71.1 44.0 34.3 46.4 26.1 4.5 72.6 8.1 25.4 78.0 53.9 0.0 40.6 58.5 0.8 64.3 82.0 35.2 13.2 34.2 LwF [36] 94.1 85.6 58.7 91.2 59.1 76.3 84.4 80.3 94.1 39.3 93.5 52.3 91.7 95.3 84.0 82.3 76.5 84.1 48.2 68.4 69.6 65.8 28.3 64.0 LwF-MC [51] 99.8 65.5 0.4 63.1 10.7 39.6 53.1 35.3 78.4 0.5 66.5 1.5 77.8 72.0 34.0 13.1 14.4 65.9 0.7 25.9 1.0 38.5 1.0 36.7 ILT [42] 93.5 88.2 59.5 94.3 77.1 83.2 72.0 81.5 96.2 38.7 93.5 55.9 93.8 94.2 84.9 85.7 79.0 91.3 47.1 77.0 63.4 66.9 23.4 64.8 CIL [33] 91.9 77.6 68.6 90.8 66.0 67.6 46.0 67.9 97.3 31.3 95.8 48.6 95.4 94.6 78.9 87.7 82.1 86.4 48.2 68.2 82.1 62.6 18.1 60.5 MiB [4] 89.8 95.0 91.6 97.7 83.9 93.0 93.7 91.2 96.9 52.3 94.2 60.8 96.8 96.2 95.5 88.0 81.9 88.5 56.7 83.6 73.8 67.1 26.1 65.1 SDR (ours) 95.0 90.1 66.5 95.1 67.9 87.7 88.0 83.0 96.4 44.9 93.0 61.3 95.9 95.3 82.7 86.8 81.8 92.9 53.3 72.9 57.9 69.9 37.3 68.4 SDR+MiB 93.1 96.0 86.9 97.3 85.5 91.5 92.1 90.5 96.7 48.8 92.4 58.6 95.7 94.8 91.3 88.9 78.9 90.3 56.1 84.4 69.5 70.8 31.4 68.9 offline 96.1 96.6 85.4 94.4 87.2 92.2 94.7 93.5 96.9 50.2 95.4 56.5 95.8 91.8 94.7 90.8 80.8 92.1 54.8 89.5 83.5 75.5 73.5 75.4 Table S4. Per-class IoU of compared methods in disjoint experimental protocol on scenario 15-5 of Pascal VOC 2012. 27.2 0.0 1.6 15.1 11.3 0.0 4.1 0.5 0.0 0.0 0.0 0.0 0.2 0.2 0.0 27.0 25.6 28.9 33.5 52.2 8.4 33.5 14.4 LwF [36] 83.4 59.1 21.7 16.7 36.8 47.0 18.7 62.5 52.3 6.6 4.8 37.7 35.9 44.9 55.5 51.6 22.6 27.8 25.3 39.6 51.1 39.7 33.3 38.2 LwF-MC [51] 85.4 54.2 16.9 59.7 29.7 46.0 34.4 65.9 38.1 5.2 35.9 7.5 62.4 44.3 48.7 29.1 11.4 37.3 8.9 42.1 27.1 41.5 25.4 37.6 ILT [42] 81.7 47.6 18.4 1.6 29.7 19.4 3.8 52.5 56.7 0.5 4.6 20.7 43.1 35.4 33.6 54.8 22.7 22.4 15.9 30.1 34.3 31.5 25.1 30.0 CIL [33] 81.0 45.4 28.8 30.4 31.1 54.5 9.4 67.8 52.1 10.5 9.2 47.9 53.0 35.3 66.3 58.4 23.9 33.3 25.2 39.1 53.9 42.6 35.1 40.8 MiB [4] 78.4 58.3 30.8 52.5 35.5 60.5 60.2 74.8 38.2 14.0 21.6 41.8 42.9 34.8 67.4 48.8 23.2 31.0 24.4 46.3 45.8 47.5 34.1 44.3 SDR (ours) 88.7 82.9 40.5 82.4 62.8 69.2 83.8 88.2 91.6 28.9 71.1 54.2 86.8 80.3 79.7 84.4 39.4 51.4 23.7 63.3 58.7 73.5 47.3 67.2 SDR + MiB 89.4 87.1 39.9 84.8 67.3 75.2 85.1 88.2 91.3 29.9 67.8 54.4 86.1 81.8 80.5 85.0 33.8 43.6 24.7 61.7 56.6 74.6 44.1 67.3 offline 92.5 89.9 39.2 87.6 65.2 77.3 91.1 88.5 92.9 34.8 84.0 53.7 88.9 85.0 85.1 84.9 60.0 79.7 47.0 82.2 73.5 77.5 68.5 75.4 Table S5. Per-class pixel accuracy of compared methods in disjoint experimental protocol on scenario 15-5 of Pascal VOC 2012.</figDesc><table><row><cell>Method FT LwF [36] MiB [4] SDR (ours) 89.6 85.3 35.9 78.6 55.2 73.6 86.2 81.9 89.1 34.2 71.4 56.6 86.5 72.7 78.0 83.0 54.1 71.0 45.5 70.4 37.3 69.9 37.3 68.4 backgr. aero bike bird boat bottle bus car cat chair cow din. table dog horse mbike person plant sheep sofa train tv old new all 72.4 62.4 6.7 45.0 47.1 39.5 33.7 40.9 25.7 4.3 54.0 8.0 25.0 50.4 50.6 0.0 35.3 43.0 0.8 59.5 13.2 35.2 13.2 34.2 87.6 1 60.5 86.9 73.5 35.7 64.0 50.5 71.0 89.5 87.0 84.8 33.7 62.9 56.9 82.1 61.8 79.5 82.4 56.2 62.0 46.0 75.9 26.0 67.0 26.0 65.1 SDR+MiB 89.5 84.4 39.0 76.5 53.6 75.1 89.1 87.6 89.0 33.7 67.8 55.4 85.2 72.8 80.8 83.4 57.8 71.3 46.3 78.4 31.4 70.8 31.4 68.9 offline 92.5 89.9 39.2 87.6 65.2 77.3 91.1 88.5 92.9 34.8 84.0 53.7 88.9 85.0 85.1 84.9 60.0 79.7 47.0 82.2 73.5 75.5 73.5 75.4 Method backgr. aero bike bird boat bottle bus car cat chair cow din. table dog horse mbike person plant sheep sofa train tv old new all FT 91.5 79.9 7Method backgr. aero bike bird boat bottle bus car cat chair cow din. table dog horse mbike person plant sheep sofa train tv old new all FT 74.2 Method backgr.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>5 14.4   LwF<ref type="bibr" target="#b35">[36]</ref> 91.9 79.<ref type="bibr" target="#b3">4</ref> 35.4 16.9 50.9 49.0 19.4 71.0 78.8 8.0 5.2 39.7 36.3 78.5 59.2 53.3 67.1 91.2 74.2 81.6 76.5 39.7 33.3 38.2 LwF-MC [51] 96.6 80.7 30.3 68.5 62.0 60.4 37.7 79.7 62.5 10.8 46.2 9.2 73.2 84.4 64.8 31.7 11.4 39.7 9.1 60.1 27.1 41.5 25.4 37.6 ILT [42] 94.6 61.4 26.4 1.6 30.8 19.5 4.0 57.7 71.7 0.5 5.1 20.9 45.9 43.7 34.6 56.7 42.5 86.0 38.8 71.0 44.9 31.5 25.1 30.0 CIL [33] 85.0 80.5 56.3 31.6 57.2 59.5 10.0 81.9 87.6 16.6 12.3 53.9 58.1 86.1 74.1 61.5 84.4 95.7 88.8 93.5 87.1 42.6 35.1 40.8 MiB [4] 80.7 92.6 64.8 64.5 74.0 68.3 65.0 84.3 93.7 23.6 36.2 50.9 49.8 91.2 85.7 52.0 73.9 86.6 87.6 89.9 83.7 47.5 34.1 44.3 SDR (ours) 91.2 95.1 82.1 96.5 80.1 86.3 93.3 92.2 97.0 51.8 93.0 64.3 96.0 91.0 92.0 91.1 68.9 64.1 69.6 74.0 82.9 73.5 47.3 67.2 SDR + MiB 91.7 94.7 80.1 93.4 79.1 88.7 90.6 91.4 96.3 51.0 82.4 64.6 94.9 90.2 91.7 91.8 68.6 67.8 70.3 79.7 81.3 74.6 44.1 67.3 offline 96.1 96.6 85.4 94.4 87.2 92.2 94.7 93.5 96.9 50.2 95.4 56.5 95.8 91.8 94.7 90.8 80.8 92.1 54.8 89.5 83.5 77.5 68.5 75.4 Table S6. Per-class IoU of compared methods in disjoint experimental protocol on scenario 15-1 of Pascal VOC 2012.</figDesc><table><row><cell>Method</cell><cell>backgr.</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>din. table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>old new all</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S7 .</head><label>S7</label><figDesc>Per-class pixel accuracy of compared methods in disjoint experimental protocol on scenario 15-1 of Pascal VOC 2012.</figDesc><table><row><cell>Method</cell><cell>backgr.</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>din. table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>old new all</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>8 4.9 6.7 1.2 5.4 CIL [33] 90.1 16.8 40.0 48.4 15.3 32.7 9.0 28.2 60.1 17.1 75.0 20.4 53.8 28.7 13.5 60.0 31.0 11.8 49.7 50.1 87.0 33.3 15.9 29.1 MiB [4] 72.7 61.7 58.6 60.7 52.3 69.4 45.8 59.2 88.3 30.2 62.3 53.9 68.6 60.7 70.9 0.1 7.0 84.3 28.8 84.9 65.6 39.0 15.0 33.3 SDR (ours) 92.7 47.6 72.3 91.9 44.5 69.2 76.5 74.7 89.3 60.9 92.8 53.1 94.9 75.5 88.3 73.8 11.5 5.1 3.0 35.7 76.6 59.2 12.9 48.1 SDR+MiB 92.7 33.2 45.0 84.7 47.0 67.6 72.1 65.2 96.6 59.1 95.7 45.1 85.3 80.5 83.5 84.2 4.4 2.8 17.2 57.1 76.6 59.4 14.3 48.7 offline 96.1 96.6 85.4 94.4 87.2 92.2 94.7 93.5 96.9 50.2 95.4 56.5 95.8 91.8 94.7 90.8 80.8 92.1 54.8 89.5 83.5 77.5 68.5 75.4</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Life-long disentangled representation learning with cross-domain latent homologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9873" to="9883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selfless sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling the background for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cermelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="532" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08383</idno>
		<title level="m">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cluster alignment with a teacher for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9944" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08051</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Learning without memorizing. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-shot semantic segmentation with prototype learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on deep learning techniques for image and video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A review of semantic segmentation using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="831" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-learning representations for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1820" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Abitino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2008" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Philipp Donn, and Tim Fingscheidt. Class-incremental learning for semantic segmentation re-using neither old data nor old labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>B?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rilod: near real-time incremental learning for object detection at the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serafettin</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE Symposium on Edge Computing</title>
		<meeting>the 4th ACM/IEEE Symposium on Edge Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distant supervised centroid shift: A simple and efficient approach to visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incremental Learning Techniques for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge distillation for incremental learning in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11321" to="11329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Extending pretrained segmentation networks with additional anatomical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firat</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orcun</forename><surname>Goksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1187" to="1195" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalized domain-adaptive dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3400" to="3409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Incremental Learning for Semantic Segmentation of Large-Scale Remote Sensing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Tasar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliya</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alliez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12448</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Generalized few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05210,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation in semantic segmentation via orthogonal and clustered embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Panet: Few-shot image semantic segmentation with prototype alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hao Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Growing a brain: Fine-tuning by increasing model capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2471" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improving domain-specific classification by collaborative learning with adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hau-San</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5450" to="5457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00853</idno>
		<title level="m">Incremental classifier learning with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
