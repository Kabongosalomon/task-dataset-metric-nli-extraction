<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where are the Masks: Instance Segmentation with Image-level Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issam</forename><forename type="middle">H</forename><surname>Laradji</surname></persName>
							<email>issamou@cs.ubc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
							<email>dvazquez@elementai.com</email>
							<affiliation key="aff0">
								<orgName type="department">Element AI Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
							<email>schmidtm@cs.ubc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Where are the Masks: Instance Segmentation with Image-level Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LARADJI, VAZQUEZ, &amp; SCHMIDT: WHERE ARE THE MASKS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major obstacle in instance segmentation is that existing methods often need many per-pixel labels in order to be effective. These labels require large human effort and for certain applications, such labels are not readily available. To address this limitation, we propose a novel framework that can effectively train with image-level labels, which are significantly cheaper to acquire. For instance, one can do an internet search for the term "car" and obtain many images where a car is present with minimal effort. Our framework consists of two stages: (1) train a classifier to generate pseudo masks for the objects of interest; (2) train a fully supervised Mask R-CNN on these pseudo masks. Our two main contribution are proposing a pipeline that is simple to implement and is amenable to different segmentation methods; and achieves new state-of-the-art results for this problem setup. Our results are based on evaluating our method on PASCAL VOC 2012, a standard dataset for weakly supervised methods, where we demonstrate major performance gains compared to existing methods with respect to mean average precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent progress in Deep Neural Networks (DNNs) and segmentation frameworks has given us major improvements in the task of instance segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. Their success was demonstrated in various applications such as autonomous driving <ref type="bibr" target="#b8">[9]</ref>, scene understanding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, and medical imaging <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>. Nonetheless, these methods require a large number of training data with per-pixel labels, or labels which distinguish between object categories and instances in the image. As acquiring them is often prohibitively expensive, the effectiveness of these methods is limited to a small range of datasets and object categories.</p><p>Many weakly supervised methods emerged to overcome the need for per-pixel labels. Instead, they only require weaker labels ranging from bounding boxes <ref type="bibr" target="#b16">[17]</ref>, scribbles <ref type="bibr" target="#b22">[23]</ref>, and image-level <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref> annotations. This makes acquiring datasets a significantly more scalable endeavour. According to Bearman et al. <ref type="bibr" target="#b2">[3]</ref>, it requires 20 sec/img to acquire imagelevel labels (which are labels that only indicate whether an object class appears in an image) for PASCAL VOC <ref type="bibr" target="#b11">[12]</ref>, compared to 239.7 sec/img for acquiring per-pixel labels. To date, c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1907.01430v1 [cs.CV] 2 Jul 2019 only two weakly supervised methods address instance segmentation with image-level labels, making our work one of the few that tackles a relatively unexplored research area.</p><p>Perhaps the first work to address this problem setup is PRM <ref type="bibr" target="#b42">[43]</ref>. It trains a classifier which can then identify local regions belonging to different objects of the same category. It extends CAM-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38]</ref> by not only identifying large regions where objects are vaguely located, but also identifying peaks that represent the specific locations of the object instances. At test time, the trained PRM obtains the object masks in two steps. First, it uses the gradient with respect to the input to get a rough mask of the objects using a process called peak backpropagation. This results in a peak response map. Then, the masks in this map are replaced by the best matching proposal masks, which are generated from a pretrained object proposal method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>. However, their results are much worse than that of fully supervised methods, leaving a large room for improvement <ref type="table" target="#tab_1">(Table 1)</ref>.</p><p>Our Weakly-supervised Instance SEgmentation method (WISE) builds on PRM by using its output pseudo masks to train a fully-supervised method, namely, Mask R-CNN <ref type="bibr" target="#b27">[28]</ref>. Our intuition as to why this procedure is effective is that Mask R-CNN is potentially robust to noisy pseudo masks, and the noisy labels within these masks might be ignored during training as they are potentially uncorrelated. The success of such a de-noising strategy has been demonstrated in semantic segmentation and object localization <ref type="bibr" target="#b16">[17]</ref>.</p><p>We show that simple techniques for obtaining the pseudo masks lead to a surprisingly effective supervision for Mask R-CNN. We summarize our contributions as follows. <ref type="formula" target="#formula_1">(1)</ref> We present a novel framework that can effectively train a fully supervised method on pseudo mask labels obtained from image-level class labels; <ref type="bibr" target="#b1">(2)</ref> we show that our framework is amenable to different localization and segmentation methods (for example, a density-based PRM <ref type="bibr" target="#b7">[8]</ref> can be used for localization and RetinaMask <ref type="bibr" target="#b12">[13]</ref> can be used for instance segmentation), and (3) we achieve new state-of-the-art results on a standard weakly-supervised instance segmentation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Instance segmentation is widely studied within the computer vision community <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>. However, an ongoing challenge is that it is time-consuming and expensive to obtain the required per-pixel labels needed by most instance segmentation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. Current trends to overcome this issue leverage weaker labels (such as image-level labels) and pseudo labels obtained with the help of object proposal methods. While most of these methods are for object detection and semantic segmentation, we review them below as they are relevant.</p><p>Instance segmentation. Instance segmentation is one the most challenging tasks in computer vision. The task is to classify every object pixel into its corresponding category and distinguish between object instances <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>. Most recent methods rely on deep networks and follow a two step procedure <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, where they first detect objects and then segment them. For instance, Mask-RCNN <ref type="bibr" target="#b14">[15]</ref> uses Faster-RCNN <ref type="bibr" target="#b35">[36]</ref> for detection and an FCN network <ref type="bibr" target="#b25">[26]</ref> for segmentation. In this work, we use Mask R-CNN as our fully supervised method and train it on pseudo masks instead of the costly per-pixel labels.</p><p>Learning with weak supervision. Due to the taxing task of acquiring per-pixel labels, many weakly supervised methods emerged that can leverage labels that are much cheaper to acquire <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. These labels range from bounding boxes <ref type="bibr" target="#b16">[17]</ref>, scribbles <ref type="bibr" target="#b22">[23]</ref>, points [3, 21, 22], <ref type="figure">Figure 1</ref>: Framework overview. Our Weakly-supervised Instance SEgmentation (WISE) method learns to perform instance segmentation with image-level supervision. First, a classifier is trained with a peak stimulation layer to identify peaks at which the objects are located (row 2). A proposal gallery (such as MCG <ref type="bibr" target="#b1">[2]</ref>) is used to obtain rough masks for the located objects, which are then used as pseudo masks to train Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> (row 3). Row 4 shows the output of a Mask R-CNN trained on the noisy pseudo mask labels. and image-level annotation <ref type="bibr" target="#b42">[43]</ref>. Our setup considers one of the weakest forms of annotation, image-level labels.</p><p>Image-level labels as weak supervision. Acquiring image-level labels is an attractive form of annotation due to its extremely cheap cost. The annotator only needs to indicate whether a certain object class appears in an image, regardless of how many of them appear. While this form of annotation has gained steam within the research community, most of the proposed methods are for semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40]</ref>. Perhaps the lack of such research for instance segmentation is partially accounted for by the fact that instance segmentation is a more challenging task. Only recently did two works emerge for this problem setup <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>. They extend the Class Activation Map (CAM) <ref type="bibr" target="#b41">[42]</ref>, by not only identifying a heatmap that vaguely represents the regions where objects are located, but also identifying peaks of that heatmap that represent the locations of different objects. At test time, they adopt a postprocessing step that matches each located object with a proposal, generated from an object proposal method. These proposals are considered as the final instance segmentation output. In contrast, we use these outputs as pseudo masks to train a fully supervised method.</p><p>Learning with pseudo labels. Our method adopts the following pipeline, generate pseudolabels and then training a model on these labels in a fully-supervised manner. While this is novel for instance segmentation, similar approaches were used for object detection <ref type="bibr" target="#b38">[39]</ref> and semantic segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref> in weakly supervised settings. However, these methods cannot be directly applied to instance segmentation, as they do not distinguish between object instances. Many such methods also rely on object proposals <ref type="bibr" target="#b15">[16]</ref> to ease the task of detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>. Object proposals are class-agnostic methods <ref type="figure">Figure 2</ref>: WISE training. The first component (shown in blue) learns to classify the images in the dataset. The classifier first outputs a class activation map (CAM); then, obtains CAM's local maximas using a peak stimulation layer (PSL). To train the classifier, the classification loss is computed using the average of these local maximas. As the CAM peaks represent located objects, we select a proposal for each of these objects to obtain pseudo masks. The second component (shown in green) trains a Mask R-CNN on these pseudo masks.</p><p>that can output thousands of object candidates per image and have progressed significantly over the last decade <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. Similar to PRM <ref type="bibr" target="#b42">[43]</ref> and PRM+Density <ref type="bibr" target="#b7">[8]</ref>, we also leverage object proposals to generate the pseudo masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Our approach to instance segmentation with image-level supervision consists of two main steps: (1) obtain pseudo masks for the training images given their ground-truth image-level labels; and (2) train a fully supervised instance segmentation method on these pseudo masks ( <ref type="figure">Figure 2</ref>). In particular, this framework is based on two components: a network that obtains the pseudo masks by training a PRM <ref type="bibr" target="#b42">[43]</ref> on the image-level labels and leveraging object proposal methods <ref type="bibr" target="#b1">[2]</ref>; and a Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> as a fully supervised instance segmentation method. We show the training steps of our framework in Algorithm 1.</p><p>At test time, we can predict the object instance masks using the trained Mask R-CNN only, discarding the PRM component. In this setup, we are interested in segmenting C classes of objects. For a training image, the image-level label is given as Y = [y 1 , y 2 , ..., y C ], where y i = 1 or 0, indicating whether the image has an object of class i. We describe our components in more detail below, and also investigate a post-processing steps that can improve Mask R-CNN's final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pseudo Mask Generation Branch</head><p>We rely on PRM <ref type="bibr" target="#b42">[43]</ref> to generate segmentation seeds that identify salient parts of the objects. These seeds help in generating pseudo masks as a source of supervision for Mask R-CNN. Following PRM's methodology, we train a CAM-based classifier which has a fully convolutional network (FCN) followed by a peak stimulation layer (PSL). As shown in <ref type="figure">Figure 2</ref>, the FCN outputs a class activation map (CAM) which specifies the class confidence at Generate a set of proposals P for I; <ref type="bibr">5:</ref> Use PSL on C to obtain the set of peaks L for I; <ref type="bibr">6:</ref> Initialize an empty list of Targets T ; <ref type="bibr">7:</ref> for (i k , j k ) ? L do 8:</p><p>Select a proposal (G k , b k ) randomly using Eq. 1, it has to intersect with (i k , j k ); <ref type="bibr">9:</ref> Add G k to list T ; <ref type="bibr">10:</ref> end for <ref type="bibr">11:</ref> Compute L(I, T, ? ) as in Eq. 2;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update the weights for ? using back-propagation; <ref type="bibr">13:</ref> end while each image location. Then, PSL outputs N c local maximas of CAM within a window size r, namely, L c = {(i 1 , j 1 ), (i 2 , j 2 ), ..., (i N c , j N c )} which represents locations in the CAM for the c-th object class (more details in Zhou et al. <ref type="bibr" target="#b42">[43]</ref>). In order to boost the activations of these local maximas, their average activation is first computed as,</p><formula xml:id="formula_0">s c = 1 N c ? (i k , j k )?L c M c i k , j k ,</formula><p>where M c is the activation map corresponding to class c. This average is then used for binary classification, specifically the multi-label soft-margin loss <ref type="bibr" target="#b19">[20]</ref>. This classification component is trained until convergence.</p><p>We then generate the pseudo masks for the training images by using the trained classifier and an off-the-shelf object proposal method (specified as the dotted line in <ref type="figure">Figure 2</ref>). The peaks obtained from PSL, which represent object locations in the image, are replaced with proposal masks based on their "objectness", which are scores given by the proposal method as confidence measure for being objects. We adopt a de-noising strategy where we select a proposal randomly based on its objectness score: proposals with higher objectness are more likely to be selected. More formally, to obtain the mask for an object located at peak (i, j), we first generate a set of n proposals whose masks intersect with (i, j), namely, P = {(G 1 , b 1 ), (G 2 , b 2 ), ..., (G n , b n )} with mask G k and objectness score b k . Then, the probability of selecting a proposal mask G k is,</p><formula xml:id="formula_1">P(G k ) = b k ? n j=1 b j<label>(1)</label></formula><p>The rationale behind selecting proposals randomly is that they have common pixels that correspond to the salient parts of the located object, despite the fact that they have different objectness. While proposal masks are not originally associated with a class label, we get the object class label information from CAM and assign it to the corresponding proposals. These proposals can be used as pseudo masks to train a fully supervised instance segmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fully Supervised Segmentation Branch</head><p>We can construct the segmentation labels for all the training images by using the trained pseudo mask generation branch. These are used as supervision to train a Mask R-CNN <ref type="figure">Figure 3</ref>: Inference. At test time, only the trained Mask-RCNN is required to output the prediction masks in the image. As an optional refinement step, the predicted masks can be replaced with the object proposals of highest Jaccard similarity.</p><p>(shown as green components in <ref type="figure">Figure 2</ref>). Depending on the application, other choices of fully supervised methods can be used instead of Mask R-CNN: if the goal is to perform instance segmentation at real-time, one can consider training a YOLACT <ref type="bibr" target="#b4">[5]</ref>, and for semantic segmentation, one can consider training a DeepLab <ref type="bibr" target="#b6">[7]</ref> segmentation network.</p><p>Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> combines Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> and FCN-based <ref type="bibr" target="#b25">[26]</ref> methods to first detect the objects and then segment them. For an image I, with target pseudo masks T , Mask R-CNN with parameters ? is trained by optimizing the following objective function,</p><formula xml:id="formula_2">L(I, T, ? ) = L cls + L box + L mask ,<label>(2)</label></formula><p>where L cls is the classification loss for the detected objects, L box is the localization loss for the detected objects, L mask is their segmentation loss. These terms are explained in more detail in the original Mask R-CNN paper <ref type="bibr" target="#b14">[15]</ref>. At test time, we can simply use the trained Mask R-CNN to predict the object masks for an unseen image. To refine these masks, we leverage the same object proposal method as that used in training. In turn, we replace each predicted object mask with the proposal of highest Jaccard similarity. <ref type="figure">Figure 3</ref> illustrates how this refinement process can lead to a better object mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the efficacy of our method by comparing it against previous methods and analyzing the pseudo masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We follow the setup by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref> for a fair benchmark, where the model only has access to an off-the-shelf proposal method and image-level labels for the training set. Also from their setup, we adopt the evaluation metric, mean average precision for Intersection-over-Union (IoU) of 0.25, 0.5, and 0.75.</p><p>Like other works in the literature of weakly supervised methods, we perform all comparisons on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b11">[12]</ref>. The dataset represents a diverse set of everyday scenes. It is divided into 1442 images for training, and 1449 images for validation. Annotators for this dataset acquired per-pixel labels for 20 objects, ranging from inanimate objects such as airplanes and bikes, and living objects such as humans and horses. However, we only use the image-level labels to train our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Supervision mAP25 mAP50 mAP75 ABO Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> pixel-level 58.9 51.4 32.4 -DeepMask <ref type="bibr" target="#b16">[17]</ref> pixel-level -41.7 09.7 -PRM <ref type="bibr" target="#b42">[43]</ref> image   <ref type="table">Table 2</ref>: PASCAL VOC 2012. Per-class comparison against the mAP 50 metric on PASCAL VOC 2012 validation set. Mask R-CNN was trained with the ground-truth per-pixel labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We discuss our method's procedure and parameters below. We also plan to make the code publicly available.</p><p>Network architecture. As a common practice, we use the ResNet-50 <ref type="bibr" target="#b13">[14]</ref> that is pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref> as the backbone for PRM and Mask R-CNN. Unlike PRM, Mask R-CNN's backbone is equipped with a feature pyramid network <ref type="bibr" target="#b24">[25]</ref> that extracts features at different resolutions. The pretrained weights, along with the rest of the parameters, are then finetuned on the PASCAL VOC 2012 training set. The remaining parameters of PRM and Mask R-CNN are in the implementation details discussed in Zhou et al. <ref type="bibr" target="#b42">[43]</ref>, and He et al. <ref type="bibr" target="#b14">[15]</ref>, respectively.</p><p>Optimization parameters. Following the official code of Mask R-CNN, we scale its input images so that the short axis has a minimum of 800px and the long axis a maximum of 1333px. Using a single GPU of TitanX, we set our batch size as 1 and train using the SGD optimizer with a learning rate of 0.00125 for 50K iterations. This learning rate was adjusted from He et al. <ref type="bibr" target="#b14">[15]</ref>, where they used a bigger batch size. We also augment the dataset with horizontal flips and color jittering as recommended by Deng et al. <ref type="bibr" target="#b10">[11]</ref>. PRM was trained as described in Zhou et al. <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to Previous Work</head><p>We first quantitatively compare our approach against previous methods that use the same supervision as ours; that is, image-level labels, an object proposal method, and a ResNet-50 backbone pretrained on ImageNet. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results on the PASCAL VOC 2012 dataset. Our method significantly outperforms the current state-of-the-art by a large margin with respect to Average Best Overlap (ABO) <ref type="bibr" target="#b32">[33]</ref>, mAP25, mAP50, and mAP75. Further, WISE without refinement also beats current state-of-the-art. Even more so, our method outperforms Cholakkal et al. <ref type="bibr" target="#b7">[8]</ref> which uses slightly stronger labels than image-level. Their labels distinguish between images with 0, 1, 2-4, and 4-or more objects. <ref type="figure" target="#fig_1">Figure 4</ref> visualizes qualitative results of WISE for each category. We further report the per-class results in <ref type="table">Table 2</ref> and compare it against Mask R-CNN trained on the true masks with respect to mAP50. This illustrates that our results are competitive against fully-supervised methods.</p><p>Our method can also compete with those that use stronger supervision. Against Deep-Mask <ref type="bibr" target="#b16">[17]</ref>, our method outperforms two of their methods, one that uses bounding boxes as labels, and the other that uses full supervision as labels (see <ref type="table" target="#tab_1">Table 1</ref>). Compared to Mask R-CNN trained on the pixel-level labels, our method still has a large room for improvement, which can be bridged by either improving the object localization component or the object proposal method.</p><p>While the overall results suggests that Mask R-CNN can effectively train from noisy, incomplete labels. The labels are noisy because the proposal masks are not perfect, and incomplete because PRM does not locate all the objects in the image. Indeed, we hypothesize that using a better object localizer such as that of Cholakkal et al. <ref type="bibr" target="#b7">[8]</ref> would lead to better results. But we leave that for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Analysis of Pseudo masks</head><p>We measure the generated pseudo masks performance by computing the mAP50 between the ground-truth and the generated masks. We also compute the mean absolute error to determine  <ref type="table">Table 3</ref>: PASCAL VOC 2012 training set. Comparison of the generated pseudo masks, and WISE's predicted masks with respect to mAP50. WISE was trained on a set of pseudo masks, and was able to output better masks for the same training images. Mask R-CNN + GT was trained on the ground-truth per-pixel labels. the number of identified objects in the images. These results are summarized in <ref type="table">Table 3</ref>, which show that a large room for improvement is required for both metrics. Examples of the synthesized masks are shown in <ref type="figure">Figure 1</ref>, where one can see that the pseudo masks are not of high quality, yet the trained Mask R-CNN is able to output good masks in <ref type="figure" target="#fig_1">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Ablation Studies</head><p>The object sizes and the number of objects in an image can have severe impact on the performance of an instance segmentation model. <ref type="figure" target="#fig_2">Figure 5</ref> shows that WISE struggles with segmenting small objects, and when the number of objects is larger than 4. In fact, there is a heavy decline in performance when the number of objects is more than 1. More robust than WISE, a Mask R-CNN trained on per-pixel labels is able to maintain higher performance with small objects and with images with larger number of objects. In addition, such Mask R-CNN performs significantly better than WISE for small objects. This suggests that the pseudo masks trained by WISE are likely far from accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a weakly supervised instance segmentation method that follows a two-stage pipeline for training on image-level labels. In the first stage, it uses class activation maps with a peak stimulation layer to locate the objects in the training images, and then object proposals to generate pseudo masks for these objects. In the second stage, we use Mask R-CNN to train on the pseudo masks in a fully supervised manner. We evaluate on PASCAL VOC 2012, a standard benchmark for weakly supervised methods, where Mask R-CNN trained on pseudo masks outperformed not only methods with the same level of supervision, image-level labels, but also methods that use counts and bounding boxes in their supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 WISE training 1 : 3 :</head><label>13</label><figDesc>Train a CAM-based classifier C until convergence as in PRM [43]; 2: while iter &lt; max_iter do Randomly sample a training image I; 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results. Qualitative results of WISE on PASCAL VOC 2012 val. set. The images illustrate the predicted masks of the trained Mask R-CNN for different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Statistical Analysis. The left figure illustrates the performance of WISE and a Mask R-CNN trained on per-pixel labels across various object sizes; and the right figure illustrates the same benchmark but across images with different number of objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PASCAL VOC 2012. Comparison of our framework (WISE) against other methods on various levels of supervision. WISE+Refine uses the refinement step shown in Figure 3. Mask R-CNN and DeepMask use full supervision, whereas PRM uses image-level labels. Same as WISE, PRM and PRM+Density leverage a pretrained proposal method. Requiring stronger supervision than WISE, DeepMask and PRM+Density have access to bounding box and image-level counts, respectively.</figDesc><table><row><cell>Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>motor</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>Avg.</cell></row><row><cell cols="22">Mask R-CNN 71.2 0.3 72.2 53.2 29.8 68.7 47.3 77.1 13.3 54.7 41.0 65.5 51.5 69.6 57.8 31.0 46.9 45.6 69.7 61.4 51.4</cell></row><row><cell>WISE</cell><cell cols="21">59.2 0.6 62.6 38.6 18.8 57.3 31.7 66.9 8.3 40.5 11.0 55.5 48.7 60.2 34.4 24.4 38.3 33.1 61.7 56.9 40.4</cell></row><row><cell cols="22">WISE+Refine 63.2 0.3 60.7 39.1 21.0 59.4 31.9 68.6 9.2 43.1 15.6 58.0 48.6 62.3 36.4 21.9 38.8 34.3 65.5 56.9 41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mask R-CNN + GT 92.<ref type="bibr" target="#b3">4</ref> 15.1 97.4 87.9 91.4 94.4 93.8 100 68.2 93.4 88.8 97.4 96.4 95.3 92.8 89.3 92.3 97.7 100 100 89.2 Pseudo Masks 24.5 1.0 29.1 18.7 11.3 38.6 26.6 43.1 8.0 35.6 6.1 38.8 46.2 23.8 10.7 7.4 35.9 29.4 41.6 39.1 25.8 WISE 43.8 3.2 43.8 35.9 16.8 51.9 36.3 56.8 7.3 45.8 15.1 53.5 59.8 45.5 18.2 10.9 47.3 38.9 61.5 58.5 37.5</figDesc><table><row><cell>Metric</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>motor</cell><cell>person</cell><cell>plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>Avg.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What???s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02689</idno>
		<title level="m">Real-time instance segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object counting and instance segmentation with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Fahad Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03353</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<title level="m">Piotr Doll?r, and Bernt Schiele. What makes for effective detection proposals? T-PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Instance segmentation of fibers from low resolution ct scans via 3d deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tomasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorben</forename><surname>Konopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1533" to="1554" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Instance segmentation with point supervision. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribblesupervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Kevis-Kokitsi Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018-03-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmentation of medical images using adaptive region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Pohle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">D</forename><surname>Toennies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIIP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting object proposals: From pascal to coco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tuset</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hilaire Sean</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
