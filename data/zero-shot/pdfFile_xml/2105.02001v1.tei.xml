<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Marsden</surname></persName>
							<email>robert.marsden@iss.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Signal Processing and System Theory</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bartler</surname></persName>
							<email>alexander.bartler@iss.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Signal Processing and System Theory</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>D?bler</surname></persName>
							<email>mario.doebler@iss.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Signal Processing and System Theory</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
							<email>bin.yang@iss.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Signal Processing and System Theory</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning and Self-Training for Unsupervised Domain Adaptation in Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks have considerably improved state-of-the-art results for semantic segmentation. Nevertheless, even modern architectures lack the ability to generalize well to a test dataset that originates from a different domain. To avoid the costly annotation of training data for unseen domains, unsupervised domain adaptation (UDA) attempts to provide efficient knowledge transfer from a labeled source domain to an unlabeled target domain. Previous work has mainly focused on minimizing the discrepancy between the two domains by using adversarial training or self-training. While adversarial training may fail to align the correct semantic categories as it minimizes the discrepancy between the global distributions, self-training raises the question of how to provide reliable pseudo-labels. To align the correct semantic categories across domains, we propose a contrastive learning approach that adapts category-wise centroids across domains. Furthermore, we extend our method with selftraining, where we use a memory-efficient temporal ensemble to generate consistent and reliable pseudo-labels. Although both contrastive learning and self-training (CLST) through temporal ensembling enable knowledge transfer between two domains, it is their combination that leads to a symbiotic structure. We validate our approach on two domain adaptation benchmarks: GTA5 ? Cityscapes and SYNTHIA ? Cityscapes. Our method achieves better or comparable results than the state-of-the-art. We will make the code publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal in semantic image segmentation is to assign the correct class label to each pixel. This makes it suitable for complex image-based scene analysis that is required in applications like automated driving. However, in order to generate a labeled training dataset, pixel-level annotation must first be performed by humans. Since detailed manual label-ing can take 90 minutes per image <ref type="bibr" target="#b7">[8]</ref>, it is associated with high costs. A potential workaround would be to generate the images and corresponding segmentation maps synthetically using computer game environments like Grand Theft Auto V (GTA5) <ref type="bibr" target="#b28">[29]</ref>. However, even current segmentation models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1]</ref> do not generalize well to data from a different domain. In fact, their segmentation performance decreases drastically when there is a discrepancy between the training and test distribution as in a synthetic-to-real scenario.</p><p>The research field of unsupervised domain adaptation (UDA) studies how to transfer knowledge from a labeled source domain to an unlabeled target domain. The aim is to achieve the best possible results in the target domain, whereas the performance in the source domain is not considered. Current methods for UDA address the problem by minimizing the distribution discrepancy between the domains while doing a supervised training on source data. The distribution alignment can take place in the pixel space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>, feature space <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>, output space <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24]</ref>, or even in several spaces in parallel.</p><p>While adversarial training (AT) <ref type="bibr" target="#b9">[10]</ref> is commonly used to minimize the distribution discrepancy between domains, it can fail to align the correct semantic categories. This is because adversarial training minimizes the mismatch between global distributions rather than class-specific ones, which can negatively affect the results <ref type="bibr" target="#b6">[7]</ref>. This is also true for discrepancy measures such as the maximum mean discrepancy <ref type="bibr" target="#b30">[31]</ref>, which can be minimized without aligning the correct class distributions across domains <ref type="bibr" target="#b18">[19]</ref>. To address the problem of misaligned classes across domains, we rely on contrastive learning (CL) <ref type="bibr" target="#b11">[12]</ref>. The basic idea of CL is to encourage positive data pairs to be similar and negative data pairs to be apart. To perform domain adaptation and match the features of the correct semantic categories, positive pairs consist of features from the same class but different domains while negative pairs are from different classes and possibly from different domains <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Due to the lack of labels in the target domain, the class of each target feature must be determined based on the predictions of the model. However, since different class prior probabilities can bias the final segmentation layer towards the source domain, we extend our approach with self-training (ST), i.e. using target predictions as pseudolabels. We start from the observation that target pixels are predicted with high uncertainty <ref type="bibr" target="#b39">[40]</ref>. Moreover, these uncertain predictions may also vary between different classes during training and are therefore usually not considered for self-training. By using a memory-efficient temporal ensemble that combines the predictions of a single network over time <ref type="bibr" target="#b20">[21]</ref>, we obtain the predictive tendency of the model. This allows us to create robust pseudo-labels even for the uncertain predictions that have high information content. The temporal ensemble has the additional advantage that pseudo-labels are updated directly during training, which reduces the computational complexity compared to a separate stage-wise recalculation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>We summarize our contributions as follows: First, we extend contrastive learning and self-training using a memoryefficient temporal ensemble to UDA for semantic segmentation. Second, we empirically show that both approaches are able to transfer knowledge between domains. Furthermore, we show that combining our contrastive learning and self-training (CLST) approach leads to a symbiotic setup that yields competitive and superior state-of-the-art results for GTA5 ? Cityscapes and SYNTHIA ? Cityscapes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>While the focus of UDA was initially on classification, interest in UDA for semantic segmentation has grown rapidly in the last few years. Since this work investigates UDA for semantic segmentation, the following literature review will mainly focus on this topic. In addition, we give a short review on self-ensembling and contrastive learning.</p><p>Adversarial Learning: AT can be applied to align the distributions of two domains in pixel space, feature space, and output space. In pixel space, the main idea is to transfer the appearance of one domain to the style of other. Thus, it is assumed that the geometric structure in both domains is approximately the same and the difference is mainly in texture and color. A very common approach uses a CycleGanbased architecture <ref type="bibr" target="#b47">[48]</ref> to transfer source images to the style of the target domain <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>. Since the transformation does not change the content, the source labels can be reused to train the model on target-like images in a supervised manner. Similarly, <ref type="bibr" target="#b6">[7]</ref> uses adaptive instance normalization <ref type="bibr" target="#b17">[18]</ref> in combination with a single Generative Adversarial Network <ref type="bibr" target="#b9">[10]</ref> to augment source images into different targetlike styles. Another option is to use adversarial training to minimize the discrepancy between feature distributions. In this case, the discriminator takes on the role of a domain classifier, which must decide whether the features belong to the source or target domain. For semantic segmentation, this approach was first proposed in <ref type="bibr" target="#b15">[16]</ref> and is also used in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref>. In <ref type="bibr" target="#b5">[6]</ref>, this approach is extended by additionally giving each class its own domain classifier, which further matches the individual class distributions between domains. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref> use adversarial training to align the output spaces on pixel-level and patch level, respectively. Meanwhile, aligning the output distributions with adversarial training is also used in several publications as either a basic component or for warm-up <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Training:</head><p>In ST, the target predictions are converted into pseudo-labels, which are then used to minimize the cross-entropy. Since the quality of the pseudo-labels is crucial to this approach, <ref type="bibr" target="#b43">[44]</ref> combines the predictions of three models and iteratively repeats the training of the models, followed by the recalculation of the pseudo-labels. Similarly, <ref type="bibr" target="#b45">[46]</ref> improves their quality by averaging the predictions of two different outputs of the same network. Another commonly used strategy tries to convert only the correct target predictions into pseudo-labels. <ref type="bibr" target="#b31">[32]</ref>, for example, attempts to find them by combining the output of two discriminators with the confidence of a segmentation classifier. <ref type="bibr" target="#b46">[47]</ref> does this by considering the pixel-wise uncertainty of the predictions, which is estimated during training. <ref type="bibr" target="#b44">[45]</ref> converts predictions into pseudo-labels only if the target features are within a certain range of the nearest category-wise source feature mean. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> use the softmax output as confidence measure and incorporate only predictions above a certain threshold into the training process. In doing so, they assume that a higher prediction probability is associated with higher accuracy. In order to make self-training less sensitive to incorrect pseudo-labels, <ref type="bibr" target="#b48">[49]</ref> uses soft pseudo-labels and smoothed network predictions. Unlike the previously mentioned approaches, <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b2">[3]</ref> do not explicitly create pseudo-labels but exploit entropy minimization and a maximum squares loss to conduct self-training. Again, both methods can further improve performance when only confident samples are considered.</p><p>Self-Ensembling: An ensemble considers the outputs of multiple independent models for the same input sample to arrive at a more reliable prediction. The basic idea is that different models make different errors, which can be compensated for in the majority. Ensembling can be employed in ST to create better pseudo-labels for the next training stages <ref type="bibr" target="#b43">[44]</ref>. In semi-supervised learning, a special variant called self-ensembling has shown remarkable results <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. In this case, there is usually only one trainable model that minimizes an additional consistency loss between two different predictions of the same sample. While one prediction remains the output of the trainable network, current methods differ mainly in the creation of the second prediction. <ref type="bibr" target="#b20">[21]</ref> uses an exponential moving average (EMA) to combine predictions generated at different times during training. This is also known as temporal ensemble.</p><p>[21] also proposes to generate the second prediction with the same model using a different dropout mask and augmentations. <ref type="bibr" target="#b34">[35]</ref> extends this idea and proposes a Mean Teacher (MT) framework where there is a second (non-trainable) model whose weights are updated with an EMA over the actual trainable weights. While <ref type="bibr" target="#b8">[9]</ref> extends the former idea to UDA for classification, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref> apply the MT framework to UDA for semantic segmentation.</p><p>Contrastive Learning: CL <ref type="bibr" target="#b11">[12]</ref> is a framework that learns representations by contrasting positive pairs against negative pairs. It has recently dominated the field of selfsupervised representation learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Contrastive learning encourages representations of positive pairs (typically different augmentations from an image, preserving semantic information) to be similar and negative pairs (different image instances) to be apart. In the area of domain adaptation (DA) for classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, CL has been applied to align the feature space of different domains. Positive pairs are generated by using samples of the same class, but different domains. Negative pairs are chosen so that they belong to different classes and potentially different domains. In <ref type="bibr" target="#b26">[27]</ref>, the contrastive loss is realized through minimizing a Frobenius norm. <ref type="bibr" target="#b18">[19]</ref> modifies the maximum mean discrepancy <ref type="bibr" target="#b30">[31]</ref> to correspond to a contrastive loss. Whereas existing ideas in DA compute the contrastive loss in the feature space, recent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> shows that using a separate projection space for the contrastive loss is very beneficial in the setting of self-supervised representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Definitions: In UDA, we have a set</p><formula xml:id="formula_0">X s = {x s i , y s i } N i=1 of N source images x s i with corresponding segmenation maps y s i , as well as M unlabeled target images X t = {x t i } M i=1 .</formula><p>The indices s and t denote the source and target domain. For simplicity, the image dimensions of both domains are described by x s i , x t i ? R H?W ?3 . Furthermore, we explicitly divide the network into a feature extractor F with parameters ? F and a segmentation head D with parameters ? D . The latter outputs a softmax probability map p i ? R H?W ?C , where C is the total number of classes. The corresponding hard prediction? i and the source segmentation maps have dimensions? i , y s i ? {0, 1} H?W ?C and are thus one-hot encoded. As shown in <ref type="figure">Fig. 2 a)</ref>, we follow recent findings in self-supervised representation learning and integrate a projector P with parameters ? P into our network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Its task is to project the extracted feature maps f i ? R H ?W ?M into a projection space z i ? R H ?W ?K , where K is usually smaller than M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Training</head><p>To train the network in a supervised manner, the segmentation maps y s i of the source domain are used to compute a weighted pixel-wise cross-entropy (CE) loss</p><formula xml:id="formula_1">L s CE = ?1 HW C H h=1 W w=1 C c=1 ? s c y s(h,w,c) i log(p s(h,w,c) i ),<label>(1)</label></formula><p>where ? s c is a class balancing term that will be explained later. Although there is no label information available in the target domain, it is possible to minimize a second CE loss by converting the network predictions into pseudo-label? y t i . The cross-entropy loss is then given by</p><formula xml:id="formula_2">L t CE = ?1 HW C H h=1 W w=1 C c=1 ? t c? t(h,w,c) i log(p t(h,w,c) i ),</formula><p>(2) where ? t c is again a weighting term. Minimizing both losses jointly will close the domain gap, if reliable peudo-labels can be provided. Although there may be many noisy predictions, especially in the early stages of the training, it is likely that some pixels will be correctly predicted by the model with some confidence. This is because not all pixels have the same transfer difficulty and some may be easier to transfer than others <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. To find these pixels during training, we use the entropy of the softmax values</p><formula xml:id="formula_3">e t(h,w) i = ? C c=1 p t(h,w,c) i log(p t(h,w,c) i ).<label>(3)</label></formula><p>Then, only the predictions that are among the most certain 30% in terms of their entropy are converted into pseudolabels, while all other predictions are ignored. This approach has already proven successful for MinEnt <ref type="bibr" target="#b39">[40]</ref>. Although using the entropy as a guidance for selecting more reliable pseudo-labels works, their quality may remain moderate because there can be false predictions with high confidence. Therefore, we switch to the following strategy after a few epochs.</p><p>Temporal Ensemble: As mentioned earlier, most of the current self-training approaches try to find the correct target predictions by using confidence measures like the entropy or softmax probability. While this strategy can help to avoid including too many false predictions in the training, the information of uncertain but mostly correct predictions is also neglected. Since it is the uncertain forecasts that contain a lot of information, we try to generate reliable pseudo-labels for them as well. This is achieved by considering the predictive tendency of the model for a given sample over time. More precisely, this tendency is extracted by using a temporal ensemble. It considers the numerous target predictions made by the model during training for a specific image. This leads to a smoothing of the noisy predictions. Note that due to the high uncertainty contained in target predictions <ref type="bibr" target="#b39">[40]</ref>, they may even vary between a few classes, especially at the beginning of the training. For a given image x t i , the temporal ensemble is realized through a tensor t i ? N H?W ?C 0 that additively collects the target predictions? t i ? {0, 1} H?W ?C at time n. (Eq. 4). As the quality of the predictions improves during training, y t i is multiplied by a stepwise increasing integer ? n ? N</p><formula xml:id="formula_4">t i,n ? t i,n?1 + ? n? t i,n .<label>(4)</label></formula><p>This ensures that more recent predictions have a larger impact on the final decision. To generate pseudo-labels from the tensor t i , majority-voting is used. It selects the class with the most votes as hard prediction? t i . Although it would be possible to exclude pseudo-labels that have only a slight majority in the temporal ensemble, we do not apply any thresholds here and leave this open to further research.</p><p>The advantage of this realization is that the ensemble can be implemented in a memory-efficient way. First, depending on the number of epochs and ?, it allows to use the uint8 format for t i . (In our experiments, ? n did not exceed 4). Second, due to the one-hot encoding of? t i , the ensemble can be realized as a sparse tensor. This drastically reduces memory requirements when the number of classes C is large. For example, in one of our experiments with C = 19, only 7.2% of the elements were non-zero.</p><p>Class Balancing: The motivation for the class balancing terms ? s c and ? t c in Eq. 1 and Eq. 2 arises from the possibility that there may be a discrepancy between the source and target class prior probabilities <ref type="bibr" target="#b42">[43]</ref>. Such a difference can bias the final segmentation layer towards the source domain and thus negatively affect the segmentation of target samples <ref type="bibr" target="#b44">[45]</ref>. To circumvent this problem, we use the source labels and the target pseudo-labels to compute ? s c and ? t c . Since pseudo-labels can better capture the presence of a class in an image than its exact number of pixels, the following strategy is used for both source and target domain. Instead of computing pixel-wise class prior probabilities, only the occurence probability o s c , o t c ? [0, 1] of each class c in the dataset is considered. For the source domain, it is given by o s c = N c / N , where N c is the number of images containing class c, while N is the total number of images in the dataset <ref type="bibr" target="#b42">[43]</ref>. To save computational resources, o s c and o t c are approximated online during training starting from probability one for each class. The weighting terms for the source and target domain are then defined by</p><formula xml:id="formula_5">? c (o c ) = min(1/o c , ?) C i=1 min(1/o i , ?) ,<label>(5)</label></formula><p>where ? ? 1 is a hyperparamter to avoid a too large weighting of very rare classes. The class balancing terms have the following effects. First, by using ? t c , the model now focuses less on classes that occur frequently in the target dataset. This allows for better knowledge transfer for classes that are rare and perhaps more difficult to transfer <ref type="bibr" target="#b49">[50]</ref>. Sec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class 1 Class 2 Centroids</head><p>Before Adaptation Contrastive Adaptation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive learning</head><p>Similar to <ref type="bibr" target="#b44">[45]</ref>, our method is based on the observation that pixels of the same class cluster in the feature space. However, due to the discrepancy between the feature distributions, this observation only applies to features of the source domain and not across domains. This circumstance is also reflected on the left side of <ref type="figure" target="#fig_0">Figure 1</ref>. To cluster features of the same class across domains, we use a contrastive loss (right side of <ref type="figure" target="#fig_0">Fig. 1</ref>). Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> and in contrast to previous CL approaches proposed for UDA for classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, the contrastive loss is computed in a projection space z i ? R H ?W ?K , see <ref type="figure">Fig. 2 a)</ref>.</p><p>First, class-wise mean projections are calculated using all source images in the current batch. For this, it is assumed that the source segmentation masks can be used to assign a class to each projection. Since projections of misclassified features can have an unfavorable effect on the corresponding class means, only the correctly segmented ones are considered. They can be extracted by comparing each prediction with its corresponding ground truth, as in <ref type="bibr" target="#b40">[41]</ref>;</p><formula xml:id="formula_6">y s(h,w,c) i = 1 if? s(h,w,c) i = y s(h,w,c) i = 1, 0 otherwise.<label>(6)</label></formula><p>Note that the predictions and labels from Eq. 6 can have a different hight and width than the source projections z s i . In this case,? s i and y s i are first resized using nearest neighbor interpolation. Then for each class contained in the current batch, class-wise mean projections are calculated. This is accomplished by averaging over the height H and width W of all B source projection maps in the current batch</p><formula xml:id="formula_7">m s c = B i=1 h,w? s(h,w,c) i z s(h,w) i B i=1 h,w? s(h,w,c) i , ? R K<label>(7)</label></formula><p>This approach is also illustrated in <ref type="figure">Fig. 2 b)</ref> for a batch consisting of only one source image. While Eq. 7 could be calculated using the entire source dataset rather than B samples, this would require propagating all source images through the network once. Furthermore, this procedure would have to be repeated several times, since the projections and thus their mean values will most likely change during training. To still extract a global source centroid g s c ? R K for each class c without much computational effort, an exponential moving average is used</p><formula xml:id="formula_8">g s c = (1 ? ?)g s c + ?m s c ,<label>(8)</label></formula><p>where ? is a momentum term. These global source centroids are also shown in yellow in <ref type="figure" target="#fig_0">Fig. 1</ref>. For the target domain, Eq. 7 is slightly modified. First,? s i is replaced by (resized) pseudo-labels? t i ? {0, 1} H ?W ?C . Second, a separate class-wise mean projection m t i,c is computed for each target image x t i in the current batch. The reason is that each m t i,c should roughly cluster around its respective source centre g s c . Finally, the contrastive loss is given by</p><formula xml:id="formula_9">L CL = ? C c=1 ? t c log exp(sim(g s c , m t i,c )) C j=1 1 [j =c] exp(sim(g s j , m t i,c ))<label>(9)</label></formula><p>where 1 [j =c] ? {0, 1} is the indicator function evaluating to 1 iff j = c and sim(u, v) = u T v/ u v denotes a cosine similarity, which is the dot product between two l 2normalized vectors u and v. Note that a similar loss was already used in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>. It can be shown that L CL becomes minimal if the cosine similarity becomes maximal for projections of the same class (positive pairs) and minimal for different classes (negative pairs). Thus, the intra-class variance across domains is minimized while the inter-class variance is maximized. This is also denoted by the arrows in <ref type="figure" target="#fig_0">Fig. 1</ref>. However, the maximization of the inter-class variance is only implicit because it is the target mean representations m t i,c which force the global source centroids g s c to be apart. Since some classes may not always be present in a batch of samples, the contrastive loss is weighted by ? t c (Eq. 5). In this case, it encourages the model to focus more on aligning and separating rare classes when they appear in the batch. Finally, note that although a contrastive loss is used, only at most C 2 distances need to be calculated for each target image. This makes the approach suitable for models with large feature maps in terms of height and width.</p><p>By combining self-training through temporal ensembling and contrastive learning, we create a symbiotic framework in which both methods can directly benefit each other. ST mitigates the bias of the last segmentation layer towards the source domain improving the target predictions. Needless to say, both ST and CL profit from better pseudo-labels. Additionally, for CL, better pseudo-labels improve the quality of the target class means m i,c . This allows the semantic categories to be clustered more accurately across domains, resulting again in a better target prediction? t i . To sum up, the following loss function is minimized</p><formula xml:id="formula_10">L = L s CE + ? ST L t CE + ? CL L CL ,<label>(10)</label></formula><p>where ? ST and ? CL are two hyperparameters balancing the influence of self-training and contrastive learning. Our overall training procedure is shown in Algorithm 1. The derivatives of the losses are computed as</p><formula xml:id="formula_11">? ? F L, ? ? D (L s CE , L t CE ) and ? ? P L CL .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Network Architecture: Similar to <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3]</ref> we deploy the DeepLab-V2 <ref type="bibr" target="#b0">[1]</ref> framework with a ResNet-101 as feature extractor F. As is common practice, we initialize F with weights pre-trained on ImageNet and freeze all batch normalization layers. The final segmentation head D consists of an Atrous Spatial Pyramid Pooling (ASPP) head. We fix the dilation rates of the ASPP head to {6, 12, 18, 24} as it was done in previous work. To create a symmetrical network architecture, we use a similar ASPP head for the projector P as described before. The only difference is that the projector outputs a tensor with K = 256 channels and does not use any non-linearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 CLST Training Procedure</head><p>Require: Iterations T , Iterations for warm-up R, Use ensemble pseudo-labels after K iterations 1: Initialize ? D and ? P randomly 2: Initialize ? F with ImageNet pre-trained weights <ref type="bibr">3:</ref> Initialize ensemble tensor t with zeros 4: for j = 1, 2, . . . , T do Update source weighting ? s c using Eq. 5 <ref type="bibr">7:</ref> Calculate L s CE using Eq. 1 <ref type="bibr">8:</ref> Calculate m s c using Eq. 7 <ref type="bibr">9:</ref> Update ? using a cosine decay <ref type="bibr">10:</ref> Update global source centers g s c using Eq. 8 <ref type="bibr">11:</ref> Get target softmax probability maps p t i 12:</p><p>if j &gt; R then <ref type="bibr">13:</ref> Update ensemble using Eq. 4 <ref type="bibr">14:</ref> end if <ref type="bibr">15:</ref> if j &gt; K then <ref type="bibr">16:</ref> Create pseudo-labels from ensemble 17: Update target weighting ? t c using Eq. 5 <ref type="bibr">21:</ref> if j &gt; R then <ref type="bibr">22:</ref> Calculate L t CE and L t CL using Eq. 2 and 9 <ref type="bibr">23:</ref> end if <ref type="bibr">24:</ref> Update parameters ? F , ? D , ? P 25: end for Implementation Details: Our implementation adopts the PyTorch deep learning framework <ref type="bibr" target="#b27">[28]</ref>. Most of the hyperparameters are taken directly from the base architecture from <ref type="bibr" target="#b37">[38]</ref>. We train the network using SGD with Nesterov acceleration to speed up convergence. We deploy a polynomial learning rate decay with an initial learning rate set to 2.5 ? 10 ?4 and an exponent of 0.9. The momentum of the optimizer is set to 0.9 and we use a weight decay of 5 ? 10 ?4 . Furthermore, we use 0.1 for both ? ST and ? CL . For the momentum term ? in Eq. 8 we use a cosine decay starting from 0.02. For training and testing, we rescale all source images to size 720 ? 1280 and all target images to 512 ? 1024 <ref type="bibr" target="#b37">[38]</ref>. In addition, we also investigate the impact of color jittering and Gaussian blur, which was recently used in <ref type="bibr" target="#b36">[37]</ref>. We train our model using batches with 2 source images and 2 target images. The model is pretrained for 6k iterations on the source domain before we switch to the loss function from Eq. 10. The pseudo-labels from the temporal ensemble are used after 15k iterations. ? is initially set to 1 and incremented by 1 every 25k iterations. All iteration-related parameters were chosen to be approximately multiples of the number of target images and received no tuning. We set ? to 5, which limits the weight-ing of classes that occur in less than 20% of the images in the dataset. This was set by inspecting the class occurence probabilities in the GTA5 dataset and was also not tuned.</p><p>Datasets and Metric: We evaluate our approach in the challenging synthetic-to-real scenario, where Cityscapes <ref type="bibr" target="#b7">[8]</ref> is used as the real-world target domain. Cityscapes contains 2975 training and 500 validation images with a resolution of 1024 ? 2048. One of the synthetic source datasets is GTA5 <ref type="bibr" target="#b28">[29]</ref>, which contains 24966 synthesized frames of size 1052 ? 1914 of the well known Grand Theft Auto V video game. We evaluate GTA5 ? Cityscapes using the common 19 classes. The second synthetic source dataset is SYNTHIA <ref type="bibr" target="#b29">[30]</ref>, which has 9400 images in total and only shares 16 classes with Cityscapes. Following <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26]</ref>, we evaluate this transfer with respect to all 16 classes and for a subset consisting of 13 classes. We train our model using all source images and evaluate it on the Cityscapes validation set. As a metric, we use the widely adopted mean-intersection-over-union (mIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>We begin by examining the impact of contrastive learning and self-training on the final results by setting either ? ST or ? CL to zero. <ref type="table" target="#tab_1">Table 1</ref> a) shows the best mIoU for GTA5 ? Cityscapes. In addition, it includes the results of the adversarial approach AdaptSegNet <ref type="bibr" target="#b37">[38]</ref> that uses two discriminators to align the output distributions of the segmentation model at different depths. As can be seen, both CL and ST outperform the adversarial baseline and enable knowledge transfer between domains. When CL is combined with ST (CLST), the result improves by 1.7% mIoU compared to plain ST and we reach 49.1% mIoU. This illustrates that CL and ST can indeed benefit from each other. To test, why the contribution of contrastive learning is much lower than that of self-training, we examined the influence of the two class balancing terms ? s c and ? t c . We observed  a drop in performance of 1.0% mIoU when only ? t c was used and more than 3% mIoU when neither L s CE nor L t CE were weighted. Since contrastive learning cannot mitigate harmful biases in the final segmentation layer towards the source domain, or easy-to-transfer classes, the quality of the pseudo-labels remains moderate. As a result, the target means m t i,c include more false predictions, causing a worse alignment. Finally, we added color jittering and Gaussian blur (+Aug), which increased the performance further.</p><formula xml:id="formula_12">a) GTA5 ? Cityscapes Method L CL L t CE</formula><p>Similar trends can be observed for SYNTHIA ? Cityscapes, where the results with respect to all 16-and only 13-classes (mIoU*) are presented in <ref type="table" target="#tab_1">Table 1</ref> b). Again, the contribution of L t CE is crucial, but the results can be improved by 1.3% mIoU by adding the contrastive component L CL . In this case, the effect of the augmentations is even larger and an increse of 2.3% mIoU can be observed.</p><p>To investigate the influence of our ASPP projector, we calculated the contrastive loss directly on the feature space f i . As it turned out, the results without projector were around 0.5% mIoU worse for GTA5 ? Cityscapes. We also experimented with different non-linear projectors having different number of layers and kernels. We found that none of them performed better than our linear ASPP projector, which is symmetrical to our segmentation head and therefore may calculate similar local and global features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Space Visualization</head><p>To visualize the quality of our learned representations, we compute for each target image in the Cityscapes validation set its own category-wise feature mean and visualize all of them using UMAP <ref type="bibr" target="#b24">[25]</ref>. This allows us to make a direct comparison with a model trained on source data only. The results are illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. If a model is trained on source data only a), the clusters are close to each other and are overlapping. As can be seen in b), our CLST algorithm forms much more separable clusters. In c), the global source centroids g s c are now visible in black. Both source and target domain are in similar regions of the feature space, suggesting successful transfer. Note that these experiments were conducted without any data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with state-of-the-art methods</head><p>In <ref type="table">Table 2</ref>, we compare our method with the current state-of-the-arts. We mainly show results that also use the DeepLab-V2 with a ResNet-101 as backbone. The only exception to this is CAG <ref type="bibr" target="#b44">[45]</ref>, which uses the more powerful DeepLabv3+ <ref type="bibr" target="#b1">[2]</ref>. However, since this method minimizes an l 2 -norm between source and target features and also uses self-training, it was included in the comparison. Furthermore, we cite the results of other approaches directly from the corresponding papers. It is worth mentioning that the cited results may include several training stages <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> or transfered source images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>GTA5 ? Cityscapes: As shown in <ref type="table">Table 2</ref> a), by using similar augmentations like DACS <ref type="bibr" target="#b36">[37]</ref>, our method (CLST+Aug) beats most of the state-of-the-arts without exiting the training loop to recompute pseudo-labels. While this makes our method significantly less computationally expensive, it comes with slightly worse performance. This is due to the batch normalization layers, which in our experiments give better results in validation mode than in the training mode that is used for the temporal ensemble. If performance is most important, the results can be further improved by creating new pseudo-labels and then fine-tune (FT) the model on the target domain, minimizing only L t CE from Eq. 2. This approach was also used by IAST and improves our results by 1% mIoU for a model that was trained with augmentation (CLST + Aug + FT) and without augmentation (CLST + FT) in the previous stage. Note that the pseudo-labels were created without applying a certainty based threshold. Although other self-training based methods like CAG or IAST may yield competitive results, they rely on an adversarial warm-up. For example, CAG reports a drop of 6.3% mIoU when the model was not pre-trained  <ref type="table">Table 2</ref>. Comparison to state-of-the-art results for Cityscapes validation set and task a) GTA5 ? Cityscapes and b) SYNTHIA ? Cityscapes. In the latter case, we report the mIoU with respect to all 16-and only 13-classes, excluding all classes marked with "*".</p><p>with AT. Our approach, on the other hand, does not require such a warm-up, but may also benefit from it.</p><p>SYNTHIA ? Cityscapes: Similar results can be observed in <ref type="table">Table 2</ref> b), where we show the mIoU with respect to all 16-and only 13-classes, excluding the classes marked with "*". Again, CLST + Aug outperforms most of the other methods, including DACS. If we apply the same strategy as explained before and fine-tune our model on the target domain, we observe an increase of 1.3% mIoU for a model that was trained with augmentation in the first stage. When trained without augmentation, the increase was 1.6% mIoU. Overall, we achieve equivalent results to IAST for 16 classes and better results for 13 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a symbiotic setup for UDA for semantic segmentation. It combines recent ideas in contrastive learning, using a separate projection space, with self-training. For reliable and consistent pseudo-labels over time a memory-efficient temporal ensemble is used. Each individual component already contributes to a knowledge transfer between domains. It is their combination that yields better or equivalent results than the state-of-the-art on two common synthetic-to-real benchmarks: GTA5 ? Cityscapes and SYNTHIA ? Cityscapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: before adaptation, right: after aligning categorywise centroids across domains using a contrastive loss. ond, by additionally using ? s c , the class prior probabilities of both domains are approximately aligned<ref type="bibr" target="#b42">[43]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 KFigure 2</head><label>22</label><figDesc>. a) Network architecture of our proposed approach, consisting of a feature extractor F, a segmentation head D as well as a projector P. b) Procedure for calculating category-wise mean projections mi,c using a resized segmentation map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 :</head><label>5</label><figDesc>Sample minibatch {x s i , y s i }, {x t i } from X s and X t 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>UMAP feature space visualization of class-wise mean representations for Cityscapes validation set. a) After training on source data only. b) Using our contrastive and self-training approach (CLST). c) CLST with global source centers shown in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>mIoU</cell></row></table><note>Component analysis for a) GTA5 ? Cityscapes and b) SYNTHIA ? Cityscapes. In the latter case, the results are shown with respect to all 16-and only 13-classes (mIoU*).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 ADVENT [40] 89.4 33.1 81.0 26.6 26.8 27.2 33.5 24.7 83.9 36.7 78.8 58.7 30.5 84.8 38.5 44.5 1.7 31.6 32.4 45.5 CBST [50] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 MaxSquare [3] 89.4 43.0 82.1 30.5 21.3 30.3 34.7 24.0 85.3 39.4 78.2 63.0 22.9 84.6 36.4 43.0 5.5 34.7 33.5 46.4 PatchAlign [39] 92.3 51.9 82.1 29.2 25.1 24.5 33.8 33.0 82.4 32.8 82.2 58.6 27.2 84.3 33.4 46.3 2.2 29.5 32.3 46.5 PLCA [20] 84.0 30.4 82.4 35.3 24.8 32.2 36.8 24.5 85.5 37.2 78.6 66.9 32.8 85.5 40.4 48.0 8.8 29.8 41.8 47.7 FT 92.8 53.5 86.1 39.1 28.1 28.9 43.6 39.4 84.6 35.7 88.1 63.9 38.3 86.0 41.6 50.6 0.1 30.4 51.7 51.6 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8 42.6 48.9 MaxSquare [3] 82.9 40.7 80.3 10.2 0.8 25.8 12.8 18.2 82.5 82.2 53.1 18.0 79.0 31.4 10.4 35.6 41.4 48.2 PatchAlign [39] 82.4 38.0 78.6 8.7 0.6 26.0 3.9 11.1 75.5 84.6 53.5 21.6 71.4 32.6 19.3 31.7 40.0 46.5 PLCA [20] 82.6 29.0 81.0 11.2 0.2 33.6 24.9 18.3 82.8 82.3 62.1 26.5 85.6 48.9 26.8 52.2 46.8 54.0 MRNet [46] 83.1 38.2 81.7 9.3 1.0 35.1 30.3 19.9 82.0 80.1 62.8 21.1 84.4 37.8 24.5 53.3 46.5 36.4 80.6 13.3 0.3 25.5 22.4 14.9 81.8 77.4 56.8 25.9 80.7 45.3 29.9 52.0 45.2 52.9 CAG [45] 84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3 44.5 -IAST [26] 81.9 41.5 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7 49.8 57.0 DACS [37] 80.5 25.1 81.9 21.4 2.8 37.2 22.6 23.9 83.6 90.7 67.6 38.3 82.9 38.9 28.4 47.5 48.3 54.8 CLST 79.0 36.7 81.3 12.7 0.3 30.4 26.3 21.5 83.7 86.2 56.4 21.1 84.6 44.8 20.7 53.4 46.2 53.5 CLST + FT 81.1 39.1 81.9 15.1 0.5 30.6 27.5 23.3 84.5 86.3 58.0 23.6 85.7 48.1 25.1 55.5 47.8 55.3 CLST + Aug 85.9 45.7 81.9 13.8 0.3 29.6 30.5 24.2 83.6 87.9 57.7 25.3 85.2 46.9 22.3 54.8 48.5 56.3 CLST + Aug + FT 88.0 49.2 82.2 16.3 0.4 29.2 31.8 23.9 84.1 88.0 59.1 27.2 85.5 46.6 28.9 56.5 49.8 57.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">a) GTA5 ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="21">AdaptSeg [38] 86.5 MRNet [46] 90.5 35.0 84.6 34.3 24.0 36.8 44.1 42.7 84.5 33.6 82.5 63.1 34.4 85.8 32.9 38.2 2.0 27.1 41.8 48.3</cell></row><row><cell>BDL [23]</cell><cell cols="20">91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5</cell></row><row><cell>SIM [41]</cell><cell cols="20">90.1 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5 1.9 30.4 39.0 49.2</cell></row><row><cell>CCM [22]</cell><cell cols="20">93.5 57.6 84.6 39.3 24.1 25.2 35.0 17.3 85.0 40.6 86.5 58.7 28.7 85.8 49.0 56.4 5.4 31.9 43.2 49.9</cell></row><row><cell>CAG [45]</cell><cell cols="20">90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2</cell></row><row><cell>IAST [26]</cell><cell cols="20">93.8 57.8 85.1 39.5 26.7 26.2 43.1 34.7 84.9 32.9 88.0 62.6 29.0 87.3 39.2 49.6 23.2 34.7 39.6 51.5</cell></row><row><cell>DACS [37]</cell><cell cols="20">89.9 39.6 87.8 30.7 39.5 38.5 46.4 52.7 87.9 43.9 88.7 67.2 35.7 84.4 45.7 50.1 0.0 27.2 33.9 52.1</cell></row><row><cell>CLST</cell><cell cols="20">90.5 42.6 83.8 35.0 26.5 24.5 40.8 35.5 84.7 37.2 81.8 63.2 36.4 85.4 41.1 51.7 0.1 25.2 47.4 49.1</cell></row><row><cell>CLST + FT</cell><cell cols="20">91.2 44.5 84.4 35.9 27.4 24.0 41.2 37.3 85.3 39.7 83.1 63.7 37.6 85.9 43.2 50.8 0.1 27.7 49.7 50.1</cell></row><row><cell>CLST + Aug</cell><cell cols="20">92.6 52.8 85.6 35.3 27.4 28.4 42.5 37.0 84.4 36.2 87.8 63.2 35.9 86.2 43.6 49.7 0.4 25.1 48.5 50.6</cell></row><row><cell cols="12">CLST + Aug + b) SYNTHIA ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall*</cell><cell>fence*</cell><cell>pole*</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell></cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell cols="3">mIoU mIoU*</cell></row><row><cell>AdaptSeg [38]</cell><cell cols="3">84.3 42.7 77.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.7</cell><cell cols="10">7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3</cell><cell>-</cell><cell></cell><cell>46.7</cell></row><row><cell>ADVENT [40]</cell><cell cols="3">85.6 42.2 79.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.4</cell><cell cols="10">8.1 80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0</cell><cell>-</cell><cell></cell><cell>48.0</cell></row><row><cell>CBST [50]</cell><cell cols="20">68.0 53.8</cell></row><row><cell>BDL [23]</cell><cell cols="3">86.0 46.7 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3</cell><cell>-</cell><cell></cell><cell>51.4</cell></row><row><cell>SIM [41]</cell><cell cols="3">83.0 44.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">17.1 15.8 80.5 81.8 59.9 33.1 70.2 37.3 28.5 45.8</cell><cell>-</cell><cell></cell><cell>52.1</cell></row><row><cell>CCM [22]</cell><cell>79.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This publication was created as part of the research project "KI Delta Learning" (project number: 19A19013R) funded by the Federal Ministry for Economic Affairs and Energy (BMWi) on the basis of a decision by the German Bundestag.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain transfer through deep activation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="590" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pixel-level cycle association: A new perspective for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="440" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12197</idno>
		<title level="m">stance adaptive self-training for unsupervised domain adaptation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and rkhs-based statistics in hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Regularizing proxies with multi-adversarial training for unsupervised domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12282</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">103889</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12635" to="12644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-ensembling attention networks: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5581" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11164</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
