<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GA-Nav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divya</forename><surname>Kothandaraman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Adarsh</roleName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagan</forename><surname>Sathyamoorthy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasun</forename><surname>Weerakoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
						</author>
						<title level="a" type="main">GA-Nav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(Code and Videos at https://gamma.umd.edu/offroad)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient learning-based method for identifying safe and navigable regions in off-road terrains and unstructured environments from RGB images. Our approach classifies terrains based on their navigability levels using coarsegrained semantic segmentation. We propose GA-Nav, a novel group-wise attention mechanism to distinguish between navigability levels of different terrains, which can improve different backbone designs. Our group-wise attention loss enables the network to explicitly focus on the different groups' features with low spatial resolution for efficient inference while maintaining a high level of accuracy compared to other SOTA methods. We show through extensive evaluations on the RUGD and RELLIS-3D datasets that our learning algorithm improves visual perception accuracy in off-road terrains for navigation. We compare our approach with prior work on these datasets and achieve an improvement over the state-of-the-art mIoU by 2.25-39.05% on RUGD and 5.17-19.06% on RELLIS-3D. In addition, we deploy GA-Nav on a Clearpath Jackal and a Husky robot for real-world navigation demonstrations. Our approach improves the performance of the navigation algorithm in terms of success rate by 10% and results in smoother trajectories while maintaining the best surface selection capabilities above 80%. Further, our method reduces the false positive rate of forbidden regions by 37.79%. Code, videos, and a full technical report are available at gamma.umd.edu/offroad.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent developments in autonomous driving and mobile robots have significantly increased the interest in outdoor navigation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. In many applications such as mining, disaster relief <ref type="bibr" target="#b2">[3]</ref>, agricultural robotics <ref type="bibr" target="#b3">[4]</ref>, or environmental surveying, the robot must navigate in uneven terrains or scenarios that lack a clear structure or well-identified navigation features.</p><p>A key issue in developing autonomous navigation capabilities in off-road environments is finding safe and navigable regions that can be used by a mobile robot. For instance, terrains like concrete or asphalt are smooth and highly navigable, while rocks or gravel are typically bumpy and may not be navigable. Our goal in this work is to learn to visually differentiate various terrains in unstructured outdoor environments and classify them based on their navigability.</p><p>An important aspect of classifying navigable regions in unstructured environments is using visual perception capabilities such as semantic segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Semantic segmentation is a pixel-level task that assigns a label for every pixel in the image. Prior works in segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> have been limited to structured environments <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> and may not be well-suited, in general, for aiding robot navigation in off-road terrains for several reasons. <ref type="figure" target="#fig_1">Fig. 1</ref>: We highlight the performance of our algorithm on unstructured outdoor terrains. We classify the navigable regions (shown in different colors) using our segmentation algorithm that uses RGB images as the input in real time. The top images correspond to different outdoor terrains where the robots are navigating in an autonomous manner. The bottom images show the terrain segmentation results. Our model achieves an improvement of 2.25 ? 39.05% in terms of mIoU over prior segmentation algorithms on complex outdoor terrains and improves navigation in the real world by 10% in terms of success rate.</p><p>Firstly, many off-road scenes require differentiating terrain classes that have highly similar appearances (e.g., water and a puddle) with overlapping boundaries. In some cases, certain terrain classes could be occupying a small portion of the image, which require highly accurate segmentation. In terms of existing methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, such scenarios lead to similar feature embeddings for multiple classes that can ultimately result in wrong classifications. Furthermore, for navigation, misclassifications of dangerous or forbidden regions for the robot could lead to disastrous consequences.</p><p>Secondly, perception methods that aid off-road robot navigation must manage their computational efficiency without compromising on their segmentation accuracy. Thirdly, it has been shown that in unstructured environments, navigation systems may not require fine-grained semantic segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. For example, it is sufficient to recognize trees and poles as obstacles for collision avoidance rather than segmenting them individually as different types of objects. A coarser approach that classifies various types of objects based on their navigation characteristics is more appropriate for aiding navigation, which is the focus of our method. Main Results: We present GA-Nav, a novel and efficient learning-based approach for identifying the navigability of different terrains from RGB images or videos. Our approach is designed to identify different terrain groups in off-road and unstructured outdoor terrains. We present a novel architecture that finds a good balance between segmentation accuracy and computational overhead and outperforms existing transformer-based segmentation methods. Unlike previous methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, which only focus on finegrained classification, our method implicitly learns how to classify and cluster different classes simultaneously according to annotations, which leads to improved accuracy among different terrains. We show that our method has an advantage over existing methods on coarse-grained segmentation task used for navigation. To demonstrate GA-Nav's benefits for real-time robot navigation, we integrate it with a learningbased navigation approach and evaluate the performance on Clearpath Jackal and Husky robot in unstructured outdoor environments.</p><p>The key contributions of our work include: 1) We propose a novel architecture which fuses a multiscale feature extractor with a transformer architecture. Our group-wise segmentation head can fuse visual features from different scales and explicitly focus on different terrain types, which leads to better accuracy on different surfaces of varying areas. Our approach is compatible with different feature extractors and can improve performance on various SOTA multi-scale backbone designs. We outperform prior methods for semantic segmentation and navigable region classifications in terms of mIoU by 2.25 ? 39.05% on RUGD and 5.17 ? 19.06% on RELLIS-3D. 2) We introduce a group-wise attention loss for fast and accurate inference. We show that with our group-wise attention (GA) loss, we can improve the inference complexity without much performance degradation. Our proposed method results in better performance than existing transformer-based and other segmentation methods with one of the lowest run-times and complexity requirements. 3) We integrate GA-Nav with TERP <ref type="bibr" target="#b12">[13]</ref>, an outdoor navigation planner to highlight the benefits of our segmentation approach for robot navigation in complex outdoor environments. We show that our modified navigation method outperforms other navigation methods by 10% in terms of success rate and, 2-47% in terms of selecting the surface with the best navigability and a decrease of 4.6-13.9% in trajectory roughness. Further, GA-Nav reduces the false positive rate of forbidden regions by 37.79%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Semantic Segmentation</head><p>Semantic segmentation is an important task in computer vision that involves assigning a label to each pixel in an image. The problem has been widely studied in the literature <ref type="bibr" target="#b13">[14]</ref>. Many deep learning architectures have been proposed, including OCRNet <ref type="bibr" target="#b5">[6]</ref>, PSPNet <ref type="bibr" target="#b4">[5]</ref>, etc. Most recently, there have been many transformer-based methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref> that demonstrate better performance than other CNN-based methods but are more computationally expensive. Therefore, there has been increased discussion of more efficient design based on transformers for fast inference time and lower computational cost, including <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. While most of these architectures work well on structured datasets like CityScapes, they do not work well for off-road datasets due to ill-defined boundaries and confusing class features. Segmentation methods like Global Convolution Networks <ref type="bibr" target="#b15">[16]</ref> are designed to deal with complex boundaries and do not scale well in terms of performance due to convoluted and overlapping boundaries that are generally not found in structured datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Navigation in Outdoor Scenes</head><p>Prior work in uneven terrain navigation includes techniques for everything from mobile robots <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> to large vehicles <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. In these algorithms, learning the navigation characteristics of the terrain and its traversability is a crucial step. At a broad level, there are three types <ref type="bibr" target="#b21">[22]</ref> of approaches that are used to determine the terrain features: 1) proprioceptive-based methods, 2) geometric methods, and 3) appearance-based methods. The proprioceptive-based methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> use frequency domain vibration information gathered by the robot sensors to classify the terrains using machine learning techniques. However, these methods require the robot to navigate through the region to collect data and assume that a mobile robot can navigate over the entire terrain. The geometric-based methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref> generally use Lidar and stereo cameras to gather 3D point cloud and depth information of the environment. This information can be used to detect the elevation, slope, and roughness of the environment as well as obstacles in the surrounding area. Nevertheless, the accuracy of the detection outcome is usually governed by the range of the sensors. Appearance-based methods <ref type="bibr" target="#b25">[26]</ref> usually extract road features with SIFT or SURF features and use MLP to classify a set of terrain classes. Our goal is to focus on the visual perceptive, leveraging the performance and efficiency for robot navigation. Our method computes the navigable regions from RGB images in unstructured environments and outputs a safe trajectory in this condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Off-road Datasets</head><p>Recent developments in semantic segmentation have achieved high accuracy on object datasets like PASCAL VOC <ref type="bibr" target="#b26">[27]</ref>, COCO <ref type="bibr" target="#b27">[28]</ref>, and ADE20K <ref type="bibr" target="#b28">[29]</ref>, as well as driving datasets like Cityscape <ref type="bibr" target="#b0">[1]</ref> and KITTI <ref type="bibr" target="#b1">[2]</ref>. However, there has not been much work on recognition or segmentation in unstructured off-road scenes, which is important for navigation. Perception in an unstructured environment is more challenging since many object classes (e.g., puddles or asphalt) lack clear boundaries. RUGD <ref type="bibr" target="#b29">[30]</ref> and RELLIS-3D <ref type="bibr" target="#b30">[31]</ref> are two recent datasets available for off-road semantic segmentation. The RUGD dataset consists of various scenes like trails, creeks, parks, and villages with finegrained semantic segmentation annotations. The RELLIS-3D dataset is derived from RUGD and includes unique terrains We introduce a novel group-wise attention head for multi-scale fusion. After we convert multi-scale features into several groups of features, the group-wise attention loss acts between the multi-head attention outputs Ag corresponding to group g and the corresponding group-wise binary ground truth YB g to explicitly guide the network towards accurate predictions of different groups. like puddles. In addition, RELLIS-3D includes Lidar data and 3D Lidar annotations. We use these datasets to evaluate the performance of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GA-NAV FOR TERRAIN CLASSIFICATION</head><p>We present GA-Nav, an approach for classifying the navigability of different terrains in off-road environments via coarse-grained segmentation. Most of the existing segmentation methods fail in unstructured environments due to highly similar visual features and unclear boundary of the objects. Use of multi-head self-attention is popular in neural network design due to its superior performance over prior state-of-the-art methods in segmentation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b10">[11]</ref>, image classification <ref type="bibr" target="#b31">[32]</ref>, and object detection <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, but the quadratic nature of its computational complexity has been an issue for real-time use like navigation.</p><p>To solve performance degradation in unstructured environment and computation limitation, we design a simple and lightweight architecture, which uses self-attention at the multi-scale fusion step and generated attention maps to calculate loss for better performance and low level of computational overhead.</p><p>The rest of this section is structured as follows. We begin by stating our problem in Section III-A. We then discuss the various components of our approach, namely the backbone architecture (Section III-B), GA-Nav segmentation head (Section III-C), and an attention-based loss function (Section III-D). Finally, we clarify the difference between GA-Nav and other attention-based methods and briefly explain its integration with planning and navigation modules for navigation demonstration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>The input consists of an RGB image I ? R 3?H?W and the corresponding ground-truth semantic segmentation labels Y ? Z H?W denoting the category to which each pixel belongs among G different groups. We use new coarse-grain labels Y G ? Z H?W based on sematic labels provided by the dataset. For each group, we also compute the binary mask Y Bg ? {0, 1} H?W for g = 1, ..., G. Our goal is to perform coarse-grain semantic segmentation on terrain images I and compute probability maps P ? R G?H?W . Each entry in P is a probability distribution of a pixel in that entry location belonging to one of the G groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Backbone Design</head><p>Vision Transformers (ViTs) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref> are a type of deep neural network (DNN) for feature extraction and are designed as an alternative to the widely used ResNet <ref type="bibr" target="#b35">[36]</ref> architecture via Multi-Head Self-Attention (MHSA). MHSA, or self-attention applied on image patches, is the main component of ViTs, contributing to much of their success over ResNets in computer vision-based tasks. Our approach also uses a transformer-based backbone architecture and leverages the MHSA module to extract and fuse multi-scale features. Our architecture can adapt to any backbone design with multi-scale features; in particular, we use Mixed Transformer (MiT) <ref type="bibr" target="#b6">[7]</ref> with some modifications as our backbone. In addition, to show the advantages of our design, we also show two alternative backbones with our design, ResNet50 <ref type="bibr" target="#b35">[36]</ref> and Bottleneck Transformer (BoT) <ref type="bibr" target="#b32">[33]</ref>.</p><p>Given an RGB image I ? R 3?H?W , we first divide it into several patches as input to the transformer encoders. Unlike ViT and MiT, which take non-overlapping patches of shape 16 by 16 and 4 by 4, respectively, we choose patches of size 7 by 7 with a stride of 4 in our design. After passing the patches to the first transformer encoder blocks, we continue to take 2 by 2 patches on the output features. Therefore, after the initial encoders, each following block of transformer encoders reduces the spatial resolution further by a factor of 2 on both height and width. After each transformer encoder block, we obtain multi-scale features F i with spatial resolution</p><formula xml:id="formula_0">H i ? W i = H 2 i+1 ? W 2 i+1</formula><p>, where the feature channel C i = {32, 64, 160, 256} and i = {1, 2, 3, 4}. We utilize all those multi-scale features in our segmentation head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GANav Segmentation Head</head><p>The RGB image I is passed through the feature extractor to obtain multi-scale feature maps F i ? R Ci?Hi?Wi after i-th encoder block. Spatial Alignment and Resolution Reduction: To prepare for our fusion, we need to combine features at the same spatial locality. We use bi-linear interpolation to resize</p><formula xml:id="formula_1">F i into shape (C i ? H f ? W f )</formula><p>, where H f and W f are equal to H i and W i with any chosen i for all scales. The final feature before fusion, F f use , is concatenated over channel dimension with shape (</p><formula xml:id="formula_2">C i ? H f ? W f )).</formula><p>To reduce the computational complexity, our efficient design prefers to align patch features from different scales to one scale with a smaller spatial resolution as the network bottleneck (i.e., choose a smaller H f and W f ). As we expect, as we reduce H f and W f , the spatial resolution of F f use reduces so the complexity goes down with some performance degradation. In Section III-D, we also introduce the group-wise attention loss to not only improve the overall performance but also mitigate the performance drop from choosing a smaller spatial resolution. Multi-scale Fusion with Group-wise Attention: Given an input feature vector A in ? R N ?Cin , the output self-attention map A out ? R N ?Cout is computed as follows:</p><formula xml:id="formula_3">A out = sof tmax(k(A in ) T ? q(A in )) T ? v(A in ), (1) where k(A in ), q(A in )</formula><p>, v(A in ) represent the key, query, and value feature maps, respectively, and k, q, v are linear projections, as in the self-attention <ref type="bibr" target="#b36">[37]</ref> literature.</p><p>This feature map F f use is reshaped (and transposed) into </p><formula xml:id="formula_4">F f lat ? R H f W f ? Ci ,</formula><formula xml:id="formula_5">), A 1 , A 2 , . . . , A G ? [0, 1] H f W f ?H f W f .</formula><p>The final output P ? R G?H?W is obtained through a standard procedure of a segmentation network by a series of 1 ? 1 convolutions and up-sampling from F out .</p><p>As demonstrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, in our detection head, we use MHSA to fuse multi-scale features and generate different attention maps for each group. We use those attention maps as an additional branch in the detection head and train the detection head to resemble the group distribution by explicitly guiding each attention map towards a corresponding category using a binary cross-entropy loss. Our network skips the attention map processing branch during inference time for efficiency.</p><p>Attention maps can also capture the relevancy between two pixel locations. Intuitively, A g, <ref type="bibr">[x,y]</ref> represents the amount of attention that the pixel at the x th position must pay to the pixel at the y th position. Given an attention map A g , its main diagonal represents the relevance of each location with respect to the attention head h g . We train the attention network to learn feature maps that are most relevant to its corresponding group through group-wise attention loss.</p><p>We use group-wise attention as a fusion method for multiscale features, which has been shown to be quite powerful in combining different modalities <ref type="bibr" target="#b37">[38]</ref>, different scales and representations <ref type="bibr" target="#b33">[34]</ref>, and temporal information <ref type="bibr" target="#b38">[39]</ref> for various tasks. We will show that our novel design of group-wise attention for multi-scale fusion can outperform most existing transformer-based methods with outstanding efficiency on coarse-grain segmentation tasks in challenging off-road environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Group-wise Attention Loss</head><p>Whenever we choose to reduce the resolution during the spatial alignment step, performance degradation is unavoidable. Based on the structure of our group-wise attention head, we propose group-wise attention loss to boost the performance and mitigate such effects.</p><p>For each attention head h i , we have its corresponding</p><formula xml:id="formula_6">attention map A i ? [0, 1] L?L , where L = H f ?W f .</formula><p>We take the main diagonal of A i and reshape it to B i ? [0, 1] H?W using bi-linear image resizing. Each pixel in B i represents the self-attention score with respect to h i .</p><p>To guide each attention-map in the multi-head selfattention module, we apply a binary cross-entropy loss function:</p><formula xml:id="formula_7">L g GA = ? h,w y G log(B g ),<label>(2)</label></formula><p>where y G ? Y G . This equation calculates loss between the predictions of the self-attention output and the corresponding group's binary ground truth with respect to the group. The purpose of group-wise attention loss is not to use attention maps to accurately predict the group's distribution; instead, as an intermediate layer, it aims to provide guidance regarding which region an attention head should focus on for further classification.</p><p>As in traditional segmentation models, we optimize our network with a multi-class cross-entropy loss and an auxiliary loss: Cross-Entropy Loss: This is the standard semantic segmentation cross-entropy loss, defined as follows:</p><formula xml:id="formula_8">L CE = ? h,w g?G y GT log(P g ),<label>(3)</label></formula><p>where h, w represent the dimensions of the image, G represents the set of groups, P g denotes the output probability map corresponding to group g, and y GT corresponds to the ground-truth annotations. Auxiliary Loss via Deep Supervision: Deep supervision was first proposed in <ref type="bibr" target="#b39">[40]</ref> and has been widely used in the task of segmentation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Adding an existing good-performing segmentation decoder head like FCN <ref type="bibr" target="#b41">[42]</ref> in parallel to our GA head during training not only provides strong regularization but also reduces training time.</p><p>E. GA-Nav v.s. Prior Methods</p><p>In this section we highlight the difference between some transformer-based methods and the proposed method, GA-Nav.</p><p>SETR <ref type="bibr" target="#b14">[15]</ref>, DPT <ref type="bibr" target="#b10">[11]</ref>: The designs are significantly different, particularly regarding the computational overhead of those methods. In addition, their method uses positional embedding in the encoding stage while our method removes any positional embedding in the backbone.</p><p>Segformer <ref type="bibr" target="#b6">[7]</ref>, Segmenter <ref type="bibr" target="#b7">[8]</ref>: Those methods, as well as GA-Nav, also focus on efficient training and inference. Segformer does not have any parameters dedicated to different classes except for the final classification layer. In addition, the decoder consists of several MLP layers for simple and efficient design. Segmenter and GA-Nav both have trainable parameters dedicated to specific classes/groups: Segmenter uses G class embeddings that correspond to each semantic class and appends those tokens to the end of the entire image patch sequence for the decoder, while GA-Nav splits feature channels into G portions of equal size for different attention heads that belong to different groups. Finally, GA-Nav uses a smaller spatial resolution in the decoder bottleneck than the other two methods to improve the run-time and compensated for the complexity brought by MHSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. GA-Nav-based Segmentation for Robot Navigation</head><p>We combine GA-Nav with TERP <ref type="bibr" target="#b12">[13]</ref>, an outdoor navigation planner to highlight the benefits of our segmentation algorithm for robot navigation. TERP utilizes the robot's pose, goal information and a point cloud based elevation map to generate a cost map from an attention based perception module. Before interfacing with TERP, we convert GA-Nav's segmentation results into a segmentation cost map. This is performed based on the following steps: 1. Projecting GA-Nav's segmented output onto the ground plane in front of the robot using homography transformations, and 2. Assigning discrete cost values for the different groups. We assign zero cost for smooth regions and increase the non-zero costs for rough, bumpy, forbidden and obstacle regions.</p><p>The computed segmentation cost map is element-wise added with the elevation cost map generated from TERP's perception module to obtain the final navigation cost map. This resulting cost map is fed into TERP's navigation module, which computes least-cost trajectories towards the robot's goal. The overall system architecture is highlighted in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND ANALYSIS A. Implementation Details</head><p>We use two off-road terrain datasets, RUGD <ref type="bibr" target="#b29">[30]</ref> and RELLIS-3D <ref type="bibr" target="#b30">[31]</ref>, as benchmarks for all experiments and comparisons. Optimization is done using a stochastic gradient descent optimizer with a learning rate of 0.01 and a decay of 0.0005. We adopt the polynomial learning rate policy with a power of 0.9. We augment the data with horizontal random flip and random crop. In the ablations and comparisons, for fairness, we benchmark all our models at 240K iterations. <ref type="figure">Fig. 3</ref>: GA-Nav-Based Outdoor Navigation: We integrate GA-Nav with an elevation based planner TERP <ref type="bibr" target="#b12">[13]</ref> to highlight benefits of our segmentation algorithm. Our architecture decouples semantic segmentation and elevation data for better perception and planning. The advantages of this formulation are highlighted in Section IV-F.</p><p>For the RUGD dataset, we use a batch size of 8 and a crop size of 300 ? 375 (the resolution of the original image is 688 ? 550). On the other hand, for the RELLIS-3D dataset, due to the high resolution of the image, we train models with a batch size of 2 and a crop size 375 ? 600 (the resolution of the original image is 1920 ? 1600). Our implementation is based on MMSeg <ref type="bibr" target="#b42">[43]</ref> and will be released.</p><p>We benchmark our models on the standard segmentation metrics: Intersection over Union (IoU), mean IoU (mIoU), mean pixel accuracy (mAcc), and average pixel accuracy (aAcc). For an image I, let P (x, y) be the predicted label at pixel location (x, y), G(x, y) be the ground truth label at We evaluate our methods with 6 coarse-grain semantic classes: smooth (concrete, asphalt), rough (gravel, grass, dirt), bumpy (small rocks, rock-bed), forbidden (water, bushes), obstacle, and background. Some visualizations for method comparisons are provided in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>Effect of Multi-scale Fusion Design: In <ref type="table" target="#tab_2">Table I</ref>, we show the effect of our multi-scale fusion design with group-wise attention. We show an improvement of 14.76% and 11.43% in terms of mIoU on RUGD and RELLIS-3D, respectively. Group-wise Attention (GA) Design: In <ref type="table" target="#tab_2">Table II</ref>, we show the effect of our segmentation head design with different <ref type="figure">Fig. 4</ref>: Qualitative Results: Each row highlights the original image, the ground truth (GT) label, the predictions of two SOTA methods, and the prediction of our method. We observe that our method can detect different navigable regions and is robust with respect to objects with similar colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Multi-scale Fusion method mIoU ? mAcc ?  backbones on RUGD and RELLIS-3D datasets. We observe that, regardless of the backbone choice, group-wise attention improves performance (in terms of mIoU) by 6.67 ? 10.85% and 4.18 ? 10.31% on RUGD and RELLIS-3D, respectively. Ablation on Spatial Reduction with GA Head: In <ref type="table" target="#tab_2">Table III</ref>, we show the relations between performance and resolution change during spatial alignment. We can see that without GA heads, as we change the resolution to 1/16 and 1/32, there are performance drops of 1.3% and 4.0% in terms of mIoU on RELLIS-3D. The performance drops do not appear for RUGD when we change the spatial reduction from 1/8 to 1/16; the main reason may be that the overall performance is low for RUGD before using GA heads, or the original resolution for RUGD is lower than RELLIS-3D, so the performance drop due to spatial information loss has not started to appear. We can still see the performance drop when reducing it to 1/32. After we add the GA component, the mIoU performance improves by 7.46 ? 10.85% and 2.85 ? 5.85% on RUGD and RELLIS-3D, respectively. In addition, we no longer see the performance drop on RELLIS-3D with the proposed GA heads. Use of GA heads allows us to reduce spatial resolution for efficient inference without compromising the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUGD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Run-time Analysis and Computational Cost</head><p>In <ref type="figure" target="#fig_2">Fig. 5</ref>, we highlight the model efficiency of our method while maintaining SOTA performance. We can see that, even if we compromise the performance for inference rate in GA-Nav-r16 and GA-Nav-r32, our method still outperforms other  TABLE III: Ablation studies on GA heads with different spatial reduction on RUGD and RELLIS-3D val sets: We show that higher spatial reduction can decrease the perception accuracy and the group-wise attention can mitigate the performance drop due to resolution reduction. " 1 N ? 1 N " in the second column means the bottleneck spatial resolution is reduced by a factor of N on height and width compared to the input resolution. methods in terms of mIoU.</p><p>In <ref type="table" target="#tab_2">Table IV</ref>, we give more details on model parameter size, GFLOPS, cost during inference, and run-time. While our method doesn't always have the smallest model size and GLOPS, our method has a very low memory requirement. Our method also has the best inference time among listed transformer-based methods and most of the CNN-based methods, given that our method achieves state-of-the-art accuracy, as shown in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and State-of-the-art Comparisons</head><p>The results on different coarse-grain classes are presented in <ref type="table" target="#tab_8">Table V</ref>. Our architecture improves the state-of-the-art mIoU by 1.66 ? 44.35% on RUGD. We can see that our method has the best overall mIoU scores. While GA-Nav does not always give the best results for each separate class and the performance is sometimes close to the second best IoU score, our method has very stable performance for each class. GA-Nav performs well on detecting different terrains, while other methods only have good performance on one of the terrain classes. Similarly, on the RELLIS dataset, GA-Nav demonstrates an improvement of 5.17?19.06% in terms of mIoU.  For each measurement, we underline one best number among CNNbased methods, one among transformer-based methods, and one among variations of GA-Nav.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Outdoor Navigation</head><p>We evaluate the benefits of our GA-Nav segmentation algorithm for robot navigation in real-world outdoor scenarios using a Clearpath Jackal and a Husky robot mounted with an Intel RealSense camera. The camera has a field of view (FOV) of 70 degrees. We use an Alienware laptop with an Intel i9 CPU (2.90GHz x 12) and an Nvidia GeForce RTX 2080 GPU mounted on the robot to run GA-Nav. We also utilize a velodyne 3D Lidar for robot localization and elevation map generation. We test the navigation performance of our segmentation and elevation-based planner on a variety of terrains around a university campus, including concrete walkways, asphalt trails, dirt roads, and off-road terrains.</p><p>The following metrics are used to compare the navigation performance of our method with the Dynamic Window Approach (DWA) <ref type="bibr" target="#b48">[49]</ref>, TERP <ref type="bibr" target="#b12">[13]</ref>, Segformer <ref type="bibr" target="#b6">[7]</ref>, and OCRNet <ref type="bibr" target="#b5">[6]</ref> based planners.</p><p>? Success Rate: The number of successful attempts for the robot to reach its goal while avoiding relatively nontraversable regions and collisions over the total number of trials. ? Trajectory Roughness -The cumulative vertical motion gradient experienced by the robot along a trajectory. ? Trajectory Selection: The percentage of the robot's trajectory length along the most navigable surface out of the total trajectory length (i.e., the selection of smoother regions over others). ? False Positive of the Forbidden Region -The percentage of the frames that include a misclassified forbidden region out of all the frames during navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Navigation Comparisons</head><p>The navigation results for GA-Nav, DWA <ref type="bibr" target="#b48">[49]</ref>, TERP <ref type="bibr" target="#b12">[13]</ref>, Segformer <ref type="bibr" target="#b6">[7]</ref>, and OCRNet <ref type="bibr" target="#b5">[6]</ref> based planners are shown in <ref type="table" target="#tab_2">Table VI</ref>. We observe that all three segmentation-based planners perform well in terms of success rate under Walkway and Trail scenarios due to the clear margins between the different surfaces. Even though Segformer <ref type="bibr" target="#b6">[7]</ref> and OCR-Net <ref type="bibr" target="#b5">[6]</ref> display reasonable performances under scenarios similar to trained datasets, we observe significant performance degradation in terms of trajectory selection during off-road conditions. However, GA-Nav displays significant improvement in terms of all three metrics under cluttered and unseen scenarios in <ref type="figure" target="#fig_3">Fig. 6</ref> (c) and 6 (d). Specifically, accurate segmentation from GA-Nav increases the utilization of the most navigable surface during navigation. This behavior leads to better trajectory selection percentages than the other methods, as shown in the <ref type="table" target="#tab_2">Table VI.</ref> Our baseline planner and TERP incorporate terrain elevation data to perform the navigation task. Hence, such methods cannot differentiate between the surface properties in multi-terrain scenarios with similar elevation conditions. This leads to trajectory generation along less navigable surfaces, in contrast to segmentation-based methods. Therefore, we observe a noticeable degradation in the trajectory smoothness. However, the usage of segmentation data for the baseline planner significantly enhances the navigable trajectory selection capabilities of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS, LIMITATIONS, AND FUTURE WORK</head><p>We present a learning-based method for classifying various terrain types in off-road environments. We propose a novel and efficient segmentation network that can provide robust predictions and distinguish regions with different semantic meanings using group-wise attention. We demonstrate improvement in both accuracy and efficiency over SOTA methods on complex, unstructured terrain datasets and show that our method can be used in real-world scenarios and result in higher success rate for navigation.</p><p>Our approach has some limitations. For example, our current work focuses on perception and integrates with a learning-based planner implementation for real-world scenarios. As a result, our current performance analysis is mostly based on perception metrics and not takes into account many navigation metrics. In addition, we need to make sure that the data is labeled correctly and consistently. For example, trees can either be obstacles or background objects. An annotated dataset might not label them separately.  06% on RELLIS-3D. Further, we show that our grouping method can improve the accuracy of classes like smooth region, bumpy region, and forbidden regions by large margins, thus improving safe navigation. We bold the best number and underline one best result from our method and one from other methods. The suffix of our method shows different designs of our network. For example, "r8" means we reduce height and width of the spatial resolution by a factor of 8 at the spatial alignment step. . We observe that the trajectories generated by GA-Nav maximize the navigation along the smoothest surface of the available terrains while maintaining the highest success rate.</p><p>As part of our future work, we need to consider how to evaluate the accuracy of our method with more navigation metrics. We want to explore how this learning method can be applied and customized to other planning and navigation schemes and extend our approach to multiple sensor inputs for designing a more robust navigation scheme. Acknowledgement. This research was supported by Army Cooperative Agreement No. W911NF2120076 and ARO grant W911NF2110026.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. UNSTRUCTURED ENVIRONMENTS: LEVELS OF NAVIGABILITY</head><p>There are certain characteristics, e.g., texture, color, temperature, etc., that highlight the differences between various terrains. For robot navigation, it is important to identify different terrains based on the navigability of that terrain, which is indicated by its texture <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Furthermore,  there may be terrains with similar textures and navigability that similarly affect navigation capabilities but may pose challenges to visual perception systems such as semantic segmentation (due to the long-tailed distribution problem). In such cases, it is advantageous to group terrains with similar navigability into a single category. Given a dataset with C different semantic classes, we regroup them into G classes using the following criteria. Terrain navigability varies for different robot systems. For instance, large autonomous vehicles may be better able to navigate on dirt roads than smaller mobile robots that would find such dirt roads difficult to traverse. Therefore, our approach is generally designed to identify different groupings of terrain categories and is not restricted to a fixed set of groupings. In Table VII, we show an example of a grouping based on terrain texture and other characteristics. Below, we describe in detail the characteristics of different terrains based on their texture and navigability.   <ref type="bibr" target="#b8">[9]</ref>, and most mobile robots should be able to navigate in these terrains. ? Gravel, Grass, Dirt, Sand (Rough): These terrains have been termed rough on account of the increased friction encountered while traversing them <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Most existing perception modules in off-road environments <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> either do not consider these factors or may conservatively avoid such regions <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref> resulting in a sub-optimal solution, particularly in off-road navigation environments. To handle a large class of navigation methods, we want to be able to explicitly distinguish between smooth surfaces and rough surfaces. For example, in the presence of a smooth navigable region, a planning algorithm could prioritize it over a rough navigable region to reduce energy loss. ? Small rocks, Rock-bed (Bumpy): Autonomous vehicles or large robots may be able to navigate through rocks, while smaller robots with weaker off-road capabilities may face issues in such terrains <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b51">[52]</ref>. We provide a safe and flexible option where the planning scheme can be customized according to different scenes and different hardware characteristics of the robot. Specifically, this level of grouping can be ignored for off-road robots or vehicles by re-allocating these terrains to a different navigability group such as rough (for larger robots) or forbidden (for smaller robots). Our approach is general and handles dynamic groupings. ? Water, Bushes, etc. (Forbidden Regions): These are regions that the agent must avoid to prevent damage to the robot hardware. ? Obstacles: Detecting obstacles such as trees, poles, etc.</p><p>is critical for safe navigation. There has been a lot of work <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b19">[20]</ref> on obstacle and hazardous terrain detection. ? Background: We use this buffer group to include background and non-navigable classes, including void, sky, and sign, that are not commensurate with any of the earlier definitions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture of our proposed network GA-Nav:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label>1</label><figDesc>(x, y), 1(X) be the indicator function, and B be a set of the class labels: Intersection over Union (IoU) for class i: IoU i = I x,y 1(P (x, y) = i and G(x, y) = i) I x,y 1(P (x, y) = i or G(x, y) = i) Mean IoU (mIoU): mIoU = i mIoU i B 1 Mean Pixel Accuracy (mAcc): mAcc = i?B ( I x,y,G(x,y)=i 1(P (x, y) = G(x, y))) B Averaged Pixel Accuracy (aAcc): aAcc = I x,y 1(P (x, y) = G(x, y)) I x,y 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Performance vs. Inference Time on RELLIS-3D: The run-times are calculated based on the average time of 200 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Navigation Results: Robot trajectories when navigating with Ours (red), Segformer (orange), OCRNet (yellow), TERP (blue), and DWA (green). (a) Walkway; (b) Trail; (c) Dirt road; (d) Off-road:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Concrete, Asphalt (Smooth): Concrete and asphalt are smooth terrains commonly found in urban roads. These terrains correspond to the "flat" category in the grouping provided by the CityScapes [1] autonomous driving dataset. These terrains are navigable in outdoor envi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which is passed through the MHSA block with G attention heads. The MHSA component fuses flatten multi-scale feature F f lat to produce the new feature maps F out ? R Cout?H f ?W f and generates G attention maps (one for each group</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Ablation studies on multi-scale fusion on RUGD and RELLIS-3D val sets: The "Multi-scale" column shows whether we use a single or all scale features from the feature extractor, and the "Fusion method" column shows either we use a simple linear layer or the proposed segmentation head for the fusion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Ablation studies on GA heads with different backbones on RUGD and RELLIS-3D val sets: We observe that the group-wise attention mechanism is successful in improving mIoU by 7.26 ? 11.44% and 4.18 ? 10.31% on the RUGD and RELLIS datasets, respectively, regardless of the backbone choice.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Spatial Reduction GW-Attn. mIoU ? mAcc ? aAcc ?</cell></row><row><cell></cell><cell>1 8 ? 1 8</cell><cell>77.64 89.08</cell><cell>86.78 93.55</cell><cell>93.47 95.66</cell></row><row><cell>RUGD</cell><cell>1 16 ? 1 16</cell><cell>79.61 87.07</cell><cell>87.76 91.74</cell><cell>92.85 94.99</cell></row><row><cell></cell><cell>1 32 ? 1 32</cell><cell>76.81 84.9</cell><cell>85.14 90.36</cell><cell>92.24 94.24</cell></row><row><cell></cell><cell>1 8 ? 1 8</cell><cell>68.91 71.76</cell><cell>81.5 81.62</cell><cell>85.57 90.31</cell></row><row><cell>RELLIS-3D</cell><cell>1 16 ? 1 16</cell><cell>67.61 71.62</cell><cell>79.95 84.51</cell><cell>84.03 90.26</cell></row><row><cell></cell><cell>1 32 ? 1 32</cell><cell>64.91 70.76</cell><cell>75.56 84.4</cell><cell>83.51 90.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Model Complexity: We show the model parameter size, GLOPS, GPU memory cost during inference, and run-time.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Comparison with state-of-the-art methods on RUGD and RELLIS-3D: We compare the performance of our method with transformer-based methods (with *) as well as other methods in terms of IoU and aAcc. GA-Nav improves the state-of-the-art mIoU by 2.25 ? 44.94% on RUGD and 5.17 ? 19.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Navigation Comparisons: We compare the performance of our method with state-of-the-art algorithms on four different scenarios, each with at least ten trials. GA-Nav consistently outperforms other methods in terms of success rate, trajectory roughness, and false positive rate of forbidden regions even in challenging unstructured environments. We observe an average increase of 10% in terms of success rate, 43.5% in terms of the best navigable trajectory selection and an average decrease of 10.82% in trajectory roughness. Further, GA-Nav reduces the false positive rate of forbidden regions by 37.79%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Texture-based Terrain Classification: We show an example of a grouping of terrain classes based on their texture and navigability. Our approach is general and designed for dynamic groupings in which different terrains may be re-classified into different hierarchy levels depending on the type of robot.</figDesc><table><row><cell></cell><cell></cell><cell>ronments</cell></row><row><cell cols="2">Hierarchy level</cell><cell>Classes</cell></row><row><cell></cell><cell>Smooth</cell><cell>Concrete, Asphalt</cell></row><row><cell>Navigable</cell><cell>Rough</cell><cell>Gravel, Grass, Dirt, Sand</cell></row><row><cell></cell><cell>Bumpy</cell><cell>Rock, Rock Bed</cell></row><row><cell cols="2">Forbidden</cell><cell>Water, Bushes, Tall Vegetation</cell></row><row><cell cols="2">Obstacles</cell><cell>Trees, Poles, Logs, etc.</cell></row><row><cell cols="2">Background</cell><cell>Void, Sky, Sign</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) creek-1 (b) GT (c) Ours (d) creek-2 (e) GT (f) Ours </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Disaster robotics</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Research and development in agricultural robotics: A perspective of digital farming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shamshiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weltzien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Balasundram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pitonakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chowdhary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th European Conference Computer Vision (ECCV 2020)</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traversable region estimation for mobile robots in an outdoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsubouchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Robotics Syst</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="453" to="463" />
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Obstacle detection and terrain classification for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Terp: Reliable planning in uneven outdoor environments using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weerakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Sathyamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A brief survey on semantic segmentation with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning terrain segmentation with classifier ensembles for autonomous robot navigation in unstructured environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grudic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Badgr: An autonomous selfsupervised learning-based navigation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1312" to="1319" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robot navigation in multiterrain outdoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pimenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chaimowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Corr?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="331" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Traversability analysis for offroad vehicles using stereo and radar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rouveure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Industrial Technology (ICIT)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="540" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Off-road terrain traversability analysis and hazard avoidance for ugvs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards speed selection for high-speed operation of autonomous ground vehicles on rough off-road terrains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Surface classification based on vibration on omni-wheel mobile base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Zhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frequency response method for terrain classification in autonomous ground vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="337" to="347" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Terrain roughness identification for high-speed ugvs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automation and Control Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Traversable region estimation for mobile robots in an outdoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsubouchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Robotic Syst</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="453" to="463" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: common objects in context</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A rugd dataset for autonomous navigation and visual perception in unstructured outdoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wigness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Osteen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wigness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saripalli</surname></persName>
		</author>
		<title level="m">Rellis-3d dataset: Data, benchmarks and analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Meta pseudo labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="16" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">M3detr: Multi-representation, multi-scale, mutualrelation 3d object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page" from="772" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal fusion transformers for interpretable multi-horizon time series forecasting</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1748" to="1764" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mmsegmentation, an open source semantic segmentation toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cgnet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yizhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The dynamic window approach to collision avoidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Mag</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A new terrain classification framework using proprioceptive sensors for mobile robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep multi-layer perception based terrain classification for planetary exploration rovers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training a terrain traversability classifier for a planetary rover through simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ellery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Ruiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1729881417735401</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
