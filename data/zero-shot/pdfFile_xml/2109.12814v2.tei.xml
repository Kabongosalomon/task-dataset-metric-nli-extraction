<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Non-local Features for Neural Constituency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhangyue@westlake.edu.cnsenyang.stu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhejiang</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Non-local Features for Neural Constituency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features. Recently, it has been shown that non-local features in CRF structures lead to improvements. In this paper, we investigate injecting non-local features into the training process of a local span-based parser, by predicting constituent n-gram non-local patterns and ensuring consistency between non-local patterns and local constituents. Results show that our simple method gives better results than the self-attentive parser on both PTB and CTB. Besides, our method achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and strong performance on CTB (92.31 F1). Our parser also achieves better or competitive performance in multilingual and zero-shot cross-domain settings compared with the baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Constituency parsing is a fundamental task in natural language processing, which provides useful information for downstream tasks such as machine translation <ref type="bibr" target="#b28">(Wang et al., 2018)</ref>, natural language inference <ref type="bibr" target="#b0">(Chen et al., 2017)</ref>, text summarization <ref type="bibr" target="#b29">(Xu and Durrett, 2019)</ref>. Over the recent years, with advance in deep learning and pre-training, neural chart-based constituency parsers <ref type="bibr" target="#b22">(Stern et al., 2017a;</ref><ref type="bibr" target="#b11">Kitaev and Klein, 2018)</ref> have achieved highly competitive results on benchmarks like Penn Treebank (PTB) and Penn Chinese Treebank (CTB) by solely using local span prediction.</p><p>The above methods take the contextualized representation (e.g., BERT) of a text span as input, and use a local classifier network to calculate the scores of the span being a syntactic constituent, together with its constituent label. For testing, the output layer uses a non-parametric dynamic programming <ref type="figure">Figure 1</ref>: An example of the non-local n-gram pattern features: the 3-gram pattern (3, 11, {VBD NP PP}) is composed of two constituent nodes and one partof-speech node; the 2-gram pattern (7, 11, {NP PP}) is composed of two constituent nodes.</p><p>algorithm (e.g., CKY) to find the highest-scoring tree. Without explicitly modeling structured dependencies between different constituents, the methods give competitive results compared to non-local discrete parsers <ref type="bibr" target="#b22">(Stern et al., 2017a;</ref><ref type="bibr" target="#b11">Kitaev and Klein, 2018)</ref>. One possible explanation for their strong performance is that the powerful neural encoders are capable of capturing implicit output correlation of the tree structure <ref type="bibr" target="#b22">(Stern et al., 2017a;</ref><ref type="bibr" target="#b6">Gaddy et al., 2018;</ref><ref type="bibr" target="#b25">Teng and Zhang, 2018)</ref>.</p><p>Recent work has shown that modeling non-local output dependencies can benefit neural structured prediction tasks, such as NER <ref type="bibr" target="#b14">(Ma and Hovy, 2016)</ref>, CCG supertagging <ref type="bibr" target="#b2">(Cui and Zhang, 2019)</ref> and dependency parsing <ref type="bibr" target="#b33">(Zhang et al., 2020a)</ref>. Thus, an interesting research question is whether injecting non-local tree structure features is also beneficial to neural chart-based constituency parsing. To this end, we introduce two auxiliary training objectives. The first is Pattern Prediction. As shown in <ref type="figure">Figure 1</ref>, we define pattern as the n-gram constituents sharing the same parent. <ref type="bibr">1</ref> We ask the model to predict the pattern based on its span representation, which directly injects the non-local constituent tree structure to the encoder.</p><p>To allow stronger interaction between non-local patterns and local constituents, we further propose a Consistency loss, which regularizes the cooccurrence between constituents and patterns by collecting corpus-level statistics. In particular, we count whether the constituents can be a sub-tree of the pattern based on the training set. For instance, both NNS and NP are legal to occur as sub-trees of the 3-gram pattern {VBD NP PP} in <ref type="figure">Figure 1</ref>, while S or ADJP cannot be contained within this pattern based on grammar rules. Similarly, for the 2-gram pattern {NP PP} highlighted in <ref type="figure">Figure 1</ref>, both IN and NP are consistent constituents, but JJ is not. The Consistency loss can be considered as injecting prior linguistic knowledge to our model, which forces the encoder to understand the grammar rules. Non-local dependencies among the constituents that share the same pattern are thus explicitly modeled. We denote our model as Injecting Non-local Features for neural Chart-based parsers (NFC).</p><p>We conduct experiments on both PTB and CTB. Equipped with BERT, NFC achieves 95.92 F1 on PTB test set, which is the best reported performance for BERT-based single-model parsers. For Chinese constituency parsing, NFC achieves highly competitive results (92.31 F1) on CTB, outperforming the baseline self-attentive parser (91.98 F1) and a 0-th order neural CRF parser (92.27 F1) <ref type="bibr" target="#b34">(Zhang et al., 2020b)</ref>. To further test the generalization ability, we annotate a multi-domain test set in English, including dialogue, forum, law, literature and review domains. Experiments demonstrate that NFC is robust in zero-shot cross-domain settings. Finally, NFC also performs competitively with other languages using the SPMRL 2013/2014 shared tasks, establishing the best reported results on three rich resource languages. We release our code and models at https://github.com/ RingoS/nfc-parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Constituency Parsing. There are mainly two lines of approaches for constituency parsing. Transition-based methods process the input words sequentially and construct the output constituency tree incrementally by predicting a series of local transition actions <ref type="bibr" target="#b35">(Zhang and Clark, 2009;</ref><ref type="bibr" target="#b1">Cross and Huang, 2016;</ref><ref type="bibr" target="#b13">Liu and Zhang, 2017)</ref>. For these methods, the sequence of transition actions make traversal over a constituent tree. Although transition-based methods directly model partial tree structures, their local decision nature may lead to error propagation  and worse performance compared with methods that model long-term dependencies <ref type="bibr" target="#b16">(McDonald and Nivre, 2011;</ref><ref type="bibr" target="#b36">Zhang and Nivre, 2012)</ref>. Similar to transition-based methods, NFC also directly models partial tree structures. The difference is that we inject tree structure information using two additional loss functions. Thus, our integration of nonlocal constituent features is implicit in the encoder, rather than explicit in the decoding process. While the relative effectiveness is empirical, it could potentially alleviate error propagation.</p><p>Chart-based methods score each span independently and perform global search over all possible trees to find the highest-score tree given a sentence. <ref type="bibr" target="#b4">Durrett and Klein (2015)</ref> represented nonlinear features to a traditional CRF parser computed with a feed-forward neural network. <ref type="bibr" target="#b23">Stern et al. (2017b)</ref> first used LSTM to represent span features. <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> adopted a self-attentive encoder instead of the LSTM encoder to boost parser performance. <ref type="bibr" target="#b17">Mrini et al. (2020)</ref> proposed label attention layers to replace self-attention layers. <ref type="bibr" target="#b38">Zhou and Zhao (2019)</ref> integrated constituency and dependency structures into head-driven phrase structure grammar. <ref type="bibr" target="#b26">Tian et al. (2020)</ref> used span attention to produce span representation to replace the subtraction of the hidden states at the span boundaries. Despite their success, above work mainly focuses on how to better encode features over the input sentence. In contrast, we take the encoder of <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> intact, being the first to explore new ways to introduce non-local training signal into the local neural chart-based parsers.</p><p>Modeling Label Dependency. There is a line of work focusing on modeling non-local output dependencies. <ref type="bibr" target="#b32">Zhang and Zhang (2010)</ref> used a Bayesian network to encode the label dependency in multilabel learning. For neural sequence labeling, <ref type="bibr" target="#b37">Zhou and Xu (2015)</ref> and <ref type="bibr" target="#b14">Ma and Hovy (2016)</ref> built a CRF layer on top of neural encoders to capture label transition patterns. <ref type="bibr" target="#b20">Pislar and Rei (2020)</ref> introduced a sentence-level constraint to encourage the model to generate coherent NER predictions. <ref type="bibr" target="#b2">Cui and Zhang (2019)</ref> investigated label attention network to model the label dependency by producing label distribution in sequence labeling tasks. <ref type="bibr" target="#b8">Gui et al. (2020)</ref> proposed a two-stage label decoding framework based on Bayesian network to model long-term label dependencies. For syntactic parsing, <ref type="bibr" target="#b34">Zhang et al. (2020b)</ref> demonstrated that structured Tree CRF can boost parsing performance over graph-based dependency parser. Our work is in line with these in the sense that we consider non-local structure information for neural structure prediction. To our knowledge, we are the first to inject sub-tree structure into neural chart-based encoders for constituency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline</head><p>Our baseline is adopted from the parsing model of <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> and . Given a sentence X = {x 1 , ..., x n }, its corresponding constituency parse tree T is composed by a set of labeled spans</p><formula xml:id="formula_0">T = {(i t , j t , l c t )}| |T | t=1</formula><p>(1)</p><p>where i t and j t represent the t-th constituent span's fencepost positions and l c t represents the constituent label. The model assigns a score s(T ) to tree T , which can be decomposed as</p><formula xml:id="formula_1">s(T ) = (i,j,l)?T s(i, j, l c )<label>(2)</label></formula><p>Following , we use BERT with a self-attentive encoder as the scoring function s(i, j, ?), and a chart decoder to perform a globaloptimal search over all possible trees to find the highest-scoring tree given the sentence. In particular, given an input sentence X = {x 1 , ..., x n }, a list of hidden representations H n 1 = {h 1 , h 2 , . . . , h n } is produced by the encoder, where h i is a hidden representation of the input token x i . Following previous work, the representation of a span (i, j) is constructed by:</p><formula xml:id="formula_2">v i,j = h j ? h i<label>(3)</label></formula><p>Finally, v i,j is fed into an MLP to produce realvalued scores s(i, j, ?) for all constituency labels:</p><formula xml:id="formula_3">s(i, j, ?) = W c 2 RELU(W c 1 v i,j + b c 1 ) + b c 2 (4) where W c 1 , W c 2 , b c 1 and b c 2 are trainable parame- ters, W c 2 ? R |H|?|L c |</formula><p>can be considered as the constituency label embedding matrix <ref type="bibr" target="#b2">(Cui and Zhang, 2019)</ref>, where each column in W c 2 corresponds to the embedding of a particular constituent label. |H| represents the hidden dimension and |L c | is the size of the constituency label set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Encoder</head><formula xml:id="formula_4">! ! ! " ! # ! $ ? ? ? " ",# = ! # ? ! "</formula><p>constituency score pattern score Training. The model is trained to satisfy the margin-based constraints</p><formula xml:id="formula_5">%(', )) +" ,# Consistency matrix -&amp;'() -*+, Span representation --./ . ! . " . # . $ ? ? ?</formula><formula xml:id="formula_6">s(T * ) ? s(T ) + ?(T, T * )<label>(5)</label></formula><p>where T * denotes the gold parse tree, and ? is Hamming loss. The hinge loss can be written as</p><formula xml:id="formula_7">Lcons = max 0, max T =T * [s(T ) + ?(T, T * )] ? s(T * ) (6)</formula><p>During inference time, the most-optimal tre?</p><formula xml:id="formula_8">T = argmax T s(T )<label>(7)</label></formula><p>is obtained using a CKY-like algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional Training Objectives</head><p>We propose two auxiliary training objectives to inject non-local features into the encoder, which rely only on the annotations in the constituency treebank, but not external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instance-level Pattern Loss</head><p>We define n-gram constituents, which shares the same parent node, as a pattern. We use a triplet (i p , j p , l p ) to denote a pattern span beginning from the i p -th word and ending at j p -th word. l p is the corresponding pattern label. Given a constituency parse tree in <ref type="figure">Figure 1</ref>, (3, 11, {VBD NP PP}) is a 3-gram pattern. Similar to Eq 4, an MLP is used for transforming span representations to pattern prediction probabilities:</p><formula xml:id="formula_9">pi,j = Softmax W p 2 RELU(W p 1 vi,j + b p 1 ) + b p 2<label>(8)</label></formula><p>where W p 1 , W p 2 , b p 1 and b p 2 are trainable parameters, W p 2 ? R |H|?|L p | can be considered as the pattern label embedding matrix, where each column in W p 2 corresponds to the embedding of a particular pattern label. |L p | represents the size of the pattern label set. For each instance, the crossentropy loss between the predicted patterns and the gold patterns are calculated as</p><formula xml:id="formula_10">L pat = ? n i=1 n j=1 p i,j logp i,j<label>(9)</label></formula><p>We use the span-level cross-entropy loss for patterns (Eq 9) instead of the margin loss in Eq 6, because our pattern-prediction objective aims to augment span representations via greedily classifying each pattern span, rather than to reconstruct the constituency parse tree through dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corpus-level Consistency Loss</head><p>Constituency scores and pattern probabilities are produced based on a shared span representation; however, the two are subsequently separately predicted. Therefore, although the span representations contain both constituent and pattern information, the dependencies between constituent and pattern predictions are not explicitly modeled. Intuitively, constituents are distributed non-uniformly in patterns, and such correlation can be obtained in the corpus-level statistic. We propose a consistency loss, which explicitly models the non-local dependencies among constituents that belong to the same pattern. As mentioned in the introduction, we regard all constituent spans within a pattern span as being consistent with the pattern span. Take 2gram patterns for example, which represents two neighboring subtrees covering a text span. The constituents that belong to the two subtrees, including the top constituent and internal sub constituents, are considered as being consistent. We consider only the constituent labels but not their corresponding span locations for this task.</p><p>This loss can be understood first at the instance level. In particular, if a constituent span</p><formula xml:id="formula_11">(i t , j t , l c t ) is a subtree of a pattern span (i t , j t , l p t ), i.e. i t &gt;= i t and j t &lt;= j t , where l c t = L c [a]</formula><p>(the a-th constituent label in L c ) and l p t = L p [b] (the b-th pattern label in L p ), we define L c [a] and L p [b] to be consistent (denoted as y a,b = 1). Otherwise we consider it to be non-consistent (denoted as y a,b = 0). This yields a consistency matrix Y ? R |L c |?|L p | for each instance. The gold consistency matrix Y provides information regarding non-local dependencies among constituents and patterns.</p><p>An intuitive method to predict the consistency matrix Y is to make use of the constituency label embedding matrix W p 2 (see Eq 4 for definition), the pattern label embedding matrix W c 2 (see Eq 8 for definition) and the span representations V (see Eq 3 for definition):</p><formula xml:id="formula_12">Y = Sigmoid (W c 2 T U 1 V)(V T U 2 W p 2 ) (10) where U 1 , U 2 ? R |H|?|H| are trainable parame- ters.</formula><p>Intuitively, the left term, W c 2 T U 1 V, integrates the representations of the pattern span and all possible constituent label embeddings. The second term, V T U 2 W p 2 , integrates features of the span and all pattern embeddings. Each binary element in the resulting? ? R |L c |?|L p | denotes whether a particular constituent label is consistent with a particular pattern in the given span context. Eq 10 can be predicted on the instance-level for ensuring consistency between patterns and constituent. However, this naive method is difficult for training, and computationally infeasible, because the span representation matrix V ? R |H|?n 2 is composed of n 2 span representations v i,j ? R |H| and the asymptotic complexity is:</p><formula xml:id="formula_13">O (|L p | + |L c |)(|H| 2 + n 2 |H|) + |L p ||L c |n 2</formula><p>(11) for a single training instance.</p><p>We instead use a corpus-level constraint on the non-local dependencies among constituents and patterns. In this way, Eq 10 is reduced to be independent of individual span representations:</p><formula xml:id="formula_14">Y = Sigmoid W c 2 UW p 2 T<label>(12)</label></formula><p>where U ? R |H|?|H| is trainable. This trick decreases the asymptotic complexity to O(|L c ||H| 2 + |L p ||L c ||H|). The cross-entropy loss between the predicted consistency matrix and gold consistency labels is used to optimize the model:</p><formula xml:id="formula_15">L reg = ? |L c | a=1 |L p | b=1 y a,b log? a,b<label>(13)</label></formula><p>The corpus-level constraint can be considered as a prior linguistic knowledge statistic from the treebank, which forces the encoder to understand the grammar rules.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>Given a constituency tree, we minimize the sum of the three objectives to optimize the parser:</p><formula xml:id="formula_16">L = L cons + L pat + L reg<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational Cost</head><p>The number of training parameters increased by</p><formula xml:id="formula_17">NFC is W p 1 ? R |H|?|H| , W p 2 ? R |H|?|L p | , b p 1 ? R |H| and b p 2 ? R |L p | in Eq 8 and U ? R |H|?|H| in Eq 12.</formula><p>Taking training model on PTB as an example, NFC adds less than 0.7M parameters to 342M parameters baseline model <ref type="bibr" target="#b11">(Kitaev and Klein, 2018</ref>) based on BERT-large-uncased during training. NFC is identical to our baseline selfattentive parser <ref type="bibr" target="#b11">(Kitaev and Klein, 2018)</ref> during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We empirically compare NFC with the baseline parser in different settings, including in-domain, cross-domain and multilingual benchmarks. <ref type="table" target="#tab_1">Table 1</ref> shows the detailed statistic of our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>In-domain. We conduct experiments on both English and Chinese, using the Penn Treebank <ref type="bibr" target="#b15">(Marcus et al., 1993)</ref> as our English dataset, with standard splits of section 02-21 for training, section 22 for development and section 23 for testing. For Chinese, we split the Penn Chinese Treebank (CTB) 5.1 <ref type="bibr" target="#b30">(Xue et al., 2005)</ref>, taking articles 001-270 and 440-1151 as training set, articles 301-325 as development set and articles 271-300 as test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-domain.</head><p>To test the robustness of our methods across difference domains, we further annotate five test set in dialogue, forum, law, literature and review domains. For the dialogue domain, we randomly sample dialogue utterances from Wizard of Wikipedia <ref type="bibr" target="#b3">(Dinan et al., 2019)</ref>, which is a chit-chat dialogue benchmark produced by humans. For the forum domain, we use users' communication records from Reddit, crawled and released by <ref type="bibr" target="#b27">V?lske et al. (2017)</ref>. For the law domain, we sample text from European Court of Human Rights Database <ref type="bibr" target="#b24">(Stiansen and Voeten, 2019)</ref>, which includes detailing judicial decision patterns. For the literature domain, we download literary fictions from Project Gutenberg 2 . For the review domain, we use plain text across a variety of product genres, released by SNAP Amazon Review Dataset <ref type="bibr" target="#b9">(He and McAuley, 2016)</ref>. After obtaining the plain text, we ask annotators whose majors are linguistics to annotate constituency parse tree by following the PTB guideline. We name our dataset as Multidomain Constituency Treebank (MCTB). More details of the dataset are documented in <ref type="bibr" target="#b31">Yang et al. (2022)</ref>.</p><p>Multi-lingual. For the multilingual testing, we select three rich resource language from the SPMRL 2013-2014 shared task <ref type="bibr" target="#b21">(Seddah et al., 2013)</ref>: French, German and Korean, which include at least 10,000 training instances, and three lowresource language: Hungarian, Basque and Polish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setup</head><p>Our code is based on the open-sourced code of <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref>  <ref type="bibr">3</ref> . The training process gets terminated if no improvement on development F1 is obtained in the last 60 epochs. We evaluate the models which have the best F1 on the development set. For fair comparison, all reported results and baselines are augmented with BERT. We adopt BERT-large-uncased for English, BERT-base for Chinese and BERT-multi-lingual-uncased for other languages. Most of our hyper-parameters are adopted from <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> and <ref type="bibr" target="#b5">Fried et al. (2019)</ref>. For scales of the two additional losses, we set the scale of pattern loss to 1.0 and the scale of consistency loss to 5.0 for all experiments.</p><p>To reduce the model size, we filter out those non-   local pattern features that appear less than 5 times in the PTB training set and those that account for less than 0.5% of all pattern occurrences in the CTB training set. The out-of-vocabulary patterns are set as &lt; UNK &gt;. This results in moderate pattern vocabulary sizes of 841 for PTB and 514 for CTB. For evaluation on PTB, CTB and cross-domain dataset, we use the EVALB script for evaluation. For the SPMRL datasets, we follow the same setup in EVALB as <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">In-domain Experiments</head><p>We report the performance of our method on the test sets of PTB and CTB in <ref type="table" target="#tab_3">Table 2</ref> and 3, respectively. Compared with the baseline parser <ref type="bibr" target="#b11">(Kitaev and Klein, 2018)</ref>, our method obtains an absolute improvement of 0.20% F1 on PTB (p&lt;0.01) and 0.33% F1 on CTB (p&lt;0.01), which verifies the effectiveness of injecting non-local features into neural local span-based constituency parsers. Note that the proposed method adds less than 0.7M parameters to the 342M parameter baseline model using BERT-large.</p><p>The parser trained with both the pattern loss (Section 4.1) and consistency loss (Section 4.2) outperforms the one trained only with pattern loss by 0.14% F1 (p&lt;0.01). This suggests that the constraints between constituents and non-local pattern features are crucial for injecting non-local features into local span-based parsers. One possible explanation for the improvement is that the constraints may bridge the gap between local and non-local supervision signals, since these two are originally separately predicted while merely sharing the same encoder in the training phase.</p><p>We further compare our method with the recent state-of-the-art parsers on PTB and CTB. <ref type="bibr" target="#b13">Liu and Zhang (2017)</ref> propose an in-order transitionbased constituency parser. <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> use self-attentive layers instead of LSTM layers to boost performance. <ref type="bibr" target="#b38">Zhou and Zhao (2019)</ref> jointly optimize constituency parsing and dependency parsing objectives using head-driven phrase structure grammar. <ref type="bibr" target="#b17">Mrini et al. (2020)</ref> extend <ref type="bibr" target="#b38">Zhou and Zhao (2019)</ref> by introducing label attention layers. <ref type="bibr" target="#b34">Zhang et al. (2020b)</ref> integrate a CRF layer to a chart-based parser for structural training (without non-local features). <ref type="bibr" target="#b26">Tian et al. (2020)</ref> use span attention for better span representation.</p><p>Compared with these methods, the proposed method achieves an F1 of 95.92%, which exceeds previous best numbers for BERT-based singlemodel parsers on the PTB test set. We further compare experiments for five runs, and find that NFC significantly outperforms <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref>  <ref type="figure">(p&lt;0.01)</ref>. The test score of 92.31% F1 on CTB significantly outperforms the result (91.98% F1) of the baseline (p&lt;0.01). Compared with the CRF parser of <ref type="bibr" target="#b34">Zhang et al. (2020b)</ref>, our method gives better scores without global normalization in training. This shows the effectiveness of integrating non-local information during training using our simple regularization. The result is highly competitive with the current best result <ref type="bibr" target="#b17">(Mrini et al., 2020)</ref>, which is obtained by using external dependency parsing data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cross-domain Experiments</head><p>We compare the generalization of our methods with baselines in <ref type="table" target="#tab_6">Table 4</ref>. In particular, all the parsers are trained on PTB training and validated on PTB development, and are tested on cross-domain test in the zero-shot setting. As shown in the table, our model achieves 5 best-reported results among 6 cross-domain test sets with an averaged F1 score of 89.85%, outperforming our baseline parser by 2.97% points. This shows that structure information is useful for improving cross-domain performance, which is consistent with findings from previous work <ref type="bibr" target="#b5">(Fried et al., 2019)</ref>.</p><p>To better understand the benefit of pattern features, we calculate Pearson correlation of n-gram pattern distributions between the PTB training set and various test sets in <ref type="figure">Figure 3</ref>. First, we find that the correlation between the PTB training set and the PTB test set is close to 1.0, which verifies the effectiveness of the corpus-level pattern knowledge during inference. Second, the 3-gram pattern correlation of all domains exceeds 0.75, demonstrating that n-gram pattern knowledge is robust across domains, which supports the strong performance of NFC in the zero-shot cross-domain setting. Third, pattern correlation decreases significantly as n increases, which suggests that transferable non-local information is limited to a certain window size of n-gram constituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multilingual Experiments</head><p>We compare NFC with <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> and <ref type="bibr" target="#b18">Nguyen et al. (2020)</ref> on SPMRL. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. <ref type="bibr" target="#b18">Nguyen et al. (2020)</ref> use pointer network to predict a sequence of pointing decisions for constituency parsing. As can be seen, <ref type="figure">Figure 3</ref>: Pearson correlation of n-gram pattern distribution between PTB training set and different test set. <ref type="bibr" target="#b18">Nguyen et al. (2020)</ref> do not show obvious advantages over <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref>. NFC outperforms these two methods on three rich resource languages. For example, NFC achieves 89.07% F1 on Korean, outperforming <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> by 0.27% F1, suggesting that NFC is generally effective across languages. However, NFC does not give better results compared with <ref type="bibr" target="#b11">Kitaev and Klein (2018)</ref> on low-resource languages. One possible explanation is that it is difficult to obtain prior linguistic knowledge from corpus-level statistics by using a relatively small number of instances.</p><p>6 Analysis 6.1 n-gram Pattern Level Performance <ref type="figure" target="#fig_2">Figure 4</ref> shows the pattern-level F1 before and after introducing the two auxiliary training objectives. In particular, we calculate the pattern-level F1 by calculating the F1 score for patterns based  on the constituency trees predicted by CKY decoding. Although our baseline parser with BERT achieves 95.76% F1 scores on PTB, the patternlevel F1 is 80.28% measured by 3-gram. When testing on the dialogue domain, the result is reduced to only 57.47% F1, which indicates that even a strong neural encoder still has difficulties capturing constituent dependency from the input sequence alone. After introducing the pattern and consistency losses, NFC significantly outperforms the baseline parser measured by 3-gram pattern F1. Though there is no direct supervision signal for 2-gram pattern, NFC also gives better results on pattern F1 of 2-gram, which are subsumed by 3-gram patterns. This suggests that NFC can effectively represent sub-tree structures. <ref type="figure">Figure 5</ref>: F1 scores versus minimum constituent span length on PTB test set. Note that constituent spans shorter than 30 accounts for approximately 98.5% of all for the PTB test set. <ref type="figure">Figure 6</ref>: Exact matching (EM) score across different domains. EM indicates the percentage of sentences whose predicted trees are entirely correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">F1 against Span Length</head><p>We compare the performance of the baseline and our method on constituent spans with different word lengths. <ref type="figure">Figure 5</ref> shows the trends of F1 scores on the PTB test set as the minimum constituent span length increases. Our method shows a minor improvement at the beginning, but the gap becomes more evident when the minimum span length increases, demonstrating its advantage in capturing more sophisticated constituency label dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Exact Match</head><p>Exact match score represents the percentage of sentences whose predicted trees are entirely the same as the golden trees. Producing exactly matched trees could improve user experiences in practical scenarios and benefit downstream applications on other tasks <ref type="bibr" target="#b19">(Petrov and Klein, 2007;</ref><ref type="bibr" target="#b12">Kummerfeld et al., 2012)</ref>. We compare exact match scores of NFC with that of the baseline parser. As shown in <ref type="figure">Figure 6</ref>, NFC achieves large improvements in exact match score for all domains. For instance, NFC gets 33.40% exact match score in the review domain, outperforming the baseline by 10.2% points. We assume that this results from the fact that NFC successfully ensures the output tree structure by modeling non-local correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Efficiency</head><p>As mentioned in Section 4.4, NFC only introduces a few training parameters to the baseline model <ref type="bibr" target="#b11">(Kitaev and Klein, 2018)</ref>. For PTB, NFC takes about 19 hours to train with a single RTX 2080Ti, while the baseline takes about 13 hours. For CTB, the approximate training time is 12 hours for NFC and 7 hours for the baseline. Our inference time is the same as that of the baseline parser, since no further computational operations are added to the inference phase. Both take around 11 seconds to parse the PTB section 23 (2416 sentences, an average of 23.5 tokens per sentence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We investigated graph-based constituency parsing with non-local features -both in the sense that features are not restricted to one constituent, and in the sense that they are not restricted to each training instance. Experimental results verify the effectiveness of injecting non-local features to neural chart-based constituency parsing. Equipped with pre-trained BERT, our method achieves 95.92% F1 on PTB and 92.31% F1 on CTB. We further demonstrated that the proposed method gives better or competitive results in multilingual and zero-shot cross-domain settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The three training objectives in NFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) F1 scores measured by 3-gram pattern.(b) F1 scores measured by 2-gram pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Pattern-level F1 on different English datasets. Noted that we train NFC based on 3-gram pattern in English. There is no direct supervision signal for 2gram pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. # -number of sentences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance (w/ BERT) on the test set of PTB. ? indicates our reproduced results, which is also the baseline that our method is built upon. * indicates training with extra supervision from dependency parsing data. indicates that the results are reported by the re-implementation of<ref type="bibr" target="#b5">Fried et al. (2019)</ref>.</figDesc><table><row><cell>Model</cell><cell>LR</cell><cell>LP</cell><cell>F1</cell></row><row><cell>Liu and Zhang (2017)</cell><cell>-</cell><cell>-</cell><cell>91.81</cell></row><row><cell>Kitaev and Klein (2018)</cell><cell cols="3">91.55 91.96 91.75</cell></row><row><cell>Zhang et al. (2020b)</cell><cell cols="3">92.04 92.51 92.27</cell></row><row><cell>Zhou and Zhao (2019)</cell><cell cols="3">91.14 93.09 92.10</cell></row><row><cell>Tian et al. (2020)</cell><cell cols="3">92.14 92.25 92.20</cell></row><row><cell cols="2">This work</cell><cell></cell><cell></cell></row><row><cell cols="4">Kitaev and Klein (2018)  ? 91.80 92.23 91.98</cell></row><row><cell>NFC w/o L reg</cell><cell cols="3">91.87 92.40 92.13</cell></row><row><cell>NFC</cell><cell cols="3">92.17 92.45 92.31</cell></row><row><cell cols="3">w/ External Dependency Supervision</cell><cell></cell></row><row><cell>Zhou and Zhao (2019) *</cell><cell cols="3">92.03 92.33 92.18</cell></row><row><cell>Mrini et al. (2020)*</cell><cell cols="3">91.85 93.45 92.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Constituency parsing performance (w/ BERT)</cell></row><row><cell>on the test set of CTB 5.1. The symbols ( ?, * and ) are</cell></row><row><cell>explained in Table 2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Constituency parsing results with BERT (F1 scores) on the cross-domain test set.</figDesc><table><row><cell>Model</cell><cell cols="3">Rich resource French German Korean</cell><cell>Avg</cell><cell cols="3">Low Resource Hungarian Basque Polish</cell><cell>Avg</cell><cell>Avg</cell></row><row><cell>Kitaev and Klein (2018)</cell><cell>87.42</cell><cell>90.20</cell><cell>88.80</cell><cell>88.81</cell><cell>94.90</cell><cell>91.63</cell><cell>96.36</cell><cell cols="2">94.30 91.55</cell></row><row><cell>Nguyen et al. (2020)</cell><cell>86.69</cell><cell>90.28</cell><cell>88.71</cell><cell>88.56</cell><cell>94.24</cell><cell>92.02</cell><cell>96.14</cell><cell cols="2">94.13 91.34</cell></row><row><cell>Kitaev and Klein (2018)  ?</cell><cell>87.38</cell><cell>90.25</cell><cell>88.91</cell><cell>88.85</cell><cell>94.56</cell><cell>91.66</cell><cell>96.14</cell><cell cols="2">94.12 91.48</cell></row><row><cell>NFC</cell><cell>87.51</cell><cell>90.43</cell><cell>89.07</cell><cell>89.00</cell><cell>94.95</cell><cell>91.73</cell><cell>96.33</cell><cell cols="2">94.34 91.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Multilingual Experiment results on SPMRL test-sets. ? indicates our reproduced baselines.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Patterns are mainly composed of n-gram constituents but also include part-of-speech tags as auxiliary.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.gutenberg.org/ 3 Available at https://github.com/nikitakit/ self-attentive-parser.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We appreciate the insightful comments from the anonymous reviewers. We thank Zhiyang Teng for the insightful discussions. We gratefully acknowledge funding from the National Natural Science Foundation of China (NSFC No.61976180).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>As mentioned in Section 5.1, we collected the raw data from free and publicly available sources that have no copyright or privacy issues. We recruited our annotators from the linguistics departments of local universities through public advertisement with a specified pay rate. All of our annotators are senior undergraduate students or graduate students in linguistic majors who took this annotation as a part-time job. We manually shuffled the data so that all batches of to-be-annotated data have similar lengths on average. An annotator could annotate around 25 instances per hour. We pay them 50 CNY an hour. The local minimum salary in the year 2021 is 22 CNY per hour for part-time jobs.</p><p>Our annotated data only involves factual information (i.e., syntactic annotation), but not opinions, attitudes or beliefs. Therefore, the annotation job does not belong to human subject research; and IRB approval is not required.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchicallyrefined label attention network for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1422</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4115" to="4128" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural CRF parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain generalization of neural constituency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What&apos;s going on in neural constituency parsers? an analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="999" to="1010" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00237</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertainty-aware label refinement for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichu</forename><surname>Fei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2316" to="2326" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web, WWW &apos;16</title>
		<meeting>the 25th International Conference on World Wide Web, WWW &apos;16<address><addrLine>Republic and Canton of Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual constituency parsing with self-attention and pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3499" to="3505" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parser showdown at the Wall Street corral: An empirical investigation of error types in parser output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1048" to="1059" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In-order transition-based constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00070</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="413" to="424" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno>abs/1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analyzing and integrating dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00039</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="197" to="230" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking self-attention: Towards interpretability in neural parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Mrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapa</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakashole</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.65</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient constituency parsing by pointing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Tung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3284" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing both the forest and the trees: Multi-head attention for joint classification on different compositional levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miruna</forename><surname>Pislar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3761" to="3775" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich?rd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Koldo Gojenola Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Przepi?rkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Woli?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wr?blewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clergerie</forename><surname>Villemonte De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1076</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A minimal span-based neural constituency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1076</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?yvind</forename><surname>Stiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Voeten</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/OBYUO5</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ECtHR judgments</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two local models for neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving constituency parsing with span attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.153</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TL;DR: Mining Reddit to Learn Automatic Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>V?lske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4508</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop on New Frontiers in Summarization at EMNLP 2017</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tree-based decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1509</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4772" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural extractive text summarization with syntactic compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3292" to="3303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
		<idno type="DOI">10.1017/S135132490400364X</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Challenges to open-domain constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-label learning by exploiting label dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1835804.1835930</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10</title>
		<meeting>the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient second-order TreeCRF for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3295" to="3305" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and accurate neural crf constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transition-based parsing of the Chinese treebank using a global discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies (IWPT&apos;09)</title>
		<meeting>the 11th International Conference on Parsing Technologies (IWPT&apos;09)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analyzing the effect of global learning and beam-search on transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1391" to="1400" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Posters</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Head-Driven Phrase Structure Grammar parsing on Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2396" to="2408" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
