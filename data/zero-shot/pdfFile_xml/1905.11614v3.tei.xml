<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-based Continual Learning with Adaptive Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjoon</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Cha</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<postCode>16419</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<postCode>16419</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Moon</surname></persName>
							<email>tsmoon@skku.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<postCode>16419</postCode>
									<settlement>Suwon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-based Continual Learning with Adaptive Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian meanfield approximation. Based on the interpretation, we propose the notion of nodewise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continual learning, also called as lifelong learning, is a long-standing open problem in machine learning in which data from multiple tasks continuously arrive and the learning algorithm should constantly adapt to new tasks as well as not forget what it has learned in the past. The main challenge is to resolve the so-called stability-plasticity dilemma <ref type="bibr">[2,</ref><ref type="bibr" target="#b17">18]</ref>. Namely, a learning agent should be able to preserve what it has learned, but focusing too much on the stability may hinder it from quickly learning a new task. On the other hand, when the agent focuses too much on the plasticity, it tends to quickly forget what it has learned. Particularly, for the artificial neural network (ANN)-based models, which became the mainstream of the machine learning methods, it is well-known that they are prone to such catastrophic forgetting phenomenon <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4]</ref>. As opposed to the ANNs, humans are able to maintain the obtained knowledge while learning a new task, and the forgetting in human brain happens gradually rather than drastically. This difference motivates active research in developing neural network based continual learning algorithms.</p><p>As given in a comprehensive survey <ref type="bibr" target="#b19">[20]</ref> on this topic, approaches for tackling the catastrophic forgetting in neural network based continual learning can be roughly grouped into three categories: regularization-based <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref>, dynamic network architecture-based <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>, and dual memory system-based <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>. While each category has its own merit, of particular interest are the regularization-based methods, since they pursue to maximally utilize the limited network capacity by imposing constraints on the update of the network given a new task. Computationally, they typically are realized by adding regularization terms that penalize the changes in the network parameters when learning a new task. This approach makes sense since it is well-known that neural network models are highly over-parametrized, and once successful, it can be also complementary to other approaches since it can lead to the efficient usage of network capacity as the number of tasks grows, as in <ref type="bibr" target="#b24">[25]</ref>.</p><p>The recent state-of-the-art regularization-based methods typically implement the per-parameter regularization parameters based on several different principles inferring the importance of each parameter for given tasks; e.g., diagonal Fisher information matrix for EWC <ref type="bibr" target="#b11">[12]</ref>, variance term associated with each weight parameter for VCL <ref type="bibr" target="#b18">[19]</ref>, and the path integrals of the gradient vector fields for SI <ref type="bibr" target="#b29">[30]</ref>. While these methods are shown to be very effective in several continual learning benchmarks, a common caveat is that the amount of the memory required to store the model is twice the original neural network parameters, since they need to store the individual regularization parameters. We note that this could be a limiting factor for being deployed with large network size.</p><p>In this paper, we propose a new regularization-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), that stores much smaller number of additional parameters for regularization terms than the recent state-of-the-arts, but achieves much better performance in several benchmark datasets. Followings summarize our key contributions.</p><p>? We adopt the standard Bayesian online learning framework, but make a fresh interpretation of the Kullback-Leibler (KL) divergence term of the variational lower bound for the Gaussian mean-field approximation case. ? We define a novel notion of "uncertainty" for each hidden node in a network by tying the learnable variances of the incoming weights of a node. Moreover, we add two additional regularization terms to freeze the weights that are identified to be important and to gracefully forget what was learned before and control the actively learning weights. ? We achieve state-of-the-art performances on a number of continual learning benchmarks, including supervised learning (SL) tasks with deep convolutional neural networks and reinforcement learning (RL) tasks with different state-action spaces. Performing well on both SL and RL continual learning tasks is a unique strength of our UCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Continual learning There are numerous approaches in continual learning and we refer the readers to <ref type="bibr" target="#b19">[20]</ref> for an extensive review. We only list work relevant to our method. The main approach of regularization-based methods in continual learning is to identify the important weights for the learned tasks and penalize the large updates on those weights when learning a new task. LwF <ref type="bibr" target="#b13">[14]</ref> contains task-specific layers, and keeps the similar outputs for the old tasks by utilizing knowledge distillation <ref type="bibr" target="#b8">[9]</ref>. In EWC <ref type="bibr" target="#b11">[12]</ref>, the diagonal of the Fisher information matrix at the learned parameter of the given task is used for giving the relative regularization strength. An extended version of EWC, IMM <ref type="bibr" target="#b12">[13]</ref>, merged the posteriors based on the mean and the mode of the old and new parameters. SI <ref type="bibr" target="#b29">[30]</ref> computes the parameter importance considering a path integral of gradient vector fields during the parameter updates. VCL <ref type="bibr" target="#b18">[19]</ref> also adopts Bayesian online learning framework as ours, but simply applies standard techniques that results in some drawbacks, which are elaborated in Section 3.1.</p><p>Some work approached continual learning differently than the regularization-based method for the limited network capacity case. PackNet <ref type="bibr" target="#b15">[16]</ref> picks out task-specific weights based on the weight pruning method, which requires saving the learnable binary masks for the weights. HAT <ref type="bibr" target="#b25">[26]</ref> employs node-wise attention mechanism per layer using the task identifier embedding, but requires a knowledge on the number of tasks a priori, which is a critical limitation.</p><p>Variational inference In standard Bayesian learning, the main idea of learning is efficiently approximating the posterior distribution on the models. <ref type="bibr" target="#b5">[6]</ref> introduces a practical variational inference technique for neural networks, which suggested that variational parameters can be learned using back-propagation. Another approach in variational inference is <ref type="bibr" target="#b10">[11]</ref> which introduces the approximated lower bound of likelihood, and learn variational parameter using re-parameterization trick.</p><p>In <ref type="bibr">[1]</ref>, they introduce Unbiased Monte Carlo, which also uses back-propagation, but many kinds of priors can be used in the Unbiased Monte Carlo. In addition, there are several practical methods for variational inference in neural networks, such as using dropout <ref type="bibr" target="#b4">[5]</ref> or Expectation-Propagation <ref type="bibr" target="#b7">[8]</ref>.</p><p>3 Uncertainty-regularized Continual Learning (UCL)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and a review on Bayesian online learning</head><p>Consider a discriminative neural network model, p(y|x, W), that returns a probability distribution over the output y given an input x and parameters W. In standard Bayesian learning, W is assumed to be sampled from some prior distribution p(W|?) that depends on some parameter ?, and after observing some data D = {(x i , y i )} n i=1 , obtaining the posterior p(W|?, D) becomes the central problem to learn the model parameters. Since exactly obtaining the posterior becomes intractable, variational inference <ref type="bibr">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref> instead tries to approximate this posterior with a more tractable distribution q(W|?). The approximation is done by minimizing (over ?) the so-called variational free energy, which can be written as</p><formula xml:id="formula_0">F(D, ?) =E q(W|?) [? log p(D|W)] + D KL (q(W|?)||p(W|?)),<label>(1)</label></formula><p>in which log p(D|W) is the log-likelihood of the data D determined by the model p(y|x, W), and D KL (?) is the Kullback-Leibler divergence. Moreover, the commonly used q(W|?) is the so-called Gaussian mean-field approximation, q(W|?) = i N (w i |? i , ? i ) with ? = (?, ?), and ? can be learned via reparameterization trick <ref type="bibr" target="#b10">[11]</ref> and the standard back-propagation.</p><p>In Bayesian online learning framework, standard variational inference can be applied to the continual learning setting. Namely, when a dataset for task t, D t arrives, the framework solves to minimize</p><formula xml:id="formula_1">F(D t , ? t ) =E q(W|?t) [? log p(D t |W)] + D KL (q(W|? t )||q(W|? t?1 ))<label>(2)</label></formula><p>over ? t = (? t , ? t ), in which q(W|? t?1 ) stands for the posterior learned after observing D t?1 acting as a prior for learning q(W|? t ). Note in <ref type="formula" target="#formula_1">(2)</ref>, we can observe that the KL-divergence term naturally acts as a regularization term. In VCL <ref type="bibr" target="#b18">[19]</ref>, they showed that the network learned by sequentially solving (2) by using projection operator of the variational inference for each task t can successfully combat the catastrophic forgetting problem to some extent.</p><p>However, we argue that this Bayesian approach of VCL has several drawbacks as well. First, due to the Monte-Carlo sampling of the model weights for computing the likelihood term in (2), the time and space complexity for learning grows with the sample size. Second, since the variance term is defined for every weight parameter, the number of parameters to maintain becomes exactly twice the size of network weights. This becomes problematic when deploying a large-sized network, as is the case in modern deep learning. In this paper, we present a novel approach which can resolve above problems. Our key idea is rooted in a fresh interpretation of the closed form of KL-divergence term in (2) for the Gaussian mean-field approximation and the Bayesian neural network pruning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interpreting KL-divergence and motivation of UCL</head><p>While the KL divergence in (2) acts as a generic regularization term, we give a closer look at it, particularly for the Gaussian mean-field approximation case. Namely, after some algebra and evaluating the Gaussian integral, the closed-form of D KL (q(W|? t ) q(W|? t?1 )) becomes:</p><formula xml:id="formula_2">1 2 L l=1 ? (l) t ? ? (l) t?1 ? (l) t?1 2 2 (a) + 1 ? (l) t ? (l) t?1 2 ? log ? (l) t ? (l) t?1 2 (b) ,<label>(3)</label></formula><p>in which L is the number of layers in the network, (?</p><formula xml:id="formula_3">(l) t , ? (l)</formula><p>t ) are the mean and standard deviation of the weight matrix for layer l that are subject to learning for task t, (?</p><formula xml:id="formula_4">(l) t?1 , ? (l)</formula><p>t?1 ) are the same quantity that are learned up to the previous task, the fraction notation means the element-wise division between tensors, and ? 2 2 stands for the Frobenius norm of a matrix. The detailed derivation of (3) is given in the Supplementary Materials. the term (a) in (3) can be interpreted as a square of the Mahalanobis distance between the vectorized ?   </p><formula xml:id="formula_5">t = (? (l) t , ? (l) t )</formula><p>, the inverse of the variance learned up to task (t ? 1) is acting as per-weight regularization strengths for ? (l) t deviating from ? (l) t?1 . This makes sense since each element of (? (l) t?1 ) 2 can be regarded as an uncertainty measure for the corresponding mean weight of ? (l) t?1 , and a weight with small uncertainty should be treated as important such that high penalty is imposed when significantly getting updated for a new task t. Moreover, the term (b) in <ref type="bibr" target="#b2">(3)</ref>, which is convex in (? (l) t ) 2 and is minimized when ?</p><formula xml:id="formula_6">(l) t = ? (l)</formula><p>t?1 , is acting as a regularization term for ? 2 t . Note it promotes to preserve the learned uncertainty measure when updating for a new task. This also makes sense for preventing catastrophic forgetting since the weights identified as important in previous tasks should be kept as important for future tasks as well such that the weights do not get updated too much by the term (a). Based on this interpretation, we modify each term and devise a new loss function for UCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modifying the term (a)</head><p>We modify the term (a) in (3) based on the following three intuitions. First, instead of maintaining the uncertainty measure for each mean weight parameter of ? t , we devise a notion of uncertainty for each node of the network. Second, based on the node uncertainty, we set the high regularization strength for a weight when either of the nodes it connects has low uncertainty. Third, we add additional 1 -regularizer such that a weight gets even more stringent penalty for getting updated when the weight has large magnitude or low uncertainty, inspired by <ref type="bibr">[1,</ref><ref type="bibr" target="#b5">6]</ref>. We elaborate each of these intuitions below.</p><p>While it is plausible to maintain the weight-level importance as in other work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, we believe maintaining the importance (or uncertainty in our case) at the level of node makes more sense, not only for the purpose of reducing the model parameters, but also because the node value (or the activation) is the basic unit for representing the learned information from a task. A similar intuition of working at node-level also appears in HAT <ref type="bibr" target="#b25">[26]</ref>, which devised a hard attention mechanism for important nodes, or dropout <ref type="bibr" target="#b27">[28]</ref>, which randomly drops nodes while training. In our setting, we define the uncertainty of a node as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>; first constrain the incoming weights to the node to have the same standard deviation parameters as in node j of layer (l ? 1) in <ref type="figure" target="#fig_1">Figure 1</ref>, then set the variance as the uncertainty of the node. For the Gaussian mean-field approximation case, this constraint corresponds to adding zero-mean i.i.d Gaussian noise (with difference variances for different nodes) to the incoming weights when sampling for the variational learning.  For our second intuition, we derive the weight-level regularization scheme based on the following arguments. Namely, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, suppose a node is identified as important (the orange nodes), i.e., has low uncertainty, for the past tasks, and the learning of a new task is taking place. We believe there are two major sources that can cause the catastrophic forgetting of the past tasks when a weight update for a new task happens; 1) the negative transfer (blue region) happening in the incoming weights of an important node, and 2) the information loss (pink region) happening in the outgoing weights of an important node. From the perspective of the important node, it is clear that when any of the incoming weights are significantly updated during the learning of the new task, the node's representation of the past tasks will significantly get altered as the node will differently combine information from the lower layer, and hurt the past tasks accuracy. On the other hand, when the outgoing weights of the important node are significantly updated, the information of that node will get washed out during forward propagation, hence, it may not play an important role in computing the prediction, causing the accuracy drop for the past task.</p><p>From above argument, we devise the weight-level regularization such that weight gets high regularization strength when either of the node it connects has low uncertainty. This is realized by replacing the term (a) of (3) with the following:</p><formula xml:id="formula_7">1 2 L l=1 ? (l) (? (l) t ? ? (l) t?1 ) 2 2 , where ? (l) ij max ? (l) init ? (l) t?1,i , ? (l?1) init ? (l?1) t?1,j ,<label>(4)</label></formula><p>in which ? (l) init is the initial standard deviation hyperparameter for all weights on the l-th layer, L is the number of layers in the network, ? (l) t is the mean weight matrix for layer l and task t, is the element-wise multiplication between matrices, and the matrix ? (l) defines the regularization strength for the weight ? (l) t,ij ; i.e., when either ?</p><formula xml:id="formula_8">(l) t?1,i or ? (l?1) t?1,j is small, ? (l)</formula><p>t,ij gets high regularization strength. We note setting ? (l) init correctly is important to control the stability of the learning process. While (4) is a sensible replacement of the term (a) in (3), our third intuition above is based on the observation that (4) does not take into account of the magnitude of the learned weights, i.e., ? (l) t?1 . In <ref type="bibr">[1,</ref><ref type="bibr" target="#b5">6]</ref>, they applied a heuristic for pruning network weights learned by variational inference; i.e., only keeps the weight if the magnitude of the ratio ?/? is large, and prunes otherwise. Inspired by the pruning heuristic, we devise an additional 1 -norm based regularizer</p><formula xml:id="formula_9">L l=1 (? (l) init ) 2 ? (l) t?1 ? (l) t?1 2 (? (l) t ? ? (l) t?1 ) 1 ,<label>(5)</label></formula><p>in which the division and square inside the 1 -norm should be understood as the element-wise operations. Note ? (l)</p><p>t?1 has the same dimension as ?</p><formula xml:id="formula_10">(l) t?1 , and the i-th row of ? (l)</formula><p>t?1 has the same variance value associated with the i-th node in layer l. Thus, in <ref type="bibr" target="#b4">(5)</ref>, if the ratio (? (l)</p><formula xml:id="formula_11">t?1,ij /? (l) t?1,i ) 2 is large, the 1 -norm will promote sparsity and ? (l) t,ij will tend to freeze to ? (l) t?1,ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modifying the term (b)</head><p>Regarding the term (b) in <ref type="formula" target="#formula_2">(3)</ref>, we can also devise a similar loss on the uncertainties associated with nodes. As mentioned in Section 3.2, the loss will promote ?</p><formula xml:id="formula_12">(l) t = ? (l)</formula><p>t?1 , meaning that once a node becomes important at task (t ? 1), it tends to stay important for a new task as well. While this makes sense for preventing the catastrophic forgetting as it may induce high regularization parameters for penalties in <ref type="formula" target="#formula_7">(4)</ref> and <ref type="formula" target="#formula_9">(5)</ref>, one caveat is that the network capacity can quickly fill up when the number of tasks grows. Therefore, we choose to add one more regularization term to the term (b) in <ref type="formula" target="#formula_2">(3)</ref>,</p><formula xml:id="formula_13">1 2 1 (? (l) t ) 2 ? log(? (l) t ) 2 ,<label>(6)</label></formula><formula xml:id="formula_14">which inflates ? (l) t to get close to ? 2? (l)</formula><p>t?1 when minimized together with the term (b). The detailed derivation of the minimizer is given in the Supplementary Materials. Therefore, if a node becomes uncertain when training current task, the regularization strength becomes smaller. Since our initial standard deviation ? (l) init is usually set to be small, the additional term in (6) compared to the term (b) in (3) will tend to increase the number of "actively" learning nodes that have incoming weights with sufficiently large standard deviation values for exploration. Moreover, when a new task arrives while most of the nodes have low uncertainty, (6) will force some of them to increase the uncertainty level to learn the new task, resulting in gracefully forgetting the past tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Init Task1 Task2</head><p>Output <ref type="figure" target="#fig_19">Figure 2</ref>: Colored hidden nodes and edges denote important nodes and highly regularized weights due to (4), respectively. The width of colored edge denotes the regularization strength of (5). Note as new task comes the uncertainty level of a node can vary due to <ref type="bibr" target="#b5">(6)</ref>, respresented with color changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Final loss function for UCL</head><p>Combining (4), <ref type="bibr" target="#b4">(5)</ref>, and (6), the final loss function for our UCL for task t becomes</p><formula xml:id="formula_15">? log p(D t |W) + L l=1 1 2 ? (l) (? (l) t ? ? (l) t?1 ) 2 2 + (? (l) init ) 2 ? (l) t?1 ? (l) t?1 2 (? (l) t ? ? (l) t?1 ) 1 + ? 2 1 ? (l) t ? (l) t?1 2 ? log ? (l) t ? (l) t?1 2 + (? (l) t ) 2 ? log(? (l) t ) 2 ,<label>(7)</label></formula><p>which is minimized over {?</p><formula xml:id="formula_16">(l) t , ? (l)</formula><p>t } L l=1 and has two hyperparameters, {? (l) init } L l=1 and ?. The former serves as pivot values determining the degree of uncertainty of each node, and the latter controls the increasing or decreasing speed of ? (l) t . As elaborated in above sections, it is clear that the uncertainty of a node plays a critical role in setting the regularization parameters, hence, justifies the name UCL. Illustration of the regularization mechanism of UCL is given in <ref type="figure" target="#fig_19">Figure 2</ref>. At the beginning epoch of task t, we sample from q(W|? t ) with ? t = ? t?1 , then continue to update ? t in the subsequent iterations. The model parameters are sampled every iteration, like in the usual Monte Carlo sampling, but we set the number of sampling to 1 for each iteration. This is an important differentiation that enables the application of UCL to reinforcement learning tasks, which was impossible for VCL <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Supervised learning</head><p>We evaluate the performance of UCL together with EWC <ref type="bibr" target="#b11">[12]</ref>, SI <ref type="bibr" target="#b29">[30]</ref>, VCL <ref type="bibr" target="#b18">[19]</ref>, and HAT <ref type="bibr" target="#b25">[26]</ref>. We also make a comparison with Coreset VCL proposed in <ref type="bibr" target="#b18">[19]</ref>. The number of sampling weights was 10 for VCL, and 1 for UCL. All of the results are averaged over 5 different seeds. For the experiments with MNIST datasets, we used fully-connected neural networks (FNN), and with CIFAR-10/100 and Omniglot datasets, we used convolutional neural networks (CNN). The detailed architectures are given in each experiment section. Moreover, the initial standard deviations for UCL, {?   Permuted / Row Permuted MNIST We first test on the popular Permuted MNIST dataset. We used single-headed FNN that has two hidden layers with 400 nodes and ReLU activations for all methods. We compare the average test accuracy over the learned tasks in <ref type="figure" target="#fig_7">Figure 3</ref> (left). After training on 10 tasks sequentially, EWC, SI, and VCL show little difference of performance among them achieving 91.8%, 91.1%, and 91.3% respectively. Although VCL with the coreset size of 200 makes an improvement of 2%, UCL outperforms all other baselines achieving 94.5%. Interestingly, HAT keeps almost the same average accuracy as UCL until the first 5 tasks, but it starts to significantly deteriorate after task 7. This points out the limitation of applying HAT in a single-headed network. As a variation of Permuted MNIST, we shuffled only rows of MNIST images instead of shuffling all the image pixels, and we denoted it as Row Permuted MNIST. We empirically find that all algorithms are more prone to forgetting in Row Permuted MNIST. Looking at the accuracy scale of <ref type="figure" target="#fig_7">Figure 3</ref> (right), all the methods show severe degradation of performance compared to Permuted MNIST. This may be due to permuting of the correlated row blocks causing more weight changes in the network. After 10 tasks, UCL again achieved the highest average accuracy, 86.5%, in this experiment as well.</p><p>For a better understanding of our model, <ref type="figure" target="#fig_8">Figure 4</ref> visualize the learned standard deviations of nodes in all layers as the training proceeds. After the model trained on task 1, we find that just a few of them become smaller than the initialized value of 0.06, and most of them become much larger in the first hidden layer. Interestingly, the uncertain nodes in layer 1 show a drastic decline of their standard deviations at a specific task as the learning progresses, which means the model had to make them certain for adapting to the new task. On the other hand, all the nodes in the output layer had to reduce their uncertainty as early as possible considering even a small randomness can lead to a totally different prediction. Most of the nodes in layer 2, in addition, do not show a monotonic tendency. This can be interpreted as many of them need not belong to a particular task. As a result, this gives the plasticity and gracefully forgetting trait of our UCL.  <ref type="figure" target="#fig_24">Figure 5</ref>: Ablation study in Permuted MNIST. Each line denotes the test accuracy.</p><p>We also carry out an ablation study on UCL's additional regularization terms. <ref type="figure" target="#fig_24">Figure 5</ref> shows the results of three variations that lack one of the ingredients of the proposed UCL on Permuted MNIST. "UCL w/o upper freeze" stands for using ? <ref type="formula" target="#formula_7">(4)</ref>, and we observe regularizing the outgoing weights of an important node in UCL very important. "UCL w/o (5)" stands for the removing (5) from <ref type="formula" target="#formula_15">(7)</ref>, and we clearly see the pruning heuristic based weight freezing is also very important. "UCL w/o (6)" stands for not using <ref type="bibr" target="#b5">(6)</ref> and it shows that while the accuracy of Task 1 &amp; 2 are even higher than UCL, but the accuracy drastically decreases after Task 3. This is because due to the rapid decrease of model capacity since "actively" learning weights reduce when (6) is not used.</p><formula xml:id="formula_17">(l) ij = ? (l) init /? (l) t?1,i in</formula><p>Split MNIST We test also in the splitted dataset setting that each task consists of 2 consecutive classes of MNIST dataset. This benchmark was used in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> and has total 5 tasks. We used multi-headed FNN hat has two hidden layers with 256 nodes and ReLU activations for all methods.</p><p>In <ref type="figure" target="#fig_9">Figure 6</ref> (top), we compare the test accuracy of each task together with the average accuracy over all observed tasks at the right end. UCL accomplishes the same 5 tasks average accuracy as HAT; 99.7%, which is slightly better than the results of SI and VCL with coreset, 99.0%, and 98.7%, respectively. Note UCL significantly outperforms EWC and VCL. We also point out that HAT makes a critical assumption to know the number of tasks a priori, while UCL need not.</p><p>Split notMNIST Here, we make an assessment on another splitted dataset tasks with notMNIST dataset, which has 10 character classes. We split the characters of notMNIST into 5 groups same as VCL <ref type="bibr" target="#b18">[19]</ref>: A/F, B/G, C/H, D/I, and E/J. We used multi-headed FNN hat has four hidden layers with 150 nodes and ReLU activations for all methods. Unlike the previous experiments, SI shows similar results to EWC around 84% average accuracy, and VCL attains a better result of 90.1% (in <ref type="figure" target="#fig_9">Figure 6</ref>) (bottom). Our UCL again achieves a superior an outstanding result of 95.7%, that is higher than HAT and VCL with coreset: 95.2% and 93.7%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split CIFAR and Omniglot</head><p>To check the effectiveness of UCL beyond the MNIST tasks, we experimented our UCL on three additional datasets, Split CIFAR-100, Split CIFAR10/100 and Omniglot. For Split CIFAR-100, each task consists of 10 consecutive classes of CIFAR-100, for Split CIFAR-10/100, we combined CIFAR-10 and Split CIFAR-100, and for Omniglot, each alphabet is treated as a single task, and we used all 50 alphabets. For Omniglot, as in <ref type="bibr" target="#b24">[25]</ref>, we rescaled all images to 28 ? 28 and augmented the dataset by including 20 random permutations (rotations and shifting) for each image. For these datasets, unlike in the previous experiments using FNNs, we used deeper CNN architectures, in which the notion of uncertainty in the convolution layer is defined for each channel (i.e., filter). We used multi-headed outputs for all experiments, and 8 different random seed runs are averages for all datasets. The details of experiments using CNNs, including the architectures and hyperparameters, are given in the Supplementary Materials. In <ref type="figure" target="#fig_11">Figure 7</ref>, we compared UCL with EWC and SI and carried out extensive hyperparameter search for fair comparison. We did not compare with VCL since it did not have any results on vision datasets with CNN architectures.  In Split CIFAR-100, EWC and SI achieve 60.5% and 60.0% respectively. However, UCL outperforms SI and EWC achieving 63.4%. In a slightly different task, Split CIFAR-10/100, which prevents overfitting on Split CIFAR-100 using a model pre-trained on CIFAR-10, UCL also outperforms baselines by achieving 73.2%. In Omniglot, although UCL becomes slightly unstable for the first task, it eventually achieves 83.9% average accuracy on all 50 tasks. However, EWC and SI only achieves 68.1% and 74.2% respectively, much lower than UCL. From above three results, we clearly observe UCL clearly outperforms the baselines for more diverse and sophisticated vision datasets and for deeper CNN architectures.  <ref type="table" target="#tab_1">Table 1</ref> shows the number of model parameters in each experiment. Vanilla stands for the base network architecture of all methods. It is shown that UCL has fewer parameters than other regularization-based approaches. Especially, UCL has almost half the number of VCL, which is based on the similar variational framework. Although HAT shows the least number of parameters, we stress it has the critical drawback of requiring to know the number of tasks a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of model parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reinforcement learning</head><p>Here, we also tested UCL for the continual reinforcement learning tasks. Roboschool <ref type="bibr" target="#b23">[24]</ref> consists of 12 tasks and each task has a different shape of the state and continuous action space, and goal. From these tasks, we randomly chose eight tasks and sequentially learned each task (with 5 million update steps) in the following order, {Walker-HumanoidFlagrun-Hooper-Ant-InvertedDoublePendulum-Cheetah-Humanoid-InvertedPendulum}. We trained a FNN model using PPO <ref type="bibr" target="#b23">[24]</ref> as a training algorithm and selected EWC and Fine-tuning as baselines. All baselines were experimented in exactly the same condition, and we carried out an extensive hyperparameter search for fair comparison.  More experimental details, network architectures, and hyperparameters are given in the Supplementary Materials. <ref type="figure" target="#fig_13">Figure 8</ref> shows the cumulative normalized rewards up to the learned task, and <ref type="figure" target="#fig_15">Figure 9</ref> shows the normalized rewards for each task with vertical dotted lines showing the boundaries of the tasks. The normalization in the figures was done for each task with the maximum rewards obtained by EWC (? = 10). The high cumulative sum thus corresponds to effectively combating the catastrophic forgetting (CF), and we note Fine-tuning mostly suffers from CF (e.g., Task2 or Task4). Note we show two versions of UCL, with different ? hyperparameter values. In <ref type="figure" target="#fig_13">Figure 8</ref>, we observe both versions of UCL significantly outperform both EWC and Fine-tuning. We believe the reason why EWC does not excel as in <ref type="figure" target="#fig_8">Figure 4B</ref> of the original EWC paper <ref type="bibr" target="#b11">[12]</ref> is because we consider pure continual learning setting, while <ref type="bibr" target="#b11">[12]</ref> allows learning tasks multiple times in a recurring fashion. Moreover, a possible reason why UCL achieves such high rewards in RL setting may be due to the by-product of our weight sampling procedure; namely, the Gaussian perturbation of the weights for the variational inference enables an effective exploration of policies for RL as suggested in <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure" target="#fig_15">Figure 9</ref> shows UCL overwhelmingly surpasses EWC particularly for Task1 and Task3 (by both achieving high rewards and not forgetting), and it contributes to the significant difference between EWC in <ref type="figure" target="#fig_13">Figure 8</ref>. We also experimentally checked the role of ? for gracefully forgetting; although UCL with ? = 5 ? 10 ?6 results in overall better rewards, with ? = 5 ? 10 ?5 does better in learning new tasks, e.g., Task5/7/8, by adding more plasticity to the network. To the best of our knowledge, this result shows for the first time that the pure continual learning is possible for reinforcement learning with continuous action space and different observation shapes. We stress that there are very few algorithms in the literature that work well on both SL and RL continual learning setting, and our UCL is very competitive in that sense.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed UCL, a new uncertainty-based regularization method for overcoming catastrophic forgetting. We proposed the notion of node-wise uncertainty motivated from the Bayesian online learning framework and devised novel regularization terms for dealing with stability-plasticity dilemma. As a result, UCL convincingly outperformed other state-of-the-art baselines in both supervised and reinforcement learning benchmarks with much fewer additional parameters. Let's assume q(W|? t ) and q(W|? t?1 ) as below by the Gaussian mean-field approximation.</p><formula xml:id="formula_18">q(W|? t ) = N (W|? t , ? 2 t ) = D d=1 1 2?? 2 t,d exp ? 1 2 (W d ? ? t,d ) 2 ? 2 t,d q(W|? t ) = N (W|? t?1 , ? 2 t?1 ) = D d=1 1 2?? 2 t?1,d exp ? 1 2 (W d ? ? t?1,d ) 2 ? 2 t?1,d</formula><p>Then, D KL (q(W|? t )||q(W|? t?1 )) becomes as follows. For simplicity, we decompose D KL (q(W|? t )||q(W|? t?1 )) as <ref type="formula" target="#formula_0">(1)</ref> and <ref type="bibr">(2)</ref>. First, the closed form of the integral in (1) becomes as follows.</p><formula xml:id="formula_19">D KL (q(W|? t )||q(W|? t?1 )) = q(W|? t ) log q(W|? t ) q(W|? t?1 ) dW = N (W|? t , ? t ) log N (W|? t , ? 2 t ) N (W|? t?1 , ? 2 t?1 ) dW = D d=1 N (W|? t , ? 2 t ) log N (W d |? t , ? 2 t ) N (W d |? t?1,d , ? 2 t?1,d ) dW = D d=1 N (W d |? t,d , ? 2 t,d ) log N (W d |? t,d , ? 2 t,d ) N (W d |? t?1,d , ? 2 t?1,d ) dW d D i =d N (W i |? t,i , ? 2 t,i )dW i = D d=1 N (W d |? t,d , ? t,d ) log N (W d |? t,d , ? t,d ) N (W d |? t?1,d , ? t?1,d ) dW d = D d=1 N (W d |? t,d , ? 2 t,d ) log N (W d |? t,d , ? 2 t,d )dW d (1) ? N (W d |? t,d , ? 2 t,d ) log N (W d |? t?1,d , ? 2 t,d )dW d<label>(</label></formula><formula xml:id="formula_20">N (W|? t,d , ? 2 t,d ) log N (W|? t,d , ? 2 t,d )dW d = N (W|? t,d , ? 2 t,d ) log 1 2?? 2 t,d exp ? 1 2 (W d ? ? t,d ) 2 ? 2 t,d dW d = N (W d |? t,d , ? 2 t,d ) log 1 2?? 2 t,d exp ? 1 2 (W d ? ? t,d ) 2 ? 2 t,d dW d = log 1 2?? 2 t,d + N (W d |? t,d , ? 2 t,d ) ? 1 2 (W d ? ? t,d ) 2 ? 2 t,d dW d = log 1 2?? 2 t,d ? 1 2? 2 t,d V ar[W d ] N (? t,d ,? 2 t,d ) = ? 1 2 log 2?? 2 t,d ? 1 2 (3)</formula><p>Next, the integral in (2) becomes as follows.</p><formula xml:id="formula_21">N (W|? t,d , ? 2 t,d ) log N (W|? t?1,d , ? 2 t?1,d )dW d = N (W|? t,d , ? 2 t,d ) log 1 2?? 2 t?1,d exp ? 1 2 (W d ? ? t?1,d ) 2 ? 2 t?1,d dW d = N (W d |? t,d , ? 2 t,d ) log 1 2?? 2 t?1,d exp ? 1 2 (W d ? ? t?1,d ) 2 ? 2 t?1,d dW d = log 1 2?? 2 t?1,d + N (W d |? t,d , ? 2 t,d ) ? 1 2 (W d ? ? t?1,d ) 2 ? 2 t?1,d dW d = log 1 2?? 2 t?1,d ? 1 2? 2 t?1,d E[(W d ? ? t?1,d ) 2 ] N (W d |? t,d ,? 2 t,d ) = ? 1 2 log 2?? 2 t?1,d ? 1 2? 2 t?1,d ? 2 t,d + (? t,d ? ? t?1,d ) 2 (? E[(X ? a) 2 ] = V ar[X] + (E[X] ? a) 2 )<label>(4)</label></formula><p>Therefore, combining <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_7">(4)</ref>, D KL (q(W|? t )||q(W|? t?1 )) becomes</p><formula xml:id="formula_22">D KL (q(W|? t )||q(W|? t?1 )) = D d=1 1 2? 2 t?1,d (? t,d ? ? t?1,d ) 2 + 1 2 ? 2 t,d ? 2 t?1,d ? log ? 2 t,d ? 2 t?1,d + C = 1 2 ? t ? ? t?1 ? t 2 2 + 1 2 1 ? 2 t ? 2 t?1 ? log ? 2 t ? 2 t?1 + C<label>(5)</label></formula><p>where 1 stands for all-1 vector with D dimensions. Therefore, (5) becomes the same as Eq. (3) in main paper when we decompose into the terms for each layer in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Detailed explanation on initializing standard deviation</head><p>As mentioned in Section 4.1, due to using much deeper architecture in convolutional neural networks, we used another initialization technique which is adaptive to model architecture. Our initialization method is highly motivated by <ref type="bibr">[1]</ref>, and the notations on derivations are similar to <ref type="bibr">[1]</ref>. As discussed in <ref type="bibr">[1]</ref>, we mainly consider ReLU activation for deriving the initialization method.</p><p>Forward propagation case Let assume a forward propagation in l-th layer is y l = W l h l?1 + b l , in which, W l is sampled from some prior distribution p(W|?) which is a symmetric distribution with zero mean, h l?1 is the activation value of the previous layer, and b l is bias. As in <ref type="bibr">[1]</ref>, we assume that all activation values in h l?1 are i.i.d., and h l?1 and W l are independent. Since W l has zero mean,</p><formula xml:id="formula_23">Var[y l ] = n l ? Var[w l ? h l?1 ] = n l ? Var[w l ] ? E[h 2 l?1 ],<label>(6)</label></formula><p>where n l is the number of nodes in the previous layer. Note that in convolutional neural network, n l is k 2 c-by-1 vector, in which c is the number of channels, and k is the filter size of the layer. If we assume b l = 0, then y l?1 has zero mean and symmetric distribution. Therefore, considering ReLU activation, E[h 2 l?1 ] can be replaced by <ref type="bibr">1 2</ref> Var[y l?1 ] in <ref type="bibr" target="#b5">(6)</ref>. Putting all together with L layers, we have</p><formula xml:id="formula_24">Var[y L ] = Var[y 1 ] ? L l=2 1 2 n l ? Var[w l ] .<label>(7)</label></formula><p>Note that, in <ref type="formula" target="#formula_15">(7)</ref>, the softmax output of the network can be increased or decreased exponentially, which leads to unstable training. </p><formula xml:id="formula_25">Var[w l ] = Var[? l + ? (l) init ] = Var[? l ] + (? (l) init ) 2 = 2 n l .</formula><p>To maintain this condition, we initialize b l = 0, (?</p><formula xml:id="formula_26">(l) init ) 2 = r ? 2 n l , and Var[? l ] = (1 ? r) ? 2 n l , in which 0 ? r ? 1 is a hyperparameter.</formula><p>Backward propagation case For backpropagation, the gradient of loss in convolution layer is</p><formula xml:id="formula_27">?h l?1 =? l ?y l ,<label>(8)</label></formula><p>where ?h l?1 is ?L ?h l?1 , ?y l is ?L ?h l?1 , and? l is transposed version of W l . Similar as forward propagation case, we can compute the variance of the gradient in <ref type="bibr" target="#b7">(8)</ref>, which is</p><formula xml:id="formula_28">Var[?h l?1 ] =n l Var[w l ]Var[?y l ] = 1 2n l Var[w l ]Var[?h l ].</formula><p>Note that we denote the number of nodes in the upper layer asn l . In convolution layer,n l is k 2 d-by-1 vector. As mentioned in <ref type="bibr" target="#b6">(7)</ref>, the variance of gradient in the input layer is</p><formula xml:id="formula_29">Var[?h 1 ] = Var[?h L ] L l=1 Var[w l ] .<label>(9)</label></formula><p>To avoid the signals exponentially increasing or decreasing, we set the variance of the weights as</p><formula xml:id="formula_30">Var[w l ] = Var[? l + ? (l) init ] = Var[? l ] + (? (l) init ) 2 = 2 n l .</formula><p>Same as in forward propagation case, we initialize b l = 0, (? (l) init ) 2 = r ? 2 n l and Var[? l ] = (1?r)? 2 n l , in which the range of r is 0 ? r ? 1.</p><p>When carrying out experiments, we empirically find that initializing (? (l) init ) 2 = r ? 2 n l for convolution layers and (? (l) init ) 2 = r ? 2 n l for fully connected layers achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detailed explanation on Eq. (6)</head><p>The modified term of term (b) by adding <ref type="formula" target="#formula_13">(6)</ref> is</p><formula xml:id="formula_31">L l=1 1 2 1 ? (l) t ? (l) t?1 2 ? log ? (l) t ? (l) t?1 2 + (? (l) t ) 2 ? log(? (l) t ) 2 .<label>(10)</label></formula><p>To verify the optimal point of (10), we first convert (10) to generalized form, which is</p><formula xml:id="formula_32">L l=1 1 2 1 ? (l) t ? (l) t?1 2 ? log ? (l) t ? (l) t?1 2 + ( ? (l) t {p (l) } 2 ) ? log( ? (l) t {p (l) } 2 ) ,<label>(11)</label></formula><p>where p (l) is any vector satisfying p</p><formula xml:id="formula_33">(l) i ? (l)</formula><p>t?1,i for all i. Since (11) is a convex function, the gradient with respect to ? (l) t at optimal point is 0. The gradient of (11) is</p><formula xml:id="formula_34">? ? (l) t L l=1 1 2 1 ? (l) t ? (l) t?1 2 ? log ? (l) t ? (l) t?1 2 + ( ? (l) t {p (l) } 2 ) ? log( ? (l) t {p (l) } 2 ) = ? (l) t {? (l) t?1 } 2 ? 1 ? (l) t + ? (l) t {p (l) } 2 ? 1 ? (l) t = ? (l) t 1 {? (l) t?1 } 2 + 1 {p (l) } 2 ? 2 {? (l) t } 2 .<label>(12)</label></formula><p>The point which makes <ref type="bibr" target="#b11">(12)</ref> equal to 0 is the optimal point. Therefore, the optimal point is</p><formula xml:id="formula_35">? (l) t = 2 1 {? (l) t?1 } 2 + 1 {p (l) } 2 = 2 ? {p (l) } 2 {? (l) t?1 } 2 {? (l) t?1 } 2 + {p (l) } 2 ? ? 2? (l) t?1 (? {p (l) } 2 {? (l) t?1 } 2 ),<label>(13)</label></formula><p>in which is element-wise comparison. In <ref type="bibr" target="#b12">(13)</ref>, selecting any vector p (l) which satisfies p (l)</p><formula xml:id="formula_36">? (l) t?1 can achieve ? (l) t ? ? 2? (l)</formula><p>t?1 . Therefore, for simplicity, we select p (l) = 1 for all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional experimental results</head><p>We carry out additional experiments to see the effect of adaptive initialization in the fully connected layer network. In <ref type="figure" target="#fig_1">Figure 1</ref>   We also carry out an additional ablation study in Split CIFAR10/100 using convolutional neural network. <ref type="figure" target="#fig_7">Figure 3</ref> shows the additional results on the ablation study in Split CIFAR 10/100. In <ref type="figure" target="#fig_7">Figure  3</ref>, we initialized {?  In <ref type="figure" target="#fig_7">Figure 3</ref>, we can observe that "UCL w/o upper freeze" does not effectively prevent catastrophic forgetting in early tasks. However, since it gives low regularization strength on outgoing weights, a large number of "active learners" tend to help train future tasks effectively, which leads to achieving high average accuracy. "UCL w/o (5)" has much lower retention capability on Task 1 than original UCL. However, in terms of average accuracy, both of them achieve almost the same accuracy. "UCL w/o (6)" shows that the accuracy of Task 1 is even higher than UCL. However, since it does not have any gracefully forgetting techniques, the accuracy drastically decreases after Task 1. We trained all of our baselines with mini batch size of 256 for 100 epochs other than VCL, which used 200 epochs. We also optimized them with learning rate 0.001 by Adam optimizer <ref type="bibr">[2]</ref>. But for HAT, we updated it by stochastic gradient descent(SGD) with learning rate 0.05. For EWC, the Fisher information matrix were computed using all training samples of a task. Regularization hyperparameters are compared as below :</p><p>? UCL ? : {0.0001, 0.001, 0.01, 0.02 (best for Row Permuted MNIST), 0.03 (best for Permuted MNIST)}</p><p>? EWC ? : {40, 400 (best), 4000, 40000}</p><p>? SI c : {0.01, 0.03 (best), 0.1, 0.3, 0.5, 0.7, 1.0}</p><p>? HAT -c : {0.25, 0.5, 0.75( best), 1.0}</p><p>? VCL -not needed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split MNIST</head><p>We use the whole training dataset of a task for a batch size of VCL, and trained it for 120 epochs. The others are trained in the same way as previous experiment. Regularization hyperparameters are compared as below :</p><p>? UCL ? : {0.0001 (best), 0.001, 0.01, 0.02, 0.03 (best)} ? VCL -not needed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split notMNIST</head><p>The training settings are equal to those in Split MNIST. Hyperparameters are compared as below :</p><p>? UCL ? : {0.0001, 0.001 (best), 0.01, 0.02, 0.03(best)}</p><p>? EWC ? : {40, 400, 4000 (best), 40000}</p><p>? SI c : {0.01, 0.03, 0.1, 0.3 (best), 0.5, 0.7, 1.0}</p><p>? HAT c : {0.25, 0.5, 0.75 (best), 1.0}</p><p>? VCL -not needed Split CIFAR-10/100 / Split CIFAR-100</p><p>Same as in Permuted MNIST experiment, we trained UCL and all our baselines with mini-batch size of 256 for 100 epochs. We also optimized them with learning rate 0.001 using the Adam optimizer <ref type="bibr">[2]</ref>. The network architecture used for Split CIFAR-10/100 and Split CIFAR-100 experiments is given in  Omniglot Same as in Split CIFAR-10/100 experiment, we trained UCL and all our baselines with mini batch size of 256 for 100 epochs. We also optimized them with learning rate 0.001 using the Adam optimizer <ref type="bibr">[2]</ref>. The network architecture used in Omniglot experiments is given in <ref type="table" target="#tab_7">Table 2</ref>. Since the number of classes for each task is different, we denoted the classes of ith task as C i . The hyperparameters used     <ref type="table" target="#tab_9">Table 3</ref> shows details on environments that we used in the experimental section on reinforcement learning. We trained UCL and each baseline using eight tasks in <ref type="table" target="#tab_9">Table 3</ref>. We used two fully connected hidden layers with 16 nodes, one input layer and multiple output layers. To initialize the model parameters of baselines, we used He initialization. The number of nodes in the input layer is 44, which is the maximum size of the state space in selected eight tasks. Depending on the tasks, unused areas are filled with zero-masks. Each output layer is equal to the size of the action space of each task, and we used a multivariate Gaussian distribution layer, which learns mean and standard deviation for a continuous action, as an output layer. We evaluated each task for 10 episodes and report the averaged rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Hyperparameters of PPO</head><p>We used PPO (Proximal Policy Optimization) to train a model for reinforcement learning. <ref type="figure" target="#fig_8">Figure  4</ref> shows hyperparameters we used and these hyperparameters are applied to the all baselines in reinforcement learning experiments equally.   init is more involved in the initial capacity of model in view of the level of uncertainty. As a result, when we set appropriate ? (l) init , as in the paper, we get higher accumulated rewards. As in an additional experiments, <ref type="figure" target="#fig_24">Figure 5</ref> and 6 shows the experimental results with ? (l) init = 1x10 ?3 for UCL. The results of other baselines are the same as the paper and all experimental results of each task are normalized by the maximum and minimum value of EWC. Compared to <ref type="figure" target="#fig_15">Figure 9</ref> in the manuscript, <ref type="figure" target="#fig_9">Figure 6</ref> shows similar results: UCL model with small ? can overcome catastrophic forgetting effectively. However, we also find two different results from this experiment. First, compared to <ref type="figure" target="#fig_15">Figure 9</ref> in the manuscript, Task 1 and 3 get much lower rewards. We believe that giving smaller initial uncertainty on the nodes limits the exploration capacity of the model, hence, results in the lower rewards. Another different result is in Task 8. From <ref type="figure" target="#fig_15">Figure 9</ref> in the manuscript, the result of different ? shows that we can control the level of gracefully forgetting by selecting ?. However, Task 8 in <ref type="figure" target="#fig_9">Figure 6</ref> has a difficulty to learn a new task in both ? cases. This result shows that, if the uncertainty of the model is small, the model will have difficulty to learn a new task even if the model forgets gracefully. In conclusion, we stress that it is important to set ? (l) init and ? values properly to achieve successful continual reinforcement learning in both goals, overcoming catastrophic forgetting by selecting ? and achieving highly enough rewards by selecting ? Task8 UCL ( (l) init = 1x10 3 , = 5x10 5 ) UCL ( (l) init = 1x10 3 , = 5x10 6 ) EWC ( = 10) Fine-tuning <ref type="figure" target="#fig_9">Figure 6</ref>: Normalized rewards for each task throughout learning of 8 RL tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 ,</head><label>1</label><figDesc>in which the covariance matrix is diag((?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) 2 ), and it acts as a regularization term for ? (l) t . Namely, when minimizing (3) over ?(l)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Information loss and negative transfer of an important node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(l)init } L l=1 , were set to be 0.06 for FNNs and adaptively set like the He initialization<ref type="bibr" target="#b6">[7]</ref> for deeper CNNs, of which details are given in the Supplementary Material. The hyperparameter selections among the baselines are done fairly, and we list the selected hyperparameters in the Supplementary Materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Experimental results on Permuted / Row Permuted MNIST with a single headed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Standard deviation histogram in the Permuted MNIST experiment. We randomly selected 100 standard deviations for layer 1 and 2. In layer 3, all 10 nodes are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Experimental results on Split MNIST(top) and Split notMNIST(bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Experiments on supervised learning using convolutional neural network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Cumulative normalized rewards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Normalized rewards for each task throughout learning of 8 RL tasks. Each task is learned with 5 million training steps. UCL excels in both not forgetting past tasks and learning new tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>and 2, "UCL constant" represents initializing ? (l) init = c and "UCL adaptive" represents initializing ? (l) init as adaptive to layer size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 1 :</head><label>1</label><figDesc>Additional experimental results on various Permuted MNIST with single head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>(l) init } L l=1 adaptive to layer size. The legend onFigure 3is same as in the manuscript.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 2 :</head><label>2</label><figDesc>Additional experimental results on Split MNIST(top) and Split notMNIST(bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study in Split CIFAR10/100 using adaptive initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>?</head><label></label><figDesc>EWC ? : {40, 400, 4000 (best), 40000} ? SI c : {0.01, 0.03, 0.1, 0.3, 0.5, 0.7, 1.0 (best)} ? HAT c : {0.25, 0.5, 0.75 (best), 1.0}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 4 :</head><label>4</label><figDesc>Implementation details on convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>init = 1x10 3 , = 5x10 5 ) UCL ( (l) init = 1x10 3 , = 5x10 6 ) EWC ( = 10) Fine-tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 :</head><label>5</label><figDesc>Cumulative normalized rewards. The combination of ? (l) init and ? controls the model capacity for continual learning, and also ? (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The number of parameters used for each benchmark.</figDesc><table><row><cell>Dataset\Methods Permuted MNIST Split MNIST Split notMNIST Split CIFAR10/100 Omniglot</cell><cell>Vanilla 478K 270K 187K 839K 1773K 1995K 1995K EWC SI 1435K 1435K 486K 1914K HAT VCL 808K 808K 272K 1077K 559K 559K 190K 749K 2467K 2467K ----</cell><cell>UCL 960K 538K 375K 1655K 1884K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>The hyperparameters used in Split CIFAR-10/100 &amp; Split CIFAR-100 experiments are compared as below :</figDesc><table><row><cell>? UCL -? : {0.0001, 0.0002 (best for CIFAR-10/100), 0.001, 0.0002 (best for CIFAR-100)} -r : {0.5, 0.125 (best)} -lr(?) : {0.01 (best), 0.02}</cell></row><row><cell>? EWC -? : {25, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000 (best for CIFAR-100), 25000 (best for CIFAR-10/100), 50000, 75000, 100000}</cell></row><row><cell>? SI -c : {0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7 (best for CIFAR-10/100), 0.75, 0.8, 0.85, 0.9, 0.95, 1.0 (best for CIFAR-100)}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Network architecture for Split CIFAR-10/100 and Split CIFAR-100</figDesc><table><row><cell>Layer 32?32 input Conv 1 Conv 2 MaxPool Conv 3 Conv 4 MaxPool Conv 5 Conv 6 MaxPool Dense 1 Task 1 : Dense 10</cell><cell>Channel Kernel Stride Padding Dropout 3 32 3?3 1 1 32 3?3 1 1 2 0 0.25 64 3?3 1 1 64 3?3 1 1 2 0 0.25 128 3?3 1 1 128 3?3 1 1 2 1 0.25 256</cell></row><row><cell>? ? ? Task i : Dense 10</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Network architecture on OmniglotFigure 4shows the implementation of UCL for convolutional neural networks (CNN). Instead of giving uncertainty on nodes, we define uncertainty for convolution channels. Colored channels and filters denote important channels and highly regularized filters due to Eq.(4) in the paper. Suppose when training Task 1 is finished, assume the orange colored channels represent channels identified to be important (i.e., certain) for Task 1. Based on Eq.(4) in the manuscript, the whole filters which mainly contribute to making orange colored channels and filter coefficients which use the orange colored channels as inputs getting high regularization strengths for Task 1. Then, after training Task</figDesc><table><row><cell>Layer 28?28 input Conv 1 Conv 2 MaxPool Conv 3 Conv 4 MaxPool Task 1 : Dense C1</cell><cell>Channel Kernel Stride Padding Dropout 1 64 3?3 1 0 64 3?3 1 0 2 0 0 64 3?3 1 0 64 3?3 1 0 2 0 0</cell></row><row><cell>? ? ? Task i : Dense Ci</cell><cell></cell></row><row><cell cols="2">in Omniglot experiments are compared as below :</cell></row><row><cell cols="2">? UCL -? : {0.00001 (best), 0.0001, 0.001, 0.01} -r : {0.5 (best), 0.125} -lr(?) : {0.01, 0.02 (best)}</cell></row><row><cell cols="2">? EWC -? : {25, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 25000, 50000, 75000, 100000 (best)}</cell></row><row><cell cols="2">? SI -c : {0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0 (best)}</cell></row><row><cell cols="2">5.1.2 UCL for convolutional neural networks</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>2, assume the green-colored channels represent channels specific to Task 2. Similar to the case of Task 1, filters connected with green-colored channels are dedicated to Task 2. However, since filters dedicated to Task 1 already get high regularization strengths, these filters tend to keep their values. Therefore, except for filters which already get high regularization strength for Task 1, filters connected to the green-colored channels are expected to be dedicated to Task 2. And gray shaded filters that do not belong to neither Task 1 nor Task 2 are active learners which can be trained future tasks actively. Information on the selected eight tasks and our FNN model</figDesc><table><row><cell>5.2 Reinforcement learning</cell></row><row><cell>5.2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Details on environments.</figDesc><table><row><cell>Task number</cell><cell>Task name</cell><cell cols="2">Observation shape Action shape</cell><cell>Goal</cell></row><row><cell>1 2 3 4 5 6 7 8</cell><cell>Walker2d HumanoidFlagrun Hopper Ant InvertedDoublePendulum HalfCheetah Humanoid InvertedPendulum</cell><cell>(22,) (44,) (15) (28) (3,) (28,) (22,) (3,)</cell><cell>(3,) (17,) (3,) (8,) (1,) (8,) (6,) (1,)</cell><cell>Make robot run as fast as possible Make a 3D humanoid robot run towards a target Make the hopper hop as fast as possible Make the creature walk as fast as possible Keep a pole upright by moving the 1-D cart Make the creature walk as fast as possible Make robot run as fast as possible Keep a pole upright by moving the 1-D cart</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Details on hyperparameters of PPO.</figDesc><table><row><cell>Hyperparameters</cell><cell>Value</cell></row><row><cell># of steps of each task # of processes # of steps per iteration PPO epochs entropy coefficient value loss coefficient ? for accumulated rewards ? for GAE mini-batch size</cell><cell>5m 128 64 10 0 0.5 0.99 0.95 64</cell></row><row><cell>5.3 Additional experimental results with ?</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Art 2: Self-organization of stable category recognition codes for analog input patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4919" to="4930" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keeping neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT-93</title>
		<meeting>COLT-93</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">07</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? Miguel Hern?ndez-Lobato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fearnet: Brain-inspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4652" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Mermillod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lia</forename><surname>Bugaiska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bonin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">504</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">Ignacio</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno>abs/1802.07569</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamim</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4528" to="4537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
