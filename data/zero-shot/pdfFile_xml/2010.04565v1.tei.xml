<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Table Structure Recognition using Top-Down and Bottom-Up Cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Raja</surname></persName>
							<email>sachinraja13@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajoy</forename><surname>Mondal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
							<email>jawahar@iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Information Technology</orgName>
								<orgName type="department" key="dep2">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Table Structure Recognition using Top-Down and Bottom-Up Cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Document image</term>
					<term>table detection</term>
					<term>table cell detection</term>
					<term>row and column association</term>
					<term>table structure recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tables are information-rich structured objects in document images. While significant work has been done in localizing tables as graphic objects in document images, only limited attempts exist on table structure recognition. Most existing literature on structure recognition depends on extraction of meta-features from the pdf document or on the optical character recognition (ocr) models to extract low-level layout features from the image. However, these methods fail to generalize well because of the absence of meta-features or errors made by the ocr when there is a significant variance in table layouts and text organization. In our work, we focus on tables that have complex structures, dense content, and varying layouts with no dependency on meta-features and/or ocr. We present an approach for table structure recognition that combines cell detection and interaction modules to localize the cells and predict their row and column associations with other detected cells. We incorporate structural constraints as additional differential components to the loss function for cell detection. We empirically validate our method on the publicly available real-world datasetsicdar-2013, icdar-2019 (ctdar) archival, unlv, scitsr, scitsr-comp, tablebank, and pubtabnet. Our attempt opens up a new direction for table structure recognition by combining top-down (table cells detection) and bottom-up (structure recognition) cues in visually understanding the tables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have shown promising results in understanding document layouts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. However, more needs to be done for structural and semantic understanding. Among these, the problem of table structure recognition has been of high interest in the community <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Table structure recognition refers to representation of a table in a machinereadable format, where its layout is encoded according to a pre-defined standard <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. It can be represented in the form of either physical <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>  or logical formats <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>. While logical structure contains every cells' row and column spanning information, physical structure additionally contains bounding box coordinates. Table structure recognition is a precursor to contextual table understanding, which has a myriad of applications in business document analysis, information retrieval, visualization, and human-document interactions, as motivated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Table structure recognition is a challenging problem due to complex structures and high variability in table layouts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Early attempts in this space are dependent on extraction of hand-crafted features and meta-data extracted from the pdfs on top of heuristic/rule-based algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">22,</ref><ref type="bibr">23</ref>,24] to locate tables and understanding tables by predicting/recognizing structures. These methods, however, fail to extend to scanned documents as they rely on meta-data information contained in the pdfs. They also make strong assumptions about the structure of the tables. Some of these methods are also dependent on textual information analysis which make them domain dependent. While textual features are useful, visual analysis becomes imperative for analysis of complex page objects. Inconsistency of size and density of tables, presence and location of table cell borders, variation in table cells' shapes and sizes, table cells spanning multiple rows and/or columns and multiline content are some challenges (refer <ref type="figure" target="#fig_2">Figure 2</ref> for some examples) that need to be addressed to solve the problem using visual cues <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr">22,</ref><ref type="bibr">23,</ref><ref type="bibr">24]</ref>.</p><p>We pose the table structure recognition problem as the generation of xml containing table's physical structure in terms of bounding boxes along with spanning information and, additionally, digitized content for every cell (see <ref type="figure" target="#fig_0">Figure 1</ref>). Since our method aims to predict this table structure given the table image only (without using any meta-information), we employ a two-step process -(a) top-down: where we decompose the <ref type="table">table image into fundamental table objects,  which are table cells</ref>   the top-down process, along with their row and column associations with every other cell. We represent row and column associations of table cells using row and column adjacency matrices.</p><p>Though table detection has observed significant success <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">25,</ref><ref type="bibr">26,</ref><ref type="bibr">27,</ref><ref type="bibr">28]</ref>, detection of table cells remains a challenging problem. This is because of (i) large variation in sizes and aspect ratios of different cells present in the same table, (ii) cells' inherent alignment despite high variance in text amount and text justification, (iii) lack of linguistic context in cells' content, (iv) presence of empty cells and (v) presence of cells with multi-line content. To overcome these challenges, we introduce a novel loss function that models the inherent alignment of cells in the cell detection network; and a graph-based problem formulation to build associations between the detected cells. Moreover, as detection of cells and building associations between them depend highly on one another, we present a novel end-to-end trainable architecture, termed as tabstruct-net, for cell detection and structure recognition. We evaluate our model for physical structure recognition on benchmark datasets: scitsr <ref type="bibr" target="#b13">[14]</ref>, scitsr-comp <ref type="bibr" target="#b13">[14]</ref>, icdar-2013 table recognition <ref type="bibr" target="#b17">[18]</ref>, icdar-2019 (ctdar) archival <ref type="bibr" target="#b18">[19]</ref>, and unlv <ref type="bibr">[29]</ref>. Further, we extend the comparative analysis of the proposed work for logical structure recognition on tablebank <ref type="bibr" target="#b10">[11]</ref> dataset. Our method sets up a new direction for table structure recognition as a collaboration of cell detection, establishing an association between localized cells and, additionally, cells' content extraction.</p><p>Our main contributions can be summarised as follows:</p><p>-We demonstrate how the top-down (cell detection) and bottom-up (structure recognition) cues can be combined visually to recognize table structures in document images. -We present an end-to-end trainable network, termed as tabstruct-net for training cell detection and structure recognition networks in a joint manner. -We formulate a novel loss function (i.e., alignment loss) to incorporate structural constraints between every pair of table cells and modify Feature Pyra- <ref type="figure">Fig. 3</ref>. Block diagram of our approach. Table detection is a precursor to table structure recognition and our method assumes that table is already localized from the input document image. The end-to-end architecture predicts cell bounding boxes and their associations jointly. From the outputs of cell detection and association predictions, xml is generated using a post-processing heuristic.</p><p>mid Network (fpn) to capture better low-level and long-range features for cell detection. -We enhance the visual features representation for structure recognition (built on top of model <ref type="bibr" target="#b8">[9]</ref>) through lstm. -We unify results from previously published methods on table structure recognition for a thorough comparison study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the space of document images, researchers have been working on understanding equations <ref type="bibr">[30,</ref><ref type="bibr">31]</ref>, figures <ref type="bibr">[32,</ref><ref type="bibr">33]</ref> and tables <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Diverse table layouts, tables with many empty cells and multi-row/column spanning cells are some challenges that make table structure recognition difficult.</p><p>Research in the domain of table understanding through its structure recognition from document images dated back to the early 1990s when algorithms based on heuristics were proposed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">22,</ref><ref type="bibr">23,</ref><ref type="bibr">24,</ref><ref type="bibr">34,</ref><ref type="bibr">35,</ref><ref type="bibr">36]</ref>. These methods were primarily dependent on hand-crafted features and heuristics (horizontal and vertical ruling lines, spacing and geometric analysis). To avoid heuristics, Wang et al. <ref type="bibr" target="#b4">[5]</ref> proposed a method for table structure analysis using optimization methods similar to the x-y cut algorithm. Another technique based on column segmentation, header detection, and row segmentation to identify the table structure was proposed by Hu et al. <ref type="bibr" target="#b3">[4]</ref>. These methods make strong assumptions about table layouts for a domain agnostic algorithm. Many cognitive methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">37,</ref><ref type="bibr">38,</ref><ref type="bibr">39,</ref><ref type="bibr">40,</ref><ref type="bibr">41,</ref><ref type="bibr">42,</ref><ref type="bibr">43]</ref> have also been presented to understand table structures as they are robust to the input type (whether being scanned images or native digital). These also do not make any assumptions about the layouts, are data-driven, and are easy to fine-tune across different domains. Minghao et al. <ref type="bibr" target="#b10">[11]</ref> proposed one class of deep learning methods to directly predict an xml from the table image using the image-tomarkup model. Though this method worked well for small tables, it was not robust enough to dense and complex tables. Another set of methods is invoice specific table extraction <ref type="bibr">[39,</ref><ref type="bibr">40]</ref>, which were not competent for a more generic  use-cases. To overcome this challenge, a combination of heuristics and cognitive methods has also been presented in <ref type="bibr" target="#b11">[12]</ref>. Chris et al. <ref type="bibr" target="#b9">[10]</ref> presented another interesting deep model, called splerge, which is based on the fundamental idea of first splitting the table into sub-cells, and then merging semantically connected sub-cells to preserve the complete table structure. Though this algorithm showed considerable improvements over earlier methods, it was still not robust to skew present in the table images. Another interesting direction was presented by Vine et al. <ref type="bibr">[42]</ref>, where they used conditional generative adversarial networks to obtain table skeleton and then fit a latent table structure into the skeleton using a genetic algorithm. Khan et al. <ref type="bibr" target="#b14">[15]</ref>, through their gru based sequential models, showed improvements over several cnn based methods for table structure extraction. Recently, many works have preferred a graph-based formulation of the problem as the graph is inherently an ideal data structure to model structural associativity. Qasim et al. <ref type="bibr" target="#b8">[9]</ref> proposed a solution where they used graph neural networks to model table-level associativity between words. The authors validate their method on synthetic table images. Chi et al. <ref type="bibr" target="#b13">[14]</ref> proposed another graphbased problem formulation and solution using a graph attention mechanism. While these methods made significant progress towards understanding complex structured tables, they made certain assumptions like availability of accurate word bounding boxes, accurate document text, etc. as additional inputs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. Our method does not make any such assumptions. We use the table image as the input and produce xml output without any other information. We demonstrate results on complex tables present in unlv, icdar-2013, icdar-2019 ctdar archival, scitsr, scitsr-comp tablebank, and pubtabnet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TabStruct-Net</head><p>Our solution for table structure recognition progresses in three steps -(a) detection of table cells; (b) establishing row/column relationships between the detected cells, and (c) post-processing step to produce the xml output as desired. <ref type="figure">Figure 3</ref> depicts the block diagram of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Top-Down: Cell Detection</head><p>The first step of our solution for table structure recognition is localization of individual cells in a table image, for which we use the popular object detection paradigm. The difference from natural scene images, however, is an inherent association between table cells. Recent success of r-cnns [44] and its improved modifications (Fast r-cnn [45], Faster r-cnn [46], Mask r-cnn [47]) have shown significant success in object detection in natural scene images. Hence, we employ Mask r-cnn [47] for our solution with additional enhancements -(a) we augment the Region Proposal Network (rpn) with dilated convolutions [48,49] to better capture long-range row and column visual features of the table. This improves detection of multi-row/column spanning and multi-line cells ; (b) inspired by [50], we append the feature pyramid network with a top-down pathway, which propagates high-level semantic information to low-level feature maps. This allows the network to work better for cells with varying scales; and (c) we append additional losses during the training phase in order to model the inherent structural constraints. We formulate two ways of incorporating this information -(i) through an end-to-end training of cell detection and the structure recognition networks (explained next), and (ii) through a novel alignment loss function. For the latter, we make use of the fact that every pair of cells is aligned horizontally if they span the same row and aligned vertically if they span the same column. For the ground truth, where tight bounding boxes around the cells' content are provided <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>, we employ an additional ground truth pre-processing step to ensure that bounding boxes of cells in the same row and same column are aligned vertically and horizontally, respectively. We model these constraints during the training in the following manner:</p><formula xml:id="formula_0">L 1 = r?SR ci,cj ?r ||y1 ci ? y1 cj || 2 2 , L 2 = r?ER ci,cj ?r ||y2 ci ? y2 cj || 2 2 L 3 = c?SC ci,cj ?c ||x1 ci ? x1 cj || 2 2 and L 4 = c?EC ci,cj ?c ||x2 ci ? x2 cj || 2 2</formula><p>Here, SR, SC, ER and EC represent starting row, starting column, ending row and ending column indices as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Also, c i and c j denote two cells in a particular row r or column c; x1 ci , y1 ci , x2 ci and y2 ci represent bounding box coordinates X-start, Y-start, X-end and Y-end respectively of the cell c i . These losses (L 1 , L 2 , L 3 , L 4 ) can be interpreted as constraints that enforce proper alignment of cells beginning from same row, ending on same row, beginning from same column and ending on same column respectively. Alignment loss is defined as</p><formula xml:id="formula_1">L align = L 1 + L 2 + L 3 + L 4 .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bottom-Up: Structure Recognition</head><p>We formulate the table structure recognition using graphs similar to <ref type="bibr" target="#b8">[9]</ref>. We consider each cell of the table as a vertex and construct two adjacency matrices -a row matrix M row and a column matrix M col which describe the association between cells with respect to rows and columns. M row , M col ? R N cells ?N cells . M rowi,j = 1 or M coli,j = 1 if cells i, j belong to the same row or column, else 0. The structure recognition network aims to predict row and column relationships between the cells predicted by the cell detection module during training and testing. During training, only those predicted table cells are used for structure recognition which overlap with the ground truth table cells having an IoU greater than or equal to 0.5. This network has three components:</p><p>-Visual Component: We use visual features from P2 layer (refer <ref type="figure" target="#fig_4">Figure 5)</ref> of the feature pyramid based on the linear interpolation of cell bounding boxes predicted by the cell detection module. In order to encode cells' visual characteristics across their entire height and width, we pass the gathered P2 features for every cell along their centre horizontal and centre vertical lines using lstm [51] to obtain the final visual features (refer <ref type="figure" target="#fig_4">Figure 5)</ref>  We train the cell detection and structure recognition networks in a joint manner (termed as tabstruct-net) to collectively predict cell bounding boxes along with row and column adjacency matrices. Further, the two structure recognition pathways for row and column adjacency matrices are put together in parallel. The visual features prepared using lstms for every vertex are duplicated for both the pathways, after which they work in a parallel manner. The overall empirical loss of tabstruct-net is given by:</p><formula xml:id="formula_2">L = L box + L cls + L mask + L align + L gnn ,<label>(2)</label></formula><p>where L box , L cls and L mask are bounding box regression loss, classification loss and mask loss, respectively defined in Mask r-cnn [47], L align is alignment loss which is modeled as a regularizer (defined in Eq. 1) and L gnn is the cross-entropy loss back propagated from the structure recognition module of tabstruct-net.</p><p>The additional loss components help the model in better alignment of cells belonging to same rows/columns during training, and in a sense fine-tunes the predicted bounding boxes that makes it easier for post-processing and structure recognition in the subsequent step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-Processing</head><p>Once all the cells and their row/column adjacency matrices are predicted, we create the xml interpretable output as a post-processing step. From the cell coordinates along with row and column adjacency matrix, SR, SC, ER and EC indexes are assigned to each cell, which indicate spanning of that cell along rows and columns. We use Tesseract [53] to extract the content of every predicted cell. The xml output for every table image finally contains coordinates of predicted cell bounding boxes and along with cell spanning information and its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use various benchmark datasetsscitsr <ref type="bibr" target="#b13">[14]</ref>, scitsr-comp <ref type="bibr" target="#b13">[14]</ref>, icdar-2013 table recognition <ref type="bibr" target="#b17">[18]</ref>, icdar-2019 ctdar archival <ref type="bibr" target="#b18">[19]</ref>, unlv [29], Marmot extended <ref type="bibr" target="#b11">[12]</ref>, tablebank <ref type="bibr" target="#b10">[11]</ref> and pubtabnet <ref type="bibr" target="#b12">[13]</ref> datasets for extracting structure information of tables. Statistics of these datasets are listed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>scitsr scitsr icdar icdar-2013 icdar unlv unlv-Marmot </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare the performance of our tabstruct-net against seven benchmark methodsdeepdesrt <ref type="bibr" target="#b6">[7]</ref>, tablenet <ref type="bibr" target="#b11">[12]</ref>, graphtsr <ref type="bibr" target="#b13">[14]</ref>, splerge <ref type="bibr" target="#b9">[10]</ref>, dgcnn <ref type="bibr" target="#b8">[9]</ref>, Bi-directional gru <ref type="bibr" target="#b14">[15]</ref> and Image-to-Text <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>tabstruct-net 1 has been trained and evaluated with table images scaled to a fixed size of 1536?1536 while maintaining the original aspect ratio as the input. While training, cell-level bounding boxes along with row and column adjacency matrices (prepared from start-row, start-column, end-row and end-column indices) are used as the ground truth. We use nvidia titan x gpu with 12 gb memory for our experiments and a batch-size of 1. Instead of using 3?3 convolution on the output feature maps from the fpn, we use a dilated convolution with filter size of 2?2 and dilation parameter of 2. Also, we use the resnet-101 backbone that is pre-trained on ms-coco [54] dataset. Dilated convolution blocks of filter size 7 are used in the fpn. To compute region proposals, we use 0.5, 1 and 2 as the anchor scale and anchor box sizes of 8, 16, 32, 64 and 128. lstms used to gather visual features have a depth of 128. The final memory state of the lstm layers is concatenated with the cell's coordinates to prepare features for the interaction network. Further, for generation of the row/column adjacency matrices, we use 2400 as the maximum number of vertices keeping in mind dense tables. Next, features from 40 neighboring vertices are aggregated using an edge convolution layer followed by a dense layer of size 64 with ReLu activation. Since every input table may contain hundreds of table cells, training can be a time consuming process. To achieve faster training, we employ a two-stage training process. In the first stage, we use 2014 anchors and 512 rois, and in the second stage, we use with 3072 anchors and 2048 rois. During both the stages, we use 0.001 as the learning rate, 0.9 as the momentum and 0.0001 as the weight decay regularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Measures</head><p>We use various existing measures -precision, recall and F1 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">29</ref>] to evaluate the performance of our model for recognition of physical structure of tables. For recognition of logical structure of tables, we use bleu [55] score as used in <ref type="bibr" target="#b10">[11]</ref> and Tree-Edit-Distance-based similarity (teds) <ref type="bibr" target="#b12">[13]</ref>. Since xml is our final output for evaluation. It is also important to note that in our evaluation, we do not take into account content of the cell and the confusion matrix is computed solely on the basis of IoU overlap with the ground truth cell box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Setup</head><p>One major challenge in the comparison study with the existing methods is the inconsistent use of additional information (e.g., meta-features extracted from the pdfs <ref type="bibr" target="#b9">[10]</ref>, content-level bounding boxes from ground truths <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> and cell's location features generated from synthetic dataset <ref type="bibr" target="#b8">[9]</ref>). Hence, we do experiments in two different setups -Setup-A (S-A): using only table image as the input -Setup-B (S-B): using table image along with additional information (e.g., cell bounding boxes) as the input. For this, instead of removing the cell detection component from the network, we ignore the predicted boxes and use the ground truth ones as input for structure recognition. <ref type="table">Table Structure Recognition   Tables 2 and 3</ref> summarize the performance of our model on standard datasets used in the space of table structure recognition. <ref type="table" target="#tab_4">Table 4</ref> presents results on icdar-2013 dataset. In S-A, we observe that our model outperforms deepdesrt <ref type="bibr" target="#b6">[7]</ref> method by a 27.5% F1 score. This is because cell coordinates for the latter are obtained by row and column intersections, making it unable to recognize cells that span multiple rows/columns. For dense tables (small inter-row spacing), row segmentation results of deepdesrt combined multiple rows into one in several instances. split+heuristic <ref type="bibr" target="#b9">[10]</ref> method outperforms tabstruct-net by a small margin, however, it requires icdar-2013 dataset-specific cell merging heuristics and is trained on a considerably larger set of images. Therefore, a direct comparison of (split+heuristic) with our method is not fair. Nevertheless, comparable results of tabstruct-net indicates its robustness to icdar-2013 dataset, without using any kind of dataset-specific postprocessing. However, if compared under the same training environment and no post-processing, our model outperforms splerge with a 3% average F1 score. splerge works well for datasets where ground truth bounding boxes are annotated at the content-level instead of cell-level. This is because it allows for a wider area for a prospective prediction of a row/column separator. Further, since it is based on cell detection through the row and column separators, it is not agnostic to input image noise such as skew and rotations. This method is susceptible to dataset-specific post-processing as opposed to ours, where no post-processing is needed. In S-B, tabstruct-net sets up a state-of-the-art benchmark on the icdar-2013 dataset, outperforming all the existing methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>. It is further interesting to note that our technique outperforms split-pdf+heuristic model also without needing any post-processing. It is because our enhancements to the dgcnn [9] model can capture the visual characteristics of a cell across a larger span through lstms. We observe that our model achieves significantly improved performance when content-level bounding boxes are used instead of cell-level, which are much easier to obtain with the help of ocr tools and pdf meta-information.  <ref type="table" target="#tab_5">Table 5</ref> shows the performance of our technique under the varying IoU thresholds. It can be inferred from the table that our model achieves an F1 score of 79.1% on structure recognition with an IoU threshold value of as high as 0.7. For the IoU values of 0.5 and 0.6, our model's performance is 91.9% and 90.6%, respectively. It demonstrates the robustness of our model. <ref type="figure" target="#fig_5">Figures 6 and 7</ref> display some qualitative outputs of our method on the datasets discussed in Section 4.1. <ref type="figure">Figure 8</ref> shows some of the failure cases of cell detection by our method. It can be seen that our model fails for table images that have large amounts of empty spaces. <ref type="table">Table 6</ref> shows the outcome of our enhancements to Mask r-cnn [47] and dgcnn <ref type="bibr" target="#b8">[9]</ref> models for both cell detection and structure recognition networks under S-A and S-B. From the table, it can be observed that our additions to the networks result in a significant increase of 4% average F1 scores on cell detection and structure recognition tasks. The novel alignment loss, along with the use of top-down and bottom-up pathways in the fpn results in an improvement of 2.3% F1 score for cell detection and 2.4% on structure recognition. Use of lstms and P2 layer output to prepare visual features for structure recognition results in a 2.1% improvement of F1 scores. Interestingly, because both models are trained together in an end-to-end fashion, cell detection's effect is also observed in the form of a 1.5% average F1 score. This empirically bolsters our claim of using an end-to-end architecture for cell detection and, in turn, structure recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We formulate the problem of table structure recognition as a combination of cell detection (top-down) and structure recognition (bottom-up) tasks. For cell detection, we make a modification to the rpn of original Mask r-cnn and introduce a novel alignment loss function (formulated for every pair of table cells) to enforce structural constraints. For structure recognition, we improve input representation for the dgcnn network by using lstm, pre-trained ResNet-101 backbone and rpn of cell detection network. Further, we propose an end-to-end trainable architecture to collectively predict cell bounding boxes along with their row and column adjacency matrices to predict structure. We demonstrate our results on multiple public datasets on both digital scanned as well as archival handwritten table images. We observe that our approach fails to handle tables containing a large number of empty cells along both horizontal and vertical directions. In conclusion, we encourage further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partly supported by meity, Government of India. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: TabStruct-Net</head><p>Our tabstruct-net is a data-driven and an end-to-end trainable architecture for the prediction of table structure from a given table image, that combines topdown and bottom-up methods. As a first step, the input table image is broken down into individual cells using the cell detection network of the tabstruct-net. We call this as the top-down step of the process. After detecting individual cells, the next step is to obtain the entire table structure by building relevant row and column associations between the detected cells. This is done using the structure recognition network of the tabstruct-net and we call this as the bottom-up step of the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-processing to Get XML Output</head><p>After the cell bounding boxes along with the row and column adjacency matrices are obtained, an xml file is generated using an heuristic based algorithm. It works as follows:</p><p>-For row assignments, sort all bounding boxes by their start y coordinates, and initialize a row belonging list for every cell.</p><p>-Assign a row belonging index (starting from 0) to the cell c i and assign the same row index to all other cells that are connected to c i in the row adjacency matrix. EC is the maximum of indexes in the column belonging list.</p><p>We use Tesseract [53] to extract the content of every predicted cell. Once SR, ER, SC and EC values (referred to as cell spanning values) and its content are obtained for every predicted cell, an xml file is created with these cell spanning values along with bounding box coordinates (top-left and bottom-right of the cell) and its content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We use various benchmark table structure recognition datasetsscitsr <ref type="bibr" target="#b13">[14]</ref>, scitsr-comp <ref type="bibr" target="#b13">[14]</ref>, icdar-2013 table recognition <ref type="bibr" target="#b17">[18]</ref>, icdar-2019 ctdar archival <ref type="bibr" target="#b18">[19]</ref>, unlv [29], Marmot extended <ref type="bibr" target="#b11">[12]</ref>, tablebank <ref type="bibr" target="#b10">[11]</ref> and pubtabnet <ref type="bibr" target="#b12">[13]</ref> datasets for extracting structure information of tables. Statistics of these datasets are listed in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Our tabstruct-net makes an assumption that all cells belonging to the same column are aligned with respect to x coordinates and cells belonging to the same row are aligned with respect to y coordinates. scitsr <ref type="bibr" target="#b13">[14]</ref>, scitsr-comp <ref type="bibr" target="#b13">[14]</ref> and icdar-2013 <ref type="bibr" target="#b17">[18]</ref> datasets have ground truth bounding boxes at the level of cell's content (box is the smallest rectangular block that encapsulates the cell's content). To handle this, we expand the bounding boxes of every cell in a row and column to get maximum sized content-level box in a particular row and column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We compare the performance of our tabstruct-net against seven following benchmark methods. DeepDeSRT <ref type="bibr" target="#b6">[7]</ref>: This method leverages semantic segmentation approach to localize each row and column from the table image. This method outputs table as a grid-like structure and fails to identify multiple row and multiple column spanning cells. Since no code is available, we implement our own version of this method. Since this method works by predicting every row and column, the scitsr training dataset is pre-processed to obtain row and column level coordinates before training.</p><p>TableNet <ref type="bibr" target="#b11">[12]</ref>: This method uses semantic segmentation approach to extract table and column masks, and segments rows by identifying words present in different columns (extracted using tesseract ocr <ref type="bibr">[53]</ref>) that occur at the same horizontal level. For comparison against other methods, we directly use the results reported by the authors.</p><p>GraphTSR <ref type="bibr" target="#b13">[14]</ref>: This method consists of edge-to-vertex and vertex-to-edge graph attention blocks to compute vertex and edge representations in a latent space, and finally classify each edge as 'horizontal', 'vertical' or 'no-relation'. It uses absolute and relative positions of every cell extracted from the pdf to compute initial vertex and edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPLERGE [10]</head><p>: This method comprises of two deep learning networks that perform splitting and merging operations in sequence to predict fine grid-like table structure and to predict merged cells which span multiple rows/columns. Split method shows an improved performance when additional pdf extracted meta-features are provided along with the table image. For the split model (to obtain the basic grid of the table), authors pre-process the ground truth by maximizing the row and column separator regions without intersecting any non multiple row or column spanning cell. For the merge model (to identify cells that span multiple rows or columns), the authors prepare the ground truth by identifying grid elements that span multiple cells and set the merging probability in the respective directions. Further, for evaluating this method on icdar-2013 <ref type="bibr" target="#b17">[18]</ref> dataset, the authors realized that merge method did not work with a good performance, and hence, introduced the following specific heuristics to merge cells instead:</p><p>-Merge cells where separators pass through text.</p><p>-Merge adjacent blank columns with cells that have been formed by merging many cells. -Merge adjacent blank rows with content cells.</p><p>-Split columns that have a consistent white-space gap between vertically aligned text.</p><p>DGCNN * <ref type="bibr" target="#b8">[9]</ref>: Authors formulate the problem as a graph learning problem to predict whether every pair of words belongs to the same cell, row and/or column or not. Their architecture consists of a visual network, an interaction network and a classification network. For evaluation purposes, table image along with word-level bounding boxes is provided as inputs.</p><p>Bi-directional GRU <ref type="bibr" target="#b14">[15]</ref>: Given the table image, this method uses two bidirectional grus to establish row and column boundaries in a context driven manner. This method however fails to localize multiple row and/or multiple column spanning cells.</p><p>Image-to-Text <ref type="bibr" target="#b11">[12]</ref>: This method utilizes an Image-to-Markup model to predict a markup-like output of a given table image. It consists of a cnn based encoder to compute visual features and an lstm based decoder to produce markup output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Our tabstruct-net model has been trained and evaluated with table images scaled to a fixed size of 1536?1536 while maintaining the original aspect ratio as the input. While training, cell-level bounding boxes along with row and column adjacency matrices (prepared from start-row, start-column, end-row and endcolumn indices) are used as the ground truth. We use nvidia titan x gpu with 12 gb memory for our experiments and a batch-size of 1. Instead of using 3?3 convolution on the output feature maps from the fpn, we use a dilated convolution with filter size of 2?2 and dilation parameter of 2. Also, we use the resnet-101 backbone that is pre-trained on ms-coco To achieve faster training, we employ a 2-stage training process. In the first step, we use 2014 anchors and 512 rois. With this setting, the model is able to learn high and low level features but resulted in a large number of false negatives. To combat this, network is trained with 3072 anchors and 2048 rois. This significantly reduces the number of false negatives. For the first step, we train a total of 30 epochs, for the first 8, we train all fpn and subsequent layers, for the next 15, we train fpn + last 4 layers of resnet-101 and for the last 7 epochs, we train all the layers of the model. For the second step, we train a total of 10 epochs, for the first 3, we train all fpn and subsequent layers, for the next 4, we train fpn + last 4 layers of resnet-101 and for the last 3 epochs, we train all the layers of the model. During both the stages, we use 0.001 as the learning rate, 0.9 as the momentum and 0.0001 as the weight decay regularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Measures</head><p>Details of Evaluation Criteria: For comparison against most of the existing methods, we use the precision, recall and F1 score <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">29]</ref>. Before evaluating performance of structure recognition, it is important to understand the specific cases in which detected cells are taken into consideration for structure recognition:</p><p>-We consider a detected cell to be a true positive if it overlaps with the ground truth cell bounding box is more than 0.5. -During evaluation of structure recognition, cells that have no content (i.e., empty or blank cells) are discarded. It means that adjacency relations that involve a blank cell are not taken into consideration.</p><p>To evaluate the performance of structure recognition, adjacency relations between every cell (with content) are generated with their horizontal and vertical neighbors. This predicted relation list is then compared with the ground truth list to generate precision, recall and F1 measures.</p><p>As per <ref type="bibr" target="#b17">[18]</ref>, this method accounts for the standard evaluation measures for table structure recognition for the following reasons:</p><p>-It provides for a simple way to account for errors in the scenarios of complex table layouts containing blank cells, and cells that span multiple rows and/or columns. -It accounts for evaluation of both physical as well as logical structure prediction methods as it is not dependent on the bounding box coordinates information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>One major challenge in the comparison study with the existing methods is the inconsistent use of additional information (e.g., meta-features extracted from the pdfs <ref type="bibr" target="#b9">[10]</ref>, content-level bounding boxes from ground truths <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> and cell's location features generated from synthetic dataset <ref type="bibr" target="#b8">[9]</ref>).</p><p>For the unification of fair comparison for table structure recognition, we divide all inconsistencies into several levels -(i) inconsistency with respect to input modalities, (ii) inconsistency with respect to annotation levels, (iii) inconsistency with respect to representation of table structure, (iv) inconsistency with respect to evaluation methods, and (v) inconsistency with respect to way of computing evaluation scores.</p><p>Inconsistency with respect to input modalities: Section 3.2 describes that many methods for table structure recognition work with table images alone <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, several other assume additional information in the form of meta-features or bounding boxes around every word or cell-content extracted from the pdfs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. This makes it difficult to compare these methods under a unified scenario. To take care of this problem, we define two different experimental setups -(a) Setup-A (S-A) where only table image is used as an input to the structure recognition model and (b) Setup-B (S-B) where table image along with additional meta-features such as low-level content bounding boxes are used as an input to the structure recognition model. We present our results under both the experimental setups for a thorough comparison of our work against most of the recent methods in this space. To achieve this, we train our model for cell detection as well as structure recognition collectively for S-A. For evaluation in S-B, instead of predicting cell bounding boxes from the image, we use the table image and the low-level bounding box information from ocr or ground truth to be able to directly and fairly compare our method.</p><p>Inconsistency with respect to annotation levels: It is important to note that training of tabstruct-net assumes cell-level bounding boxes in a way that all cells that (a) have the same SR indices having same y1 coordinates, (b) have the same SC indices having same x1 coordinates, (c) have the same ER indices having same y2 coordinates, and (d) have the same EC indices having same x2 coordinates. This assumption is necessary for our alignment loss function to work properly. However, different datasets for physical table structure recognition have ground truth annotations defined in different ways. unlv and icdar-2019 archival datasets have ground truth annotated at the cell-level. scitsr <ref type="bibr" target="#b13">[14]</ref> and icdar-2013 <ref type="bibr" target="#b17">[18]</ref> datasets have ground truth annotation defined at the content-level (cells' bounding box is the smallest rectangle that covers entire content of the cell). To be able to use those for training, we pre-process the ground-truth to obtain cell-level bounding boxes (as explained in Section 3.1). Please note that this pre-processing step is only done for the training process. Similarly, ground-truth bounding boxes of the synthetic dataset proposed in <ref type="bibr" target="#b8">[9]</ref> are provided at the word-level. To obtain cell-level bounding boxes, we use the ground-truth cell adjacency matrix and word-level bounding boxes to obtain content-level bounding boxes. During the testing time in S-A, however, to compute if a detected cell is a true positive, we use the original ground-truth bounding boxes (either at cell-level or content-level), and not the pre-processed ones. Similarly while testing in S-B, we use the original content-level or cell-level bounding boxes as the additional input instead of the pre-processed ones. This ensures consistency while comparing our methods against previously published ones.</p><p>Inconsistency with respect to representation of table structure: We broadly classify table structure methods into two categories -(a) physical structure predicting methods that predict cell-level bounding boxes along with their associations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref> and (b) logical structure predicting methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> that predict only cell associations. In our work, we standardize our representation as described in Section 3.5, which allows us to directly compare methods in both the experimental setups. To compare the results of tabstruct-net on logical structure prediction, we generate the mark-up string from the postprocessed xml output of tabstruct-net in the same format as tableBank <ref type="bibr" target="#b10">[11]</ref> and pubtabnet <ref type="bibr" target="#b12">[13]</ref> by extracting only the structure without cells' coordinates and content.</p><p>Inconsistency with respect to evaluation methods: While most of the previously published methods for table structure recognition use precision, recall and F1 scores as described in <ref type="bibr" target="#b17">[18]</ref>, there are some inconsistencies in this aspect as well. In <ref type="bibr" target="#b8">[9]</ref>, authors use true positive rate (tpr), false positive rate (fpr) and absolute accuracy on the predicted adjacency matrix to compute performance. In order to standardize evaluation with <ref type="bibr" target="#b8">[9]</ref>, we infer neighboring cell relations from their output to ensure consistency. Further, <ref type="bibr" target="#b11">[12]</ref> use bleu scores to compare their output with the ground truth. Since our method generates and xml output from the model's predictions, we bring our output to the same format as <ref type="bibr" target="#b11">[12]</ref> to ensure a direct and fair comparison on the tablebank dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>Inconsistency with respect to way of computing evaluation scores: To fairly compare tabstruct-net against previous methods, we list both micro averaged results on the test datasets. However, it is important to note that for tableBank <ref type="bibr" target="#b10">[11]</ref> and pubtabnet <ref type="bibr" target="#b12">[13]</ref> datasets, where we evaluate our results on the markup output of the model, we only consider document-averaged results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: Results on Table Structure Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Micro-averaged Results</head><p>Tables 7-10 show the micro-averaged results of various methods for structure recognition on multiple datasets. From the tables, it can be observed that our method outperforms previously published works under multiple kinds of experimental settings. Further, it is important to note that the tables use an IoU threshold of 0.6 to identify true positive cells for experiment setup S-A. We also show the precision, recall and F1 measures on various IoU thresholds to better interpret the performance of the cell detection module of tabstruct-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Exp. P? R? F1? Dataset #Images Setup deepdesrt <ref type="bibr" target="#b6">[7]</ref> icdar-2013-partial 0.124K S-A 0.959 0.874 0.914 splerge <ref type="bibr" target="#b9">[10]</ref> icdar-2013-partial 0.124K S-A 0.917 0.911 0.914 Bi-directional gru <ref type="bibr" target="#b14">[15]</ref>  We present our results on the output xml file that contains -(a) bounding box coordinates, (b) start and end row indices, (c) start and end column indices,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Experimental Setup Training Dataset #Images TEDS? Acrobat Pro <ref type="bibr" target="#b12">[13]</ref> S-A --0.537 wygiwys <ref type="bibr" target="#b12">[13]</ref> S-A pubtabnet 399K 0.786 edd <ref type="bibr" target="#b12">[13]</ref> S-A pubtabnet 399K 0.883 tabstruct-net (our) S-A scitsr <ref type="bibr" target="#b13">[14]</ref> 12K 0.901 <ref type="table" target="#tab_2">Table 12</ref>. Comparison of results for logical structure recognition on pubtabnet dataset <ref type="bibr" target="#b12">[13]</ref>. TEDS: indicates averaged tree edit distance based similarity <ref type="bibr" target="#b12">[13]</ref>. and (d) content for every predicted cell given the table image. To evaluate our method, we compare this xml against the ground-truth prepared in the same format using bleu, cidrr and rouge scores as presented in <ref type="table" target="#tab_2">Table 13</ref>. The table also compares our results against dgcnn <ref type="bibr" target="#b8">[9]</ref> when cells detected from tabstructnet are provided as the input to their model.         <ref type="figure" target="#fig_0">Figure 19</ref> shows some failure cases of our model in presence of empty spaces along both horizontal and vertical axes.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Failure Examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The figure depicts the problem of recognizing table structure from it's image. This opens up many applications including information retrieval, graphical representation and digitizing for editing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>using a cell detection network and (b) bottom-up: where we re-build the entire tableas a collection of all the table cells localized from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Examples of complex table images from unlv and icdar-2013 datasets. Complex tables are ones which contain partial or no ruling lines, multi-row/column spanning cells, multi-line content, many empty dense cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visual illustration of cell spanning information along rows and columns of a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Our tabstruct-net. Modified rpn in cell detection network, which consists of both top-down and bottom-up pathways to better capture low-level visual features. P2 layer of the optimized feature pyramid is used in the structure recognition network to extract visual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Sample intermediate cell detection results of tabstruct-net on table images of icdar-2013, icdar-2019 ctdar and unlv, scitsr, scitsr-comp and tablebank datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Sample structure recognition output of tabstruct-net on table images of icdar-2013, icdar-2019 ctdar archival and unlv datasets. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .Table 6 .</head><label>86</label><figDesc>Sample intermediate cell detection results of tabstruct-net on table images of icdar-2013, icdar-2019 ctdar, unlv, scitsr, scitsr-comp and tablebank datasets illustrate failure of tabstruct-net. ES CD Network SR Network CD Scores SR Scores P? R? F1? P? R? F1? Mask r-cnn dgcnn 0.885 0.890 0.887 0.871 0.860 0.865 Mask r0.920 0.917 0.906 0.885 0.895 Mask r-cnn+td+bu+aldgcnn+p2+lstm 0.9210.9260.9240.9150.8970.906-nadgcnn -na--na--na-0.972 0.983 0.977 S-B -na-dgcnn+p2 -na--na--na-0.973 0.983 0.978 -na-dgcnn+p2+lstm -na--na--na-0.9760.9850.981 Ablation study for physical structure recognition on icdar-2013 dataset. ES: Experimental Setup, CD: Cell Detection, SR: Structure Recognition, P2: using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: use of Top-Down and Bottom-Up pathways in the fpn, AL: addition of alignment loss as a regularizer to tabstruct-net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>22. Green, E., Krishnamoorthy, M.: Recognition of tables using table grammars. In: Annual Symposium on Document Analysis and Information Retrieval. (1995) 23. Kieninger, T.G.: Table structure recognition based on robust block segmentation. In: Document Recognition V. (1998) 24. Tupaj, S., Shi, Z., Chang, C.H., Alam, H.: Extracting tabular information from text files. EECS Department, Tufts University, Medford, USA (1996) 25. Gilani, A., Qasim, S.R., Malik, I., Shafait, F.: Table detection using deep learning. In: ICDAR. (2017) 26. Dong, H., Liu, S., Han, S., Fu, Z., Zhang, D.: TableSense: Spreadsheet table detection with convolutional neural networks. In: AAAI. (2019) 27. Kavasidis, I., Pino, C., Palazzo, S., Rundo, F., Giordano, D., Messina, P., Spampinato, C.: A saliency-based convolutional neural network for table and chart detection in digitized documents. In: ICIAP. (2019) 28. Saha, R., Mondal, A., Jawahar, C.V.: Graphical object detection in document images. In: ICDAR. (2019) 29. Shahab, A., Shafait, F., Kieninger, T., Dengel, A.: An open approach towards the benchmarking of table structure recognition systems. In: DAS. (2010) 30. Zanibbi, R., Blostein, D., Cordy, J.R.: Recognizing mathematical expressions using tree transformation. IEEE Trans. on PAMI (2002) 31. Zhang, J., Du, J., Dai, L.: Multi-scale attention with dense encoder for handwritten mathematical expression recognition. In: ICDAR. (2018) 32. Siegel, N., Horvitz, Z., Levin, R., Divvala, S., Farhadi, A.: FigureSeer: Parsing result-figures in research papers. In: ECCV. (2016) 33. Tang, B., Liu, X., Lei, J., Song, M., Tao, D., Sun, S., Dong, F.: DeepChart: Combining deep convolutional networks and deep belief networks in chart classification. Signal Processing (2015) 34. Harit, G., Bansal, A.: Table detection in document images using header and trailer patterns. In: ICVGIP. (2012) 35. Gatos, B., Danatsas, D., Pratikakis, I., Perantonis, S.J.: Automatic table detection in document images. In: CVPR. (2005) 36. Ohta, M., Yamada, R., Kanazawa, T., Takasu, A.: A cell-detection-based tablestructure recognition method. In: ACM Symposium on Document Engineering. (2019) 37. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientific table recognition. In: ICDAR. (2019) 38. Adiga, D., Bhat, S.A., Shah, M.B., Vyeth, V.: Table structure recognition based on cell relationship, a bottom-up approach. In: RANLP. (2019) 39. Riba, P., Dutta, A., Goldmann, L., Fornes, A., Ramos, O., Llados, J.: Table detection in invoice documents by graph neural networks. In: ICDAR. (2019) 40. Hole?ek, M., Hoskovec, A., Baudi?, P., Klinger, P.: Line-items and table understanding in structured documents. arXiv (2019) 41. Deng, L., Zhang, S., Balog, K.: Table2Vec: Neural word and entity embeddings for table population and retrieval. In: SIGIR. (2019) 42. Le Vine, N., Zeigenfuse, M., Rowan, M.: Extracting tables from documents using conditional generative adversarial networks and genetic algorithms. In: IJCNN. (2019) 43. Sage, C., Aussem, A., Elghazel, H., Eglin, V., Espinas, J.: Recurrent neural network approach for table field extraction in business documents. In: ICDAR. (2019) 44. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR. (2014) 45. Girshick, R.: Fast R-CNN. In: ICCV. (2015) 46. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NIPS. (2015) 47. He, K., Gkioxari, G., Doll?r, P., Girshick, R.: Mask R-CNN. In: CVPR. (2017) 48. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv (2015) 49. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE trans. on PAMI (2017) 50. Woo, S., Hwang, S., Jang, H.D., Kweon, I.S.: Gated bidirectional feature pyramid network for accurate one-shot detection. Machine Vision and Applications (2019) 51. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation (1997) 52. Qasim, S.R., Kieseler, J., Iiyama, Y., Pierini, M.: Learning representations of irregular particle-detector geometry with distance-weighted graph networks. arXiv (2019) 53. Smith, R.: An overview of the Tesseract OCR engine. In: ICDAR. (2007) 54. Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. CoRR (2014) 55. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: BLEU: a method for automatic evaluation of machine translation. In: AMACL. (2002) 56. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr: Consensus-based image description evaluation. In: CVPR. (2015) 57. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out (2004)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>-</head><label></label><figDesc>Increment the row index and repeat the above step until all the cells are assigned at least one row belonging index. -For each cell, SR is the minimum of indexes in the row belonging list, and ER is the maximum of indexes in the row belonging list. -Similarly, for column assignments, sort all bounding boxes by their start x coordinates, and initialize a column belonging list for every cell. -Assign a column belonging index (starting from 0) to the cell c i and assign the same column index to all other cells that are connected to c i in the column adjacency matrix. -Increment the column index and repeat the above step until all the cells are assigned at least one column belonging index. -For each cell, SC is the minimum of indexes in the row belonging list, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Ground truth unification. content-level bounding boxes are given in ground truth as shown in First Row. We make content-level bounding boxes into cell-level bounding boxes as shown in Second Row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Sample structure recognition output of tabstruct-net on table images of icdar-2013 dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Sample structure recognition output of tabstruct-net on table images of icdar-2019 dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Sample structure recognition output of tabstruct-net on table images of scitsr dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Sample structure recognition output of tabstruct-net on table images of scitsr-comp dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Sample structure recognition output of tabstruct-net on table images of tablebank dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Sample structure recognition output of tabstruct-net on table images of pubtabnet dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 .</head><label>18</label><figDesc>Sample structure recognition output of tabstruct-net on table images of unlv dataset. First Row: prediction of cells which belong to the same row. Second Row: prediction of cells which belong to the same column. Cells marked with orange colour represent the examine cells and cells marked with green colour represent those which belong to the same row/column of the examined cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig</head><label></label><figDesc>Fig. 19. Sample intermediate cell detection results of tabstruct-net on table images of icdar-2013, icdar-2019 ctdar, scitsr, scitsr-comp, tablebank, pubtabnet and unlv datasets illustrate failure of tabstruct-net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>18 .Table 19 .Table 21 .</head><label>181921</label><figDesc>Ablation study for physical structure recognition on icdar-2013-partial dataset. ES: indicates Experimental Setup, CD: indicates Cell Detection, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score. r-cnn+td+bu dgcnn+p2 0.781 0.768 0.774 0.756 0.721 0.738 Mask r-cnn+td+bu dgcnn+p2+lstm 0.803 0.790 0.796 0.782 0.754 0.768 Mask r-cnn+td+bu+aldgcnn 0.821 0.814 0.817 0.797 0.748 0.772 Mask r-cnn+td+bu+aldgcnn+p2 0.823 0.818 0.820 0.800 0.753 0.776 Mask r-cnn+td+bu+aldgcnn+p2+lstm 0.8400.8360.8380.8220.7870.804 -nadgcnn -na--na--na-0.904 0.889 0.896 S-B -na-dgcnn+p2 -na--na--na-0.932 0.921 0.927 -na-dgcnn+p2+lstm -na--na--na-0.9750.9580.966 Ablation study for physical structure recognition on icdar-2019 dataset. ES: indicates Experimental Setup, CD: indicates Cell Detection, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score. r-cnn+td+bu+aldgcnn+p2+lstm 0.8580.8640.8610.8490.8280.839 -nadgcnn -na--na--na-0.921 0.898 0.909 S-B -na-dgcnn+p2 -na--na--na-0.950 0.935 0.942 -na-dgcnn+p2+lstm -na--na--na-0.9920.9940.993 Table 20. Ablation study for physical structure recognition on unlv-partial dataset. ES: indicates Experimental Setup, CD: indicates Cell Detection, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score. r-cnn+td+bu dgcnn+p2 0.905 0.917 0.911 0.896 0.882 0.889 Mask r-cnn+td+bu dgcnn+p2+lstm 0.918 0.924 0.921 0.905 0.898 0.902 Mask r-cnn+td+bu+aldgcnn 0.908 0.919 0.913 0.908 0.894 0.901 Mask r-cnn+td+bu+aldgcnn+p2 0.921 0.926 0.923 0.913 0.901 0.907 Mask r-cnn+td+bu+aldgcnn+p2+lstm 0.9320.9380.9350.9270.9130.920 -nadgcnn -na--na--na-0.970 0.981 0.976 S-B -na-dgcnn+p2 -na--na--na-0.973 0.986 0.979 -na-dgcnn+p2+lstm -na--na--na-0.9890.9930.991 Ablation study for physical structure recognition on scitsr dataset. ES: indicates Experimental Setup, CD: indicates Cell Detection, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>table from unlv dataset. Left Image: shows original table image in unlv and Right Image: illustrates ground-truth cell spanning information.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the datasets used for our experiments.</figDesc><table><row><cell>table pubtabnet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>table structure recognition, we also use bleu [55], cider [56]and rouge [57] scores to compare generated xml and ground truth xml on spanning information and content of every cell. We first calculate these scores separately on each table and then compute averaged score as the final result. We consistently use an IoU threshold of 0.6 to compute the confusion matrix. Please note that only non-empty table cells are considered similar to<ref type="bibr" target="#b17">[18]</ref> for theTable 2. shows the performance of our tabstruct-net for physical table structure recognition on various benchmark datasets.</figDesc><table><row><cell>Test Dataset</cell><cell cols="2">Train Dataset</cell><cell>S-A</cell><cell>S-B</cell></row><row><cell></cell><cell></cell><cell cols="3">P? R? F1? P? R? F1?</cell></row><row><cell>icdar-2013</cell><cell>scitsr</cell><cell cols="3">0.915 0.897 0.906 0.976 0.985 0.981</cell></row><row><cell cols="2">icdar-2013-partial scitsr</cell><cell cols="3">0.930 0.908 0.919 0.991 0.993 0.992</cell></row><row><cell>scitsr</cell><cell>scitsr</cell><cell cols="3">0.927 0.913 0.920 0.989 0.993 0.991</cell></row><row><cell>scitsr-comp</cell><cell>scitsr</cell><cell cols="3">0.909 0.882 0.895 0.981 0.987 0.984</cell></row><row><cell>unlv-partial</cell><cell>scitsr</cell><cell cols="3">0.849 0.828 0.839 0.992 0.994 0.993</cell></row><row><cell>icdar-2019</cell><cell>scitsr</cell><cell cols="3">0.595 0.572 0.583 0.924 0.899 0.911</cell></row><row><cell>icdar-2019</cell><cell>icdar-2019</cell><cell cols="3">0.803 0.768 0.785 0.975 0.957 0.966</cell></row><row><cell>icdar-2019</cell><cell cols="4">scitsr+icdar-2019 0.822 0.787 0.804 0.975 0.958 0.966</cell></row><row><cell cols="2">Test Dataset</cell><cell cols="3">Train Dataset Metric Score</cell></row><row><cell cols="2">tablebank-word</cell><cell>scitsr</cell><cell>bleu</cell><cell>0.914</cell></row><row><cell cols="2">tablebank-latex</cell><cell>scitsr</cell><cell>bleu</cell><cell>0.937</cell></row><row><cell cols="3">tablebank-word+latex scitsr</cell><cell>bleu</cell><cell>0.916</cell></row><row><cell cols="2">pubtabnet</cell><cell>scitsr</cell><cell>teds</cell><cell>0.901</cell></row></table><note>Table 3. shows the performance of our tabstruct-net for logical table structure recog- nition on various benchmark datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of results for physical structure recognition on icdar-2013 dataset. #Images: indicates number of table images in the training set. Heuristic: indicates dataset specific cell merging rules for various models in<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Training</cell><cell cols="2">Experimental P?</cell><cell>R? F1?</cell></row><row><cell></cell><cell>Dataset</cell><cell cols="2">#Images Setup</cell></row><row><cell>deepdesrt [7]</cell><cell>scitsr</cell><cell></cell><cell>12K S-A</cell><cell>0.631 0.619 0.625</cell></row><row><cell>splerge [10]</cell><cell>scitsr</cell><cell></cell><cell>12K S-A</cell><cell>0.883 0.875 0.879</cell></row><row><cell cols="2">split+heuristic [10] Private [10]</cell><cell></cell><cell>83K S-A</cell><cell>0.938 0.922 0.930</cell></row><row><cell cols="2">tabstruct-net (our) scitsr</cell><cell></cell><cell>12K S-A</cell><cell>0.915 0.897 0.906</cell></row><row><cell>tablenet [12]</cell><cell>Marmot Extended</cell><cell></cell><cell>1K S-B</cell><cell>0.922 0.899 0.910</cell></row><row><cell>graphtsr [14]</cell><cell>scitsr</cell><cell></cell><cell>12K S-B</cell><cell>0.885 0.860 0.872</cell></row><row><cell>split-pdf [10]</cell><cell>Private [10]</cell><cell></cell><cell>83K S-B</cell><cell>0.920 0.913 0.916</cell></row><row><cell>split-pdf</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+heuristic [10]</cell><cell>Private [10]</cell><cell></cell><cell>83K S-B</cell><cell>0.959 0.946 0.953</cell></row><row><cell>dgcnn [9]</cell><cell>scitsr</cell><cell></cell><cell>12K S-B</cell><cell>0.972 0.983 0.977</cell></row><row><cell cols="2">tabstruct-net (our) scitsr</cell><cell></cell><cell>12K S-B</cell><cell>0.976 0.985 0.981</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Network</cell><cell>SR Network</cell><cell>IoUCD Scores</cell><cell>SR Scores</cell></row><row><cell></cell><cell></cell><cell cols="2">TH P? R? F1? P? R? F1?</cell></row><row><cell></cell><cell></cell><cell cols="2">0.5 0.9350.9420.9380.9270.9110.919</cell></row><row><cell></cell><cell></cell><cell cols="2">0.6 0.921 0.926 0.923 0.915 0.897 0.906</cell></row><row><cell cols="4">Mask r-cnn+td+bu+aldgcnn+p2+lstm0.7 0.815 0.820 0.817 0.797 0.785 0.791</cell></row><row><cell></cell><cell></cell><cell cols="2">0.8 0.638 0.653 0.645 0.629 0.615 0.622</cell></row><row><cell></cell><cell></cell><cell cols="2">0.9 0.275 0.312 0.292 0.247 0.236 0.241</cell></row></table><note>. Physical structure recognition results on icdar-2013 dataset for varying IoU thresholds to demonstrate tabstruct-net's robustness. ES: Experimental Setup, CD: Cell Detection, TH: IoU threshold value, SR: Structure Recognition, P2: using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: use of Top-Down and Bottom-Up pathways in the fpn, AL: addition of alignment loss as a regularizer to tabstruct-net.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>[53] dataset. To compute region proposals, we use 0.5, 1 and 2 as the anchor scale and anchor box sizes of 8, 16, 32, 64 and 128. Further, for generation of the row/column adjacency matrices, we use 2400 as the maximum number of vertices keeping in mind dense tables. Since every input table may contain hundreds of table cells, training can be a time consuming process.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .Table 8 .Table 9 .Table 10 .Table 11 .</head><label>7891011</label><figDesc>Comparison of results for physical structure recognition on icdar-2013partial dataset. P: indicates precision, R: indicates recall, F1: indicates F1 Score and #Images: indicates number of table images in the training set. Comparison of results for physical structure recognition on icdar-2019 (ctdar) archival dataset. For comparison against dgcnn<ref type="bibr" target="#b8">[9]</ref>, we use the cell bounding boxes detected from tabstruct-net for a fair comparison. P: indicates precision, R: indicates recall, F1: indicates F1 Score and #Images: indicates number of table images in the training set. Comparison of results for physical structure recognition on unlv-partial dataset. P: indicates precision, R: indicates recall, F1: indicates F1 Score. All models are trained on scitsr and fine-tuned on unlv-partial datasets. Comparison of results for physical structure recognition on scitsr and scitsr-comp datasets. P: indicates precision, R: indicates recall, F1: indicates F1 Score. All the models are trained on scitsr dataset.Tables 11-12 present compare our results for logical structure prediction from the table image on tablebank and pubtabnet dataset, respectively. The scores are obtained by averaging the score for every table across all the tables in the evaluation dataset. From the tables, it can be inferred that despite trained with a much smaller set of data, our model achieves better performance than<ref type="bibr" target="#b12">[13]</ref>. Direct comparison, however would not be fair because of the use of different input modalities for training. Comparison of results for logical structure recognition on tablebank dataset.</figDesc><table><row><cell></cell><cell cols="2">icdar-2013-partial</cell><cell cols="2">0.124K S-A</cell><cell>0.969 0.901 0.934</cell></row><row><cell>tabstruct-net (our)</cell><cell cols="2">icdar-2013-partial</cell><cell cols="2">0.124K S-A</cell><cell>0.928 0.903 0.915</cell></row><row><cell>tabstruct-net (our)</cell><cell>scitsr</cell><cell></cell><cell cols="2">12.124K S-A</cell><cell>0.930 0.908 0.919</cell></row><row><cell></cell><cell cols="2">+ icdar-2013-partial</cell><cell></cell><cell></cell></row><row><cell>tablenet [12]</cell><cell cols="2">Marmot extended</cell><cell cols="2">1.016K S-B</cell><cell>0.931 0.900 0.915</cell></row><row><cell>graphtsr [14]</cell><cell>scitsr</cell><cell></cell><cell cols="2">12.124K S-B</cell><cell>0.854 0.891 0.872</cell></row><row><cell></cell><cell cols="2">+ icdar-2013-partial</cell><cell></cell><cell></cell></row><row><cell>dgcnn [9]</cell><cell>scitsr</cell><cell></cell><cell cols="2">12.124K S-B</cell><cell>0.986 0.990 0.988</cell></row><row><cell></cell><cell cols="2">+ icdar-2013-partial</cell><cell></cell><cell></cell></row><row><cell>tabstruct-net (our)</cell><cell cols="2">icdar-2013-partial</cell><cell cols="2">0.124K S-B</cell><cell>0.991 0.989 0.990</cell></row><row><cell>tabstruct-net (our)</cell><cell>scitsr</cell><cell></cell><cell cols="2">12.124K S-B</cell><cell>0.991 0.993 0.992</cell></row><row><cell></cell><cell cols="2">+ icdar-2013-partial</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Training</cell><cell>Exp.</cell><cell></cell><cell>P?</cell><cell>R? F1?</cell></row><row><cell></cell><cell>Dataset</cell><cell cols="2">#Images Setup</cell><cell></cell></row><row><cell>nlpr-pal [19]</cell><cell>ctdar</cell><cell></cell><cell>0.6K S-A</cell><cell cols="2">0.720 0.770 0.745</cell></row><row><cell>dgcnn [9]</cell><cell>ctdar</cell><cell></cell><cell>0.6K S-A</cell><cell cols="2">0.785 0.751 0.768</cell></row><row><cell>dgcnn [9]</cell><cell>scitsr</cell><cell></cell><cell>12.0K S-A</cell><cell cols="2">0.552 0.519 0.535</cell></row><row><cell>dgcnn [9]</cell><cell>ctdar + scitsr</cell><cell></cell><cell>12.6K S-A</cell><cell cols="2">0.803 0.778 0.790</cell></row><row><cell>splerge [10]</cell><cell>ctdar</cell><cell></cell><cell>0.6K S-A</cell><cell cols="2">0.774 0.783 0.778</cell></row><row><cell>splerge [10]</cell><cell>scitsr</cell><cell></cell><cell>12.0K S-A</cell><cell cols="2">0.559 0.572 0.565</cell></row><row><cell>splerge [10]</cell><cell>ctdar + scitsr</cell><cell></cell><cell>12.6K S-A</cell><cell cols="2">0.792 0.800 0.796</cell></row><row><cell cols="2">tabstruct-net (our) ctdar</cell><cell></cell><cell>0.6K S-A</cell><cell cols="2">0.803 0.768 0.785</cell></row><row><cell cols="2">tabstruct-net (our) scitsr</cell><cell></cell><cell>12.0K S-A</cell><cell cols="2">0.595 0.572 0.583</cell></row><row><cell cols="2">tabstruct-net (our) ctdar + scitsr</cell><cell></cell><cell>12.6K S-A</cell><cell cols="2">0.822 0.787 0.804</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13 .</head><label>13</label><figDesc>Results comparison of various methods for table structure recognition on various datasets.</figDesc><table><row><cell>Training Set</cell><cell cols="2">Evaluation SetModel</cell><cell cols="3">BLEU?CIDEr?ROUGE?</cell></row><row><cell>scitsr</cell><cell>scitsr</cell><cell>dgcnn</cell><cell>0.774</cell><cell>0.8</cell><cell>0.782</cell></row><row><cell></cell><cell></cell><cell cols="2">tabstruct-net 0.833</cell><cell>0.848</cell><cell>0.839</cell></row><row><cell>scitsr</cell><cell>scitsr-comp</cell><cell>dgcnn</cell><cell>0.769</cell><cell>0.795</cell><cell>0.774</cell></row><row><cell></cell><cell></cell><cell cols="2">tabstruct-net 0.826</cell><cell>0.837</cell><cell>0.830</cell></row><row><cell>scitsr + unlv-partial</cell><cell>unlv-partial</cell><cell>dgcnn</cell><cell>0.721</cell><cell>0.744</cell><cell>0.729</cell></row><row><cell></cell><cell></cell><cell cols="2">tabstruct-net 0.804</cell><cell>0.826</cell><cell>0.813</cell></row><row><cell>scitsr</cell><cell>icdar-2013</cell><cell>dgcnn</cell><cell>0.756</cell><cell>0.773</cell><cell>0.762</cell></row><row><cell></cell><cell></cell><cell cols="2">tabstruct-net 0.815</cell><cell>0.831</cell><cell>0.821</cell></row><row><cell cols="2">scitsr + icdar-2013-partialicdar-2013-</cell><cell>dgcnn</cell><cell>0.772</cell><cell>0.801</cell><cell>0.78</cell></row><row><cell></cell><cell>partial</cell><cell cols="2">tabstruct-net 0.829</cell><cell>0.845</cell><cell>0.834</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Qualitative Results of Cell Detection Figures 10-11 demonstrate some qualitative results of cell detection on all the evaluation datasets. From the figures, it can be seen that our model is able to work in the presence of archival table images, multiple row/column spanning cells, varied table layouts and multiple line spanning cells. This indicates the robustness of our method under multiple kind of table images. . 10. Sample intermediate cell detection results of tabstruct-net on table images of icdar-2013 (in First Row), icdar-2019 (in Second Row), scitsr (in Third Row), scitsr-comp (in Fourth Row) and tablebank (in Fifth Row) datasets. Fig. 11. Sample intermediate cell detection results of tabstruct-net on table images of pubtabnet (in First Row) and unlv (in Second Row) datasets. Qualitative Results of Structure Recognition Figures 12-18 demonstrate some qualitative results of structure recognition on all the evaluation datasets. From the figures, it can be seen that our model is able to work in the presence of archival table images, multi row/column spanning cells, varied table layouts and multiple line spanning cells. This indicates the robustness of our method under multiple kind of table images.</figDesc><table><row><cell>Fig</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 14 .Table 17 .</head><label>1417</label><figDesc>. 19. Sample intermediate cell detection results of tabstruct-net on table images of icdar-2013, icdar-2019 ctdar, scitsr, scitsr-comp, tablebank, pubtabnet and unlv datasets illustrate failure of tabstruct-net.6.2 Robustness of TabStruct-Net Physical structure recognition results on icdar-2013-partial dataset for varying IoU thresholds to demonstrate tabstruct-net's robustness. ES: indicates Experimental Setup, CD: indicates Cell Detection, TH: indicates IoU threshold value, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score. Physical structure recognition results on scitsr dataset for varying IoU thresholds to demonstrate tabstruct-net's robustness. ES: indicates Experimental Setup, CD: indicates Cell Detection, TH: indicates IoU threshold value, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net, P: indicates precision, R: indicates recall, F1: indicates F1 Score.</figDesc><table><row><cell>CD Network</cell><cell>SR Network</cell><cell>IoUCD Scores</cell><cell>SR Scores</cell></row><row><cell></cell><cell></cell><cell cols="2">TH P? R? F1? P? R? F1?</cell></row><row><cell></cell><cell></cell><cell cols="2">0.5 0.9420.9480.9450.9330.9150.924</cell></row><row><cell></cell><cell></cell><cell cols="2">0.6 0.937 0.941 0.939 0.930 0.908 0.919</cell></row><row><cell cols="4">Mask r-cnn+td+bu+aldgcnn+p2+lstm0.7 0.828 0.831 0.829 0.800 0.791 0.795</cell></row><row><cell></cell><cell></cell><cell cols="2">0.8 0.651 0.670 0.660 0.638 0.624 0.631</cell></row><row><cell></cell><cell></cell><cell cols="2">0.9 0.314 0.336 0.325 0.291 0.284 0.287</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/sachinraja13/TabStructNet.git</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast CNN-based document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Borges Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Palhares Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">CNN based page object detection in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Medium-independent table detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Kashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Lopresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilfong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Recognition and Retrieval VII</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<title level="m">Table structure understanding and its performance evaluation. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding the semantic structures of tables with a hybrid deep neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepDeSRT: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Table-to-text: Describing table region with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking table parsing using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinezp</surname></persName>
		</author>
		<editor>ICDAR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TableBank: Table benchmark for image-based table detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<editor>ICDAR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">TableNet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<editor>ICDAR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Yepes</surname></persName>
		</author>
		<title level="m">Image-based table recognition: data, model, and evaluation. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Mao</surname></persName>
		</author>
		<title level="m">Complicated table structure recognition. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Table structure extraction with Bi-directional Gated Recurrent Unit networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M D</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<editor>ICDAR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation for table structure recognition in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<editor>ICDAR.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">ReS2TIM: Reconstruct syntactic structures from table images. In: ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orsi</surname></persName>
		</author>
		<title level="m">ICDAR 2013 table competition. In: ICDAR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lang</surname></persName>
		</author>
		<title level="m">ICDAR 2019 competition on table detection and recognition (cTDaR). In: ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">IIIT-AR-13K: a new dataset for graphical object detection in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lipps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>DAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Table structure recognition based on textblock arrangement and ruled line position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itonori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">CD Network SR Network IoUCD Scores SR Scores TH P? R? F1? P? R? F1?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TH: indicates IoU threshold value, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net</title>
	</analytic>
	<monogr>
		<title level="m">Table 15. Physical structure recognition results on icdar-2019 dataset for varying IoU thresholds to demonstrate tabstruct-net&apos;s robustness</title>
		<imprint/>
	</monogr>
	<note>CD: indicates Cell Detection. P: indicates precision, R: indicates recall, F1: indicates F1 Score</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">CD Network SR Network IoUCD Scores SR Scores TH P? R? F1? P? R? F1?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TH: indicates IoU threshold value, SR: indicates Structure Recognition, P2: indicates using visual features from P2 layer of the fpn instead of using separate convolution blocks, lstm: indicates use of lstms to model visual features along center-horizontal and center-vertical lines for every cell, td+bu: indicates use of Top-Down and Bottom-Up pathways in the fpn, AL: indicates addition of alignment loss as a regularizer to tabstruct-net</title>
	</analytic>
	<monogr>
		<title level="m">Table 16. Physical structure recognition results on unlv-partial dataset for varying IoU thresholds to demonstrate tabstruct-net&apos;s robustness</title>
		<imprint/>
	</monogr>
	<note>CD: indicates Cell Detection. P: indicates precision, R: indicates recall, F1: indicates F1 Score</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
