<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Li</surname></persName>
							<email>lijiangnan@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
							<email>linzheng@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Fu</surname></persName>
							<email>fupeng@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Si</surname></persName>
							<email>siqingyi@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
							<email>wangweiping@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion Recognition in Conversation (ERC) is a more challenging task than conventional text emotion recognition. It can be regarded as a personalized and interactive emotion recognition task, which is supposed to consider not only the semantic information of text but also the influences from speakers. The current method models speakers' interactions by building a relation between every two speakers. However, this fine-grained but complicated modeling is computationally expensive, hard to extend, and can only consider local context. To address this problem, we simplify the complicated modeling to a binary version: Intra-Speaker and Inter-Speaker dependencies, without identifying every unique speaker for the targeted speaker. To better achieve the simplified interaction modeling of speakers in Transformer, which shows excellent ability to settle long-distance dependency, we design three types of masks and respectively utilize them in three independent Transformer blocks. The designed masks respectively model the conventional context modeling, Intra-Speaker dependency, and Inter-Speaker dependency. Furthermore, different speaker-aware information extracted by Transformer blocks diversely contributes to the prediction, and therefore we utilize the attention mechanism to automatically weight them. Experiments on two ERC datasets indicate that our model is efficacious to achieve better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Nowadays, intelligent machines to precisely capture speakers' emotions in conversations are gaining popularity, thus driving the development of Emotion Recognition in Conversation (ERC). ERC is a task to predict the emotion of the current utterance expressed by a specific speaker according to the context <ref type="bibr" target="#b16">(Poria et al. 2019b</ref>), which is more challenging than the conventional emotion recognition only considering semantic information of an independent utterance.</p><p>To precisely predict the emotion of a targeted utterance, both the semantic information of the utterance and the information provided by utterances in the context are critical. Nowadays, a number of works <ref type="bibr">(Hazarika et al. 2018a,b;</ref><ref type="bibr" target="#b13">Majumder et al. 2019;</ref><ref type="bibr" target="#b3">Ghosal et al. 2019)</ref> demonstrate that the interactions between speakers can facilitate extracting information from contextual utterances. We denote this kind of information with modeling speakers' interactions as <ref type="bibr">Copyright ? 2021</ref>, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  speaker-aware contextual information. To capture speakeraware contextual information, the state-of-the-art model Di-alogueGCN <ref type="bibr" target="#b3">(Ghosal et al. 2019)</ref> introduces Self and Inter-Speaker dependencies, which capture the influences from different speakers. As illustrated in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>, Self and Inter-Speaker dependencies establish a specific relation between every two speakers and construct a fully connected relational graph. And then a Relational Graph Convolutional Network (RGCN) <ref type="bibr" target="#b18">(Schlichtkrull et al. 2018</ref>) is applied to process such a graph. Although DialogueGCN can achieve excellent performance with Self and Inter-Speaker dependencies, this speaker modeling is easy to be complicated with the number of speakers increasing. As shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>, for a conversation clip with two speakers, the considered relations reach to 7. The number can drastically increase with more speakers involved. Thus this complicated speaker modeling is hard to deal with the condition that the number of speakers dynamically changes, and not flexible to be deployed in other models. In addition, RGCN processing the fully connected graph with multiple relations requires tremendous consumption of computation. This limitation leads to Dia-logueGCN only considering the local context in a conversation <ref type="bibr" target="#b3">(Ghosal et al. 2019)</ref>. Therefore, it is appealing to introduce a simple and general speaker modeling, which is easy to extend in all scenes and realize in other models so that long-distance context can be available.</p><p>To address the above problem, we propose a TRans-forMer with Speaker Modeling (TRMSM). First, we simplify the Self and Inter-speaker dependencies to a binary version, which only contains two relations: Intra-Speaker dependency and Inter-Speaker dependency. As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, for the speaker of the targeted utterance, Intra-Speaker dependency focuses on the influence from the same speaker, and Inter-Speaker dependency treats other speakers as a whole group instead of building a relation between every two speakers. In this way, our simplified modeling can be easy to extend in other models and deal with the scene with the dynamical number of speakers without introducing new relations between speakers. Furthermore, with the ability to settle long-distance dependency, Transformer <ref type="bibr" target="#b20">(Vaswani et al. 2017</ref>) achieves excellent performance among a great number of Natural Language Processing (NLP) problems. To better model the longdistance contextual utterances in a conversation, we utilize a hierarchical Transformer with two levels: sentence level and dialogue level. In the sentence level, BERT <ref type="bibr" target="#b1">(Devlin et al. 2019</ref>) encodes the semantic representation for a targeted utterance, and in the dialogue level, Transformer is used to capture the information from contextual utterances. To better model our simplified dependencies in the dialogue-level Transformer, we design three masks: Conventional Mask for conventional context modeling, Intra-Speaker Mask for Intra-Speaker dependency, and Inter-Speaker Mask for Inter-Speaker dependency. To realize the functions of masks, we deploy three independent Transformer blocks in dialogue level and the designed masks are respectively used in these Transformer blocks. With different speaker-aware contextual information extracted by these Transformer blocks, whose contributions to the final prediction are diverse, we utilize the attention mechanism to automatically weight and fuse them. Besides, we also apply two other simple fusing methods: Add and Concatenation to demonstrate the advancement of the attention.</p><p>Specifically, our contributions are concluded as follows:</p><p>? We simplify Self and Inter-speaker dependencies to a binary version, so that the speaker interaction modeling can be extended in hierarchical Transformer and the longdistance context can be considered.</p><p>? We design three types of masks to achieve speakers' interactions modeling in Transformer and utilize the attention mechanism to automatically pick up the important speaker-aware contextual information.</p><p>? We conduct experiments on two ERC datasets: IEMO-CAP and MELD. Our method achieves state-of-the-art performance on both datasets on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Two aspects are strongly related to our work: Emotion recognition in conversation and Utilization of mask in Transformer. Emotion recognition in conversation Hierarchical structure based on RNN <ref type="bibr" target="#b7">(Jiao, Lyu, and King 2020;</ref><ref type="bibr" target="#b8">Jiao et al. 2019)</ref> or Transformer <ref type="bibr" target="#b23">(Zhong, Wang, and Miao 2019;</ref><ref type="bibr" target="#b10">Li et al. 2020</ref>) is leveraged in ERC to capture contextual information. Except contextual information, speaker information is proven to be important to ERC. Speakers can be regarded as objects related to utterances or additional information for utterances. As objects, speakers are involved in the graph of conversation as nodes to interact with utterances <ref type="bibr" target="#b22">(Zhang et al. 2019)</ref>. As additional information, speaker information is modeled via utterances. Specifically, <ref type="bibr">Hazarika et al. (2018b,a)</ref> employ GRUs and Memory Network (Memnet) <ref type="bibr" target="#b19">(Sukhbaatar et al. 2015)</ref> to model speakers' interactions in the dyadic conversation, which is difficult to extend to multi-speaker conditions. Therefore, <ref type="bibr" target="#b13">Majumder et al. (2019)</ref> generalize speakers as parties, track them by GRU, and utilize attention mechanism to gather interactive information in multi-speaker conversations. Even so, <ref type="bibr" target="#b3">Ghosal et al. (2019)</ref> argue that <ref type="bibr" target="#b13">Majumder et al. (2019)</ref> ignored the influences from other speakers and propose Self and Inter-Speaker dependencies to formalize interactions within and between speakers. However, the complicated modeling of speakers' interactions is difficult to apply in other models, thus requiring a simplified version.</p><p>Utilization of mask in Transformer Masks in Transformer are utilized to mask the unattended elements in selfattention. Recently, masks are well-designed and leveraged in language modeling <ref type="bibr" target="#b2">(Dong et al. 2019;</ref><ref type="bibr" target="#b1">Devlin et al. 2019;</ref><ref type="bibr" target="#b17">Radford et al. 2018</ref>) and conversation structure modeling <ref type="bibr" target="#b24">(Zhu et al. 2020)</ref>. Masks are flexible and convenient to be implemented and we choose them to model the interactions of speakers in Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we will elaborate on the task definition and the structure of TRMSM which is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. Our model contains 4 parts: Sentence-Level Encoder, Dialogue-Level Encoder, Fusing Method, and Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition</head><p>ERC task includes K emotions, whose set is E = {emo 1 , emo 2 , ..., emo K }. Given a conversation C = [u 1 , u 2 , ..., u N ] containing N textual utterances, each utterance u n = [w 1 , w 2 , ..., w Ln ] within is sequentially formed by L n words. Particularly, M speakers, whose set is SP K = {spk 1 , spk 2 , ..., spk M }, participate in the conversation. For each utterance u n , a emotion label e n ? E and a speaker annotation p n ? SP K are assigned. ERC task aims to predict the emotion of every utterance in C with the information provided above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-Level Encoder</head><p>To encode a more informative and context-aware representation of a single utterance based on Transformer, we utilize . . . a BERT encoder. Limited by the max length of a sequence supported by BERT, we cannot input the concatenated sequence of all utterances in a conversation, whose length frequently exceeds 768 in cases of long conversations, to capture the global contextual information. Therefore, BERT is solely used to encode the sentence-level context in a single utterance. An utterance u n = [w 1 , w 2 , ..., w Ln ] is fed into BERT to obtain the contextualized representation of words:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Encoder</head><formula xml:id="formula_0">W = BERT (w 1 , w 2 , ..., w Ln )<label>(1)</label></formula><p>where W ? R Ln?dw is the output of the top layer of BERT and d w is the dimension of the word representation. To obtain an utterance representation for u n , a max-pooling operation followed by a projection is deployed:</p><formula xml:id="formula_1">u n = Linear(M axpooling(W ))<label>(2)</label></formula><p>where u n ? R du represents the utterance and d u is the dimension of utterance representation. By processing every utterance in a conversation, we finally obtain the representation matrix C ? R N ?du .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue-Level Encoder</head><p>In the dialogue level, we utilize three transformer blocks: Conventional Blocks for conventional context modeling, Intra-Speaker Blocks for Intra-Speaker dependency, and Inter-Speaker Blocks for Inter-Speaker dependency. Due to the same structures of all Transformer blocks, we simply introduce the general process of the first layer of Transformer blocks.</p><p>Given the conversation matrix C processed by the sentence-level encoder, to avoid the absence of positional information in C, an Absolute Positional Embedding is added to every representation in C:</p><formula xml:id="formula_2">C = C + P E(0 : N )<label>(3)</label></formula><p>where P E(0 : N ) is in the same dimension as C. Self-attention intuitively provides an interactive pattern for contextual modeling of conversations. Taking advantage of the mechanism of self-attention, the targeted utterances can be parallelly processed. Therefore, the targeted utterances are regarded as a query matrix, and the contextual utterances act as a key matrix, so that every utterance simultaneously assesses how much information shall be obtained from every contextual utterance. In this way, C is projected to query matrix Q ? R N ?da , key matrix K ? R N ?da , and value matrix V ? R N ?da by linear projections without bias:</p><formula xml:id="formula_3">[Q; K; V ] = Linear([C; C; C]) where [] is the concatenat- ing operation. Self-attention is calculated by: A(Q, K, V, M ) = sof tmax( (QK T ) * M ? d a )V<label>(4)</label></formula><p>where * denotes element-wise multiplication; M ? R N ?N is the utilized mask which is a square matrix whose noninfinite elements equal 1. We will introduce different masks used by diverse blocks later. Transformer hires multiple selfattention (Multi-Head Attention, MHA) to model different aspects of information. And then the outputs of all heads are concatenated and projected to O with the same size of C. After the Attention module, a Position-wise Feed-Forward Network (FFN) module is deployed to produce output F ? R N ?du . MHA and FFN are both residually connected. Therefore the output O 1 C of the first layer of Transformer is:</p><formula xml:id="formula_4">A = LayerN orm(O + C),<label>(5)</label></formula><formula xml:id="formula_5">F = max(0, A W 1 + b 1 )W 2 + b 2 ,<label>(6)</label></formula><formula xml:id="formula_6">O 1 C = LayerN orm(F + A ).<label>(7)</label></formula><p>O 1 C acts as the input of the second transformer layer, and by this analogy, we obtain the final output O C ? R N ?du after multiple layers. Therefore, the outputs of our 3 blocks can be denoted as:</p><formula xml:id="formula_7">O C C for Conventional Block, O RA C</formula><p>for Intra-Speaker Block, and O ER C for Inter-Speaker Block. Due to the limited space in this paper, more details about Transformer can be reviewed in <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref>.</p><p>Masks can prompt Transformer blocks to realize their different functions, and we introduce how to form these 3 masks:</p><p>Conventional Mask sets all the elements of itself to 1, which means that every targeted utterance can get access to all the contextual utterances. Conventional Mask is applied in the multi-head attention of Conventional Blocks and is illustrated in <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>. We annotate Conventional Mask as M C .</p><p>Intra-Speaker Mask only considers those contextual utterances tagged with p n , which is the speaker tag of the targeted utterance. Therefore, based on M C , Intra-Speaker Mask M RA sets positions representing other speakers to -INF. Intra-Speaker Mask is illustrated in <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>.</p><p>Inter-Speaker Mask regards other speakers different from the one of the targeted utterance as one unit due to our simplification. Therefore, based on M C , Inter-Speaker Mask M ER sets positions whose speaker is the same as the speaker tag of the targeted utterance to -INF. Inter-Speaker Mask is illustrated in <ref type="figure" target="#fig_2">Fig. 2 (c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusing Method</head><p>As blocks produce different outputs that carry various speaker-aware contextual information, we utilize 3 simple methods to fuse the information.</p><p>Add As illustrated in <ref type="figure" target="#fig_2">Fig. 2 (i)</ref>, Add equally regards the contributions of all outputs of blocks. Therefore, the fusing representation is:</p><formula xml:id="formula_8">R = O C C + O RA C + O ER C<label>(8)</label></formula><p>Concatenation Concatenation (illustrated in <ref type="figure" target="#fig_2">Fig. 2 (ii)</ref>) is also a simple but effective method to combine different information. Different from Add operation, Concatenation can implicitly choose the information which is important for the final prediction due to the following linear projection of classifier. Therefore, the fusing representation R ? R N ?3du is:</p><formula xml:id="formula_9">R = Concat(O C C , O RA C , O ER C , dim = 1) (9) Attention</formula><p>As the contributions of different speaker parties are diversely weighted, it is feasible that the model automatically chooses the more important information. Therefore, we utilize the widely used attention <ref type="bibr" target="#b11">(Lian et al. 2019</ref>  to achieve this goal. Attention mechanism takes 3 block outputting representations as inputs and produces an attention score for each representation. For simplicity, we take rep-</p><formula xml:id="formula_10">resentations O C C i ? R 1?du , O RA C i ? R 1?du , and O ER C i ? R 1?du</formula><p>of utterance i as an example. Therefore, the attention score and fusing representation are computed as:</p><formula xml:id="formula_11">O i = Concat(O C C i , O RA C i , O ER C i , dim = 0), (10) ? = sof tmax(w F O i T ),<label>(11)</label></formula><formula xml:id="formula_12">R i = ?O i .<label>(12)</label></formula><p>where O i ? R 3?du is the concatenated representations, ? ? R 1?3 is the attention score, w F ? R 1?du is a trainable parameter, and R i ? R 1?du is the fusing representation. Finally, all fusing representations of utterances are concatenated as R ? R N ?du .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>With the sentence-level and dialogue-level contextual information fully modeled by encoders, the dialogue-level output is fed to a classifier which predicts the final emotion distributions:</p><formula xml:id="formula_13">? = sof tmax(RW clf + b clf )<label>(13)</label></formula><p>where W clf ? R du?K (W clf ? R 3du?K for Concatenation), b clf ? R K and? is the matrix of emotion distributions of all utterances in conversation C. The model is trained by a cross-entropy loss function, which is calculated as:</p><formula xml:id="formula_14">L = ? 1 T l=1 N l T l=1 N l i=1 K e=1 y e i log(? e i )<label>(14)</label></formula><p>where y i is the one-hot vector denoting the emotion label of utterance i in a conversation, e denotes the dimension of each emotion, N l denotes the length of l-th conversation, and T denotes the number of conversations in a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup Datasets</head><p>We evaluate our models on two datasets: IEMOCAP <ref type="bibr" target="#b0">(Busso et al. 2008)</ref>, MELD <ref type="bibr" target="#b15">(Poria et al. 2019a)</ref>, and both of them are multi-modal datasets that contain three modalities. We solely consider the textual modality following Ghosal et al.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods</head><p>To distinguish our models with different fusing methods, we construct 3 model variants: TRMSM-Add, TRMSM-Cat, and TRMSM-Att. To show the importance of the speakerrelated information, we construct our model without Intra-Speaker Blocks and Inter-Speaker Blocks, which we denote it as TRM. Besides, our models are compared with the baselines below:</p><p>? CMN <ref type="bibr" target="#b5">(Hazarika et al. 2018b)</ref> CMN is proposed to model dyadic conversations using two sets of RNNs and Memnets to respectively track different speakers. ? DialogueRNN ) DialogueRNN is the first state-of-the-art model to address ERC of multispeakers. RNNs are deployed to track speakers' states and global state during conversations. ? AGHMN <ref type="bibr" target="#b7">(Jiao, Lyu, and King 2020)</ref> AGHMN is the state-of-the-art unidirectional model in real-time ERC.</p><p>To retain the positional information from its hierarchical structure, a GRU is constructed for attention mechanism. ? DialogueGCN <ref type="bibr" target="#b3">(Ghosal et al. 2019)</ref> To fully model the interactive information between speakers, DialogueGCN models detailed dependencies between speakers using a Relational GCN. ? KET <ref type="bibr" target="#b23">(Zhong, Wang, and Miao 2019)</ref> KET introduces the Transformer structure to model context in conversations. It also proposes an effective graph attention to extract information from commonsense knowledge bases. ? BERT <ref type="bibr" target="#b1">(Devlin et al. 2019</ref>) A vanilla BERT followed by a classifier is fine-tuned to show the importance of context. ? Other baselines Both based on CNN to extract semantic information, scLSTM <ref type="bibr" target="#b14">(Poria et al. 2017</ref>) utilizes LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber 1997)</ref> and Memnet <ref type="bibr" target="#b19">(Sukhbaatar et al. 2015)</ref> utilizes memory network to model conversational context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>For BERT and sentence-level encoder, an uncased BERTbase 1 model is adopted. For the dialogue-level encoder, the dimension of dialogue-level representation is set to 300 for IEMOCAP and 200 for MELD; the number of transformer layers is set to 6 for IEMOCAP and 1 for MELD; the number of heads is set 6 for IEMOCAP and 4 for MELD; dropout rate is set to 0.1. Additionally, models are trained using AdamW <ref type="bibr" target="#b9">(Kingma and Ba 2015;</ref><ref type="bibr" target="#b12">Loshchilov and Hutter 2019)</ref> for 10000 steps with 1000 steps for warming up, and the learning rate linearly decaying after the warm-up is set to 1e-5 for IEMOCAP and 8e-6 for MELD. Due to the parallel prediction of utterances in one conversation, batch size is set to 1 following <ref type="bibr" target="#b7">Jiao, Lyu, and King (2020)</ref>. Besides, DialogueGCN is trained in the setting of 90:10 data split on IEMOCAP, and for a fair comparison, we re-run DialogueGCN with 80:20 data split using the open-source code 2 . All of our results reported are the average values of 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussions</head><p>Overall Results</p><p>For IEMOCAP, weighted-F1 (wF1) score is used as the metric. However, the data proportion of MELD is in a severely imbalanced condition. Therefore, the weighted-F1 score is not that proper and enough for MELD. To balance the contributions of large classes and small classes, we follow <ref type="bibr" target="#b21">Zhang et al. (2020)</ref> and also use the average value of macro F1 score and micro F1 score as one metric, which is calculated by mF 1 = (F 1 macro + F 1 micro )/2. For IEMOCAP, as shown in Tab. 2, BERT attains wF1 of 54.01 which is substantially worse than our models and most state-of-the-art models considering the dialogue-level context. This result may indicate that IEMOCAP contains considerable utterances that cannot be predicted only depending on the semantic information, which is out of the  <ref type="bibr" target="#b23">(Zhong, Wang, and Miao 2019)</ref> -   <ref type="figure">Figure 3</ref>: The results of models with different blocks. wF1 for IEMOCAP; mF1 for MELD.</p><formula xml:id="formula_16">- - - - - - - - - - - - - - 58.18 - DialogueGCN(Ghosal et al. 2019) - - - - - - - - - - - - - - -</formula><p>conversational context. Furthermore, TRMSM-Att outperforms AGHMN by 2.24 wF1 and DialogueGCN (80: 20) by 3.28 wF1, which benefits from the powerful Transformer and our speaker modeling with long-distance information considered. For emotions, TRMSM-Att achieves the best F1 on Frustrated, and TRMSM-Add achieves the best F1 on Neutral. Besides, our models can attain second or third higher results among other emotions. This demonstrates that our models are competitive to achieve comprehensive performance. For MELD, as shown in Tab. 3, BERT outperforms other state-of-the-art models by a great margin, which indicates the importance of external knowledge brought by BERT. Compared with BERT, TRM attains marginally better results, which may be attributed to the limited conversational contextual information in MELD. To confirm this, comparing the results of CNN 3 and scLSTM (based on CNN), we can notice that the improvement is also limited. Although MELD provides limited contextual information in conversations, TRMSM-Att still outperforms BERT by 1.29 wF1 and 1.17 mF1, which indicates the effectiveness of our model to capture such information. For emotions, BERT beats other state-of-the-art models by a great margin in such an imbalanced circumstance, and TRMSM-Att attains the best F1 on 4 emotions including large classes Joy, Anger, Surprise, and the small class Disgust. This demonstrates that BERT can alleviate data imbalance and our model can take advantage of such a feature.</p><p>For both datasets, all TRMSM variants outperform TRM to show the importance of speaker-aware contex-3 CNN achieves 55.02 wF1 on MELD by <ref type="bibr" target="#b16">Poria et al. (2019b)</ref>. tual information. TRMSM-Att and TRMSM-Cat outperform TRMSM-Add, which indicates the importance of different aspects of speaker information requiring to be treated differently. TRMSM-Att outperforming TRMSM-Cat demonstrates that automatically and explicitly picking up speakerrelated information is better than the implicit way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Analysis</head><p>Ablation Study To better understand the influences of masks on our models, we report the results of the models removing the Transformer blocks with different masks on IEMOCAP and MELD. In this part, we denote Convention Mask as CM and Intra-Speaker, Inter-Speaker Masks as SM. Accordingly, TRMSM w/o SM is equivalent to TRM.</p><p>As seen in <ref type="figure">Fig. 3</ref>, on both datasets, TRMSM w/o CM (solely applying SM) can achieve better performance than TRMSM w/o SM (solely applying CM). We attribute it to that speaker modeling does not drop the contextual information from conversations, and on the contrary, speaker modeling can guide the model to extract more effective information to the final prediction. Furthermore, TRMSM outperforms both TRMSM w/o CM and TRMSM w/o SM, which demonstrates that all of our designed masks are critical to achieving better performance.</p><p>Effect of Range of Context To find out the influence of the range of context on our model, we train TRMSM-Att with different ranges of available context on IEMOCAP and refer the results from <ref type="bibr" target="#b3">Ghosal et al. (2019)</ref> for DialogueGCN. We utilize different windows (?x, y) to limit the context, where x, y is respectively the number of utterances in prior context and post context. As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, with the window widened, the performance increases as shown in both models. For DialogueGCN, (-10, 10) is the max window of context and therefore it cannot get access to the long- Attention from fusing method <ref type="figure">Figure 5</ref>: Heatmaps of attention from fusing method and self-attention of Intra-, Inter-Speaker Blocks for the targeted utterances (whose speakers are marked in yellow). Labels of utterances are tagged below the utterances. Predictions of TRMSM and TRM are marked in green for correctness and red for mistake. distance context. On the contrary, the performance is further improved by TRMSM-Att with all context available. This indicates that the local contextual information is critical for the prediction and the long-distance information is also important for contextual modeling to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Number of Layers</head><p>We study the effect of the number of layers to our model on different datasets. <ref type="figure" target="#fig_4">Fig.  6</ref> illustrates the radar graphs for the F1 scores of emotions in IEMOCAP and MELD by TRM and TRMSM-Att. As the number of layers increasing, the F1 scores of emotions in IEMOCAP normally expand. While in MELD, increasing the number of layers gradually hurts the performance to be 0 of F1 on emotions Fear and Disgust which are classes with the fewest data. We think the reason may be that MELD suffers from data imbalance and increasing the number of layers leads to severer overfitting on small classes. For data imbalance, methods like re-balance can be applied to alleviate it. Re-balance is out of the scope of this paper and our future work will study data imbalance of ERC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>To better understand how our model captures Intra-Speaker and Inter-Speaker dependencies, we illustrate two conversation clips ending with the targeted utterances so that the targeted utterances can only refer to the prior context. We choose TRMSM-Att without Conventional Blocks so that only speaker information related blocks are considered. Specifically, we illustrate heatmaps of attention from fusing method and self-attention 4 in Transformer blocks. For simplicity, we denote attention from fusing method as FAtt.</p><p>In the scene of <ref type="figure">Fig. 5 (a)</ref>, the speaker M keeps in frustration through the conversation and F in a neutral state has few influences on M. Therefore, FAtt pays more attention to Intra-Speaker dependency so that Intra-Speaker Blocks can extract information from M himself. We can see from the heatmap that the targeted utterance yeah grades the highest score to the farthest contextual utterance whose emotion is also frustration, which is out of the range of context that Dia-logueGCN can refer. In a sense, this indicates the importance of long-distance information.</p><p>As the condition in <ref type="figure">Fig. 5 (b)</ref>, speakers in this conversation basically keep in a neutral state except that Chandler shows other emotions like anger and surprise before the targeted utterance. Although the targeted utterance with speaker Chandler shows slight sadness from the semantic view, it is supposed to be predicted as neutral according to the conversational context. Specifically, FAtt grades Inter-Speaker Blocks with higher score and self-attention in Inter-Speaker Blocks extracts information from the neutral utterances of other speakers. This case indicates the effectiveness of our model to extract inter-speaker information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we simplify the Self and Inter-Speaker dependencies to a binary version. To achieve the simplified modeling of speakers' interactions, we design three masks: Conventional Mask, Intra-Speaker Mask, and Inter-Speaker Mask. These masks are utilized in the self-attention modules of the second-level Transformer blocks of a hierarchical Transformer. As the speaker-aware information extracted by different masks diversely contributes to the prediction, attention mechanism is utilized to weight and fuse them. Finally, our model achieves state-of-the-art results on 2 ERC datasets and further analysis shows that our model is efficacious for ERC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Relational graph for u3, u4 and u5 in Self and Inter-speaker dependencies. 7 relations are involved. Illustration of speakers' interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) illustrates a conversation clip of 3 speakers and the utterance in yellow frame is selected as a targeted utterance; (b) and (c) illustrate the relation graphs of u3, u4, u5 in different dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The structure of our proposed model, which is based on Transformer structure. Our proposed masks are utilized in the Multi-Head Attention of Dialogue-Level Encoder and are illustrated for 3 types: (a)conventional, (b)Intra-Speaker and (c)Inter-Speaker masks. The fusing methods include Attention, (i)Add, and (ii)Concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>wF1 of TRMSM-Att and DialogueGCN with different ranges of context. all means using global context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The F1 score on every emotion class by TRMSM-Att and TRM. (a)-(c) for IEMOCAP; (d)-(f) for MELD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics about IEMOCAP and MELD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Statistics about the datasets are shown in Tab. 1.Sukhbaatar et al. 2015) ?  25.72 33.53 55.53 61.77 58.12 52.84 59.32 55.39 51.50 58.30 67.20  59.00 55.72 55.10 CMN(Hazarika et al. 2018b) ? 25.00 30.38 55.92 62.41 52.86 52.39 61.76 59.83 55.52 60.25 71.13 60.69 56.56 56.13 DialogueRNN(Majumder et al. 2019) 25.69 33.18 75.10 78.80 58.59 59.21 64.71 65.28 80.27 71.86 61.15 58.91 63.40 62.75 Jiao, Lyu, and King 2020) 48.30 52.10 68.30 73.30 61.60 58.40 57.50 61.90 68.10 69.70 67.10 62.30 63.50 63.50 DialogueGCN(Ghosal et al. 2019) 40.62 42.75 89.14 84.54 61.92 63.54 67.53 64.19 65.46 63.08 64.18 66.99 65.25 64.18</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IEMOCAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Happy</cell><cell>Sad</cell><cell></cell><cell cols="2">Neutral</cell><cell cols="2">Angry</cell><cell cols="2">Excited</cell><cell cols="2">Frustrated</cell><cell cols="2">Average</cell></row><row><cell></cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>wF1</cell></row><row><cell>Memnet(KET(Zhong, Wang, and Miao 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.56</cell></row><row><cell>AGHMN(DialogueGCN(80:20)</cell><cell cols="14">52.83 42.47 79.43 77.26 60.93 58.48 61.89 57.82 66.85 74.91 56.28 56.82 63.23 62.46</cell></row><row><cell>BERT(Devlin et al. 2019)</cell><cell cols="14">42.19 39.05 60.45 59.91 49.11 52.24 55.14 54.82 64.22 55.97 54.26 55.88 54.06 54.01</cell></row><row><cell>TRM</cell><cell cols="14">43.08 42.42 77.27 74.54 58.24 60.88 65.00 60.22 68.37 66.98 61.48 59.84 62.25 62.11</cell></row><row><cell>TRMSM-Add</cell><cell cols="14">43.53 48.53 76.15 76.74 66.06 64.37 56.24 60.6 75.72 68.49 63.23 61.18 64.21 64.45</cell></row><row><cell>TRMSM-Cat</cell><cell cols="14">48.71 49.46 76.84 77.02 64.44 63.00 57.03 60.72 76.07 70.33 60.74 62.09 64.72 64.82</cell></row><row><cell>TRMSM-Att</cell><cell cols="14">43.36 50.22 81.23 75.82 66.11 64.15 60.39 60.97 77.46 72.70 62.16 63.45 65.34 65.74</cell></row></table><note>? IEMOCAP This dataset contains a series of dyadic conversation between 10 unique speakers. 6 categories of emotions are considered in our experiments: neutral, happy, sad, angry, excited, and frustrated. Following Ma- jumder et al. (2019), the training set is split into a new training set and a validation set by the ratio of 80: 20.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The results of our models on IEMOCAP. Weighted-F1 score (wF1) is used as the metric. ? means referring from<ref type="bibr" target="#b3">Ghosal et al. (2019)</ref>.</figDesc><table /><note>? MELD This dataset contains over 1400 multi-speakers conversations collected from the TV series Friends. Emo- tions in this dataset are annotated into 7 categories: neu- tral, joy, surprise, anger, disgust, sadness and fear.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b14">Poria et al. 2017</ref>)? 78.40 73.80 46.80 47.70 3.80 5.40 22.40 25.10 51.60 51.30 4.30 5.20 36.70 38.40 57.50 55.90 46.40 DialogueRNN(Majumder et al. 2019)? 72.10 73.5 54.40 49.40 1.60 1.20 23.90 23.80 52.00 50.70 1.50 1.70 41.90 41.50 56.10 55.90 45.30 AGHMN(Jiao, Lyu, and King 2020) 83.40 76.40 49.10 49.70 9.20 11.50 21.60 27.00 52.40 52.40 12.20 14.00 34.90 39.40 60.30 58.10 49.45 KET</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MELD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Neutral</cell><cell cols="2">Surprise</cell><cell>Fear</cell><cell></cell><cell cols="2">Sadness</cell><cell>Joy</cell><cell></cell><cell cols="2">Disgust</cell><cell cols="2">Anger</cell><cell></cell><cell>Average</cell></row><row><cell></cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>wF1</cell><cell>Acc.</cell><cell>wF1 mF1</cell></row><row><cell>scLSTM(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Devlin et al. 2019)  74.85 76.57 53.26 56.25 21.94 21.89 35.33 33.01 53.82 57.02 36.64 27.67 50.70 42.42  61.82 61.07 53.40 TRM 76.57 76.62 55.53 55.67 23.9 23.8 36.43 32.34 52.07 57.46 29.46 25.13 50.68 44.64 61.80 61.30 53.45 TRMSM-Add 75.88 77.71 53.40 56.49 24.67 21.14 36.39 31.56 53.97 57.83 35.07 22.62 52.36 45.95 62.93 62.06 53.96 TRMSM-Cat 75.69 77.26 52.64 56.33 25.66 22.73 38.41 34.37 57.81 58.07 35.61 22.57 48.34 45.90 62.76 62.01 54.04 TRMSM-Att 75.48 77.56 55.90 57.25 25.91 20.38 36.82 32.9 55.55 58.66 38.31 28.63 52.11 45.95 63.23 62.36 54.57</figDesc><table><row><cell>58.10</cell><cell>-</cell></row><row><cell>BERT(</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The Results of our models on MELD. MELD uses weighted-F1 (wF1) score, and the average value (mF1) of Macro-F1 and Micro-F1, as the metrics. ? means referring from<ref type="bibr" target="#b7">Jiao, Lyu, and King (2020)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>IEMOCAP</cell><cell>65.74</cell><cell></cell><cell>55.5</cell><cell></cell><cell>MELD</cell><cell></cell></row><row><cell></cell><cell>65.5</cell><cell>TRMSM+Add</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TRMSM+Add</cell><cell></cell><cell></cell></row><row><cell>W-F1</cell><cell>63.5 64.5</cell><cell>TRMSM+Cat TRMSM+Att</cell><cell>64.01 64.6 64.18</cell><cell>64.45 64.82</cell><cell>M-F1</cell><cell>53.5 54.5</cell><cell>TRMSM+Cat 53.45 TRMSM+Att</cell><cell>53.75 53.83 54.34</cell><cell>53.96 54.04 54.57</cell></row><row><cell></cell><cell>62.5</cell><cell>62.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>61.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TRMSM W/O</cell><cell>TRMSM W/O</cell><cell>TRMSM</cell><cell></cell><cell></cell><cell>TRMSM W/O</cell><cell>TRMSM W/O</cell><cell>TRMSM</cell></row><row><cell></cell><cell></cell><cell>SM</cell><cell>CM</cell><cell></cell><cell></cell><cell></cell><cell>SM</cell><cell>CM</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Oh, thank God... on the phone for half an hour ... talk to a freaking human being.All right, all right, I'll ... my girlfriend. But I'm just doing it for you guys.So big deal, so Joey's had a lot of girlfriends, it doesn't ...</figDesc><table><row><cell cols="4">M: Male F: Female</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.94 Intra-s. Inter-s. 0.061</cell><cell cols="3">Ch: Chandler Ro: Ross Mo: Monica Ra: Rachel</cell><cell></cell><cell cols="5">Attention from fusing method</cell><cell cols="4">0.38 Intra-s. Inter-s. 0.62</cell></row><row><cell cols="6">Intra-speaker Block Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Inter-speaker Block Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F</cell><cell>M M</cell><cell>F</cell><cell>M</cell><cell>F</cell><cell>M</cell><cell cols="2">F F</cell><cell>M</cell><cell cols="3">F F F</cell><cell cols="2">M M</cell><cell>F</cell><cell>M</cell><cell>F</cell><cell>M</cell><cell>:Speaker</cell><cell>Ch</cell><cell>Ro</cell><cell>Ch</cell><cell>Mo</cell><cell>Ch</cell><cell>Mo</cell><cell>Ch</cell><cell>Ra</cell><cell>Ro</cell><cell>Mo</cell><cell>Ra</cell><cell>Ch</cell><cell>:Speaker</cell></row><row><cell>0</cell><cell>0.13 0.11</cell><cell>0</cell><cell>0.12</cell><cell>0</cell><cell>0.1</cell><cell>0</cell><cell>0</cell><cell>0.11</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.095</cell><cell>0.11</cell><cell>0</cell><cell>0.12</cell><cell>0</cell><cell>0.11</cell><cell>:Attention score</cell><cell>0</cell><cell>0.13</cell><cell>0</cell><cell>0.16</cell><cell>0</cell><cell>0.11</cell><cell>0</cell><cell>0.14</cell><cell>0.16</cell><cell cols="2">0.15 0.17</cell><cell>0</cell><cell>:Attention score</cell></row><row><cell cols="2">Frustrated</cell><cell></cell><cell></cell><cell cols="3">TRMSM: Fru.</cell><cell></cell><cell></cell><cell cols="5">I got it in the email Frustrated TRMSM: Fru.</cell><cell></cell><cell cols="4">yeah Frustrated TRMSM: Fru.</cell><cell>Neutral</cell><cell cols="3">TRMSM: Neu.</cell><cell cols="5">Yeah, you should, really. Neutral TRMSM: Neu.</cell><cell></cell><cell cols="2">Neutral</cell><cell>TRMSM: Neu.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TRM: Ang.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TRM: Neu.</cell><cell></cell><cell></cell><cell cols="3">TRM: Neu.</cell><cell></cell><cell cols="3">TRM: Neu.</cell><cell></cell><cell></cell><cell></cell><cell cols="2">TRM: Neu.</cell><cell></cell><cell></cell><cell></cell><cell>TRM: Sad.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/transformers 2 https://github.com/declare-lab/convemotion/tree/master/DialogueGCN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We average the attention scores of all self-attention heads in the top layer of Transformer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unified Language Model Pre-training for Natural Language Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hi-GRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07551</idno>
		<title level="m">Hierarchical Transformer Network for Utterance-level Emotion Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conversational Emotion Analysis via Attention Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1936" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-Dependent Sentiment Analysis in User-Generated Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ESWC</title>
		<meeting>of ESWC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeuIPS</title>
		<meeting>of NeuIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeuIPS</title>
		<meeting>of NeuIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3188" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5415" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
