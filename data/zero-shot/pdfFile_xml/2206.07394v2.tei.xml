<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Adaptive Ensembling for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="202227-08-30">August 30, 2022 27 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bruno</surname></persName>
							<email>antonio.bruno@isti.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moroni</surname></persName>
							<email>davide.moroni@isti.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Martinelli</surname></persName>
							<email>massimo.martinelli@isti.cnr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bruno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Martinelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bruno</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Martinelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science and Technologies (ISTI)</orgName>
								<orgName type="institution">National Research Council of Italy (CNR)</orgName>
								<address>
									<addrLine>Via Moruzzi 1</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Adaptive Ensembling for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="202227-08-30">August 30, 2022 27 Aug 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to TBD</note>
					<note>? These authors have contributed equally to this work and share first authorship</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Ensemble</term>
					<term>Convolutional Neural Networks</term>
					<term>EfficientNet</term>
					<term>Image Classification * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent times, except for sporadic cases, the trend in Computer Vision is to achieve minor improvements over considerable increases in complexity.</p><p>To reverse this tendency, we propose a novel method to boost image classification performances without an increase in complexity.</p><p>To this end, we revisited ensembling, a powerful approach, not often adequately used due to its nature of increased complexity and training time, making it viable by specific design choices. First, we trained end-to-end two EfficientNet-b0 models (known to be the architecture with best overall accuracy/complexity trade-off in image classification) on disjoint subsets of data (i.e. bagging). Then, we made an efficient adaptive ensemble by performing fine-tuning of a trainable combination layer. In this way, we were able to outperform the state-of-the-art by an average of 0.5% on the accuracy with restrained complexity both in terms of number of parameters (by 5-60 times), and FLoating point Operations Per Second (by 10-100 times) on several major benchmark datasets, fully embracing the green AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivations</head><p>Computer vision is one of the fields that most benefit from deep learning, continuously improving the state-of-the-art (SOTA) using Convolutional Neural Networks (CNNs). In nearly all computer vision scenarios, complexity grows <ref type="bibr" target="#b4">5</ref> exponentially even for minimal improvements, both in terms of the number of parameters and in FLoating point Operations Per Second (FLOPS). <ref type="table" target="#tab_1">Table 1</ref> briefly shows the evolution of the SOTA on the ImageNet classification task.</p><p>It can be observed that the trend of improvements achieved only through high complexity growth was temporarily slowed down by the introduction of Effi- <ref type="bibr" target="#b9">10</ref> cientNet architecture (and in particular with EfficientNet-b0 attaining the best accuracy/complexity trade-off ). The previous also applies to other image classification (e.g. CIFAR) and computer vision tasks based on CNNs (e.g. object detection and segmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Ensembling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15</head><p>Ensembling is the technique that combines several models, called weak learners, in order to produce a model with better performance than any of the weak learners alone [9, <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Usually, the combination is accomplished by aggregating the output of the weak learners generally by voting (resp. averaging ) for classification (resp. regression). Other aspects, such as ensemble size (i.e. <ref type="bibr" target="#b19">20</ref> number of weak learners) and ensemble techniques (e.g. bagging, boosting, stacking), are crucial for obtaining a satisfactory result. Since it requires the training of several models, ensembles make the overall validation much more expensive, and model complexity grows at least linearly with respect to the ensemble size; moreover, ensembling is a time-consuming process, and this is the <ref type="bibr" target="#b25">25</ref> main reason preventing a more extended use in practice, especially in Computer Vision. By converse, this work shows our technique to exploit this powerful tool</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Year Accuracy Parameters FLOPs  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Efficient Adaptive Ensembling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Efficiency</head><p>At the foundations of the efficiency of the proposed method lies the basic core 40 model adopted in this work: EfficientNet <ref type="bibr" target="#b3">[4]</ref>. As the name suggests, EfficientNet improves the classification quality with lower complexity compared to models having similar classification performances. This is possible since EfficientNet performs optimised network scaling, given a predefined complexity. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in the CNN literature, there are three main types of scaling:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>45</head><p>? depth scaling, which consists in increasing the number of layers in the CNN; it is the most popular scaling method in the literature and allows detecting features at multiple levels of abstraction;</p><p>? width scaling, which consists in increasing the number of convolutional kernels and parameters or channels, giving the model the capability to 50 represent different features at the same level;</p><p>? input scaling, represented by the increase in size/resolution of the input images, which allows capturing additional details. and, at the end, the EfficientNet compound scaling method that uniformly scales all three dimensions with a fixed ratio. Image taken from the original paper <ref type="bibr" target="#b3">[4]</ref>.</p><p>Each of these scalings can be manually set or by a grid search. However, they increase the model complexity, usually exponentially, with tons of new parameters to tune and, after a certain level, scaling appears not to improve performances. The scaling method introduced in <ref type="bibr" target="#b3">[4]</ref> is named compound scaling. It suggests that strategically performing all scaling together delivers better results because it is argued that they are dependent. Intuitively, they introduce the compound coefficient ? representing the amount of resources available to the model and find the optimal scaling combination given such a constrain, following the rules in the Equation 1. In this way, the total complexity of the network is approximately proportional to 2 ? (see the original paper for more details).</p><formula xml:id="formula_0">depth: d = ? ? width: w = ? ? resolution: r = ? ? such that ? ? ? 2 ? ? 2 ? 2 and ? ? 1, ? ? 1, ? ? 1 (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adaptivity</head><p>The adaptivity is given by the fact that the ensembling is data-driven and 55 not fixed as usual. The typical way of combining weak learners is to perform voting/averaging as shown in <ref type="figure">Figure 2</ref> (predicting the output from all weak learners and then picking the most frequent output/average of outputs), respectively for classification/regression. However, in this case, the ensemble is only a static aggregator. In this work, we opted for performing an adaptive combina-60 tion. However, instead of combining the outputs ( <ref type="figure">Figure 3</ref>) of the weak learners, we combine the features that the CNNs extract from the input ( <ref type="figure">Figure 4</ref>). In this way, we further reduce the complexity of the ensemble without reducing its power and expressiveness. Indeed the combination layer is of the same type of output layer as the weak learners (i.e. Linear + LogSoftmax), and keeping both 65 would introduce redundancy. This can be seen as a fully-differentiable version of Gradient Boosting <ref type="bibr" target="#b11">[12]</ref>. However, in this way, there is no reason to perform the tree decision traversal and ensemble is performed at features-level.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>In this section, the results obtained on several major benchmark datasets on 70 image classification are described. Before showing the results, the main aspects of the experimental setup are detailed. The experiments have been implemented using the PyTorch <ref type="bibr" target="#b12">[13]</ref> open-source machine learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The proposed solution has been tested on several datasets in order to evalu-  of each dataset follows:</p><p>CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b13">[14]</ref>: the CIFAR-10 dataset consists of 60000    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transfer learning</head><p>Transfer learning <ref type="bibr" target="#b19">[20]</ref> is the technique of taking knowledge gained while solving one problem, and applying it to a different but related problem. Like the most cases for image classification, the stored knowledge is brought by pre-135 trained models from ImageNet <ref type="bibr" target="#b20">[21]</ref> task, since it has more than 14 million images belonging to 1000 generic classes. Transfer learning has been used for weak learners training only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Validation phases</head><p>Validation is divided into 2 main phases:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>140</head><p>? end-to-end weak learner overfitting training: transfer learning starting from the ImageNet pre-trained model and setting a new output module (to fit the output size). The models are trained to reach overfit in order to get high specialization;</p><p>? ensemble combination layer fine-tuning: as shown in <ref type="figure">Figure 4</ref> the weak 145 learners are frozen removing their output modules, so in this phase only the combination layer is trained.</p><p>Both phases are performed using AdaBelief <ref type="bibr">[22]</ref> optimizer which guarantees both fast convergence and generalization. AdaBelief parameters used are the following ones: learning rate 5 ? 10 ?4 , betas (0.9, 0.999), eps 10 ?16 , using weight 150 decoupling without rectify.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Avoid overfitting</head><p>In order to prevent overfitting (i.e. avoid the model being too specialized to data from the training set with poor performances on unknown data), we use early stopping (i.e. stop training after no improvements on validation set after 155 a certain number of epochs, called patience) during fine-tuning only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Data Splitting</head><p>Every dataset is provided with the "official" train-test split that is used for the ensemble fine-tuning. On the other hand, for the end-to-end overfitting training of the weak learners we perform the following data split: 3. for each subset, instantiate a weak learner and train it only on that subset (called as bagging), with overfitting. In this way every weak learner will be 170 highly specialized only on a portion of data, this could sound self-defeating but <ref type="bibr" target="#b22">[23]</ref> showed that it leads to qualitative ensemble, especially in the case of this work in which ensembling is adaptive. The choice to reach overfitting will reduce the overall validation time: with preliminary tests we noticed that EfficientNet-b0 and AdaBelief optimizer with overfitting 175 training are powerful and will always converge to the same minimum point (very likely to be the global one, due to the fact that accuracy is 100% almost always) independently on the initialization. In this way, just 2</p><p>train runs (only one initialization for each weak learner) are sufficient for every dataset. Validation and test metrics: for the validation set evaluation, we decided to use the Weighted F1-score because this takes into account both correct and wrong predictions (true/false positive/negative) and weighting allows to manage 190 any imbalance of the classes (more representative classes have a greater contribution). On the other hand, to make comparisons with previous works on test set, we used the same metric, which is Accuracy (i.e. correct prediction/total set) in all cases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>In this section results of experiments are shown and discussed. <ref type="table" target="#tab_8">Table 4</ref> shows that our work improves the SOTA in all major benchmark datasets and as expected the highest improvements (&gt; 0.5%) are obtained on the tasks which are not saturated (i.e. accuracy &lt; 99%). These results gain more evidence when 210 complexity is considered too: indeed <ref type="table" target="#tab_9">Table 5</ref> shows that our work (except for the case of CINIC-10) has 5-60 times less total number of parameters and needs  Last but not least, we present below an analysis of computation time for a single task; let:</p><p>? T end the time for a single end-to-end weak learner training;</p><p>? T f ine the time for a single fine-tuning ensemble model training;</p><p>? T f wd the time for a single forward step;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>220</head><p>? T back the time for a single backward step, if subscripted it indicates the number of parameters involved;</p><p>? T upd the time for a single optimization update step, if subscripted it indicates the number of parameters involved.</p><p>Then for a single task the total time needed is:</p><formula xml:id="formula_1">225 T = A ? T end + B ? T f ine<label>(2)</label></formula><p>Dataset SOTA parameters Our parameters SOTA FLOPs Our FLOPs</p><formula xml:id="formula_2">CIFAR-10 [24] ? 632M ? 11M (100K) ? 916G ? ? 0.9G CIFAR-100 [25] ? 480M ? 11M (100K) ? 299G * ? 0.9G Cars [26] ? 54.7M ? 11M (100K) ? 10G ? 0.9G Food-101 [25] ? 480M ? 11M (100K) ? 299G * ? 0.9G Flower102 [27] ? 277M ? 11M (100K) ? 60G ? 0.9G CINIC-10 [28] ? 8.1M ? 11M (100K) ? 1G ? 0.9G</formula><p>Pets <ref type="bibr" target="#b25">[25]</ref> ? 480M ? 11M (100K) ? 299G * ? 0.9G ? Estimation based on similar architecture with similar number of parameters. * Estimation based on the same architecture but scaling FLOPs w.r.t. the number of parameters ratio. where in our case A = 2 since end-to-end training is performed once on each of the two disjoint subset and B = 5 because we performed fine-tuning ensemble training with five random initializations.</p><p>However, it is possible to perform in parallel the end-to-end trainings halving the batch size and about take the half of the time, the same goes for the fine-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>230</head><p>tuning training running all in parallel, in this way the total time is:</p><formula xml:id="formula_3">T = T end + T fine<label>(3)</label></formula><p>and considering that a training is made of forward+backward+update steps to all training data for several epochs:</p><formula xml:id="formula_4">T end ? T fwd + T back + T upd (4) T fine ? K ? T fwd + T back 100k + T upd 100k ? ? T f wd<label>(5)</label></formula><p>the approximation in the Equation5 is done because backward and update steps involve only a small fraction of the parameters, moreover the two weak 235 learners perform forward steps in parallel since they are independent (otherwise we should have K = 2). Putting together Equations 4 and 5, the total time is:</p><formula xml:id="formula_5">T ? ? 2 ? T f wd + T back + T upd<label>(6)</label></formula><p>What said before, in terms of FLOPs is (considering only one input, just add the linear scaling factor for the training on the whole dataset):</p><formula xml:id="formula_6">F f wd = F back = 0.39 GFLOPs<label>(7)</label></formula><p>F upd ? 20 ? P ? 0.1 GFLOPs GFLOPs, and considering the <ref type="table" target="#tab_9">Table 5</ref>, the SOTA for CINIC-10 in <ref type="bibr" target="#b29">[28]</ref> that has least number of parameters (8.1M) requires requires 1 GFLOPs for one single 245 forward on an image, showing that our solution is the fastest and the speedup is much more noticeable (10-100 times) over the even more complex SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future works</head><p>In this work, we presented a method to reverse the trend in image classification of having minor improvements with a huge complexity increase. In 250 particular, we showed a revisited ensembling to outperform the SOTA with restrained complexity, both in terms of numbers of parameters and FLOPs.</p><p>Specifically, we proved how it is possible to perform bagging on two disjoint subsets of data using two EfficientNet-b0 weak learners and training them to overfit on the assigned/scheduled subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>255</head><p>In this work we pushed the ensemble size to the lower bound using only 2 weak learners: this adaptive ensemble strategy would still be the most efficient using up to 5 weak learners, and then it could be further improved by defining different bagging strategies (e.g. train weak learners on subsets split by class dimensionality, clustering or different color space mapping of inputs).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of scaling types, from left to right: a baseline network example, conventional scaling methods that only increase one network dimension (width, depth, resolution)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Ensemble by voting: the final output is obtained picking the mode among the output produced by the weak learners. In this way the weak learners are independent and it is effective with a high number of heterogeneous weak learners. Ensemble by output combination: an additional combination layer is fed with the outputs of the weak learners and combines them. In this way the weak learners are no longer independent and the combination layer can be trained to better adapt to data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Features 1 Features 2 Figure 4 :</head><label>24</label><figDesc>Our adaptive ensemble method: is an optimised version of the method shown inFigure 3because we avoid redundancy and reduce complexity by deleting the output module (dark grey filled) of weak learners and feeding the combination layer with the features. Light grey filled modules denote modules whose parameters are frozen during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>160 1 .</head><label>1</label><figDesc>set the size N of the final ensemble model (i.e. the number of weak learners to ensemble), in particular for the experiments N=2 in order to have the minimum ensemble size; 2. randomly split the dataset into N equally sized and disjoint (i.e. each data belongs exactly only to 1 subset) subsets with stratification (i.e. preserving 165 the class ratios within the subset). During the test exception has made for the Pets dataset, in which the 2 disjoint subsets were made only by cats and dogs, respectively;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>180 3 . 7 .</head><label>37</label><figDesc>Loss and MetricsTraining Loss: due to the multiclass nature of all dataset tasks, the Cross-Entropy Loss (which exponentially penalizes differences between predicted and true values, expressed as probability of class belonging) is used. For this reason, the model output has a specified size depending on the dataset (i.e. number 185 of classes) and each element output[i] represents the probability that the input sample belongs to class i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>195</head><label></label><figDesc>Some hyperparmeters have already been fixed in the previous sections (i.e. preprocessing size and normalization, optimizer parmeters and ensemble size), to further reduce the total validation time other hyperparameters have been fixed: ? early stopping patience: 10 epochs; 200 ? batch size: 55 (200 in the case of fine-tuning) and 200 (700 in the case of the fine-tuning) for the 500?500 and 256?256 images, respectively. while, for the ensemble fine-tuning, 5 different random seeds are used. In this way, for each dataset, 2 end-to-end weak training (1 for each subset) and 5 fine-tuning ensemble training are performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>205</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 8 )F ? 2 ?? 1</head><label>821</label><figDesc>the Equation 7 refers to FLOPs of EfficientNet-b0 architectures and the 240 Equation 8 refers to FLOPs of AdaBelief update step where P = 5M is the number parameters involved in the end-to-end training. Putting all together: F f wd + F back + F upd ? ? 2 ? 0.39 + 0.39 + 0.1 ? the whole pipeline on a single image requires about 1.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>260</head><label></label><figDesc>Then, the ensemble is performed by fine-tuning a trainable combination layer. The efficiency of the method is given by different reasons: efficiency of EfficientNet-b0 models, fine-tuning for ensemble and the high parallelizationcapability of the solution. The reduced number of FLOPs combined with the tiny validation space (7 total runs: 2 end-to-end + 5 fine-tuning) fully embraces 265 the idea of the green AI [29]. These results pave to investigate this kind of strategy in many fields: on Object Detection (performing the ensemble at feature extraction backbone level) and Segmentation (performing the ensemble on the encoding in typical encoderdecoder architectures).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>270</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Evolution of the state-of-the-art on the ImageNet classification task: as can be seen, complexity in models having accuracy &gt; 80% (both in the number of parameters and FLOPs) grows exponentially in spite of a minimal improvement. The same trend can be noticed on other computer vision tasks. N.B. only some architectures providing relevant improvements are shown in this table.with restrained resources (e.g. with respect to model complexity, validation time and training time).1.3. Content outline30 This work shows how applying a well-defined ensembling strategy using an efficient basic model as the core can improve the state-of-the-art in Computer Vision tasks, preserving a competitive performance/complexity trade-off. In Section 2 we describe our designing strategy in detail (e.g. model, ensembling strategy, validation), focusing on introducing the main novel aspects. Experi-35 mental results and data description are shown in Section 3, while an exhaustive discussion is provided in the last section. 3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>75ate its capability of being effective over disparate domains (e.g. type of images, number of classes, balancing, quality) as shown inTable 2. A brief description 6</figDesc><table><row><cell cols="2">Output</cell></row><row><cell>Combination Layer</cell><cell></cell></row><row><cell>Output Module</cell><cell>Output Module</cell></row><row><cell>Features 1</cell><cell>Features 2</cell></row><row><cell>Feature</cell><cell>Feature</cell></row><row><cell>Extractor</cell><cell>Extractor</cell></row><row><cell>Input</cell><cell>Input</cell></row><row><cell>Input</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>32?32 colour images in 10 classes, with 6000 images per class. There are 50000 Food-101<ref type="bibr" target="#b15">[16]</ref>: the Food-101 dataset consists of 101 food categories with 750 95 training and 250 test manually-reviewed images per category, making a total of 101000 images. On purpose, the training images contain some amount of noise that comes mainly in the form of intense colours and sometimes wrong labels.All images were rescaled to have a maximum side length of 512 pixels.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Domain</cell><cell>Input size</cell><cell cols="3">Classes Balanced Provided splits</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>Mixed (RGB)</cell><cell>32?32</cell><cell>10</cell><cell>Yes</cell><cell>Train-Test</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>Mixed (RGB)</cell><cell>32?32</cell><cell>100</cell><cell>Yes</cell><cell>Train-Test</cell></row><row><cell></cell><cell>Cars</cell><cell>Cars (RGB)</cell><cell>360?240</cell><cell>196</cell><cell>Yes</cell><cell>Train-Test</cell></row><row><cell></cell><cell>Food-101</cell><cell>Food (RGB)</cell><cell>512 larger side</cell><cell>101</cell><cell>No</cell><cell>Train-Test</cell></row><row><cell>100</cell><cell>Flower102</cell><cell>Flowers (RGB)</cell><cell>Various</cell><cell>102</cell><cell>Yes</cell><cell>Train-Valid-Test</cell></row><row><cell></cell><cell cols="5">Oxford 102 Flower [17]: the Oxford 102 Flower is an image classification CINIC-10 Mixed (RGB) 32?32 10 Yes</cell><cell>Train-Valid-Test</cell></row><row><cell></cell><cell cols="5">dataset consisting of 102 flower categories, most of them being plants com-Pets Dogs &amp; Cats (RGB) Various 37 Yes</cell><cell>Train-Valid-Test</cell></row></table><note>monly occurring in the United Kingdom. Each class consists of between 40 and 258 images. The images have large scale, pose and light variations. In ad- dition, there are categories that have significant variations within the category105 and several very similar ones. Since now, this dataset is referred as "Flower102".CINIC-10 [18]: CINIC-10 is a dataset for image classification consisting of 270000 32?32 colour images. It was compiled as a "bridge" between CIFAR-10 and ImageNet, taking 60000 images from the former and 210000 downsampled110 images from the latter. It is split into three equal subsets -train, validation, and test -each containing 90000 images. Oxford-IIIT Pet [19]: the Oxford-IIIT Pet Dataset has 37 categories with roughly 200 images for each class representing dogs or cats (25 classes for dogs and 12 for cats). Different versions of the dataset can be used for image clas-115 sification, object detection, or image segmentation. In particular, for the ex- perimentation, the fine-grained version of the image classification task has been used (i.e. predict the particular breed of the animal in the image instead of just determining if it is a dog or a cat). The images have wide variations in scale, pose and lighting. Since now, this dataset is referred as "Pets".120 3.2. Input preprocessing The models are not fed directly with the images provided by the datasets, but images are preprocessed to improve the performances. In particular the only</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Details about the datasets used in the experiments.</figDesc><table><row><cell>Dataset</cell><cell>Input size</cell><cell>Means (R,G,B)</cell><cell>Stds (R,G,B)</cell></row><row><cell>CIFAR-10</cell><cell>256?256</cell><cell cols="2">(0.491, 0.482, 0.447) (0.246, 0.243, 0.261)</cell></row><row><cell>CIFAR-100</cell><cell>256?256</cell><cell cols="2">(0.507, 0.486, 0.441) (0.267, 0.256, 0.276)</cell></row><row><cell>Cars</cell><cell>500?500</cell><cell cols="2">(0.468, 0.457, 0.450) (0.295, 0.294, 0.302)</cell></row><row><cell>Food-101</cell><cell>500?500</cell><cell cols="2">(0.550, 0.445, 0.344) (0.271, 0.275, 0.279)</cell></row><row><cell>Flower102</cell><cell>500?500</cell><cell cols="2">(0.433, 0.375, 0.285) (0.296, 0.245, 0.269)</cell></row><row><cell>CINIC-10</cell><cell>256?256</cell><cell cols="2">(0.478, 0.472, 0.430) (0.242, 0.238, 0.258)</cell></row><row><cell>Pets</cell><cell>500?500</cell><cell cols="2">(0.481, 0.448, 0.394) (0.269, 0.264, 0.272)</cell></row></table><note>two preprocessing step done are resize (size chosen after a preliminary tests) and125 normalization (in order to have all data of the same dataset described under the same distribution with pixel values in [0, 1] range and centred around the mean) which improves stability and convergence of the training. Preprocessing details for each dataset are shown in Table 3. Even if augmentation has been performed on work representing the SOTA, no augmentation is performed in130 this work to test the performances of the "pure" method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Input sizes and normalization values, for each channel, used for data preprocessing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>10-100 times less FLOPs respect to the SOTA. Moreover, in terms of trainable parameters, since it performs the fine-tuning of a combination layer, our final solution has only about 100K parameters to train.</figDesc><table><row><cell>215</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">SOTA accuracy Our accuracy Improvement</cell></row><row><cell>CIFAR-10 [24]</cell><cell>99.500%</cell><cell>99.612%</cell><cell>0.112%</cell></row><row><cell>CIFAR-100 [25]</cell><cell>96.080%</cell><cell>96.808%</cell><cell>0.728%</cell></row><row><cell>Cars [26]</cell><cell>96.320%</cell><cell>96.868%</cell><cell>0.548%</cell></row><row><cell>Food-101 [25]</cell><cell>96.180%</cell><cell>96.879%</cell><cell>0.699%</cell></row><row><cell>Flower102 [27]</cell><cell>99.720%</cell><cell>99.847%</cell><cell>0.127%</cell></row><row><cell>CINIC-10 [28]</cell><cell>94.300%</cell><cell>95.064%</cell><cell>0.764%</cell></row><row><cell>Pets [25]</cell><cell>97.100%</cell><cell>98.220%</cell><cell>1.120%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Classification test accuracy comparison between SOTA and our work on datasets used during experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Complexity, both number of parameters and FLOPs, comparison between SOTA and our work on datasets used during experiments.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="80">training images and 10000 test images. This CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) 85 and a "coarse" label (the superclass to which it belongs). In the experiments, the fine-grained version with 100 classes has been used. Stanford Cars [15]: the Stanford Cars dataset contains 16185 360?240 colour images of 196 classes of cars at the level of Make, Model, Year (e.g. Tesla, Model 90 S, 2012). The data is split into 8144 training images and 8041 testing images,where each class has been divided roughly in a 50-50 split. Since now, this dataset is referred as "Cars".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking the 275 inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.634</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Com-280 puter Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efficientnet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>K. Chaudhuri, R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<ptr target="https://arxiv.org/abs/2103.14030" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<ptr target="https://arxiv.org/abs/2102.06171" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">295</biblScope>
		</imprint>
	</monogr>
	<note>Scaling vision transformers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<ptr target="https://arxiv.org/abs/2106.04803" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.614</idno>
		<ptr target="https://doi.org/10.1613/jair.614" />
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11704-019-8208-z</idno>
		<idno>doi:10.1007/ 305 s11704-019-8208-z</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1249</idno>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1249</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting ma-310 chine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-315 performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/3209015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Cifar-10 (canadian institute for advanced research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://ai.stanford.edu/~jkrause/cars/car_dataset.html" />
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
	<note>3d object representations for fine-grained categorization</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative 330 components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<ptr target="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large 335 number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/" />
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno>ArXiv abs/1810.03505</idno>
		<title level="m">Cinic-10 is not imagenet or cifar-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<ptr target="https://www.robots.ox.ac.uk/~vgg/data/pets/" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-016-0043-6</idno>
	</analytic>
	<monogr>
		<title level="j">Journal 345 of Big Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adabelief optimizer: Adapting stepsizes by the belief in observed gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning with ensembles: How over-fitting can be 355 useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sollich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Neural Information Processing Systems, NIPS&apos;95</title>
		<meeting>the 8th International Conference on Neural Information Processing Systems, NIPS&apos;95<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="190" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021: The Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">ence on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10972</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: In-370 troducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architecture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2021.3052758</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2971" to="2989" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Green</forename><surname>Ai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381831</idno>
		<ptr target="https://doi.org/10.1145/3381831" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
