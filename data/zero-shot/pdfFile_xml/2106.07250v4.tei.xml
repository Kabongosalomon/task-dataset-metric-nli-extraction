<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
							<email>kamigaito@lr.pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
							<email>khayashi0201@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Gunma University</orgName>
								<address>
									<country>RIKEN AIP</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In knowledge graph embedding, the theoretical relationship between the softmax crossentropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.</p><p>Proposition 5. When p n (y|x) is a uniform distribution, Eq. (9) equals p d (y|x).</p><p>Proof. This is described in Appendix C of the supplemental material.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Negative Sampling (NS) <ref type="bibr">(Mikolov et al., 2013)</ref> is an approximation of softmax cross-entropy (SCE). Due to its efficiency in computation cost, NS is now a fundamental loss function for various Natural Language Processing (NLP) tasks such as used in word embedding <ref type="bibr">(Mikolov et al., 2013)</ref>, language modeling <ref type="bibr">(Melamud et al., 2017)</ref>, contextualized embedding <ref type="bibr">(Clark et al., 2020a,b)</ref>, and knowledge graph embedding (KGE) <ref type="bibr">(Trouillon et al., 2016)</ref>. Specifically, recent KGE models commonly use NS for training. Considering the current usages of NS, we investigated the characteristics of NS by mainly focusing on KGE from theoretical and empirical aspects.</p><p>First, we introduce the task description of KGE. A knowledge graph is a graph that describes the relationships between entities. It is an indispensable resource for knowledge-intensive NLP applications such as dialogue <ref type="bibr">(Moon et al., 2019)</ref> and questionanswering <ref type="bibr">(Lukovnikov et al., 2017)</ref> systems. However, to create a knowledge graph, it is necessary to consider a large number of entity combinations and their relationships, making it difficult to construct a complete graph manually. Therefore, the prediction of links between entities is an important task.</p><p>Currently, missing relational links between entities are predicted using a scoring method based on KGE <ref type="bibr" target="#b4">(Bordes et al., 2011)</ref>. With this method, a score for each link is computed on vector space representations of embedded entities and relations. We can train these representations through various loss functions. The SCE <ref type="bibr">(Kadlec et al., 2017)</ref> and <ref type="bibr">NS (Trouillon et al., 2016)</ref> loss functions are commonly used for this purpose.</p><p>Several studies <ref type="bibr" target="#b6">(Ruffinelli et al., 2020;</ref><ref type="bibr" target="#b0">Ali et al., 2020)</ref> have shown that link-prediction performance can be significantly improved by choosing the appropriate combination of loss functions and scoring methods. However, the relationship between the SCE and NS loss functions has not been investigated in KGE. Without a basis for understanding the relationships among different loss functions, it is difficult to make a fair comparison between the SCE and NS results.</p><p>We attempted to solve this problem by using the Bregman divergence <ref type="bibr" target="#b5">(Bregman, 1967)</ref> to provide a unified interpretation of the SCE and NS loss functions. Under this interpretation, we can understand the relationships between SCE and NS in terms of the model's predicted distribution at the optimal solution, which we called the objective distribution. By deriving the objective distribution for a loss function, we can analyze different loss functions, the objective distributions of which are identical under certain conditions, from a unified viewpoint.</p><p>We summarize our theoretical findings not restricted to KGE as follows:</p><p>? The objective distribution of NS with uniform noise (NS w/ Uni) is equivalent to that of SCE.</p><p>? The objective distribution of self-adversarial negative sampling (SANS) <ref type="bibr">(Sun et al., 2019)</ref> is quite similar to SCE with label smoothing (SCE w/ LS) <ref type="bibr">(Szegedy et al., 2016)</ref>.</p><p>? NS with frequency-based noise (NS w/ Freq) in word2vec 1 has a smoothing effect on the objective distribution.</p><p>? SCE has a property wherein it more strongly fits a model to the training data than NS.</p><p>To check the validity of the theoretical findings in practical settings, we conducted experiments on the FB15k-237 (Toutanova and Chen, 2015) and WN18RR <ref type="bibr" target="#b9">(Dettmers et al., 2018)</ref> datasets. The experimental results indicate that ? The relationship between SCE and SCE w/ LS is also similar to that between NS and SANS in practical settings.</p><p>? NS is prone to underfitting because it weakly fits a model to the training data compared with SCE.</p><p>? SCE causes underfitting of KGE models when their score function has a bound.</p><p>? Both SANS and SCE w/ LS perform well as pre-training methods.</p><p>The structure of this paper is as follows: Sec. 2 introduces SCE and Bregman divergence; Sec. 3 induces the objective distributions for NS; Sec. 4 analyzes the relationships between SCE and NS loss functions; Sec. 5 summarizes and discusses our theoretical findings; Sec. 6 discusses empirically investigating the validity of the theoretical findings in practical settings; Sec. 7 explains the differences between this paper and related work; and Sec. 8 summarizes our contributions. Our code will be available at https://github.com/kamigaito/ acl2021kge 2 Softmax Cross Entropy and Bregman Divergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SCE in KGE</head><p>We denote a link representing a relationship r k between entities e i and e j in a knowledge graph as (e i , r k , e j ). In predicting the links from given queries (e i , r k , ?) and (?, r k , e j ), the model must predict entities corresponding to each ? in the queries. We denote such a query as x and the entity to be 1 The word2vec uses unigram distribution as the frequencybased noise. predicted as y. By using the softmax function, the probability p ? (y|x) that y is predicted from x with the model parameter ? given a score function f ? (x, y) is expressed as follows:</p><formula xml:id="formula_0">p ? (y|x) = exp ( f ? (x, y)) ? y ?Y exp ( f ? (x, y )) ,<label>(1)</label></formula><p>where Y is the set of all predictable entities. We further denote the pair of an input x and its label y as (x, y). Let D = {(x 1 , y 1 ), ? ? ? , (x |D| , y |D| )} be observed data that obey a distribution p d (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bregman Divergence</head><p>Next, we introduce the Bregman divergence. Let ?(z) be a differentiable function; the Bregman divergence between two distributions f and g is defined as follows:</p><formula xml:id="formula_1">d ?(z) ( f , g) = ?( f )??(g)???(g) T ( f ?g). (2)</formula><p>We can express various divergences by changing ?(z).</p><p>To take into account the divergence on the entire observed data, we con-</p><formula xml:id="formula_2">sider the expectation of d ? ( f , g): B ?(z) ( f , g) = ? x,y d ?(z) ( f (y|x), g(y|x))p d (x, y).</formula><p>To investigate the relationship between a loss function and learned distribution of a model at an optimal solution of the loss function, we need to focus on the minimization of B ?(z) . Gutmann and Hirayama (2011) showed that B ?(z) ( f , g) = 0 means that f equals g almost everywhere when ?(z) is a differentiable strictly convex function in its domain. Note that all ?(z) in this paper satisfy this condition. Accordingly, by fixing f , minimization of B ?(z) ( f , g) with respect to g is equivalent to minimization of</p><formula xml:id="formula_3">B ?(z) ( f , g) = ? x,y ??(g) + ??(g) T g ? ??(g) T f p d (x, y) (3)</formula><p>We useB ? ( f , g) to reveal a learned distribution of a model at optimal solutions for the SCE and NS loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Derivation of SCE</head><p>For the latter explanations, we first derive the SCE loss function from Eq. (3). We denote a probability for a label y as p(y), vector for all y as y, vector of probabilities for y as p(y), and dimension size of z as len(z). In Eq. <ref type="formula">(3)</ref>, by setting f as p d (y|x) and g as p ? (y|x) with ?(z) = ? len(z) i=1 z i log z i <ref type="bibr" target="#b2">(Banerjee et al., 2005)</ref>, we can derive the SCE loss function as follows:</p><formula xml:id="formula_4">B ?(z) (p d (y|x), p ? (y|x)) = ? ? x,y |Y | ? i=1 p d (y i |x) log p ? (y i |x) p d (x, y) (4) = ? 1 |D| ? (x,y)?D log p ? (y|x).<label>(5)</label></formula><p>This derivation indicates that p ? (y|x) converges to the observed distribution p d (y|x) through minimizing B ?(z) (p d (y|x), p ? (y|x)) in the SCE loss function. We call the distribution of p ? (y|x) when B ?(z) equals zero an objective distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Objective Distribution for Negative Sampling Loss</head><p>We begin by providing a definition of NS and its relationship to the Bregman divergence, following the induction of noise contrastive estimation (NCE) from the Bregman divergence that was established by Gutmann and Hirayama (2011). We denote p n (y|x) to be a known non-zero noise distribution for y of a given x. Given ? noise samples from p n (y|x) for each (x, y) ? D, NS estimates the model parameter ? for a distribution G(y|x; ? ) = exp(? f ? (x, y)). By assigning to each (x, y) a binary class label C: C = 1 if (x, y) is drawn from observed data D following a distribution p d (x, y) and C = 0 if (x, y) is drawn from a noise distribution p n (y|x), we can model the posterior probabilities for the classes as follows:</p><formula xml:id="formula_5">p(C = 1, y|x; ? ) = 1 1+exp(? f ? (x, y)) = 1 1 + G(y|x; ? )</formula><p>,</p><formula xml:id="formula_6">p(C = 0, y|x; ? ) = 1? p(C = 1, y|x; ? ) = G(y|x; ? ) 1 + G(y|x; ? ) .</formula><p>The objective function NS (? ) of NS is defined as follows:</p><formula xml:id="formula_7">NS (? ) = ? 1 |D| ? (x,y)?D log(P(C = 1, y|x; ? )) + ? ? i=1</formula><p>,y i ?p n log(P(C = 0, y i |x; ? )) . <ref type="formula">(6)</ref> By using the Bregman divergence, we can induce the following propositions for NS (? ).</p><p>Proposition 1. NS (? ) can be induced from Eq. (3) by setting ?(z) as:</p><formula xml:id="formula_8">?(z) = z log(z) ? (1 + z) log(1 + z).<label>(7)</label></formula><p>Proposition 2. When NS (? ) is minimized, the following equation is satisfied:</p><formula xml:id="formula_9">G(y|x; ? ) = ? p n (y|x) p d (y|x) .<label>(8)</label></formula><p>Proposition 3. The objective distribution of P ? (y|x) for NS (? ) is</p><formula xml:id="formula_10">p d (y|x) p n (y|x) ? y i ?Y p d (y i |x) p n (y i |x) .<label>(9)</label></formula><p>Proof. We give the proof of Props. 1, 2, and 3 in Appendix A of the supplemental material.</p><p>We can also investigate the validity of Props. 1, 2, and 3 by comparing them with the previously reported result. For this purpose, we prove the following proposition: Proof. This is described in Appendix B of the supplemental material.</p><p>This observation is consistent with that by Levy and <ref type="bibr" target="#b13">Goldberg (2014)</ref>. The differences between their representation and ours are as follows. (1) Our noise distribution is general in the sense that its definition is not restricted to a unigram distribution;</p><p>(2) we mainly discuss p ? (y|x) not f ? (x, y); and (3) we can compare NS-and SCE-based loss functions through the Bregman divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Various Noise Distributions</head><p>Different from the objective distribution of SCE, Eq. (9) is affected by the type of noise distribution p n (y|x). To investigate the actual objective distribution for NS (? ), we need to consider separate cases for each type of noise distribution. In this subsection, we further analyze Eq. (9) for each separate case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">NS with Uniform Noise</head><p>First, we investigated the case of a uniform distribution because it is one of the most common noise distributions for NS (? ) in the KGE task. From Eq. (9), we can induce the following property. <ref type="bibr" target="#b11">Dyer (2014)</ref> indicated that NS is equal to NCE when ? = |Y | and P n (y|x) is uniform. However, as we showed, in terms of the objective distribution, the value of ? is not related to the objective distribution because Eq. (9) is independent of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">NS with Frequency-based Noise</head><p>In the original setting of NS <ref type="bibr">(Mikolov et al., 2013)</ref>, the authors chose as p n (y|x) a unigram distribution of y, which is independent of x. Such a frequencybased distribution is calculated in terms of frequencies on a corpus and independent of the model parameter ? . Since in this case, different from the case of a uniform distribution, p n (y|x) remains on the right side of Eq. (9), p ? (y|x) decreases when p n (y|x) increases. Thus, we can interpret frequency-based noise as a type of smoothing for p d (y|x). The smoothing of NS w/ Freq decreases the importance of high-frequency labels in the training data for learning more general vector representations, which can be used for various tasks as pretrained vectors. Since we can expect pre-trained vectors to work as a prior <ref type="bibr" target="#b12">(Erhan et al., 2010</ref>) that prevents models from overfitting, we tried to use NS w/ Freq for pre-training KGE models in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Self-Adversarial NS</head><p>Sun et al. (2019) recently proposed SANS, which uses p ? (y|x) for generating negative samples. By replacing p n (y|x) with p ? (y|x), the objective distribution when using SANS is as follows:</p><formula xml:id="formula_11">p ? (y|x) = p d (y|x) p? (y|x) ? y i ?Y p d (y i |x) p? (y i |x) ,<label>(10)</label></formula><p>where? is a parameter set updated in the previous iteration. Because both the left and right sides of Eq. (10) include p ? (y|x), we cannot obtain an analytical solution of p ? (y|x) from this equation. However, we can consider special cases of p ? (y|x) to gain an understanding of Eq. (10). At the beginning of the training, p ? (y|x) follows a discrete uniform distribution u{1, |Y |} because ? is randomly initialized. In this situation, when we set p? (y|x) in Eq. (10) to a discrete uniform distribution u{1, |Y |},</p><formula xml:id="formula_12">p ? (y|x) becomes p ? (y|x) = p d (y|x).<label>(11)</label></formula><p>Next, when we set p? (y|x) in Eq. (10) as p d (y|x),</p><formula xml:id="formula_13">p ? (y|x) becomes p ? (y|x) = u{1, |Y |}.<label>(12)</label></formula><p>In actual mini-batch training, ? is iteratively updated for every batch of data. Because p ? (y|x) converges to u{1, |Y |} when p? (y|x) is close to p d (y|x) and p ? (y|x) converges to p d (y|x) when p? (y|x) is close to u{1, |Y |}, we can approximately regard the objective distribution of SANS as a mixture of p d and u{1, |Y |}. Thus, we can represent the objective distribution of p ? (y|x) as</p><formula xml:id="formula_14">p ? (y|x) ? (1 ? ? )p d (y|x) + ? u{1, |Y |} (13)</formula><p>where ? is a hyper-parameter to determine whether p ? (y|x) is close to p d (y|x) or u{1, |Y |}. Assuming that p ? (y|x) starts from u{1, |Y |}, ? should start from 0 and gradually increase through training. Note that ? corresponds to a temperature ? for p? (y|x) in SANS, defined as</p><formula xml:id="formula_15">p? (y|x) = exp(? f ? (x, y)) ? y ?Y exp(? f ? (x, y )) ,<label>(14)</label></formula><p>where ? also adjusts p? (y|x) to be close to p d (y|x) or u{1, |Y |}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Relationships among Loss Functions 4.1 Corresponding SCE form to NS with Frequency-based Noise</head><p>We induce a corresponding cross entropy loss from the objective distribution for NS with frequencybased noise. We set T x,y = p n (y|x) ?</p><formula xml:id="formula_16">y i ?Y p d (y i |x) p n (y i |x) , q(y|x) = T ?1 x,y p d (y|x), and ?(z) = ? len(z) i=1 z i log z i .</formula><p>Under these conditions, following induction from Eq.</p><p>(4) to Eq.</p><p>(5), we can reformulat? B ?(z) (q(y|x), p(y|x)) as follows: Except that T x,y is conditioned by x and not normalized for y, we can interpret this loss function as SCE with backward correction (SCE w/ BC) <ref type="bibr">(Patrini et al., 2017)</ref>. Taking into account that backward correction can be a smoothing method for predicting labels <ref type="bibr">(Lukasik et al., 2020)</ref>, this relationship supports the theoretical finding that NS can adopt a smoothing to the objective distribution.</p><formula xml:id="formula_17">B ?(z) (q(y|x), p ? (y|x)) = ? ? x,y |Y | ? i=1 T ?1 x,y p d (y i |x) log p ? (y i |x) p d (x, y) = ? 1 |D| ? (x,y)?D T ?1 x,y log p ? (y|x).<label>(15)</label></formula><formula xml:id="formula_18">Loss Objective Distribution ?(z) or ?(z) Remarks NS w/ Uni p d (y|x) ?(z) = z log(z) ? (1 + z) log(1 + z) NS w/ Freq T ?1 x,y p d (y|x) ?(z) = z log(z) ? (1 + z) log(1 + z) T x,y = p n (y|x) ? y i ?Y p d (y i |x) p n (y i |x) SANS (1 ? ? )p d (y|x) + ? u{1, |Y |} ?(z) = z log(z) ? (1 + z) log(1 + z) Approximately derived. ? increases from zero in training. SCE p d (y|x) ?(z) = ? len(z) i=1 z i log z i SCE w/ BC T ?1 x,y p d (y|x) ?(z) = ? len(z) i=1 z i log z i T x,y = p n (y|x) ? y i ?Y p d (y i |x) p n (y i |x) SCE w/ LS (1 ? ? )p d (y|x) + ? u{1, |Y |} ?(z) = ? len(z) i=1 z i log z i ? is fixed.</formula><p>Because the frequency-based noise is used in word2vec as unigram noise, we specifically consider the case in which p n (y|x) is set to unigram noise. In this case, we can set p n (y|x) = p d (y). Since relation tuples do not appear twice in a knowledge graph, we can assume that p d (x, y) is uniform. Accordingly, we can change T ?1</p><p>x,y to</p><formula xml:id="formula_19">1 p d (y) ? y i ?Y p d (y i |x) p d (y i ) = 1 p d (y) ? y i ?Y p d (y i ,x) p d (y i )p d (x) = p d (x) p d (y)C , where C</formula><p>is a constant value, and we can reformulate Eq. (15) as follows:</p><formula xml:id="formula_20">? 1 |D| ? (x,y)?D p d (x) p d (y)C log p ? (y|x) ? ? 1 |D| ? (x,y)?D #x #y log p ? (y|x),<label>(16)</label></formula><p>where #x and #y respectively represent frequencies for x and y in the training data. We use Eq. (16) to pre-train models for SCE-based loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corresponding SCE form to SANS</head><p>We induce a corresponding cross entropy loss from the objective distribution for SANS by setting</p><formula xml:id="formula_21">q(y|x) = (1 ? ? )p d (y|x) + ? u{1, |Y |} and ?(z) = ? len(z) i=1 z i log z i .</formula><p>Under these conditions, on the basis of induction from Eq. (4) to Eq. (5), we can reformulateB ?(z) (q(y|x), p ? (y|x)) as follows:</p><formula xml:id="formula_22">B ?(z) (q(y|x), p ? (y|x)) = ? ? x,y |Y | ? i=1 (1 ? ? )p d (y i |x) log p ? (y i |x) + |Y | ? i=1 ? u{1, |Y |} log p ? (y i |x) p d (x, y) = ? 1 |D| ? (x,y)?D (1 ? ? ) log p ? (y|x) + |Y | ? i=1 ? |Y | log p ? (y i |x) .</formula><p>(17) The equation in the brackets of Eq. <ref type="formula" target="#formula_0">(17)</ref> is the cross entropy loss that has a corresponding objective distribution to that of SANS. This loss function is similar in form to SCE with label smoothing (SCE w/ LS) <ref type="bibr">(Szegedy et al., 2016)</ref>. This relationship also accords with the theoretical finding that NS can adopt a smoothing to the objective distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Understanding Loss Functions for Fair Comparisons</head><p>We summarize the theoretical findings from Sections 2, 3, and 4 in <ref type="table" target="#tab_0">Table 1</ref>. To compare the results from the theoretical findings, we need to understand the differences in their objective distributions and divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Objective Distributions</head><p>The objective distributions for NS w/ Uni and SCE are equivalent. We can also see that the objective distribution for SANS is quite similar to that for SCE w/ LS. These theoretical findings will be important for making a fair comparison between scoring methods trained with the NS and SCE loss functions. When a dataset contains low-frequency entities, SANS and SCE w/ LS can improve the link-prediction performance through their smoothing effect, even if there is no performance improvement from the scoring method itself. For comparing the SCE and NS loss functions fairly, therefore, it is necessary to use the vanilla SCE against NS w/ Uni and use SCE w/ LS against SANS.</p><formula xml:id="formula_23">0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 p d ?(z) (0.5, p) ?(z) = ? len(z) i=1 zi log zi ?(z) = z log(z) ? (1 + z) log(1 + z)</formula><p>However, we still have room to discuss the relationship between SANS and SCE w/ LS because ? in SANS increases from zero during training, whereas ? in SCE w/ LS is fixed. To introduce the behavior of ? in SANS to SCE w/ LS, we tried a simple approach in our experiments that trains KGE models via SCE w/ LS using pre-trained embeddings from SCE as initial parameters. Though this approach is not exactly equivalent to SANS, we expected it to work similarly to increasing ? from zero in training.</p><p>We also discuss the relationship between NS w/ Freq and SCE w/ BC. While NS w/ Freq is often used for learning word embeddings, neither NS w/ Freq nor SCE w/ BC has been explored in KGE. We investigated whether these loss functions are effective in pre-training KGE models 2 . Because SANS and SCE w/ LS are similar methods to NS w/ Freq and SCE w/ BC in terms of smoothing, in our experiments, we also compared NS w/ Freq with SANS and SCE w/ BC with SCE w/ LS as pre-training methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Divergences</head><p>Comparing ?(z) for NS and SCE losses is as important as focusing on their objective distributions. The ?(z) determines the distance between model-2 As a preliminary experiment, we also trained KGE models via NS w/ Freq and SCE w/ BC. However, these methods did not improve the link-prediction performance because frequency-based noise changes the data distribution drastically. predicted and data distributions in the loss. It has an important role in determining the behavior of the model. <ref type="figure" target="#fig_1">Figure 1</ref> shows the distance in Eq.</p><p>(3) between the probability p and probability 0.5 for each ? in <ref type="table" target="#tab_0">Table 1</ref> 3 . As we can see from the example, d ?(z) (0.5, p) of the SCE loss has a larger distance than that of the NS loss. In fact, Painsky and Wornell (2020) proved that the upper bound of the Bregman divergence for binary labels when</p><formula xml:id="formula_24">?(z) = ? len(z) i=1 z i log z i .</formula><p>This means that the SCE loss imposes a larger penalty on the same predicted value than the NS loss when the value of the learning target is the same between the two losses 4 .</p><p>However, this does not guarantee that the distance of SCE is always larger than NS. This is because the values of the learning target between the two losses are not always the same. To take into account the generally satisfied property, we also focus on the convexity of the functions. In each training instance, the first-order and second-order derivatives of these loss functions indicate that SCE is convex, but NS is not in their domains 5 . Since this property is independent of the objective distribution, we can consider SCE fits the model more strongly to the training data in general. Because of these features, SCE can be prone to overfitting.</p><p>Whether the overfitting is a problem depends on how large the difference between training and test data is.  p(e i |r k , e j ) = p(e i |r k ) + p(e i |e j ) on the basis of frequencies in the data then calculated p(e j |r k , e i ) in the same manner. We treated both p(e i |r k , e j ) and p(e j |r k , e i ) as p(y|x). We denote p(y|x) in the training data as P and in the test data as Q. With these notations, we calculated D KL (P||Q) as the KL divergence for p(y|x) between the test and training data. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results. There is a large difference in the KL divergence between FB15k-237 and WN18RR. We investigated how this difference affects the SCE and NS loss functions for learning KGE models. In a practical setting, the loss function's divergence is not the only factor to affect the fit to the training data. Model selection also affects the fitting. However, understanding a model's behavior is difficult due to the complicated relationship between model parameters. For this reason, we experimentally investigated which combinations of models and loss functions are suitable for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Discussion</head><p>We conducted experiments to investigate the validity of what we explained in Section 5 through a comparison of the NS and SCE losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>We evaluated the following models on the FB15k-237 and WN18RR datasets in terms of the Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 metrics: TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref>; RESCAL <ref type="bibr" target="#b4">(Bordes et al., 2011)</ref>; ComplEx (Trouillon et al., 2016); DistMult <ref type="bibr">(Yang et al., 2015)</ref>; TransE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref>; <ref type="bibr">RotatE (Sun et al., 2019)</ref>. We used LibKGE <ref type="bibr">(Broscheit et al., 2020) 6</ref> as the implementation. For each model to be able to handle queries in both directions, we also trained a model for the reverse direction that shares the entity embeddings with the model for the forward direction.</p><p>To determine the hyperparameters of these models, for RESCAL, ComplEx, DistMult, and TransE with SCE and SCE w/ LS, we used the settings that achieved the highest performance in a previous study <ref type="bibr" target="#b6">(Ruffinelli et al., 2020)</ref> for each loss function as well as the settings from the original papers for TuckER and RotatE. In TransE with NS and SANS, we used the settings used by Sun et al. <ref type="bibr">(2019)</ref>. When applying SANS, we set ? to an initial value of 1.0 for LibKGE for all models except TransE and RotatE, and for TransE and Ro-tatE, where we followed the settings of the original paper since SANS was used in it. When applying SCE w/ LS, we set ? to the initial value of LibKGE, 0.3, except on TransE and RotatE. In the original setting of RotatE, because the values of SANS for TransE and RotatE were tuned, we also selected ? from {0.3, 0.1, 0.01} using the development data in TransE and RotatE for fair comparison. Appendix D in the supplemental material details the experimental settings. <ref type="table" target="#tab_2">Table 2</ref> shows the results for each loss and model combination. In the following subsections, we discuss investigating whether our findings work in a practical setting on the basis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Characteristics of Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Objective Distributions</head><p>In terms of the objective distribution, when SCE w/ LS improves performance, SANS also improves performance in many cases. Moreover, it accords with our finding that SCE w/ LS and SANS have similar effects. For TransE and RotatE, the relationship does not hold, but as we will see later, this is probably because TransE with SCE and RotatE with SCE did not fit the training data. If the SCE does not fit the training data, the effect of SCE w/ LS is suppressed as it has the same effect as smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Divergences</head><p>Next, let us focus on the distance of the loss functions. A comparison of the results of WN18RR and FB15k-237 shows no performance degradation of SCE compared with NS. This indicates that the difference between the training and test data in WN18RR is not so large to cause overfitting problems for SCE.</p><p>In terms of the combination of models and loss functions, the results of NS are worse than those of SCE in TuckER, RESCAL, ComplEx, and Dist-Mult. Because the four models have no constraint to prevent fitting to the training data, we consider that the lower scores are caused by underfitting. This conjecture is on the basis that the NS loss weakly fits model-predicted distributions to training-data distributions compared with the SCE loss in terms of divergence and convexity.</p><p>In contrast, the performance gap between NS  and SCE is smaller in TransE and RotatE. This is because the score functions of TransE and RotatE have bounds and cannot express positive values. Since SCE has a normalization term, it is difficult to represent values close to 1 when the score function cannot represent positive values. This feature prevents TransE and RotatE from completely fitting to the training data. Therefore, we can assume that NS can be a useful loss function when the score function is bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effectiveness of Pre-training Methods</head><p>We also explored pre-training for learning KGE models. We selected the methods in <ref type="table" target="#tab_2">Table 2</ref> that achieved the best MRR for each NS-based loss and each SCE-based loss in each dataset. In accordance with the success of word2vec, we chose unigram noise for both NS w/ Freq and SCE w/ BC. <ref type="table" target="#tab_4">Table 3</ref> shows the results. Contrary to our expectations, SCE w/ BC does not work well as a pre-training method. Because the unigram noise for SCE w/ BC can drastically change the original data distribution, SCE w/ BC is thought to be effective when the difference between training and test data is large. However, since the difference is not so large in the KG datasets, as discussed in the previous subsection, we believe that the unigram noise may be considered unsuitable for these datasets.</p><p>Compared with SCE w/ BC, both SCE w/ LS and SANS are effective for pre-training. This is because the hyperparameters of SCE w/ LS and SANS are adjusted for KG datasets.</p><p>When using vanilla SCE as a pre-training method, there is little improvement in prediction performance, compared with other methods. This result suggests that increasing ? in training is not as important for improving task performance.</p><p>For RotatE, there is no improvement in pretraining. Because RotatE has strict constraints on its relation representation, we believe it may degrade the effectiveness of pre-training. NS is frequently used to train KGE models. KGE is a task to complement a knowledge graph that describes relationships between entities. Knowledge graphs are used in various important downstream tasks because of its convenience in incorporating external knowledge, such as in a language model <ref type="bibr">(Logan et al., 2019</ref><ref type="bibr">), dialogue (Moon et al., 2019</ref><ref type="bibr">), question-answering (Lukovnikov et al., 2017</ref>, natural language inference <ref type="bibr">(K M et al., 2018)</ref>, and named entity recognition <ref type="bibr">(He et al., 2020)</ref>. Thus, current KGE is important in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Due to the importance of KGE, various scoring methods including RESCAL <ref type="bibr" target="#b4">(Bordes et al., 2011)</ref>, TransE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref>, <ref type="bibr">DistMult (Yang et al., 2015)</ref>, <ref type="bibr">ComplEx (Trouillon et al., 2016)</ref>, TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019), and</ref><ref type="bibr">RotatE (Sun et al., 2019)</ref> used in our experiment, have been proposed. However, the relationship between these score functions and loss functions is not clear. Several studies <ref type="bibr" target="#b6">(Ruffinelli et al., 2020;</ref><ref type="bibr" target="#b0">Ali et al., 2020)</ref> have investigated the best combinations of scoring method, loss function, and their hyperparameters in KG datasets. These studies differ from ours in that they focused on empirically searching for good combinations rather than theoretical investigations.</p><p>As a theoretical study, <ref type="bibr" target="#b13">Levy and Goldberg (2014)</ref> showed that NS is equivalent to factorizing a matrix for PMI when a unigram distribution is selected as a noise distribution. <ref type="bibr" target="#b11">Dyer (2014)</ref> investigated the difference between NCE (Gutmann and Hyv?rinen, 2010) and NS. Gutmann and Hirayama (2011) revealed that NCE is derivable from Bregman divergence. Our derivation for NS is inspired by their work. <ref type="bibr">Meister et al. (2020)</ref> proposed a framework to jointly interpret label smoothing and confidence penalty (Pereyra et al., 2017) through investigating their divergence. <ref type="bibr">Yang et al. (2020)</ref> theoretically induced that a noise distribution that is close to the true distribution behind the training data is suitable for training KGE models in NS. They also proposed a variant of SANS in the basis of their investigation.</p><p>Different from these studies, we investigated the distributions at optimal solutions of SCE and NS loss functions while considering several types of noise distribution in NS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We revealed the relationships between SCE and NS loss functions in KGE. Through theoretical analysis, we showed that SCE and NS w/ Uni are equivalent in objective distribution, which is the predicted distribution of a model at an optimal solution, and that SCE w/ LS and SANS have similar objective distributions. We also showed that SCE more strongly fits a model to the training data than NS due to the divergence and convexity of SCE.</p><p>The experimental results indicate that the differences in the divergence of the two losses were not large enough to affect dataset differences. The results also indicate that SCE works well with highly flexible scoring methods, which do not have any bound of the scores, while NS works well with RotatE, which cannot express positive values due to its bounded scoring. Moreover, they indicate that SCE and SANS work better in pre-training than NS w/ Uni, commonly used for learning word embeddings.</p><p>For future work, we will investigate the properties of loss functions in out-of-domain data. A Proof of Proposition 1, 2, and 3</p><p>We can reformulate NS as follows:</p><formula xml:id="formula_25">NS (? ) = ? 1 |D| ? (x,y)?D log(P(C = 1, y|x; ? )) + ? ? i=1,y i ?p n log(P(C = 0, y i |x; ? )) = ? 1 |D| ? (x,y)?D log(P(C = 1, y|x; ? )) ? 1 |D| ? (x,y)?D ? ? i=1,y i ?p n log(P(C = 0, y i |x; ? )) = ? 1 |D| ? (x,y)?D log( 1 1 + G(y|x; ? ) ) ? 1 |D| ? (x,y)?D ? ? i=1,y i ?p n log( G(y i |x; ? ) 1 + G(y i |x; ? ) ) = 1 |D| ? (x,y)?D log(1 + G(y|x; ? )) + ? ?|D| ? (x,y)?D ? ? i=1,y i ?p n log(1 + 1 G(y i |x; ? ) ) = ? x,y p d (y|x) log(1 + G(y|x; ? ))p d (x) + ? x,y ? p n (y|x) log(1 + 1 G(y|x; ? ) )p d (x)<label>(18)</label></formula><p>Letting u = (x, y), f (u) = ? p n (y|x) p d (y|x) , g(u) = G(y|x; ? ), and p d (x) = 1 p d (y|x) p d (x, y), we can reformulate Eq. (18) as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>Dataset: We use FB15k-237 (Toutanova and Chen, 2015) 7 and WN18RR <ref type="bibr" target="#b9">(Dettmers et al., 2018)</ref> 8 datasets in the experiments. We followed the standard split in the original papers for each dataset.      <ref type="table">Table 9</ref>: The best MRR scores of models initialized with pre-trained embeddings on validation data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 4 .</head><label>4</label><figDesc>When Eq. (8) satisfies ? = 1 and p n (y|x) = p d (y), f ? (x, y) equals point-wise mutual information (PMI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Divergence between 0.5 and p in d ?(z) for each ?(z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>KL divergence of p d (y|x) between training and test relations for each dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Mikolov et al. (2013)  proposed the NS loss function as an approximation of the SCE loss function to reduce computational cost and handle a large vocabulary for learning word embeddings. NS is now used in various NLP tasks, which must handle a large amount of vocabulary or labels.Melamud  et al. (2017)  used the NS loss function for training a language model.Trouillon et al. (2016)  introduced the NS loss function to KGE. In contextualized pretrained embeddings,<ref type="bibr" target="#b7">Clark et al. (2020a)</ref> indicated that ELECTRA<ref type="bibr" target="#b8">(Clark et al., 2020b)</ref>, a variant of BERT<ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, follows the same manner of the NS loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of our theoretical findings. w/ Uni denotes with uniform noise, w/ Freq denotes with frequencybased noise, w/ BC denotes with backward correction, and w/ LS denotes with label smoothing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>To measure the difference between training and test data in a KG dataset, we calculated the Kullback-Leibler (KL) divergence for p(y|x) between the training and test data of commonly used KG datasets. To compute p(y|x), we first calculated</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10</cell></row><row><cell></cell><cell>NS</cell><cell>0.257</cell><cell>0.151</cell><cell>0.297</cell><cell>0.472</cell><cell>0.431</cell><cell>0.407</cell><cell>0.440</cell><cell>0.473</cell></row><row><cell>TuckER</cell><cell>SANS</cell><cell>0.330</cell><cell>0.238</cell><cell>0.365</cell><cell>0.512</cell><cell>0.445</cell><cell>0.421</cell><cell>0.455</cell><cell>0.489</cell></row><row><cell></cell><cell>SCE</cell><cell>0.338</cell><cell>0.246</cell><cell>0.372</cell><cell>0.521</cell><cell>0.453</cell><cell>0.424</cell><cell>0.465</cell><cell>0.507</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.343</cell><cell>0.251</cell><cell>0.378</cell><cell>0.529</cell><cell>0.472</cell><cell>0.441</cell><cell>0.483</cell><cell>0.528</cell></row><row><cell></cell><cell>NS</cell><cell>0.337</cell><cell>0.247</cell><cell>0.368</cell><cell>0.516</cell><cell>0.385</cell><cell>0.354</cell><cell>0.405</cell><cell>0.437</cell></row><row><cell>RESCAL</cell><cell>SANS</cell><cell>0.339</cell><cell>0.249</cell><cell>0.372</cell><cell>0.520</cell><cell>0.389</cell><cell>0.363</cell><cell>0.404</cell><cell>0.434</cell></row><row><cell></cell><cell>SCE</cell><cell>0.352</cell><cell>0.260</cell><cell>0.387</cell><cell>0.537</cell><cell>0.451</cell><cell>0.417</cell><cell>0.470</cell><cell>0.512</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.363</cell><cell>0.269</cell><cell>0.400</cell><cell>0.548</cell><cell>0.469</cell><cell>0.435</cell><cell>0.485</cell><cell>0.529</cell></row><row><cell></cell><cell>NS</cell><cell>0.296</cell><cell>0.211</cell><cell>0.324</cell><cell>0.468</cell><cell>0.394</cell><cell>0.373</cell><cell>0.403</cell><cell>0.432</cell></row><row><cell>ComplEx</cell><cell>SANS</cell><cell>0.300</cell><cell>0.214</cell><cell>0.328</cell><cell>0.472</cell><cell>0.432</cell><cell>0.407</cell><cell>0.442</cell><cell>0.480</cell></row><row><cell></cell><cell>SCE</cell><cell>0.300</cell><cell>0.218</cell><cell>0.326</cell><cell>0.466</cell><cell>0.463</cell><cell>0.434</cell><cell>0.473</cell><cell>0.521</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.318</cell><cell>0.231</cell><cell>0.348</cell><cell>0.493</cell><cell>0.477</cell><cell>0.441</cell><cell>0.491</cell><cell>0.546</cell></row><row><cell></cell><cell>NS</cell><cell>0.304</cell><cell>0.219</cell><cell>0.336</cell><cell>0.470</cell><cell>0.389</cell><cell>0.374</cell><cell>0.394</cell><cell>0.416</cell></row><row><cell>DistMult</cell><cell>SANS</cell><cell>0.320</cell><cell>0.234</cell><cell>0.352</cell><cell>0.489</cell><cell>0.410</cell><cell>0.386</cell><cell>0.419</cell><cell>0.452</cell></row><row><cell></cell><cell>SCE</cell><cell>0.342</cell><cell>0.252</cell><cell>0.374</cell><cell>0.521</cell><cell>0.438</cell><cell>0.407</cell><cell>0.447</cell><cell>0.497</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.344</cell><cell>0.254</cell><cell>0.377</cell><cell>0.526</cell><cell>0.448</cell><cell>0.410</cell><cell>0.460</cell><cell>0.527</cell></row><row><cell></cell><cell>NS</cell><cell>0.284</cell><cell>0.182</cell><cell>0.319</cell><cell>0.498</cell><cell>0.218</cell><cell>0.011</cell><cell>0.390</cell><cell>0.510</cell></row><row><cell>TransE</cell><cell>SANS</cell><cell>0.328</cell><cell>0.230</cell><cell>0.365</cell><cell>0.525</cell><cell>0.219</cell><cell>0.016</cell><cell>0.394</cell><cell>0.514</cell></row><row><cell></cell><cell>SCE</cell><cell>0.324</cell><cell>0.232</cell><cell>0.359</cell><cell>0.508</cell><cell>0.229</cell><cell>0.054</cell><cell>0.366</cell><cell>0.523</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.323</cell><cell>0.231</cell><cell>0.359</cell><cell>0.508</cell><cell>0.229</cell><cell>0.054</cell><cell>0.369</cell><cell>0.522</cell></row><row><cell></cell><cell>NS</cell><cell>0.301</cell><cell>0.203</cell><cell>0.333</cell><cell>0.505</cell><cell>0.469</cell><cell>0.429</cell><cell>0.484</cell><cell>0.547</cell></row><row><cell>RotatE</cell><cell>SANS</cell><cell>0.333</cell><cell>0.238</cell><cell>0.371</cell><cell>0.523</cell><cell>0.472</cell><cell>0.431</cell><cell>0.487</cell><cell>0.550</cell></row><row><cell></cell><cell>SCE</cell><cell>0.315</cell><cell>0.228</cell><cell>0.347</cell><cell>0.486</cell><cell>0.452</cell><cell>0.423</cell><cell>0.463</cell><cell>0.507</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.315</cell><cell>0.228</cell><cell>0.346</cell><cell>0.489</cell><cell>0.447</cell><cell>0.417</cell><cell>0.461</cell><cell>0.502</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results for each method in FB15k-237 and WN18RR datasets. Notations are same as those in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results of pre-training methods. + denotes combination of model and loss function. Other nota- tions are same as those in Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Richard Nock, and Lizhen Qu. 2017. Making deep neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</figDesc><table><row><cell>Omer Levy and Yoav Goldberg. 2014. Neural word</cell><cell>Giorgio Patrini, Alessandro Rozza, Aditya Kr-</cell></row><row><cell>embedding as implicit matrix factorization. In Pro-ceedings of the 27th International Conference on Neural Information Processing Systems -Volume 2, NIPS'14, page 2177-2185, Cambridge, MA, USA. MIT Press. Robert Logan, Nelson F. Liu, Matthew E. Peters, Matt</cell><cell>ishna Menon, Gabriel Pereyra, George Tucker, Jan Chorowski,</cell></row><row><cell>Gardner, and Sameer Singh. 2019. Barack's wife</cell><cell>Lukasz Kaiser, and Geoffrey E. Hinton. 2017. Regu-</cell></row><row><cell>hillary: Using knowledge graphs for fact-aware lan-</cell><cell>larizing neural networks by penalizing confident out-</cell></row><row><cell>guage modeling. In Proceedings of the 57th Annual</cell><cell>put distributions. CoRR, abs/1701.06548.</cell></row><row><cell>Meeting of the Association for Computational Lin-</cell><cell></cell></row><row><cell>guistics, pages 5962-5971, Florence, Italy. Associa-tion for Computational Linguistics.</cell><cell>Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. 2020. You can teach an old dog new tricks! on training knowledge graph embeddings.</cell></row><row><cell>Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon,</cell><cell>In International Conference on Learning Represen-</cell></row><row><cell>and Sanjiv Kumar. 2020. Does label smoothing mit-</cell><cell>tations.</cell></row><row><cell>igate label noise? In Proceedings of the 37th Inter-national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 6448-6458. PMLR.</cell><cell>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. In Interna-tional Conference on Learning Representations.</cell></row><row><cell>Denis Lukovnikov, Asja Fischer, Jens Lehmann, and</cell><cell></cell></row><row><cell>S?ren Auer. 2017. Neural network-based question</cell><cell></cell></row><row><cell>answering over knowledge graphs on word and char-</cell><cell></cell></row><row><cell>acter level. In Proceedings of the 26th International</cell><cell></cell></row><row><cell>Conference on World Wide Web, WWW '17, page</cell><cell></cell></row><row><cell>1211-1220, Republic and Canton of Geneva, CHE.</cell><cell></cell></row><row><cell>International World Wide Web Conferences Steering</cell><cell>Michael Gutmann and Aapo Hyv?rinen. 2010. Noise-</cell></row><row><cell>Committee.</cell><cell>contrastive estimation: A new estimation principle</cell></row><row><cell></cell><cell>for unnormalized statistical models. In Proceedings</cell></row><row><cell>Clara Meister, Elizabeth Salesky, and Ryan Cot-</cell><cell>of the Thirteenth International Conference on Artifi-</cell></row><row><cell>terell. 2020. Generalized entropy regularization or:</cell><cell>cial Intelligence and Statistics, pages 297-304.</cell></row><row><cell>There's nothing special about label smoothing. In</cell><cell></cell></row><row><cell>Proceedings of the 58th Annual Meeting of the Asso-</cell><cell>Michael U. Gutmann and Jun-ichiro Hirayama. 2011.</cell></row><row><cell>ciation for Computational Linguistics, pages 6870-</cell><cell>Bregman divergence as general framework to es-</cell></row><row><cell>6886, Online. Association for Computational Lin-</cell><cell>timate unnormalized statistical models. In Pro-</cell></row><row><cell>guistics.</cell><cell>ceedings of the Twenty-Seventh Conference on Un-</cell></row><row><cell>Oren Melamud, Ido Dagan, and Jacob Goldberger. 2017. A simple language model based on PMI ma-</cell><cell>certainty in Artificial Intelligence, UAI'11, page 283-290, Arlington, Virginia, USA. AUAI Press.</cell></row><row><cell>trix approximations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-guage Processing, pages 1860-1865, Copenhagen, Denmark. Association for Computational Linguis-tics.</cell><cell>Qizhen He, Liang Wu, Yida Yin, and Heming Cai. 2020. Knowledge-graph augmented word represen-tations for named entity recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7919-7926.</cell></row><row><cell>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean. 2013. Distributed representa-tions of words and phrases and their compositional-ity. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.</cell><cell>Annervaz K M, Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. 2018. Learning beyond datasets: Knowledge graph augmented neural net-works for natural language processing. In Proceed-ings of the 2018 Conference of the North Ameri-</cell></row><row><cell>Seungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-jen Subba. 2019. OpenDialKG: Explainable conver-sational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th An-nual Meeting of the Association for Computational</cell><cell>can Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-ume 1 (Long Papers), pages 313-322, New Orleans, Louisiana. Association for Computational Linguis-tics.</cell></row><row><cell>Linguistics, pages 845-854, Florence, Italy. Associ-</cell><cell></cell></row><row><cell>ation for Computational Linguistics.</cell><cell>Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst.</cell></row><row><cell></cell><cell>2017. Knowledge base completion: Baselines strike</cell></row><row><cell>A. Painsky and G. W. Wornell. 2020. Bregman diver-</cell><cell>back. In Proceedings of the 2nd Workshop on Rep-</cell></row><row><cell>gence bounds and universality properties of the loga-</cell><cell>resentation Learning for NLP, pages 69-74, Vancou-</cell></row><row><cell>rithmic loss. IEEE Transactions on Information The-</cell><cell>ver, Canada. Association for Computational Linguis-</cell></row><row><cell>ory, 66(3):1658-1673.</cell><cell>tics.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>lists the statistics for each dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Entities Relations</cell><cell></cell><cell>Tuples</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell cols="2">FB15k-237 14,541</cell><cell>237</cell><cell cols="3">272,115 17,535 20,466</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The hyper-parameters for each model in FB15k-237. Rel. denotes relation, P. denotes patience, sub. denotes subjective, obj. denotes objective, xn denotes xavier normal, n denotes normal, xu denotes xavier uniform, and u denotes uniform.</figDesc><table><row><cell>WN18RR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The hyper-parameters for each model in WN18RR. The notations are the same asTable 5.</figDesc><table><row><cell>Model</cell><cell>Loss</cell><cell cols="2">FB15k-237 WN18RR</cell></row><row><cell></cell><cell>SCE</cell><cell>0.345</cell><cell>0.451</cell></row><row><cell>TuckER</cell><cell>SCE w/ LS NS</cell><cell>0.350 0.261</cell><cell>0.470 0.433</cell></row><row><cell></cell><cell>SANS</cell><cell>0.337</cell><cell>0.441</cell></row><row><cell></cell><cell>SCE</cell><cell>0.359</cell><cell>0.461</cell></row><row><cell>RESCAL</cell><cell>SCE w/ LS NS</cell><cell>0.369 0.344</cell><cell>0.474 0.389</cell></row><row><cell></cell><cell>SANS</cell><cell>0.344</cell><cell>0.390</cell></row><row><cell></cell><cell>SCE</cell><cell>0.304</cell><cell>0.468</cell></row><row><cell>ComplEx</cell><cell>SCE w/ LS NS</cell><cell>0.324 0.302</cell><cell>0.478 0.399</cell></row><row><cell></cell><cell>SANS</cell><cell>0.308</cell><cell>0.433</cell></row><row><cell></cell><cell>SCE</cell><cell>0.350</cell><cell>0.441</cell></row><row><cell>DistMult</cell><cell>SCE w/ LS NS</cell><cell>0.351 0.308</cell><cell>0.451 0.391</cell></row><row><cell></cell><cell>SANS</cell><cell>0.326</cell><cell>0.412</cell></row><row><cell></cell><cell>SCE</cell><cell>0.328</cell><cell>0.227</cell></row><row><cell>TransE</cell><cell>SCE w/ LS NS</cell><cell>0.322 0.289</cell><cell>0.220 0.216</cell></row><row><cell></cell><cell>SANS</cell><cell>0.333</cell><cell>0.218</cell></row><row><cell></cell><cell>SCE</cell><cell>0.320</cell><cell>0.452</cell></row><row><cell>RotatE</cell><cell>SCE w/ LS NS</cell><cell>0.320 0.306</cell><cell>0.449 0.472</cell></row><row><cell></cell><cell>SANS</cell><cell>0.340</cell><cell>0.475</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The best MRR scores on validation data.</figDesc><table><row><cell>Dataset</cell><cell>Mehotd</cell><cell>MRR</cell></row><row><cell>FB15k-237</cell><cell cols="2">RESCAL+SCE w/BC RESCAL+NS w/ Freq 0.171 0.149</cell></row><row><cell>WN18RR</cell><cell cols="2">ComplEx+SCE w/ BC 0.361 RotatE+NS w/ Freq 0.469</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>The best MRR scores of pre-trained models on validation data.</figDesc><table><row><cell cols="2">FB15k-237</cell><cell></cell></row><row><cell>Method</cell><cell>Pretrain</cell><cell>MRR</cell></row><row><cell></cell><cell>SCE</cell><cell>0.369</cell></row><row><cell>RESCAL+SCE w / LS</cell><cell cols="2">SCE w/ BC 0.369</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.371</cell></row><row><cell></cell><cell>NS</cell><cell>0.349</cell></row><row><cell>RESCAL+SANS</cell><cell cols="2">NS w/ Freq 0.348</cell></row><row><cell></cell><cell>SANS</cell><cell>0.350</cell></row><row><cell cols="2">WN18RR</cell><cell></cell></row><row><cell>Method</cell><cell>Pretrain</cell><cell>MRR</cell></row><row><cell></cell><cell>SCE</cell><cell>0.483</cell></row><row><cell>ComplEx+SCE w/ LS</cell><cell cols="2">SCE w/ BC 0.469</cell></row><row><cell></cell><cell cols="2">SCE w/ LS 0.481</cell></row><row><cell></cell><cell>NS</cell><cell>0.472</cell></row><row><cell>RotatE+SANS</cell><cell cols="2">NS w/ Freq 0.474</cell></row><row><cell></cell><cell>SANS</cell><cell>0.475</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In this setting, we can expand ?(z) = ?len(z) i=1 z i log z i to ?(z) = z log z + (1 ? z) log (1 ? z).4 See Appendix. E for the further details.5  Goldberg and Levy (2014) discuss the convexity of the inner product in NS. Different from theirs, our discussion is about the convexity of the loss functions itself.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/uma-pi1/kge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by JSPS Kakenhi Grant nos. 19K20339, 21H03491, and 21K17801.   , 2019). For each model, we also trained a model for the reverse direction that shares the entity embeddings with the model for the forward direction. Thus, the dimension size of subject and object embeddings are the same in all models. Implementation: We used LibKGE <ref type="bibr" target="#b6">(Broscheit et al., 2020</ref>) 9 as the implementation. We used its 1vsAll setting for SCE-based loss functions and negative sampling setting for NS-based loss functions. We modified LibKGE to be able to use label smoothing on the 1vsAll setting. We also incorporated NS w/ Freq and SCE w/ BC into the implementation. Hyper-parameter: <ref type="table">Table 5</ref> and 6 show the hyper-parameter settings of each method for each dataset. In RESCAL, ComplEx, and DistMult we used the settings that achieved the highest performance for each loss function in the previous study <ref type="bibr" target="#b6">(Ruffinelli et al., 2020)</ref>  10  . In TuckER and RotatE, we follow the settings from the original paper. When applying SANS, we set ? to an initial value of 1.0 for LibKGE for all models except TransE and RotatE, and for TransE and RotatE, where we followed the settings of the original paper of SANS since SANS was used in it. When applying SCE w/ LS, we set ? to the initial value of LibKGE, 0.3, except on TransE and RotatE. In the original setting of TransE and RotatE, because the value of SANS was tuned for comparison, for fairness, we selected ? from {0.3, 0.1, 0.01} by using the development data through a single run for each value. We set the maximum epoch to 800. We calculated MRR every five epochs on the developed data, and the training was terminated when the highest value was not updated ten times. We chose the best model by using the MRR score on the development data. These hyperparameters were also used in the pre-training step. Validation Score <ref type="table">Table 7</ref>, 8, and 9 show the best MRR scores of each loss for each model on the validation dataset.</p><p>Device: In all models, we used a single NVIDIA RTX2080Ti for training. Except for RotetE with SCE-based loss functions, all models finished the training in one day. The RotetE with SCE-based loss function finished the training in at most one week.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With ?(g(u)) = g(u) log(g(u)) ? (1 + g(u)) log(1 + g(u)) and ??(g(u)) = log(g(u)) ? log(1 + g(u)), we can reformulate Eq. (19) as:</p><p>From Eq. (20), when NS (? ) is minimized, g(u) = f (u) is satisfied. In this condition, G(y|x; ? ) becomes ? p n (y|x)</p><p>, and exp( f ? (x, y)) becomes p d (y|x) ? p n (y|x) as follows:</p><p>Based on the Eq. (1) and Eq. <ref type="formula">(21)</ref>, the objective distribution for p ? (y|x) is as follows:</p><p>B Proof of Proposition 4 PMI is induced by multiplying p d (x) to the right-hand side of Eq. (8) and then computing logarithm for both sides as follows: E Note: the divergence between the NS and SCE loss functions <ref type="bibr">Painsky and Wornell (2020)</ref> proved that the upper bound of the Bregman divergence for binary labels when ?(z) = ? len(z) i=1 z i log z i . However, to compare the SCE and NS loss functions in general, we need to consider the divergence of multi labels in SCE. When ?(z) = ? len(z) i=1 z i log z i , we can derive the following inequality by using the log sum inequality:</p><p>Eq. <ref type="formula">(24)</ref> shows that the divergence of multi labels is larger than that of binary labels in SCE. As we explained, d ?(z) ( f , g) of SCE is larger than d ?(z) ( f , g) of NS in binary labels. Therefore, the SCE loss imposes a larger penalty on the same predicted value than the NS loss when the value of the learning target is the same between the two losses.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pykeen 1.0: A python library for training and evaluating knowledge graph emebddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14175</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TuckER: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering with bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srujana</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">58</biblScope>
			<biblScope unit="page" from="1705" to="1749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI&apos;11</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI&apos;11</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bregman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0041-5553(67)90040-7</idno>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="217" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LibKGE -A knowledge graph embedding library for reproducible research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pre-training transformers as energy-based cloze models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minervini</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stenetorp</forename><surname>Pontus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8251</idno>
		<title level="m">Notes on noise contrastive estimation and negative sampling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>abs/1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
