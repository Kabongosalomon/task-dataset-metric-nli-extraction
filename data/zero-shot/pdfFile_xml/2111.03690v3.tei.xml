<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DO WE STILL NEED IMAGENET PRE-TRAINING IN REMOTE SENSING SCENE CLASSIFICATION?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-25">25 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Risojevi?</surname></persName>
							<email>vladimir.risojevic@etf.unibl.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">University of Banja Luka Banja Luka</orgName>
								<orgName type="institution" key="instit2">Bosnia and Herzegovina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Stojni?</surname></persName>
							<email>vladan.stojnic@etf.unibl.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="institution" key="instit1">University of Banja Luka Banja Luka</orgName>
								<orgName type="institution" key="instit2">Bosnia and Herzegovina</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DO WE STILL NEED IMAGENET PRE-TRAINING IN REMOTE SENSING SCENE CLASSIFICATION?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-25">25 May 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks</term>
					<term>Transfer learning</term>
					<term>Domain-adaptive pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the scarcity of labeled data, using supervised models pre-trained on ImageNet is a de facto standard in remote sensing scene classification. Recently, the availability of larger high resolution remote sensing (HRRS) image datasets and progress in self-supervised learning have brought up the questions of whether supervised ImageNet pre-training is still necessary for remote sensing scene classification and would supervised pre-training on HRRS image datasets or self-supervised pre-training on ImageNet achieve better results on target remote sensing scene classification tasks. To answer these questions, in this paper we both train models from scratch and fine-tune supervised and self-supervised ImageNet models on several HRRS image datasets. We also evaluate the transferability of learned representations to HRRS scene classification tasks and show that self-supervised pre-training outperforms the supervised one, while the performance of HRRS pre-training is similar to self-supervised pre-training or slightly lower. Finally, we propose using an ImageNet pre-trained model combined with a second round of pre-training using in-domain HRRS images, i.e. domain-adaptive pre-training. The experimental results show that domain-adaptive pre-training results in models that achieve state-of-the-art results on HRRS scene classification benchmarks. The source code and pre-trained models are available at https://github.com/risojevicv/RSSC-transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Transfer learning opened up the possibility of applying deep learning to domains in which labeled data is scarce, difficult or expensive to acquire, such as remote sensing <ref type="bibr" target="#b2">(Ball et al., 2018)</ref>. A standard approach for applying transfer learning in high resolution remote sensing (HRRS) scene classification has been to start with a supervised model trained on ImageNet and either use it for feature extraction or fine-tune it to the target task <ref type="bibr" target="#b13">(Hu et al., 2015;</ref><ref type="bibr" target="#b24">Nogueira et al., 2017)</ref>. We refer to the first round of training as pre-training. In the case of feature extraction the pre-trained model is used to compute the features of the target images, which can subsequently be used e.g., for training a classifier or for image retrieval. On the other hand, in fine-tuning the entire network is optimized for the target classification task.</p><p>The most prominent benchmark in HRRS scene classification for a long time has been UCM dataset with 21 classes and only 100 images per class <ref type="bibr" target="#b37">(Yang and Newsam, 2010)</ref>, which is not enough for training a model from scratch. However, in the previous years, several larger datasets of HRRS images have appeared with a goal of establishing new benchmarks for HRRS scene classification <ref type="bibr" target="#b5">(Cheng et al., 2017;</ref><ref type="bibr" target="#b18">Li et al., 2020;</ref><ref type="bibr" target="#b27">Qi et al., 2020;</ref><ref type="bibr" target="#b35">Xia et al., 2017;</ref><ref type="bibr" target="#b42">Zhou et al., 2018)</ref>. These datasets contain more classes and more images per class than UCM and, having in mind the domain difference between everyday objects in ImageNet and remote sensing scenes, a question arises whether we still benefit from ImageNet pretraining in HRRS scene classification.</p><p>On the other hand, ImageNet has been successfully used for self-supervised pre-training and it was recently shown * Corresponding author in <ref type="bibr" target="#b7">(Ericsson et al., 2021</ref>) that self-supervised models transfer better than supervised models to a number of downstream tasks. Therefore, to obtain a complete picture of usefulness of ImageNet pre-training in remote sensing scene classification self-supervised pre-training should also be included in the analysis.</p><p>To answer the titular question we both trained from scratch and fine-tuned ImageNet pre-trained supervised and self-supervised convolutional neural networks on multiple HRRS image datasets and compared the resulting classification accuracies. Furthermore, we examined whether representations learned on HRRS image datasets transfer better to other HRRS scene classification tasks compared to the representations learned on ImageNet. We used multiple source and target HRRS datasets of various sizes and class distributions with a goal of finding the factors that influence the model transferability.</p><p>In addition, it has recently been observed that transfer performance of large unsupervised pre-trained language models can be improved by additional unsupervised pre-training on data from the target domain, an approach known as domain-adaptive pretraining <ref type="bibr" target="#b8">(Gururangan et al., 2020)</ref>. Similar results with selfsupervised and supervised learning for several computer vision tasks were presented in <ref type="bibr" target="#b29">(Reed et al., 2021)</ref>. Motivated by these results, we also investigated if domain-adaptive pre-training can improve upon pre-training on only ImageNet or HRRS images. In contrast to <ref type="bibr" target="#b8">(Gururangan et al., 2020)</ref>, we experiment with supervised domain-adaptive pre-training and show that it can improve the classification accuracies on the remote sensing test datasets without any additional model complexity. The resulting performances are similar or better than state-of-theart obtained using architectural modifications. Therefore, our results can be regarded as the new baselines and models reused as backbones in all applications where ImageNet pre-trained supervised networks have been used.</p><p>The main contributions of this paper are:</p><p>1. Comparison of training from scratch and fine-tuning Ima-geNet pre-trained supervised and self-supervised convolutional neural networks on HRRS image datasets.</p><p>2. Evaluation of transferability of representations learned from scratch on HRRS image datasets to other HRRS scene classification tasks.</p><p>3. Introduction of supervised domain-adaptive pre-training to HRRS scene classification and empirical analysis of the factors contributing to its good performance.</p><p>4. Publishing the source code and pre-trained models to facilitate new research and applications of transfer learning.</p><p>The rest of the paper is organized as follows. In Section 2 related work is reviewed. The datasets and training details are described in Section 3. The experimental results are presented in Section 4. We discuss the results in Section 5 and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>It has long been known that networks pre-trained on Image-Net can produce representations suitable for different target tasks <ref type="bibr" target="#b6">(Donahue et al., 2014;</ref><ref type="bibr" target="#b30">Sharif Razavian et al., 2014)</ref>. Consequently, the question of factors influencing the transferability of the learned features had been investigated in <ref type="bibr" target="#b1">(Azizpour et al., 2015;</ref><ref type="bibr" target="#b14">Huh et al., 2016;</ref><ref type="bibr" target="#b38">Yosinski et al., 2014)</ref>. It was shown that similarity of the source and target tasks, as well as diversity and size of the source dataset influence the performance on a target task. However, these results are not consistent across the source and target tasks, and recently in <ref type="bibr" target="#b9">(He et al., 2019)</ref> was observed that ImageNet pre-training did not improve object detection and instance segmentation performance. Additionaly, it was found in <ref type="bibr" target="#b17">(Kornblith et al., 2019)</ref> that the networks pre-trained on ImageNet did not improve performance on target tasks from significantly different domains, requiring fine-grained classification or having more training data. These results spurred the interest in examining the effects of ImageNet pre-training in medical imaging <ref type="bibr" target="#b12">(Hosseinzadeh Taher et al., 2021;</ref><ref type="bibr" target="#b16">Ke et al., 2021;</ref><ref type="bibr" target="#b28">Raghu et al., 2019)</ref>, which shares some of the problems, such as the lack of large labeled datasets and need for domain experts for labeling, with remote sensing. However, to the best of our knowledge, a systematic investigation of ImageNet pretraining in HRRS scene classification is still missing.</p><p>Models pre-trained on ImageNet have quickly gained popularity for remote sensing image classification <ref type="bibr" target="#b13">(Hu et al., 2015;</ref><ref type="bibr" target="#b19">Liang et al., 2016;</ref><ref type="bibr" target="#b22">Marmanis et al., 2015;</ref><ref type="bibr" target="#b24">Nogueira et al., 2017;</ref><ref type="bibr" target="#b25">Penatti et al., 2015;</ref><ref type="bibr" target="#b34">Tong et al., 2020;</ref><ref type="bibr" target="#b40">Zhao et al., 2017)</ref>. Although pre-training on ImageNet is a de facto standard, several papers also reported experiments with pre-training on remote sensing image datasets. In <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> a HRRS scene classification model pre-trained on a dataset constructed taking a union of RESISC45, PatternNet, and RSI-CB showed slightly improved results on UCM classification compared to ImageNet pre-training. In contrast, an analysis in (Pires de Lima and Marfurt, 2020) showed that the networks pre-trained on ImageNet outperform the ones pre-trained on PatternNet in transfer learning to AID and UCM datasets. Concurrently with this work, Million-AID, the largest HRRS image dataset to date, has been published along with the experiments showing that pre-training on Million-AID results in better transfer learning performance on RESISC45 and AID than pre-training on ImageNet <ref type="bibr" target="#b20">(Long et al., 2022)</ref>.</p><p>The work in <ref type="bibr" target="#b23">(Neumann et al., 2020)</ref> is similar to ours in the sense that transfer from both ImageNet and remote sensing image datasets is explored. The experiments had been performed on three medium-resolution (BigEarthNet <ref type="bibr" target="#b32">(Sumbul et al., 2019)</ref>, EuroSAT <ref type="bibr" target="#b11">(Helber et al., 2019)</ref>, and So2Sat <ref type="bibr" target="#b43">(Zhu et al., 2019)</ref>) as well as two high-resolution datasets (RESISC45 and UCM) and the results showed that fine-tuning the models pre-trained on remote sensing datasets resulted in better classification accuracies than fine-tuning the models pre-trained on ImageNet. Furthemore, multiresolution datasets led to more transferable representations and medium-resolution datasets did not yield good generalization to high-resolution datasets. However, since only two HRRS image datasets were used, the question of the factors that influence transferability for HRRS scene classification remained unanswered. In this paper we perform the experiments on multiple HRRS image datasets of various sizes and with different numbers of classes with a goal of identifying the factors that influence the transferability of the obtained representations the most.</p><p>Self-supervised learning holds a promise to reduce the need for large labeled datasets in training deep learning models <ref type="bibr" target="#b15">(Jing and Tian, 2020)</ref>. Recently, it has been shown that the best self-supervised ImageNet models can outperform supervised ImageNet models in transferring to various downstream tasks <ref type="bibr" target="#b7">(Ericsson et al., 2021)</ref>. Although in most cases ImageNet or larger datasets of images of everyday objects and scenes are used for self-supervised pre-training, there are also attempts to use remote sensing images for that purpose <ref type="bibr" target="#b0">(Ayush et al., 2021;</ref><ref type="bibr" target="#b21">Ma?as et al., 2021;</ref><ref type="bibr" target="#b31">Stojnic and Risojevic, 2021)</ref>. However, since the preliminary experiments with pre-trained models from <ref type="bibr" target="#b21">(Ma?as et al., 2021)</ref> resulted in poor performances, we did not use remote sensing images for self-supervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MATERIALS AND METHODS</head><p>We used six HRRS image datasets: MLRSNet <ref type="bibr" target="#b27">(Qi et al., 2020)</ref>, RESISC45 <ref type="bibr" target="#b5">(Cheng et al., 2017)</ref>, PatternNet <ref type="bibr" target="#b42">(Zhou et al., 2018)</ref>, RSI-CB , AID <ref type="bibr" target="#b35">(Xia et al., 2017)</ref>, and UCM <ref type="bibr" target="#b37">(Yang and Newsam, 2010)</ref>. The main properties of these datasets are listed in <ref type="table">Table 1</ref>. The first five datasets were used as both source and target datasets while UCM was used only as the target dataset because it is too small for training a network from scratch. MLRSNet can be used for training both single-label and multi-label classifiers so we included both scenarios in our experiments. We also used ImageNet-1k and ImageNet-100 as source datasets. ImageNet-100 is a subset of ImageNet-1k with 100 classes and total of 131,689 images . Having the size similar to MLRSNet, ImageNet-100 enabled us to assess how the source domain influences representation transferability. With ImageNet-1k as the source dataset we experimented with both supervised and self-supervised pre-training. For self-supervised pre-training we chose SwAV <ref type="bibr" target="#b3">(Caron et al., 2020)</ref>, which had shown a good transfer performance in <ref type="bibr" target="#b7">(Ericsson et al., 2021)</ref>. We did not train networks on ImageNet-1k ourselves and rather used the pretrained supervised model available in Keras as well as the PyTorch implementation and weights of a self-supervised model pre-trained using SwAV and provided by the authors of <ref type="bibr" target="#b3">(Caron et al., 2020)</ref>.</p><p>For each source dataset, we used 80% of images from each class for training/fine-tuning and the rest for testing. We either trained from scratch or fine-tuned a ResNet-50 <ref type="bibr" target="#b10">(He et al., 2016)</ref> model pre-trained on ImageNet-1k for 100 epochs using Adam with batch size 100. We linearly increased the learning rate for the first 5 epochs to 3 ? 10 ?3 , in the case of training from scratch, or 3 ? 10 ?4 , in the case of fine-tuning, and reduced it with the factor of 0.2 in the 50th, 70th, and 90th epochs.</p><p>In both cases, we applied the following augmentations: resize to 292 ? 292 pixels and random crop of a 256 ? 256 pixels block, random flip left-right and up-down, and random rotation for <ref type="bibr">{90, 180, 270, 360}</ref> degrees. At test time, the images were resized to 292 ? 292 pixels and a 256 ? 256 pixels block was cropped from the center.</p><p>In the experiments with transfer learning, we used the pretrained source models as either fixed feature extractors and trained a softmax classifier, or replaced the classification layer and fine-tuned the whole network on a target dataset. For target datasets, we used 20% of images from each class for training and the remainder for testing. Following the usual protocol <ref type="bibr" target="#b7">(Ericsson et al., 2021)</ref>, we did not perform data augmentation when training the softmax and used the same augmentations as for the source datasets when fine-tuning the models. In the case of feature extraction data augmentation is omitted in order to assess the quality of the extracted features because they can also be used in different downstream tasks, such as image retrieval.</p><p>It should be noted that a single dataset was not used as both the source and target dataset in the same experiment.</p><p>For domain-adaptive pre-training, we fine-tuned ImageNet-1k pre-trained supervised and self-supervised ResNet-50 models on MLRSNet, as described previously, and used the resulting model as the pre-trained source model for transfer learning. In these experiments we did not use MLRSNet as a target dataset.</p><p>On the target datasets, we trained the softmax classifiers or fine-tuned the whole network for 100 epochs using Adam with batch size 100. For training the softmax we used a fixed learning rate 10 ?3 and for fine-tuning we used the same learning rate schedule as for fine-tuning the network pre-trained on ImageNet-1k. We report the classification accuracies on the test set for single-label tasks and F1-measures, with threshold 0.5, for multi-label tasks. All the networks were trained or finetuned on two Nvidia GTX 1080Ti GPUs with CUDA 11.0 and Intel Core i7-8700K CPU running Ubuntu 18.04.</p><p>We used the nonparametric bootstrap to estimate 95% confidence intervals for each performance metric. We drew 1,000 replicates from the test set, and computed the performance metric on each replicate. This procedure produced a distribution for each metric, and we reported the 2.5 and 97.5 percentiles as a confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training from scratch vs. fine-tuning</head><p>In the first experiment we compare training from scratch and fine-tuning the ImageNet-1k pre-trained network on remote sensing image datasets. In <ref type="table">Table 2</ref> the test accuracies/F1measures of the models trained or fine-tuned on 80% of the images from the used source datasets are reported. From these results we can observe that fine-tuning both supervised and self-supervised models pre-trained on ImageNet outperforms training from scratch in all the cases. However, for both variants of MLRSNet, as well as for PatternNet and RSI-CB, the differences are very small, and for RESISC45 the difference is around 2%, indicating that, even for medium-sized datasets, ImageNet pre-training plays a diminishing role in HRRS scene classification. The differences between fine-tuned supervised and self-supervised models are not statistically significant. When HRRS image datasets are used for pre-training, the best results are obtained using feature extractors pre-trained on MLRSNet, both single-label and multi-label, with multilabel pre-training winning in 3 out of 5 cases. In the tests on PatternNet and RSI-CB, single-label pre-training on MLRSNet marginally outperforms multi-label pre-training but the difference is very small and the performance is already saturated. Interestingly, pre-training on RESISC45 results in only slightly worse classification accuracies than pre-training on MLRSNet, despite being around three times smaller. Furthermore, pretraining on RESISC45 is comparable to supervised pre-training on ImageNet-1k and better than pre-training on ImageNet-100. Surprisingly, pre-training on PatternNet and RSI-CB yields much worse representations compared to all the other datasets. This was not expected having in mind that these datasets are comparable in sizes with RESISC45, significantly larger than AID, and with the classes similar to the classes in the target datasets.</p><p>An intuitively plausible reason for good performance of the representations learned on HRRS datasets is that the source and target datasets contain the same or similar classes. To investigate this assumption we split MLRSNet into two subsets, one containing the classes present in UCM and the other with the rest of the classes. The sizes of these subsets are 50,197 and 58,964 images, respectively. We also made a third MLRSNet subset containing all the classes and half (54,573) the images.</p><p>We train networks on all three subsets and use them to extract features from UCM. The resulting classification accuracies are given in <ref type="table">Table 4</ref>.</p><p>We observe that pre-training the feature extractor on the subset with the classes present in UCM results in better classification accuracy compared to pre-training on the subset with different classes. Moreover, pre-training on the subset with the same classes results in higher accuracy compared to training on, significantly larger, ImageNet-100. On the other hand, when the subset with different classes is used, the performance drop Dataset Size Classes Image size Resolution (m) Annotations MLRSNet <ref type="bibr" target="#b27">(Qi et al., 2020)</ref> 109,161 46/60 256 ? 256 0.1 -10 single/multi-label RESISC45 <ref type="bibr" target="#b5">(Cheng et al., 2017)</ref> 31,500 45 256 ? 256 0.2 -30 single-label PatternNet <ref type="bibr" target="#b42">(Zhou et al., 2018)</ref> 30,400 38 256 ? 256 0.062 -4.693 single-label RSI-CB  24,000 35 256 ? 256 0.22 -3 single-label AID <ref type="bibr" target="#b35">(Xia et al., 2017)</ref> 10,000 30 600 ? 600 0.5 -8 single-label UCM <ref type="bibr" target="#b37">(Yang and Newsam, 2010)</ref> 2,100 21 256 ? 256 0.3 single-label is less than 2% compared to the subset with the same classes and around half percent compared to ImageNet-100. These results suggest that the performance indeed benefits from pre-training on the same or similar classes as in the target task.</p><p>When the subset with all the classes is used, the classification accuracy is additionally improved and is similar to the accuracies obtained by pre-training on RESISC45, full singlelabel MLRSNet, and supervised  showing that in addition to domain similarity and overlap between the classes, class diversity also plays a role in training good feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Fine-tuning</head><p>The classification accuracies obtained by fine-tuning the pre-trained networks on 20% of the images from the target datasets are given in <ref type="table" target="#tab_2">Table 5</ref>. For comparison, we also report the classification accuracies obtained by training ResNet-50 on the target datasets from scratch. We see that both supervised and self-supervised pre-training on ImageNet-1k outperform training from scratch and pre-training on HRRS datasets in all the cases, with pre-training on multilabel MLRSNet being worse by around 1%. Supervised and self-supervised pre-training on ImageNet-1k result in comparable performances, with self-supervised pre-training having a slight advantage. These results suggest that both the number of images and class diversity in ImageNet-1k contribute to obtaining a good initialization for fine-tuning in spite of the domain gap between ImageNet-1k and remote sensing images. Nevertheless, it should be noted that the differences are small challenging again the role of ImageNet-1k as an ubiquitous pretraining dataset.</p><p>Pre-training on both variants of MLRSNet outperforms training from scratch in all the cases, and pre-training on RESISC45 is worse than training from scratch only in the case when the target task is multi-label MLRSNet. Furthermore, pre-training on both variants of MLRSNet, as well as on RESISC45 and AID, outperforms pre-training on ImageNet-100 on all the target datasets except PatternNet and RSI-CB, where the differences are very small.</p><p>Similarly to the feature extraction case, when PatternNet and RSI-CB are used for pre-training the obtained results are worse than in all the other cases. Moreover, for these two source datasets, the classification accuracies on most of the target datasets are not improved compared to training from scratch. A notable exception is UCM with too few training images for training a classifier from scratch. This result is in line with (Pires de Lima and Marfurt, 2020).</p><p>By comparing the results in <ref type="table">Table 3</ref>. and 5. we see that finetuning the networks pre-trained on MLRSNet and RESISC45 for smaller target datasets only slightly improves classification performances in comparison with training only a classifier on the features extracted using the pre-trained network. On the other hand, in both experiments with MLRSNet as the target dataset, fine-tuning the network pre-trained on other HRRS source datasets results in improved classification performance compared to training only a softmax classifier. However, it should be noted that these fine-tuned networks exhibit similar performances as the networks trained from scratch on both MLRSNet tasks. Interestingly, fine-tuning the networks pretrained on both ImageNet-100 and ImageNet-1k in all the cases considerably improves classification accuracies compared to the feature extraction case.</p><p>To examine the impact of the class overlap between the source and target datasets on the fine-tuning performance we fine-tune the networks pre-trained on the subsets of MLRSNet on UCM.</p><p>The obtained results are shown in <ref type="table">Table 6</ref>. We can see that the obtained classification accuracies do not differ much compared to the feature extraction case. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain-adaptive pre-training</head><p>The results in Section 4.2.1 show that, when pre-trained models are used as feature extractors, in-domain pre-training is comparable to self-supervised ImageNet pre-training and both approaches outperform supervised ImageNet pre-training. Furthermore, all the approaches result in similarly performing finetuned models. Therefore, in order to leverage the advantages of both ImageNet and in-domain pre-training we investigated the quality of the representations obtained by domain adaptive (DA) pre-training, i.e. fine-tuning the network pre-trained on ImageNet-1k using an in-domain dataset different from the target dataset and using the resulting model as a feature extractor or additionally fine-tuning it for the target task. In this section, we used MLRSNet (single and multi-label variants) as the source dataset and RESISC45, AID, and UCM as the target datasets. We excluded PatternNet and RSI-CB from these experiments because their performances had already been saturated in the previous experiments and pre-training on these datasets did not bring improvements on the target tasks compared to training from scratch. In all the cases we used supervised fine-tuning on MLRSNet. To get better insight into the impact of class overlap between the dataset used for domain adaptation and the target dataset we also performed DA pretraining using the subsets of MLRSNet containing the same and different classes as UCM and evaluated the domain-adapted models on UCM classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Feature extraction</head><p>The results obtained by training softmax classifiers on the features extracted using domainadapted models are given in the upper half of <ref type="table">Table 7</ref> (marked with FE). In comparison with the results in <ref type="table">Table 3</ref> we can see that DA pre-training improves the classification accuracies on the target datasets compared to using only ImageNet-1k or MLRSNet pre-training. The improvement is present when both supervised and self-supervised ImageNet pre-training are used, with self-supervised pre-training outperforming supervised. In contrast to pre-training only on MLRSNet, in this case the multi-label variant of MLRSNet does not show any advantages over the single-label variant.</p><p>The classification accuracies obtained by using the feature extractors domain-adapted on different subsets of MLRSNet are given in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Fine-tuning</head><p>The results obtained by fine-tuning the domain-adapted models on the target datasets are given in the lower half of <ref type="table">Table 7</ref> (marked with FT). In comparison with <ref type="table" target="#tab_2">Table 5</ref>, we can see that DA pre-training improves the classification accuracies compared to both supervised and selfsupervised pre-training on ImageNet-1k only. Moreover, the self-supervised model makes for a better basis for DA pretraining than the supervised one.</p><p>Fine-tuning the models obtained by DA pre-training on the different subsets of MLRSNet results in the classification accuracies given in <ref type="table">Table 9</ref>. From these results similar conclusions as in the case of feature extraction can be drawn. Specifically, DA pre-training on the MLRSNet subset with the same classes as in UCM is considerably better than pre-training on the MLRSNet subset with the different classes. In this case, it is even better than DA pre-training with all the classes and half of the training images. Furthermore, it outperforms pre-training on both ImageNet-1k and MLRSNet (multi-label). Therefore, when the models are fine-tuned for the target task, DA pre-training using a dataset with class overlap with the target dataset is beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>In summary, our experimental results show that training from scratch on most of the used HRRS image datasets results in only slightly lower performance than fine-tuning the ImageNet pre-trained models on the same datasets. For example, the differences on both variants of MLRSNet, PatternNet and RSI-CB are less than 1%, with somewhat larger gaps on RESISC45 and AID. These results suggest that for larger and some mediumsized HRRS image datasets we might avoid ImageNet pretraining and still achieve competitive results.</p><p>When pre-trained networks are used as fixed feature extractors, pre-training on HRRS image datasets outperforms supervised pre-training on ImageNet. Clearly, the features obtained using the networks pre-trained on HRRS images are better suited for other HRRS scene classification tasks than the features computed using the ImageNet pre-trained network. However, when a self-supervised model is used as a feature extractor, the performances are similar to pre-training on HRRS image datasets. On the other hand, when pre-trained networks are endto-end fine-tuned, supervised ImageNet pre-training slightly outperforms pre-training on HRRS image datasets, while selfsupervised ImageNet models after fine-tuning outperform both supervised ImageNet and in-domain models.</p><p>When PatternNet and RSI-CB are used as source datasets in both transfer learning scenarios, the obtained classification accuracies are lower compared to pre-training on other datasets. However, the classification accuracies on PatternNet and RSI-CB exceed 99%, showing that the performance on a source task is not always a good predictor of the performance on a target task. It is possible that classification of PatternNet and RSI-CB is too easy, which prevents the network from learning useful features for other HRRS scene classification tasks.</p><p>The recent experiments on Million-AID <ref type="bibr" target="#b20">(Long et al., 2022)</ref> showed that a large HRRS image dataset can outperform ImageNet as a source dataset in a transfer learning scenario and our results indicate that the gap between the models trained from scratch and fine-tuned models pre-trained on ImageNet is very narrow even for medium-sized HRRS image datasets. Nevertheless, there is another benefit of ImageNet pre-training. The models pre-trained on ImageNet and fine-tuned on a HRRS image dataset, i.e. domain-adaptive pre-trained models, outperform the models pre-trained only on either ImageNet or MLRSNet in both transfer learning scenarios. Nevertheless, although the improvements can be readily achieved by domainadaptive pre-training on medium-sized datasets, our results suggest that it is important that there exists class overlap between the dataset used for domain adaptation and the target dataset.</p><p>In <ref type="table">Table 10</ref> the results on RESISC45 and AID obtained using DA pre-training are compared to recent state-of-the-art HRRS scene classification methods. Two of the methods, ResNet50+EAN  and PCNet  use the same ResNet-50 backbone as in our experiments, while GLDBS <ref type="bibr" target="#b36">(Xu et al., 2021)</ref> uses ResNet-34. The best results obtained using pre-training on Million-AID used DenseNet-169 and ResNet-101 for classification of RESISC45 and AID, respectively. We can see that domain-adaptive pretraining without any additional modifications of the model is able to surpass the classification accuracies achieved using the recently proposed methods, as well as pre-training on an order of magnitude larger Million-AID. Therefore, DA pre-training can serve as a strong baseline for HRRS scene classification. Furthermore, DA pre-training is orthogonal to other extensions of the ResNet architecture proposed in the literature and we believe that it is possible that their combination would lead to further increase in classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Target dataset RESISC45 AID ResNet50+EAN  93.51 93.64 GLDBS <ref type="bibr" target="#b36">(Xu et al., 2021)</ref> 94.46 95.45 PCNet  94.59 95.53 Million-AID <ref type="bibr" target="#b20">(Long et al., 2022)</ref> 94.26 95.40 Domain-adaptive pre-training (ours) 95.89 96.17 <ref type="table">Table 10</ref>. Comparison with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper we empirically showed that, although ImageNet pre-training in its traditional form gradually loses its appeal, we can obtain additional improvements by using a second round of pre-training using in-domain data i.e., domain-adaptive pretraining. Therefore, the answer to the question posed in the title is positive and we can still benefit from ImageNet pre-training by coupling it with domain-adaptive pre-training. Additionally, since self-supervised pre-training is on par or better than supervised pre-training, for pre-training we only need the images from ImageNet and not their labels. An important consequence of our work is that domain-pretrained networks can be used as backbones in all applications where supervised ImageNet pretrained networks have traditionally been used.</p><p>In the future work we plan to further investigate what makes a dataset suitable for pre-training, as well as why fine-tuning the network pre-trained on ImageNet outperforms fine-tuning the networks pre-trained on HRRS image datasets in spite of the better results of the latter as feature extractors. Another interesting question is the impact of the choice of the network layers for fine-tuning on the classifier performance. Finally, transferability of representations to other target tasks, such as object detection and semantic segmentation is also an interesting direction of research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 3 .</head><label>13</label><figDesc>Details of HRRS image datasets used in the experiments. Classification accuracies/F1-measures (%) on 20% of the images from the target datasets obtained by training softmax classifiers on the features extracted using the pre-trained networks. The best result for each target dataset is given in bold, and the second best is underlined.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scratch</cell><cell cols="5">Fine-tuning (supervised) Fine-tuning (SwAV)</cell></row><row><cell cols="2">MLRSNet (multi-label)</cell><cell cols="2">91.83 (91.69, 91.97)</cell><cell></cell><cell cols="2">92.41 (92.27, 92.54)</cell><cell cols="2">92.58 (92.45, 92.71)</cell></row><row><cell cols="4">MLRSNet (single-label) 97.74 (97.55, 97.95)</cell><cell></cell><cell cols="2">98.61 (98.46, 98.76)</cell><cell cols="2">98.85 (98.70, 99.01)</cell></row><row><cell>RESISC45</cell><cell></cell><cell cols="2">95.11 (94.56, 95.65)</cell><cell></cell><cell cols="2">97.04 (96.57, 97.48)</cell><cell cols="2">96.87 (96.44, 97.30)</cell></row><row><cell>PatternNet</cell><cell></cell><cell cols="2">99.49 (99.31, 99.65)</cell><cell></cell><cell cols="2">99.84 (99.74, 99.93)</cell><cell cols="2">99.82 (99.70, 99.92)</cell></row><row><cell>RSI-CB</cell><cell></cell><cell cols="2">99.39 (99.17, 99.60)</cell><cell></cell><cell cols="2">99.55 (99.35, 99.72)</cell><cell cols="2">99.64 (99.47, 99.80)</cell></row><row><cell>AID</cell><cell></cell><cell cols="2">93.92 (92.85, 95.00)</cell><cell></cell><cell cols="2">97.30 (96.55, 98.00)</cell><cell cols="2">97.85 (97.20, 98.45)</cell></row><row><cell cols="9">Table 2. Test accuracies/F1-measures (%) with 95% confidence intervals of models trained/fine-tuned on 80% of the images from the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">HRRS datasets.</cell><cell></cell><cell></cell></row><row><cell>Source dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Target dataset</cell><cell></cell></row><row><cell></cell><cell cols="2">MLRSNet (multi-label)</cell><cell cols="2">MLRSNet (single-label)</cell><cell>RESISC45</cell><cell>AID</cell><cell cols="2">PatternNet RSI-CB UCM</cell></row><row><cell>ImageNet-1k</cell><cell cols="2">83.77</cell><cell>91.69</cell><cell></cell><cell>86.94</cell><cell>90.81</cell><cell>98.63</cell><cell>98.50</cell><cell>92.86</cell></row><row><cell>ImageNet-100</cell><cell cols="2">81.23</cell><cell>88.22</cell><cell></cell><cell>82.10</cell><cell>87.21</cell><cell>98.16</cell><cell>97.81</cell><cell>90.12</cell></row><row><cell>ImageNet-1k (SwAV)</cell><cell cols="2">85.83</cell><cell>93.22</cell><cell></cell><cell>89.21</cell><cell>92.98</cell><cell>99.07</cell><cell>98.82</cell><cell>93.27</cell></row><row><cell>MLRSNet (multi-label)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>93.21</cell><cell>92.68</cell><cell>98.96</cell><cell>98.57</cell><cell>93.45</cell></row><row><cell>MLRSNet (single-label)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>92.57</cell><cell>91.09</cell><cell>99.03</cell><cell>98.87</cell><cell>92.32</cell></row><row><cell>RESISC45</cell><cell cols="2">85.19</cell><cell>91.96</cell><cell></cell><cell>-</cell><cell>90.47</cell><cell>98.56</cell><cell>98.08</cell><cell>92.14</cell></row><row><cell>AID</cell><cell cols="2">80.83</cell><cell>85.78</cell><cell></cell><cell>79.81</cell><cell>-</cell><cell>96.99</cell><cell>96.49</cell><cell>86.49</cell></row><row><cell>PatternNet</cell><cell cols="2">79.19</cell><cell>84.19</cell><cell></cell><cell>76.56</cell><cell>78.37</cell><cell>-</cell><cell>97.10</cell><cell>83.57</cell></row><row><cell>RSI-CB</cell><cell cols="2">77.66</cell><cell>80.04</cell><cell></cell><cell>69.20</cell><cell>72.09</cell><cell>95.72</cell><cell>-</cell><cell>74.82</cell></row><row><cell>Subset</cell><cell cols="2">Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Same classes</cell><cell cols="2">91.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Different classes</cell><cell cols="2">89.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>All classes</cell><cell cols="2">92.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 4. Classification accuracies on UCM when different</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">subsets of MLRSNet are used for training the feature extractor.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Classification accuracies/F1-measures (%) on 20% of the images from the target datasets obtained by fine-tuning the pre-trained networks. The best result for each target dataset is given in bold, and the second best is underlined.</figDesc><table><row><cell>Source dataset</cell><cell></cell><cell></cell><cell cols="2">Target dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MLRSNet (multi-label)</cell><cell>MLRSNet (single-label)</cell><cell>RESISC45</cell><cell>AID</cell><cell cols="3">PatternNet RSI-CB UCM</cell></row><row><cell>Scratch</cell><cell>89.19</cell><cell>93.87</cell><cell>85.44</cell><cell>79.14</cell><cell>98.04</cell><cell>97.29</cell><cell>58.93</cell></row><row><cell>ImageNet-1k</cell><cell>90.53</cell><cell>96.62</cell><cell>93.85</cell><cell>94.40</cell><cell>99.51</cell><cell>99.15</cell><cell>94.64</cell></row><row><cell>ImageNet-100</cell><cell>88.35</cell><cell>93.79</cell><cell>88.99</cell><cell>90.95</cell><cell>99.03</cell><cell>98.78</cell><cell>87.86</cell></row><row><cell>ImageNet-1k (SwAV)</cell><cell>90.81</cell><cell>97.27</cell><cell>94.48</cell><cell>95.37</cell><cell>99.65</cell><cell>99.25</cell><cell>94.29</cell></row><row><cell>MLRSNet (multi-label)</cell><cell>-</cell><cell>-</cell><cell>93.75</cell><cell>93.60</cell><cell>99.19</cell><cell>99.00</cell><cell>93.81</cell></row><row><cell>MLRSNet (single-label)</cell><cell>-</cell><cell>-</cell><cell>92.17</cell><cell>92.16</cell><cell>99.11</cell><cell>98.90</cell><cell>92.50</cell></row><row><cell>RESISC45</cell><cell>89.08</cell><cell>94.53</cell><cell>-</cell><cell>91.45</cell><cell>99.01</cell><cell>98.66</cell><cell>92.14</cell></row><row><cell>AID</cell><cell>88.46</cell><cell>93.60</cell><cell>88.52</cell><cell>-</cell><cell>98.93</cell><cell>98.42</cell><cell>88.27</cell></row><row><cell>PatternNet</cell><cell>87.71</cell><cell>91.83</cell><cell>83.76</cell><cell>83.28</cell><cell>-</cell><cell>98.13</cell><cell>86.73</cell></row><row><cell>RSI-CB</cell><cell>86.57</cell><cell>90.26</cell><cell>80.29</cell><cell>79.19</cell><cell>98.02</cell><cell>-</cell><cell>78.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>We can see that DA pre-training on the MLRSNet subset with the same classes as in UCM outperforms DA pre-training on the subset with the different classes by around 4% and achieves almost the same results as when the MLRSNet subset with all the classes and half the images is used. However, when the MLRSNet subset with different classes is used for DA pre-training, the classification accuracies on UCM are lower compared to ImageNet-1k pre-training only. Apparently, DA pre-training shifted the features towards the discrimination between the classes not present in UCM reducing their performance on UCM classification. These results suggest that feature extractors can benefit from DA pretraining in those cases when there exists class overlap between the dataset used for DA pre-training and the target dataset.Table 7. Classification accuracies (%) on the target datasets obtained by transfer learning using the domain-adapted models. FE denotes using domain-adapted models for feature extraction and FT fine-tuning the domain-adapted model.</figDesc><table><row><cell></cell><cell>Pre-training</cell><cell>Transfer</cell><cell cols="2">Target dataset</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RESISC45</cell><cell>AID</cell><cell>UCM</cell></row><row><cell cols="2">ImageNet (supervised) ? MLRSNet (multi-label)</cell><cell>FE</cell><cell>94.50</cell><cell>92.51 94.35</cell></row><row><cell cols="2">ImageNet (supervised) ? MLRSNet (single-label)</cell><cell>FE</cell><cell>94.69</cell><cell>92.99 94.17</cell></row><row><cell cols="2">ImageNet (SwAV) ? MLRSNet (multi-label)</cell><cell>FE</cell><cell>94.54</cell><cell>93.29 94.96</cell></row><row><cell cols="2">ImageNet (SwAV) ? MLRSNet (single-label)</cell><cell>FE</cell><cell>95.24</cell><cell>93.92 96.89</cell></row><row><cell cols="2">ImageNet (supervised) ? MLRSNet (multi-label)</cell><cell>FT</cell><cell>94.27</cell><cell>94.31 93.87</cell></row><row><cell cols="2">ImageNet (supervised) ? MLRSNet (single-label)</cell><cell>FT</cell><cell>95.14</cell><cell>95.54 95.12</cell></row><row><cell cols="2">ImageNet (SwAV) ? MLRSNet (multi-label)</cell><cell>FT</cell><cell>95.49</cell><cell>96.17 96.55</cell></row><row><cell cols="2">ImageNet (SwAV) ? MLRSNet (single-label)</cell><cell>FT</cell><cell>95.89</cell><cell>96.09 97.14</cell></row><row><cell>Subset</cell><cell>Accuracy (%)</cell><cell></cell><cell></cell></row><row><cell>Same classes</cell><cell>95.18</cell><cell></cell><cell></cell></row><row><cell>Different classes</cell><cell>91.25</cell><cell></cell><cell></cell></row><row><cell>All classes</cell><cell>95.30</cell><cell></cell><cell></cell></row><row><cell cols="2">Table 8. Classification accuracies on UCM when different</cell><cell></cell><cell></cell></row><row><cell cols="2">subsets of MLRSNet are used for DA training of the feature</cell><cell></cell><cell></cell></row><row><cell cols="2">extractor.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Mihajlo Savi? for insightful discussion. Valuable comments from the reviewers are gratefully acknowledged. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geography-aware self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10181" to="10190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-of-the-art and gaps for deep learning on limited training data in remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An improved pretraining strategy-based scene classification with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="844" to="848" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How well do self-supervised models transfer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A systematic benchmarking analysis of transfer learning for medical image analysis. Domain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hosseinzadeh Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What makes ImageNet good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4037" to="4058" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RSI-CB: A large-scale remote sensing image classification benchmark using crowdsourced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1594</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning for high resolution aerial image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Applied Imagery Pattern Recognition Workshop (AIPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Aerial Scene Parsing: From Tile-level Scene Classification to Pixelwise Semantic Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01953</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seasonal contrast: Unsupervised pretraining from uncurated remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ma?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9414" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning earth observation classification using ImageNet pretrained networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Esch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="109" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training general representations for remote sensing using indomain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6730" to="6733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural network for remote-sensing scene classification: Transfer learning analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pires De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marfurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MLRSNet: A multi-label high spatial resolution remote sensing dataset for semantic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Mathiopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="337" to="350" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07208</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Metzger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12718</idno>
		<title level="m">Self-Supervised Pretraining Improves Self-Supervised Pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised learning of remote sensing scene representations using contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Risojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1182" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bigearthnet: A large-scale benchmark archive for remote sensing image understanding. IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charfuelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="5901" to="5904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Land-cover classification with high-resolution remote sensing images using transferable deep models. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">111322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Remote Sensing Image Scene Classification Based on Global-Local Dual-Branch Structure Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>IEEE Geoscience and Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pairwise Comparison Network for Remote Sensing Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transfer learning with fully pretrained deep convolution networks for land-use classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1436" to="1440" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Remote sensing image scene classification based on an enhanced attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>IEEE Geoscience and Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PatternNet: A benchmark dataset for performance evaluation of remote sensing image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="197" to="209" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?berle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12171</idno>
		<title level="m">So2Sat LCZ42: A benchmark dataset for global local climate zones classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
