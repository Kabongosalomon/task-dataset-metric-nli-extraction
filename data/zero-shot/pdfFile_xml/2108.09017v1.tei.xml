<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
							<email>qiaolimeng@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Zhao</surname></persName>
							<email>zhaoyuxuan@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
							<email>lizhiyuan@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Qiu</surname></persName>
							<email>qiuxi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<email>zhangchi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection, which aims at detecting novel objects rapidly from extremely few annotated examples of previously unseen classes, has attracted significant research interest in the community. Most existing approaches employ the Faster R-CNN as basic detection framework, yet, due to the lack of tailored considerations for data-scarce scenario, their performance is often not satisfactory. In this paper, we look closely into the conventional Faster R-CNN and analyze its contradictions from two orthogonal perspectives, namely multi-stage (RPN vs. RCNN) and multi-task (classification vs. localization). To resolve these issues, we propose a simple yet effective architecture, named Decoupled Faster R-CNN (DeFRCN). To be concrete, we extend Faster R-CNN by introducing Gradient Decoupled Layer for multistage decoupling and Prototypical Calibration Block for multi-task decoupling. The former is a novel deep layer with redefining the feature-forward operation and gradientbackward operation for decoupling its subsequent layer and preceding layer, and the latter is an offline prototype-based classification model with taking the proposals from detector as input and boosting the original classification scores with additional pairwise scores for calibration. Extensive experiments on multiple benchmarks show our framework is remarkably superior to other existing approaches and establishes a new state-of-the-art in few-shot literature 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, deep neural networks have achieved state-ofthe-art on a variety of visual tasks, e.g. image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> and object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39]</ref>. However, these leaps of performance arrive only when a large amount of annotated data is available. Since it is often labor-intensive to obtain adequate labelled data, the number of available samples severely limits the applications of current vision systems. Besides, compared to the ability of human to quickly extract novel concepts from few examples, these deep models are still far from satisfactory. It is thus of attracting major research interest on few-shot learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b48">48]</ref>, which employs the idea of learning novel concepts rapidly and generalizing well in data-scarce scenario. As one of the research branches, fewshot object detection (FSOD) is a much more challenging task than both few-shot classification and object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b58">58]</ref>. At present, most FSOD approaches prefer to follow the meta-learning paradigm to acquire more task-level knowledge and generalize better to novel classes. However, these methods usually suffer from a complicated training process and data organization, which results in limited application scenarios. In contrast, the finetune-based methods that exist as another research branch of FSOD, are very simple and efficient <ref type="bibr" target="#b50">[50]</ref>. By adopting a two-stage finetuning scheme, this series is comparable to meta methods. Yet, due to most parameters are pre-trained on base domain and then frozen on novel set, they may fall down the severe shift in data distribution and underutilization of novel data.</p><p>Regardless of the meta-based or finetune-based method, Faster R-CNN <ref type="bibr" target="#b39">[39]</ref> has been widely used as the basic detector and achieved good performance. However, its original architecture is designed for conventional detection and lacks of tailored consideration for few-shot scenario, which limits the upper bound of existing approaches. Concretely, <ref type="figure">Figure 2</ref>: Comparison of Faster R-CNN and our motivation. We performs stop-gradient between RPN and backbone, meanwhile, scale-gradient between RCNN and backbone, as well as decouple conflict tasks between classifier and regressor. The yellow blocks are trainable during fine-tuning.</p><p>on the one hand, as a classic two-stage stacking architecture, (i.e., backbone, RPN and RCNN, see <ref type="figure">Fig.2</ref>), Faster R-CNN may encounter an intractable conflict when it performs joint optimization end-to-end between class-agnostic RPN and class-relevant RCNN through the shared backbone. On the other hand, as a multi-task learning paradigm (i.e., classification and localization), RCNN needs translation-invariant features for box classifier whereas translation-covariant features for box regressor. These mismatched goals potentially generate so many low-quality scores and then further lead to the reduced classification power. Moreover, since there are only a few samples available during learning, these above contradictions will be further exacerbated.</p><p>Motivated by the above observations, we extend Faster R-CNN for few-shot scenario from two orthogonal perspectives: (1) multi-stage view. As shown in <ref type="figure">Fig.2</ref>, the Faster R-CNN contains three components, i.e., backbone, RPN and RCNN, which interact with each other through featureforward and gradient-backward. Due to the contradiction mentioned above between RPN and RCNN, we present to alleviate the entire model from being dominated by one of them with tailoring the degree of decoupling between three modules through gradient. (2) multi-task view. The task conflict between classification and regression affects the quality of features, which in turn damages the performance of box head outputs, i.e., category scores and box coordinates. We employ an efficient score calibration module only on the classification branch to achieve the purpose of decoupling the above two tasks. This paper proposes a simple yet effective approach, named Decoupled Faster R-CNN (DeFRCN), to perform both multi-stage decoupling and multi-task decoupling for few-shot object detection. The overall architecture is very straightforward as demonstrated in <ref type="figure">Fig.3</ref>. Compared to the standard Faster R-CNN <ref type="bibr" target="#b39">[39]</ref>, DeFRCN additionally contains two Gradient Decoupled Layer (GDL) and an offline Prototypical Calibration Block (PCB). The former ones are inserted between the shared backbone and RPN, meanwhile, between the backbone and RCNN to adjust the degree of decoupling among three modules, and the latter is parallel to the box classifier for further score calibration. Specifically, during the forward-backward propagation, GDL performs a learnable affine transformation on the forward feature maps and simply multiplies the backward gradient by a constant, which decouples the subsequent module and preceding module efficiently. Moreover, PCB is initially equipped with a well pre-trained classification model (e.g. ImageNet Pretrain) and a set of novel support prototypes. Then it takes the region proposals from few-shot detector as input and boosts the original softmax scores with additional prototype-based pairwise scores. As an interesting by-product, we find that just adopting PCB only in the inference phase can greatly improve the performance of few-shot detectors, with no extra training effort, which makes the PCB data-efficient and plug-and-play. The main contributions of our approach are three-folds:</p><p>? We look closely into the conventional Faster R-CNN and propose a simple yet effective architecture for few-shot detection, named Decoupled Faster R-CNN, which can be learned end-to-end via straightforward fine-tuning. ? To deal with the data-scarce scenario, we further present two novel modules, i.e. GDL and PCB, to perform decoupling among multiple components of Faster R-CNN and boost classification performance respectively. ? DeFRCN is remarkably superior to SOTAs on various benchmarks, revealing the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">General Object Detection</head><p>General object detection based on deep neural networks are currently divided into two main branches, i.e., two-stage proposal-based paradigm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">39]</ref> and one-stage proposal-free one <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref>, which both have witnessed fantastic progress on numerous large-scale benchmarks. The R-CNN series falls into the former line of work, which firstly generates a set of potential objects with region proposal network (RPN) <ref type="bibr" target="#b39">[39]</ref> and then performs category classification and box localization for end-to-end detection. In contrast, one-stage detectors endeavour to directly produce final predictions from the feature map without RPN module, usually have the advantages of inference speed but the detection performance is often not as good as two-stage approaches. However, all these frameworks uniformly assume that a large amount of annotated data from seen domain can be accessed, which may be stuck in troubles in data-scarce scenarios or novel unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROI Features</head><p>? <ref type="figure">Figure 3</ref>: The architecture of Decoupled Faster R-CNN (DeFRCN) for few-shot object detection. Compared to the standard Faster R-CNN, there are two Gradient Decoupled Layers (sky-blue) and an offline Prototypical Calibration Block (red) are inserted into the framework to perform decoupling for multi-stage and multi-task, respectively. The A is the affine transformation layer in GDL and ? is score fusion operation in PCB. Moreover, yellow and dark-blue indicate that the block is trainable and frozen during fine-tuning. The orange solid and black dotted lines represent forward flow and gradient flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Few-Shot Learning</head><p>Few-shot learning, which aims at learning to learn general knowledge slowly from abundant base data and extracting novel concepts rapidly from extremely few examples of new-coming classes, has been recently featured into the meta-learning based <ref type="bibr" target="#b47">[47]</ref> and fine-tuning based <ref type="bibr" target="#b30">[30]</ref> paradigms. As a recognition case of few-shot learning, few-shot classification has been widely investigated until now. In the literature, a large amount of studies that follow the idea of meta-learning to alleviate severe over-fitting can be divided into two streams, namely, optimization approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b35">35]</ref> and metric approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b48">48]</ref>. The former intents to learn efficient parameter updating rules <ref type="bibr" target="#b35">[35]</ref> or good parameters initialization strategies <ref type="bibr" target="#b14">[15]</ref>, and the latter focuses on obtaining a generalizable embedding metric space to perform pairwise similarity of inputs. In addition to meta-based approaches, some simple fine-tuning based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">45]</ref> are attaching more and more attention in the few-shot community. These methods show that just fine-tuning a linear classifier on top of a pre-trained model surprisingly achieves competitive performance with the meta-based approaches. Compared to classification, the solutions for other tasks, such as object detection and segmentation, are still underdeveloped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Few-Shot Object Detection</head><p>Since previous detectors usually require a large amount of annotated data, few-shot detection has attracted more and more interest recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b59">59</ref>]. Similar to classification task <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>, most of the current few-shot detectors focus on the meta-learning paradigm. FSRW <ref type="bibr" target="#b20">[21]</ref> is a light-weight meta-model based on YOLOv2 <ref type="bibr" target="#b37">[37]</ref> to re-weight the importance of features with channelwise attention, and then adapt these features to promote novel detection. Yet, instead of employing attention on the whole feature map, Meta R-CNN <ref type="bibr" target="#b58">[58]</ref> focuses on the attention of each RoI feature. Furthermore, FSDView <ref type="bibr" target="#b55">[55]</ref> puts forward a novel feature aggregation scheme, which leverages on base classes feature information to improve the performance on novel classes. From the perspective of attention on RPN, FSOD <ref type="bibr" target="#b12">[13]</ref> utilizes support information to filter out most background boxes and those in non-matching categories. Although meta-based approaches have been extensively studied recently, there are still some other metafree methods. RepMet <ref type="bibr" target="#b21">[22]</ref> incorporates a modified prototypical network as classification head into a standard object detector. And TFA <ref type="bibr" target="#b50">[50]</ref> proposes a simple approach based on transfer learning, that only fine-tunes the last layer of existing detectors on rare classes, which are comparable to the previous meta-based methods. Instead, our approach, which also follows the idea of fine-tuning, jointly trains the almost entire detector with novel gradient decoupled layer and prototypical calibration block, outperforming all above meta-based and finetune-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we first introduce the setup of few-shot object detection in Section 3.1. Then we revisit conventional Faster R-CNN in Section 3.2 and elaborate our Decoupled Faster R-CNN (DeFRCN) in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setting</head><p>As in various previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b55">55]</ref>, we follow the standard problem settings of few-shot object detection in our paper. Specifically, the whole learning procedure is organized into the form of two-stage fine-tuning paradigm, which gradually collects transferable knowledge across a large base set D base with abundant annotated instances and performs adaptation quickly on novel support set D novel with only a few samples per category. Note that the base classes C base in D base and the novel classes C novel in D novel are nonoverlapping, namely, C base ? C novel = ?. Given a sample (x, y) ? D base ? D novel , where x = {o i , i = 1, ..., N } is the input image with N objects and y = {(c i , b i ), i = 1, ..., N } denotes the category c i ? C base ? C novel and the structured box annotations b i . Under this setting, the ultimate goal of our algorithm is to optimize a robust detector F based on the D base and D novel , then classify and localize unlabelled objects of a novel query set D query with classes C query , where C query ? C base ? C novel . The overall procedure, which follows the standard transfer learning, can be summarized as follow,</p><formula xml:id="formula_0">F init D base ?? F base D novel ?? F novel<label>(1)</label></formula><p>where F init , F base and F novel denote the learned detectors in initialization, base training and novel fine-tuning stages respectively. The symbol ? indicates model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Revisiting Faster R-CNN</head><p>As a two-stage stacking architecture, Faster R-CNN <ref type="bibr" target="#b39">[39]</ref> consists of three function-detached modules for end-to-end training, i.e., a shared convolutional backbone for extracting generalized features, an efficient Region Proposal Network (RPN) for generating class-agnostic proposals and a taskspecific RCNN head <ref type="bibr" target="#b15">[16]</ref> for performing class-relevant classification and localization. The whole learning procedure is illustrated in <ref type="figure">Fig.2 (a)</ref>. Concretely, the input image is first fed into the backbone to generate a high-level feature map, and then parallelly provided to the next two modules, i.e., RPN and RCNN. Second, with classifying and regressing a group of scale varying anchors of the feature map simultaneously, RPN generates a sparse set of high-quality region proposals. Finally, on top of the shared feature map and proposals, RCNN pools each region-of-interest into a fixed size feature map with RoI pooling <ref type="bibr" target="#b17">[18]</ref>, and then performs box classifier and regressor for computing the object category probabilities and fine-tuning the box boundaries respectively. All these modules are jointly optimized endto-end by minimizing an unify objective function, which follows the multi-task learning paradigm as:</p><formula xml:id="formula_1">L total = (L cls rpn + L reg rpn rpn task ) + ? ? (L cls rcnn + L reg rcnn rcnn task )<label>(2)</label></formula><p>where ? is a balanced hyper-parameter for different tasks.</p><p>Problem of multi-task learning. It can be seen that the above-mentioned three modules of Faster R-CNN constitute an unified multi-task learning (MTL) framework, yet there is a certain inconsistency among the optimization goals of these sub-networks. Specifically, with utilizing the feature maps extracted from hard-parameter shared <ref type="bibr" target="#b46">[46]</ref> backbone, RPN aims at generating class-agnostic region proposals to tell the network where to look, while RCNN targets to perform region-based detection category by category to identify what to look. Furthermore, the classification head needs translation invariant features whereas the localization head needs translation covariant features on the contrary. In spite of multi-task learning generally helps to improve the endto-end performance of object detection as shown in Faster R-CNN <ref type="bibr" target="#b39">[39]</ref>, the joint optimization with the Eq.2 may lead to possible suboptimal solution on individual tasks in order to balance the mismatched goals of them <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b53">53]</ref>. Problem of shared backbone. According to the arguments in <ref type="bibr" target="#b39">[39]</ref>, the ultimate goal of shared backbone is to extract general features that are as suitable as possible for all downstream tasks. In fact, from the perspective of gradient flow in <ref type="figure">Fig.2</ref> (a), RPN and RCNN mutually exchange information of optimization through the shared backbone. However, due to the potential contradictions between RPN and RCNN, we notice that the current architecture may lead to the reduced few-shot detection power of the entire framework. Moreover, following the setting of Eq.1, the shared backbone of few-shot novel detector F novel is usually finetuned from a base domain detector F base . During this two-stage cross-domain procedure, RPN may suffer from the foreground-background confusion, which means a proposal that belongs to background in the base training phase is likely to be foreground in the novel fine-tuning phase. Through the gradient from RPN, the shared convolutional layers propagate the tendency of over-fitting on base classes to backbone and RCNN. Although this is one of the convergence schemes to behave well on base domain, it potentially damages the ability to transfer to the novel set quickly and efficiently, especially in the data-scarce scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoupled Faster R-CNN</head><p>Motivated by the above arguments, we propose a simple yet effective approach, named Decoupled Faster R-CNN (DeFRCN), to tap into more potential of Faster R-CNN styled detectors in few-shot literature. Based on the idea of decoupling three functional modules ( i.e., backbone, RPN and RCNN) and two kinds of tasks (i.e., classification and localization), the overall architecture of our method is very straightforward as demonstrated in <ref type="figure">Fig.3</ref>, which has two Gradient Decoupled Layers (GDL) to adjust the degree of decoupling among three modules and an offline Prototypical Calibration Block (PCB) to improve the classification power of RCNN during the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Gradient Decoupled Layer</head><p>In this section, we look into a different aspect of network design -how to customize the relationship between the upstream and downstream modules of the model. From the perspective of feature-forward and gradient-backward, we introduce a novel architectural unit, denoted as the Gradient Decoupled Layer (GDL). During the forward propagation, GDL employs an affine transformation layer A, which is parameterized by learnable channel-wise weights ? and bias b, to simply enhance feature representations and perform forward-decoupling. During the backward propagation, GDL takes the gradient from the subsequent layer, multiplies it by a constant ? ? [0, 1] and passes it to the preceding layer, as illustrated in <ref type="figure">Fig.3</ref>. Concretely, along with the back-propagation process passes through the GDL, the partial derivatives of the loss L d that is downstream of the GDL with respect to the layer parameters ? u that are upstream of the GDL get multiplied by ?, i.e., ?L d ??u (denoted as ? in Eq.4) is simply replaced with ? ?L d ??u . Mathematically, we can formally treat GDL as a pseudo-function G (A,?) defined by two equations describing its forward-and backward-propagation behaviour as follows:</p><formula xml:id="formula_2">G (A,?) (x) = A(x) (3) dG (A,?) dx = ?? A<label>(4)</label></formula><p>where A is an affine transformation layer, ? ? [0, 1] is a decoupling coefficient and ? A is the Jacobian matrix from the affine layer. In general, implementing such layer with existing deep learning frameworks are extremely simple, as defining procedures for forwardprop (affine transformation) and backprop (multiplying by a constant) is trivial. We provide the pseudo-code of GDL in Algorithm 1.</p><p>Perform Decoupling with GDL. Given a standard Faster R-CNN <ref type="bibr" target="#b39">[39]</ref>, two GDLs are respectively inserted between the shared backbone and RPN (i.e., G rpn ), as well as the shared backbone and RCNN (i.e., G rcnn ), which brings the part of DeFRCN architecture depicted in <ref type="figure">Fig.3</ref>. Specifically, during the forward propagation, the feature from shared backbone is transformed into different feature spaces through A rpn and A rcnn . Moreover, during the backward propagation, we adjust the decoupling degree of three modules (i.e., backbone, RPN and RCNN) by applying different ? rpn and ? rcnn on gradients. More formally, we consider the following loss function with two separate GDLs as:</p><formula xml:id="formula_3">L = L rpn (F rpn (G rpn (F b (x; ? b )); ? rpn ), y rpn ) + ? ? L rcnn (F rcnn (G rcnn (F b (x; ? b )); ? rcnn ), y rcnn ) (5)</formula><p>Here, G ? is the Gradient Decoupled Layer we proposed in this section, ? b , ? rpn and ? rcnn are learnable parameters for the backbone, RPN and RCNN respectively. Moreover, ? is a hyper-parameter to control the trade-off between L rpn and L rcnn (usually is set to 1). Optimization with GDL. Consistent with the optimization goal of Faster R-CNN, we seek the optimal parameters ? b , ? rpn and ? rcnn , denoted as ?, for the function Eq.5 as:</p><formula xml:id="formula_4">? = arg min ? 1 N N i=1 L, ? = {? b , ? rpn , ? rcnn }<label>(6)</label></formula><p>where N is the number of training samples, and L is from the Eq.5. Concretely, a gradient descent step can be described as:</p><formula xml:id="formula_5">? b ? ? b ? ? ? 1 ?L rpn ?? b + ? 2 ?L rcnn ?? b (7) ? rpn ? ? rpn ? ? ?L rpn ?? rpn (8) ? rcnn ? ? rcnn ? ? ?L rcnn ?? rcnn<label>(9)</label></formula><p>where ? is the learning rate, ? 1 and ? 2 are decoupling coefficients for RPN and RCNN respectively. It can be seen from Eq.8 and Eq.9 that adding GDL does not affect the optimization of RPN and RCNN. However, the parameter update of sharing backbone is deeply affected by GDL in Eq.7. We mainly analyze three important situations: (1) ? 1 = 0 (or ? 2 = 0), it is equivalent to stopping gradient from RPN (or RCNN), and the update of ? b will only be dominated by RCNN (or RPN); (2) ? 1 ? (0, 1] (or ? 2 ? (0, 1]), it is equivalent to scaling gradient from RPN (or RCNN), which means that the RPN (or RCNN) has individual contributions to the update of shared backbone; (3) ? 1 = ? 2 =?, which is equivalent to multiplying the learning rate ? of backbone by a small coefficient, i.e.,?, ensures that the update speed of ? b is slower than ? rpn and ? rcnn . Note that ? &lt; 0 is meaningless for detection and more discussion about ? is mentioned in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Prototypical Calibration Block</head><p>In this section, we introduce a novel metric-based score refinement module, termed as Prototypical Calibration Block (PCB), to effectively decouple the classification and localization tasks during the inference time. In general, most of detectors parallelly deploy a classifier and a regressor on top of the shared network. However, classification needs translation invariant features whereas localization needs translation covariant features. Thus the localization branch may force the backbone to gradually learn translation covariant property, which potentially downgrades the performance of classifier. Due to model complexity, the extreme lack of annotated samples will further exacerbate this contradiction. We notice that the under-explored few-shot classification branch generates a large amount of low-quality scores, which motivates us to eliminate high-scored false positives and remedy low-scored missing samples by introducing a Prototypical Calibration Block (PCB) for score refinement. The overall pipeline is illustrated in <ref type="figure">Fig.3 (c)</ref>. Concretely, our PCB consists of a strong classifier from ImageNet pretrained model, a RoIAlign layer and a prototype bank. Given a M -way K-shot task with support set S, the PCB first extracts original image feature map and then employs RoIAlign with ground-truth boxes to produce M K instance representations. Based on these features, we shrink the support set S to a prototype bank P = {p c } M c=1 with Eq.10:</p><formula xml:id="formula_6">p c = 1 |S c | (xi, yi)?Sc x i<label>(10)</label></formula><p>where S c is a subset which contains samples with the same label c in S. Given an object proposal? i = (c i , s i , b i ) produced by fine-tuned few-shot detector, where b i is the box boundaries, c i is the predicted category and s i is the corresponding score, PCB first performs RoIAlign on predicted box b i to generate object feature x i , and then calculate the cosine similarity s cos i between x i and p ci as:</p><formula xml:id="formula_7">s cos i = x i ? p ci ?x i ??p ci ?<label>(11)</label></formula><p>In the end, we perform weighted aggregation between the s cos i from PCB and s i from few-shot detector for final classification score s ? i as follow:</p><formula xml:id="formula_8">s ? i = ? ? s i + (1 ? ?) ? s cos i<label>(12)</label></formula><p>where ? is the trade-off hyper-parameter. Moreover, we do not share any parameters between the few-shot detector and PCB module, so that the PCB can not only preserve the quality of classification-aimed translation invariance feature, but also better decouple the classification task and regression task within the RCNN. Furthermore, since the PCB module is offline without any further training, it can be plug-and-play and easily equipped to any other architectures to build stronger few-shot detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the experimental settings in Sec.4.1 and then compare our approach with previous SOTAs on multiple benchmarks in Sec.4.2. Finally, we provide comprehensive ablation studies in Sec.4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Existing benchmarks. We follow the previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b55">55]</ref> and utilize the same data splits with <ref type="bibr" target="#b50">[50]</ref> to evaluate our approach for a fair comparison. As for PASCAL VOC, we have three random split groups and each of them covers 20 categories, which are randomly divided into 15 base classes and 5 novel classes. Each novel category has K = 1, 2, 3, 5, 10 objects sampled from the combination of VOC07 and VOC12 train/val set for few-shot training. And VOC07 test set is used for evaluation. As for COCO, the 60 categories disjoint with VOC are denoted as base classes while the remaining 20 classes are used as novel classes with K = 1, 2, 3, 5, 10, 30 shots. We utilize 5k images from the validation set for evaluation and the rest for training. Evaluation setting. We take two popular evaluation protocols into consideration to access the effectiveness of our approach, including few-shot object detection (FSOD) and generalized few-shot object detection (G-FSOD). The former protocol is widely adopted by most previous methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b58">58]</ref> and only focuses on the performance of novel classes. Yet, the latter presents to not only observe the performance on novel classes, but also base and overall performance of the few-shot detector, which is more comprehensive and monitors the occurrence of catastrophic forgetting <ref type="bibr" target="#b50">[50]</ref>. For evaluation metrics, we report AP 50 for VOC and the COCO-style mAP for COCO. Moreover, all results are averaged over multiple repeated runs. Implementation details. Our approach employs Faster R-CNN <ref type="bibr" target="#b39">[39]</ref> (termed as FRCN) as the basic detection framework and ResNet-101 <ref type="bibr" target="#b18">[19]</ref> pre-trained on ImageNet <ref type="bibr" target="#b40">[40]</ref> as the backbone. We adopt SGD to optimize our network endto-end with a mini-batch size of 16, momentum of 0.9 and weight decay of 5e ?5 . The learning rate is set to 0.02 during base training and 0.01 during few-shot fine-tuning. Moreover, the ? in GDL of RPN is set to 0 for stopping gradient and the ? in GDL of RCNN is set to 0.75 during base training and 0.01 during novel fine-tuning for scaling gradient. The ? in PCB is uniformly set to 0.5 in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Results</head><p>PASCAL VOC. We present our evaluation results of VOC on three different data splits in <ref type="table">Table 1</ref>. It can be seen that, no matter under the FSOD or G-FSOD setting, our De-FRCN is significantly superior to the recent state-of-the-art approaches by a large margin (up to 21.4%), which demonstrates the effectiveness of our approach. Based on the re-  sults of <ref type="table">Table 1</ref>, we further notice that two interesting phenomena exist in few-shot detection: (1) For FSOD setting, the increment of novel shots does not necessarily lead to an advance in final performance. Take Novel Set 1 as an example, the AP 50 of 5-shot is 64.1% but 10-shot is 60.8% (-3.3%). There is a similar case in TFA. We conjecture that the quality of sample is vital in data-scarce scenario and adding low-quality samples may be harmful to the detector.</p><p>(2) For the comparison between FSOD and G-FSOD, we find that as the number of shots increases, the final performance of G-FSOD grows faster than that of FSOD (40.2% ? 66.5% vs. 53.6% ? 60.8%), which is due to the addition of more negative samples under the G-FSOD setting.  COCO. The <ref type="table" target="#tab_2">Table 2</ref> shows all evaluation results on COCO dataset with the standard COCO-style averaged AP (mAP ). Obviously, our approach consistently outperforms recent SOTAs in all setups, including FSOD and G-FSOD for K=1,2,3,5,10,30. For FSOD, we achieve around 6.0% and 7.9% improvement over the best method in 10-shot and 30shot respectively, which demonstrates the strong robustness and generalization ability of our method in the few-shot scenario. Furthermore, compared to the fine-tuning based methods, the number of learnable parameters of DeFRCN is almost the same as FRCN-ft and much more than TFA. The results in <ref type="table" target="#tab_2">Table 2</ref> reveal that our method not only guarantees the sufficient learning of these parameters, but also does not fall into the severe over-fitting. All base/overall results of G-FSOD are presented in supplementary materials. COCO to VOC. We conduct the cross-domain FSOD experiments on the standard VOC 2007 test set with following the same setting from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b52">52]</ref>, which uses the base dataset with 60 classes as in the previous COCO within-domain setting and the novel dataset with 10-shot objects for each of the 20 classes from VOC. As shown in the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Effectiveness of different modules. We conduct relative ablations in 10/30-shot scenarios on the COCO dataset to carefully analyze how much each module contributes to the ultimate performance of DeFRCN. All results are shown in <ref type="table" target="#tab_6">Table 4</ref> in great details. Specifically, the first row shows the results of plain FRCN, which only achieves 7.9%/12.2% for 10/30-shot respectively, indicating that the original model without any few-shot techniques is severely over-fitting due to the lack of training data. Next, we take four progressive steps to complete the exploration of our DeFRCN: (1) add GDL in base training phase (GDL-B). Through the results of rows 1-4 and 5-8, we find that the GDL-B improves by 0.6% on base classes and also a certain improvement (0.3% ? 2.1%) on novel classes. This indicates that a better base model is beneficial to the performance of few-shot detector.</p><p>(2) add GDL in novel fine-tuning phase <ref type="figure">(GDL-N)</ref>. The results of first row and third row show that GDL-N makes an amazing boost with 7.3%/6.8% for 10/30-shot, which are mainly from two aspects: i) more learnable parameters guarantee sufficient ability to transfer to novel domain, and ii) GDL greatly reduces the risk of over-fitting.(3) add PCB in the inference phase. As a plug-and-play module, PCB is orthogonal to GDL, so no matter which setting PCB is added, our model further gains 1.4% ? 2.6% points on mAP . (4) Finally, we integrate the above three modules into original FRCN, and the last line shows the final performance of DeFRCN. Compared to the plain results in the first row, we obtain a marvelous promotion of 10.6%/10.4% for 10/30-shot, which proves the effectiveness of our approach.  Effectiveness of the degree of decoupling. We carefully explore the influence of decoupling with setting different ? rpn and ? rcnn in GDL during both base training and novel fine-tuning, and all results are illustrated in <ref type="figure" target="#fig_1">Fig.4</ref>. No matter in the base training or the novel fine-tuning stage, the model tends to achieve higher performance when ? rpn is set to a smaller value (close to 0), while ? rcnn needs an appropriate value to ensure that the backbone can be optimized better. This observation prompts us to perform stop-gradient for RPN and scale-gradient for RCNN in DeFRCN. In addition, we further get a very interesting conclusion from four corners in <ref type="figure" target="#fig_1">Fig.4(a</ref> Can GDL boost conventional detection?</p><p>The above analysis shows that GDL brings a significant improvement on FSOD. Since the problem it solves (that is, the contradiction in Faster R-CNN) also potentially exists in conventional detection, we conjecture that our GDL is as well as effective in data-sufficient scenarios. Thus we further conduct experiments on COCO 2017 dataset with standard setup <ref type="bibr" target="#b39">[39]</ref> and the results are shown in <ref type="table" target="#tab_7">Table 5</ref>. It can be seen that the proposed GDL outperforms baselines on all evaluation metrics. Specifically, adding GDL to original FRCN gains 1.5% and 0.9% AP50 for Res-50 and Res-101 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we look closely into the visual task of fewshot object detection and propose a simple yet effective finetuning based framework, named Decoupled Faster R-CNN, which remarkably alleviates the potential contradictions of conventional Faster R-CNN in data-scarce scenario with introducing novel GDL and PCB. Despite its simplicity, our method still achieves new state-of-the-art on various benchmarks, which demonstrates its effectiveness and versatility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this supplementary material, we provide additional details which we could not include in the main paper due to space limitations, including more experimental analysis and visualization details that help us develop further insights to the proposed approach. We discuss:</p><p>? More results of generalized few-shot object detection. ? Additional analysis on Prototypical Calibration Block. ? Related extensions of Gradient Decoupled Layer. ? Qualitative visualization results of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generalized Few-Shot Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation Details</head><p>As mentioned in the main paper, we take two popular evaluation protocols into consideration to assess the effectiveness of our approach, including few-shot object detection (FSOD) and generalized few-shot object detection (G-FSOD). The difference between these two protocols is whether the performance of base classes is still required after the fine-tuning stage. Following the G-FSOD setting in TFA <ref type="bibr" target="#b50">[50]</ref>, we fine-tune our DeFRCN on a small balanced training set consisting of both base and novel classes, where each class has the same number of annotated objects (i.e., K-shot). In addition to deploying more training iterations (2?), other experimental settings for G-FSOD are exactly consistent with the FSOD in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Experimental Results of G-FSOD Setting</head><p>In this section, we show the full benchmark results of the G-FSOD setting. For each evaluation metric, we report the average results of n random splits (n = 30 for VOC and n = 10 for COCO) with the same data split in TFA as well as the 95% confidence interval estimate of the mean values. PASCAL VOC. We present the complete G-FSOD results of VOC (K = 1, 2, 3, 5, 10) in <ref type="table" target="#tab_11">Table 6</ref> and then analyze our results from the following three aspects: (1) Novel AP. The novel AP of our model is usually over 7% points higher than that of TFA in three data splits, which indicates that the proposed DeFRCN has absolute advantage on novel performance. (2) Base AP. Our approach is able to outperform TFA on split 2 (+1.9% ? +3.7% AP ), however, it is slightly worse on data split 1 and 3 (-0.3% ? -1.0% AP ). We notice that the base performance advantage of TFA comes from the strategy of fine-tuning only the last layer of detectors, which can indeed be eccentric to ensure that the base performance does not decrease too much, but it also results in the novel performance cannot be further improved. (3) Overall AP. As shown in the <ref type="table" target="#tab_11">Table 6</ref>, the proposed DeFRCN achieves the best overall performance across all settings (+1.4% ? +4.0% AP ), including data splits and shots.</p><p>COCO. The <ref type="table" target="#tab_12">Table 7</ref> shows the G-FSOD results on COCO dataset over K = 1, 2, 3, 5, 10, 30 shots. Although COCO is much more complicated than VOC, similar observations can be drawn about accuracy on both base classes and novel classes. Concretely, the performance on base classes is comparable to TFA, but we are far superior to TFA in terms of both novel and overall results. In addition, we further notice that as the number of support shots increases, our approach can bring more performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Analysis on PCB B.1. Boost Other Approaches with PCB</head><p>As a plug-and-play module, the proposed PCB is easily equipped to any other architectures to build stronger fewshot detectors. Here, we verify this argument with introducing PCB into other previous approaches, including FRCNft <ref type="bibr" target="#b58">[58]</ref>, TFA <ref type="bibr" target="#b50">[50]</ref>, MPSR <ref type="bibr" target="#b52">[52]</ref>, and all experimental results on COCO dataset are shown in the <ref type="table">Table 8</ref>. Regardless of methods or the number of shots, we observe that using PCB can consistently achieve much higher performance (+1.0% ? +3.0% points) on novel classes, which demonstrates the effectiveness and flexibility of our PCB module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Employ Other Pre-trained Models</head><p>In the main paper, we utilize the standard ImageNet pretrained model <ref type="figure" target="#fig_0">(IN-1K)</ref>, which is widely adopted in most of few-shot object detection frameworks, to initialize both Faster-RCNN and PCB. Since the core module of PCB is the generalizable feature extractor, which determines the final performance of the score calibration, we further explore other pre-trained models (see <ref type="table">Table 9</ref>) in this section. SwAV <ref type="bibr" target="#b4">[5]</ref> is an efficient method for pre-training without using annotations, i.e., self-supervised learning. IN-SwAV indicates that the model is pre-trained by SwAV on ImageNet. IG-WSL <ref type="bibr" target="#b28">[28]</ref> employs the ResNeXt <ref type="bibr" target="#b57">[57]</ref> architecture and pre-trains on a much larger social media image dataset (Instagram) with weakly-supervised learning paradigm. <ref type="table">Table  10</ref> shows the performance on VOC with utilizing the above three pre-trained models. No matter which one is exploited, the final performance is better with PCB. Moreover, we further notice that using a stronger pre-trained model, the performance of FSOD can be improved more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Paradigm # Images # Classes IN-SwAV <ref type="bibr" target="#b4">[5]</ref> ResNet-50 S-S-L 1.28M 0 IN-1K <ref type="bibr" target="#b18">[19]</ref> ResNet-101 S-L 1.28M 1000 IG-WSL <ref type="bibr" target="#b28">[28]</ref> ResNeXt-101 W-S-L 940M 1000 <ref type="table">Table 9</ref>: The comparison between different pre-trained classification models. S-S-L, S-L and W-S-L stand for self-supervised learning, supervised learning and weakly-supervised learning respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Why PCB Works ?</head><p>The PCB can be reinterpreted as a non-parameter fewshot classification model, which draws on the idea of Prototypical Network <ref type="bibr" target="#b42">[42]</ref>. Based on the COCO 10-shot task, we calculate the channel-wise cosine similarity between different few-shot RoI prototypes (C ? 1 ? 1) and the feature map (C ? H ? W ) of the test image, and then visualize the similarity map in <ref type="figure">Fig.5</ref>. We find that the prototypes from different categories can indeed activate distinct areas of the feature map, which indicates that the metric-based pairwise score in data-scarce scenario is very effective. In addition, we notice that even if the category label of novel prototype is not seen before by the pre-trained classification model, an ideal similarity map can still be obtained, e.g., the novel label 'Person' does not exist in ImageNet 1K sysnets, see the first three lines in <ref type="figure">Fig.5</ref>. Moreover, the results of IN-SwAV (i.e. self-supervised paradigm) in <ref type="table">Table 10</ref> further prove this argument. According to the visualization and above analysis, we believe that it is reasonable for PCB to utilize the pairwise score based on classification model to calibrate the softmax score from the original classification branch of fewshot detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related extensions of GDL C.1. Conventional Cross-Domain Object Detection</head><p>In the experimental section of the main paper, we have verified that the proposed GDL is not only remarkably effective for few-shot object detection ( i.e., FSOD, G-FSOD and cross-domain FSOD), but also plays a positive role in conventional object detection. In this section, we further explore the conventional cross-domain object detection and all experimental results are shown in <ref type="table">Table 11</ref>. We use the Cityscapes <ref type="bibr" target="#b8">[9]</ref> and FoggyCityscapes <ref type="bibr" target="#b41">[41]</ref> (Normal-to-Foggy) as our benchmarks and follow the same evaluation protocol in <ref type="bibr" target="#b60">[60]</ref>. By comparing the experimental results of the second row and the third row in <ref type="table">Table 11</ref>, we find that adding GDL achieves 32.8% mAP on the weather transfer task, which is +2.8% higher than the plain Faster-RCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. The value range of ?</head><p>We discuss the value range of ? into three situations.</p><p>? ? rpn ? [0, 1] and ? rcnn ? [0, 1]. This setting has been explored in our paper and achieved the best results. ? ? rpn ? (??, 0) or ? rcnn ? (??, 0). ? &lt; 0 means that the downstream module has a negative effect on the optimization direction of backbone. Without any adversarial strategy, this setup is meaningless for object detection. ? ? rpn ? (1, +?) or ? rcnn ? (1, +?). ? &gt; 1 means that the gradient from the downstream module magnifies its effect on the backbone. We notice that slightly increasing ? (e.g. 1 ? 5) will not affect the stability of detector but incite performance degradation, which is caused by the backbone's update speed faster than before and overfitting. When ? is relatively large (e.g. &gt; 5), due to overemphasizing the degree of coupling between the module and the backbone, the model will usually converge to an unreasonable saddle point and cause a collapse solution.</p><p>The value of 5 is obtained by experiments approximately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Visualization of Our Approach</head><p>We provide qualitative visualizations of the detected novel objects on COCO dataset in <ref type="figure">Fig.6</ref>. We show both success (green box) and failure cases (red box) when detecting novel objects for each image to help analyze the possible error types, including misclassifying novel objects, mislocalizing objects and missing detections.     <ref type="table">Table 11</ref>: The performance of conventional cross-domain object detection. All results in the first line refer from <ref type="bibr" target="#b60">[60]</ref> for brief comparison. Note that the Faster R-CNN model trained on the source domain only without any other information (denoted as "Source Only" in other papers). The symbol * indicates the model is re-implemented by us. <ref type="figure">Figure 6</ref>: The visualization results of our 10-shot object detection on COCO dataset. We visualize the bounding boxes with score larger than 0.7. The green and red box shows the success and failure cases of our DeFRCN respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>FSOD performance (mAP) on COCO<ref type="bibr" target="#b25">[26]</ref> novel set at different shot numbers. The proposed DeFRCN is remarkably superior to other state-of-the-art approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The effectiveness of different degree of coupling. The horizontal and vertical axis represent the ? in GDL of RPN and RCNN respectively. Note the results in (b) do not use PCB to ensure the impact of GDL is considered only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Gradient Decoupled Layer, PyTorch-like</figDesc><table><row><cell># A: learnable channel-wise affine layer</cell></row><row><cell># _lambda: gradient decoupling coefficient</cell></row><row><cell>class GradientDecoupledLayer(Function):</cell></row><row><cell># feature forward</cell></row><row><cell>def forward(ctx, x, A, _lambda):</cell></row><row><cell>ctx._lambda = _lambda</cell></row><row><cell>x = A(x)</cell></row><row><cell>return x.view_as(x)</cell></row><row><cell># gradient backward</cell></row><row><cell>def backward(ctx, grad_output):</cell></row><row><cell>grad_output = grad_output * ctx._lambda</cell></row><row><cell>return grad_output, None, None</cell></row></table><note>def decouple_layer(x, A, _lambda): return GradientDecoupleLayer(x, A, _lambda)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>10.7 12.5 24.8 38.6 12.5 4.2 11.6 16.1 33.9 13.0 15.9 15.0 32.2 38.4 FRCN-ft [58] ? 13.8 19.6 32.8 41.5 45.6 7.9 15.3 26.2 31.6 39.1 9.8 11.3 19.1 35.0 45.1 LSTD [6] ? 8.2 1.0 12.4 29.1 38.5 11.4 3.8 5.0 15.7 31.0 12.6 8.5 15.0 27.3 36.3 FSRW [21] ? 14.8 15.5 26.7 33.9 47.2 15.7 15.2 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 MetaDet [51] ? 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 Meta R-CNN [58] ? 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1 TFA [50] ? 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8</figDesc><table><row><cell cols="4">Method / Shots w/G 1</cell><cell cols="3">Novel Set 1 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 2 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 3 2 3 5</cell><cell>10</cell></row><row><cell cols="4">YOLO-ft [21] 6.6 MPSR [52] ? ? 41.7</cell><cell>-</cell><cell cols="4">51.4 55.2 61.8 24.4</cell><cell>-</cell><cell>39.2 39.9 47.8 35.6</cell><cell>-</cell><cell>42.3 48.0 49.7</cell></row><row><cell cols="2">DeFRCN (Ours)</cell><cell cols="8">? 53.6 57.5 61.5 64.1 60.8 30.1 38.1 47.0 53.3 47.9 48.4 50.9 52.3 54.9 57.4</cell></row><row><cell>FRCN-ft [58]</cell><cell></cell><cell>?</cell><cell cols="7">9.9 15.6 21.6 28.0 52.0 9.4 13.8 17.4 21.9 39.7 8.1 13.9 19.0 23.9 44.6</cell></row><row><cell>FSRW [21]</cell><cell></cell><cell cols="8">? 14.2 23.6 29.8 36.5 35.6 12.3 19.6 25.1 31.4 29.8 12.5 21.3 26.8 33.8 31.0</cell></row><row><cell>TFA [50]</cell><cell></cell><cell cols="8">? 25.3 36.4 42.1 47.9 52.8 18.3 27.5 30.9 34.1 39.5 17.9 27.2 34.3 40.8 45.6</cell></row><row><cell cols="2">FSDetView [55]</cell><cell cols="8">? 24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6</cell></row><row><cell cols="2">DeFRCN (Ours)</cell><cell cols="8">? 40.2 53.6 58.2 63.6 66.5 29.5 39.7 43.4 48.1 52.8 35.0 38.3 52.9 57.7 60.8</cell></row><row><cell cols="3">Method / Shots w/G 1</cell><cell>2</cell><cell cols="2">Shot Number 3 5</cell><cell cols="2">10 30</cell><cell></cell></row><row><cell>FRCN-ft [58]</cell><cell cols="7">? 1.0  5 11.1</cell><cell></cell></row><row><cell>FSRW [21]</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">5.6 9.1</cell><cell></cell></row><row><cell>MetaDet [51]</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">7.1 11.3</cell><cell></cell></row><row><cell cols="2">Meta R-CNN [58] ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">8.7 12.4</cell><cell></cell></row><row><cell>TFA [50]</cell><cell cols="7">? 4.4  0 13.7</cell><cell></cell></row><row><cell>MPSR [52]</cell><cell cols="7">? 5.1  8 14.1</cell><cell></cell></row><row><cell>FSDetView [55]</cell><cell>?</cell><cell cols="6">4.5 6.6 7.2 10.7 12.5 14.7</cell><cell></cell></row><row><cell>DeFRCN (Ours)</cell><cell>?</cell><cell cols="6">9.3 12.9 14.8 16.1 18.5 22.6</cell><cell></cell></row><row><cell>FRCN-ft [58]</cell><cell cols="7">? 1.7 3.1 3.7 4.6 5.5 7.4</cell><cell></cell></row><row><cell>TFA [50]</cell><cell cols="7">? 1.9 3.9 5.1 7.0 9.1 12.1</cell><cell></cell></row><row><cell>FSDetView [55]</cell><cell cols="7">? 3.2 4.9 6.7 8.1 10.7 15.9</cell><cell></cell></row><row><cell>DeFRCN (Ours)</cell><cell cols="7">? 4.8 8.5 10.7 13.6 16.8 21.2</cell><cell></cell></row></table><note>Table 1: Experimental results on VOC dataset. We evaluate DeFRCN performance (AP50) on three different splits. The term w/G indicates whether we use the G-FSOD setting [50]. RED/BLUE indicate SOTA/the second best. Note that our results are averaged over multiple runs and the base/overall performance are presented in supplementary materials, the same below.* 1.8* 2.8* 4.0* 6.* 5.4* 6.0* 7.7* 10.* 6.7* 7.4* 8.7* 9.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Experimental results on COCO dataset. We evaluate De- FRCN performance (mAP ) over multiple runs. The superscript* indicates that the results are reproduced by us.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The 10-shot cross-domain FSOD performance on COCO base set ? VOC novel set. All detection results for comparison refer from [21, 52, 58].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>, our ap-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effectiveness of different modules in DeFRCN. All results are conducted on COCO dataset. The GDL-B and GDL-N indicates that we use GDL in base training phase and novel finetuning phase respectively.BackboneAP AP 50 AP 75 AP s AP m AP l</figDesc><table><row><cell cols="2">R50-C4-1x [54] 35.7 56.1 38.0 19.2 40.9 48.7</cell></row><row><cell>+ GDL</cell><cell>36.5 57.6 39.2 19.8 41.7 50.3</cell></row><row><cell cols="2">R101-C4-3x [54] 41.1 61.4 44.0 22.2 45.5 55.9</cell></row><row><cell>+ GDL</cell><cell>41.9 62.3 45.1 22.3 46.6 57.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Conventional object detection results on COCO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>38.83 38.82 38.66 38.39 39.04 39.02 38.67 38.48 38.21 38.53 38.62 38.24 37.73 37.44 37.99 38.19 37.41 36.94 36.43 20.27 32.00 32.05 31.48 31.56 .51 7.47 7.45 8.21 12.94 12.94 12.80 11.93 10.17 16.89 16.84 16.72 14.37 10.31 16.46 16.51 16.42 14.27 9.84 15.76 15.94 16.12 14.20 9.80</figDesc><table><row><cell>1.00 0.75 0.50 0.25 0.00</cell><cell cols="5">25.0 27.5 30.0 32.5 35.0 0.10 0.01 0.001 0.00 39.01 22.5 37.5 1.00</cell><cell cols="2">7.46 7</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0.25</cell><cell>0.50</cell><cell>0.75</cell><cell>1.00</cell><cell>0.00</cell><cell>0.001</cell><cell>0.01</cell><cell>0.10</cell><cell>1.00</cell></row><row><cell cols="6">(a) The base training stage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The visualization of PCB on COCO val set. Through different kinds of prototypes, which are calculated by K-shot samples (K = 10), distinct areas of the same picture are activated. The symbol ? indicates that it is some kind of prototypes.</figDesc><table><row><cell>Original Image Original Image Original Image Original Image Original Image # shots Split 1 1 2 Figure 5: Split</cell><cell>Method FSRW [21] FRCN+ft [58] TFA [50] DeFRCN FSRW [21] FRCN+ft [58] TFA [50] DeFRCN</cell><cell>? Person ? Person ? Person ? Cat ? Bird AP 27.6?0.5 30.2?0.6 40.6?0.5 42.0?0.6 (+1.4) 28.7?0.4 30.5?0.6 42.6?0.3 44.3?0.4 (+1.7)</cell><cell>Overall #20 AP50 50.8 ?0.9 49.4?0.7 64.5?0.6 66.7?0.8 (+2.2) 52.2?0.6 49.4?0.8 67.1?0.4 70.2?0.5 (+3.1)</cell><cell>AP75 26.5?0.6 32.2?0.9 44.7?0.6 45.5?0.7 (+0.8) 27.7?0.5 32.6?0.7 47.0?0.4 48.0?0.6 (+1.0)</cell><cell cols="2">? Motorcycle ? Bird ? Dog ? Motorcycle ? Potted plant Base #15 AP 34.1?0.5 38.2?0.8 49.4?0.4 48.4?0.4 (-1.0) 22.5?1.7 (+8.3) Novel #5 AP 8.0?1.0 6.0?0.7 14.2?1.4 33.9?0.4 13.2?1.0 37.3?0.7 9.9?0.9 49.6?0.3 21.7?1.0 49.1?0.3 (-0.5) 30.6?1.2 (+8.9)</cell></row><row><cell></cell><cell>FRCN+ft [58]</cell><cell>33.1?0.5</cell><cell>53.1?0.7</cell><cell>35.2?0.5</cell><cell>38.0?0.5</cell><cell>18.4?0.8</cell></row><row><cell></cell><cell>TFA [50]</cell><cell>45.0?0.3</cell><cell>70.3?0.4</cell><cell>48.9?0.4</cell><cell>51.6?0.2</cell><cell>25.4?0.7</cell></row><row><cell></cell><cell>DeFRCN</cell><cell>47.0?0.3 (+2.0)</cell><cell>73.3?0.3 (+3.0)</cell><cell>51.0?0.4 (+2.1)</cell><cell>51.3?0.2 (-0.3)</cell><cell>34.7?0.7 (+9.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Generalized few-shot object detection (G-FSOD) performance on PASCAL VOC dataset. For each metric, we report the average and 95% confidence interval computed over 30 random samples. All comparison results refer from<ref type="bibr" target="#b50">[50]</ref>.<ref type="bibr" target="#b23">24</ref>.4?0.6 39.8?0.8 26.1?0.8 31.9?0.7 1.9?0.4 DeFRCN (Ours) 24.0?0.4 (-0.4) 36.9?0.6 (-2.9) 26.2?0.4 (+0.1) 30.4?0.4 (-1.5) 4.8?0.6 (+2.9) Ours) 26.6?0.4 (+1.3) 41.1?0.7 (+0.7) 28.9?0.4 (+1.3) 32.1?0.3 (+0.1) 10.7?0.8 (+5.6) Ours) 27.8?0.3 (+1.9) 43.0?0.6 (+1.8) 30.2?0.3 (+1.8) 32.6?0.3 (+0.3) 13.6?0.7 (+6.6) Ours) 29.7?0.2 (+3.1) 46.0?0.5 (+3.8) 32.1?0.2 (+3.1) 34.0?0.2 (+1.6) 16.8?0.6 (+7.7) Ours) 31.4?0.1 (+2.7) 48.8?0.2 (+4.1) 33.9?0.1 (+2.4) 34.8?0.1 (+0.6) 21.2?0.4 (+9.1)</figDesc><table><row><cell># shots</cell><cell>Method</cell><cell></cell><cell>Overall #80</cell><cell></cell><cell>Base #60</cell><cell>Novel #20</cell></row><row><cell></cell><cell></cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AP</cell><cell>AP</cell></row><row><cell></cell><cell>FRCN+ft [58]</cell><cell>16.2?0.9</cell><cell>25.8?1.2</cell><cell>17.6?1.0</cell><cell>21.0?1.2</cell><cell>1.7?0.2</cell></row><row><cell>1</cell><cell>TFA [50]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FRCN+ft [58]</cell><cell>15.8?0.7</cell><cell>25.0?1.1</cell><cell>17.3?0.7</cell><cell>20.0?0.9</cell><cell>3.1?0.3</cell></row><row><cell>2</cell><cell>TFA [50]</cell><cell>24.9?0.6</cell><cell>40.1?0.9</cell><cell>27.0?0.7</cell><cell>31.9?0.7</cell><cell>3.9?0.4</cell></row><row><cell></cell><cell cols="2">DeFRCN (Ours) 25.7?0.5 (+0.8)</cell><cell cols="2">39.6?0.8 (-0.5) 28.0?0.5 (+1.0)</cell><cell>31.4?0.4 (-0.5)</cell><cell>8.5?0.8 (+4.6)</cell></row><row><cell></cell><cell>FRCN+ft [58]</cell><cell>15.0?0.7</cell><cell>23.9?1.2</cell><cell>16.4?0.7</cell><cell>18.8?0.9</cell><cell>3.7?0.4</cell></row><row><cell>3</cell><cell>TFA [50]</cell><cell>25.3?0.6</cell><cell>40.4?1.0</cell><cell>27.6?0.7</cell><cell>32.0?0.7</cell><cell>5.1?0.6</cell></row><row><cell cols="2">FRCN+ft [58] DeFRCN (5 TFA [50]</cell><cell>14.4?0.8 25.9?0.6</cell><cell>23.0?1.3 41.2?0.9</cell><cell>15.6?0.8 28.4?0.6</cell><cell>17.6?0.9 32.3?0.6</cell><cell>4.6?0.5 7.0?0.7</cell></row><row><cell cols="2">FRCN+ft [58] DeFRCN (10 TFA [50]</cell><cell>13.4?1.0 26.6?0.5</cell><cell>21.8?1.7 42.2?0.8</cell><cell>14.5?0.9 29.0?0.6</cell><cell>16.1?1.0 32.4?0.6</cell><cell>5.5?0.9 9.1?0.5</cell></row><row><cell cols="2">FRCN+ft [58] DeFRCN (30 TFA [50]</cell><cell>13.5?1.0 28.7?0.4</cell><cell>21.8?1.9 44.7?0.7</cell><cell>14.5?1.0 31.5?0.4</cell><cell>15.6?1.0 34.2?0.4</cell><cell>7.4?1.1 12.1?0.4</cell></row><row><cell></cell><cell>DeFRCN (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Generalized few-shot object detection (G-FSOD) performance on COCO dataset. For each metric, we report the average and 95% confidence interval computed over 10 random samples. All comparison results refer from<ref type="bibr" target="#b50">[50]</ref>. (+2.7) 11.8 (+2.8) 15.5 (+2.1) (+2.2) 11.9 (+2.1) 15.5 (+1.0) (+1.4) 12.9 (+2.0) 14.8 (+1.4) 16.1 (+1.5) 18.5 (+1.6) 22.6 (+1.<ref type="bibr" target="#b5">6)</ref> </figDesc><table><row><cell># shots</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :Table 10 :</head><label>810</label><figDesc>Effectiveness of Prototypical Calibration Block with different approaches. We evaluate FSOD performance (mAP ) on COCO dataset with K = 1, 2, 3, 5, 10, 30 shots over multiple runs. All experimental results are reproduced by us. The term w/PCB indicates whether the method uses the PCB module. Note that the ? in PCB is set to 0.5 in all experiments.<ref type="bibr" target="#b48">48</ref>.8 52.3 57.1 55.6 22.5 31.9 42.1 45.6 42.3 42.5 48.7 48.9 51.1 52.2 IN-SwAV [5] 48.7 52.4 54.5 60.2 56.3 26.9 34.6 44.6 48.1 44.7 41.8 50.1 50.5 53.4 55.1 IN-1K [19] 53.6 57.5 61.5 64.1 60.8 30.1 38.1 47.0 53.3 47.9 48.4 50.9 52.3 54.9 57.4 DeFRCN IG-WSL [28] 62.3 64.5 66.6 69.3 68.2 37.5 44.7 53.5 57.6 54.7 54.7 57.2 59.0 60.9 62.0 Experimental results of employing different pre-trained model in PCB on PASCAL VOC dataset. All reported results are averaged over 30 random samples. IN-SwAV, IN-1K and INS-WSL denote the different pre-trained models from ImageNet self-supervised learning, conventional supervised learning and weakly-supervised learning separately. 32.9 (+1.4) 38.4 (-0.9) 47.3 (+2.1) 26.6 (+1.9) 34.3 (-1.0) 41.4 (+0.2) 17.3 (+8.5) 24.3 (+7.6) 32.8 (+2.8)</figDesc><table><row><cell>Method</cell><cell>Model</cell><cell>1</cell><cell>Novel Set 1 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 2 2 3 5</cell><cell>10</cell><cell>1</cell><cell>Novel Set 3 2 3 5</cell><cell>10</cell></row><row><cell></cell><cell>?</cell><cell>47.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This paper is supported by the National Key R&amp;D Plan of the Ministry of Science and Technology (Project No. 2020AAA0104400).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="384" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="453" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1641" to="1654" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fgn: Fully guided network for few-shot instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9172" to="9181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed; Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Dhruv Kumar Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incremental few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13846" to="13855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Any-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fewshot adaptive faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7173" to="7182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-scale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking classification and localization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10186" to="10195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Few-shot object detection with self-adaptive attention network for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J-STARS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4854" to="4865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Context-transformer: tackling object confusion for few-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12653" to="12660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cross-domain object detection through coarse-to-fine feature adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
