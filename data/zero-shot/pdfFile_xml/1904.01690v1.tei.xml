<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
							<email>alex.pon@mail.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
							<email>stevenw@utias.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present MonoPSR, a monocular 3D object detection method that leverages proposals and shape reconstruction. First, using the fundamental relations of a pinhole camera model, detections from a mature 2D object detector are used to generate a 3D proposal per object in a scene. The 3D location of these proposals prove to be quite accurate, which greatly reduces the difficulty of regressing the final 3D bounding box detection. Simultaneously, a point cloud is predicted in an object centered coordinate system to learn local scale and shape information. However, the key challenge is how to exploit shape information to guide 3D localization. As such, we devise aggregate losses, including a novel projection alignment loss, to jointly optimize these tasks in the neural network to improve 3D localization accuracy. We validate our method on the KITTI benchmark where we set new state-of-the-art results among published monocular methods, including the harder pedestrian and cyclist classes, while maintaining efficient run-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A cornerstone of 3D scene understanding in computer vision is 3D object detection-the task where objects of interest within a scene are classified and identified by their 6 DoF pose and dimensions. Existing methods vary in the data they use, which include LiDAR <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>, stereo images <ref type="bibr" target="#b4">[5]</ref>, and monocular images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">40]</ref>. Monocular methods are attractive as they have the lowest cost and the simplest setup, relying on only a single camera. These methods are therefore attractive for applications where resources are limited, or for companies wanting to bring 3D object detection to mass markets such as autonomous navigation and virtual reality.</p><p>Monocular 3D object detection methods are also the most disadvantaged; the problem formulation is underconstrained because depth information is lost when a 3D * Equal contribution. scene is projected onto an image plane. The difficulty of the problem is highlighted on the KITTI 3D Object Detection benchmark <ref type="bibr" target="#b11">[12]</ref> in the car category where the best published monocular method <ref type="bibr" target="#b15">[16]</ref> has an AP value 67% lower than the best published method using LiDAR <ref type="bibr" target="#b42">[42]</ref>. Results for the more challenging pedestrian and cyclist classes are rarely reported for monocular methods, likely due to even poorer performance.</p><p>To deal with the under-constrained monocular object detection problem, recent methods have typically used deep learning with well-informed priors. One such prior is that the predicted 3D bounding box should fit tightly within its corresponding 2D bounding box <ref type="bibr" target="#b23">[24]</ref>. This assumption, however, leads to localization inaccuracies because it causes 2D bounding box, orientation, and dimension estimation errors to propagate to the final 3D box prediction. Other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> match objects to CAD models to determine shape and pose. Unfortunately, these methods are restricted to the shape space covered by the selected CAD models, and do not easily extend to applications where models are un-available. Lastly, all state-of-the-art methods under-utilize the information available during training. Although depth maps or LiDAR scans are available because they are required to create 3D labels, only <ref type="bibr" target="#b40">[40]</ref> incorporates this form of depth information during training, but neglect to exploit the strong priors that can be formed from 2D bounding boxes, such as the ones used by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>. This paper introduces a proposal based monocular 3D object detection method that leverages the related task of shape reconstruction. We first greatly reduce the 3D search space by exploiting the robust performance of a 2D object detector by designing a non-restrictive 3D bounding box proposal per object detected in the scene. The location of the proposal is determined by considering the re-projection of the box center, and the relation between the height and depth of an object in the perspective transformation of a pinhole camera model. Compared to the 2D box constraint proposed by <ref type="bibr" target="#b23">[24]</ref>, our usage of a 2D bounding box does not lock in 2D box, orientation and dimensions inaccuracies. Instead, we use a two stage proposal regression design, in a similar manner to Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, which facilitates learning by regressing distributed anchor boxes, to obtain the final amodal, oriented 3D bounding box. We find this prior is flexible and suitable for classes that have varying dimensions and poses such as pedestrians and cyclists.</p><p>We also incorporate an Instance Reconstruction module that predicts a point cloud for each instance in a canonical object (local) coordinate system. However, it is not obvious how to gain localization information from this estimated object point cloud. In our formulation, we connect the tasks of object detection and shape reconstruction by transforming the object point cloud into the camera coordinate frame using the instance centroid regression output. We finally jointly optimize the local scale and shape of each instance with its localization in the scene through multi-task learning and a novel projection alignment loss. This loss projects the object point cloud to image space and enforces 2D-3D consistency between the object point cloud and the image. The outputs for the proposal and shape reconstruction tasks are also designed to have smaller ranges, which has been shown to make learning tasks easier in various applications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30</ref>]. An overview is provided in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We validate our method on the KITTI 3D Object Detection benchmark <ref type="bibr" target="#b11">[12]</ref> on the car, pedestrian, and cyclist categories, and perform extensive ablation studies to evaluate our design choices. In summary, our key contributions are: a) an effective non-restrictive incorporation of a 2D bounding box prior to generate high quality 3D centroid proposals; b) an instance reconstruction module, which helps in recovering the shape and localization of objects; c) a novel loss formulation to jointly optimize point cloud estimates in both object and camera coordinate frames to enforce consistency between the 2D and 3D estimations. Furthermore, we are the first to propose a learning method that jointly optimizes point cloud reconstruction and observation consistency to achieve accurate 3D localization.</p><p>These contributions lead to state-of-the-art results on the KITTI 3D Object Detection benchmark where we achieve a 68% increase over the previous monocular state-of-the-art <ref type="bibr" target="#b15">[16]</ref>. In addition, we are the first to publish 3D pedestrian and cyclist results on the test benchmark and achieve highly promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Proposal Based Methods Many successful 2D object detectors employ the use of proposals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b45">45]</ref> to generate candidate object positions. This idea is extended into 3D scenes by methods such as Mono3D <ref type="bibr" target="#b3">[4]</ref> and 3DOP <ref type="bibr" target="#b4">[5]</ref> which generate a large number of 3D proposals along an estimated ground plane, which are then projected to the image and scored by hand-crafted semantic, contextual, and shape features. Using 2D detections to reduce the search space in 3D has also shown promise because 2D detection is a mature field with robust performance; <ref type="bibr" target="#b27">[28]</ref> even suggests 2D detectors are accurate enough that detectors can be trained using data it inferences. F-PointNet <ref type="bibr" target="#b24">[25]</ref> lifts 2D detections to frustums and uses PointNets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> on these frustum points to regress 3D detections. In our monocular case, LiDAR point cloud information is not available. We instead use a pinhole model and leverage the relation of the 2D bounding box height and estimated object height to create centroid proposals that are regressed in the second stage of our network.</p><p>Geometric Priors Geometry can be used as a prior to constrain the object detection problem. Deep3DBox <ref type="bibr" target="#b23">[24]</ref> uses a neural network to predict the dimensions and pose of an object, then use a linear system of equations to enforce a constraint that the projected 3D bounding box must fit tightly in the 2D box. However, this hard constraint locks in errors from 2D bounding box, orientation, and dimension estimates when producing the 3D box. A3DODWTDA <ref type="bibr" target="#b15">[16]</ref> instead estimate the image coordinates of the 3D bounding box corners, and solve a non-linear least squares fit for the best 3D box. While these methods work well for objects that maintain a constant shape like cars, solving for the projected 3D bounding box corners is harder for classes such as pedestrians; the 3D box dimensions and the corresponding projection of its corners vary greatly based on the skeletal pose of the person. In contrast, our formulation is less restrictive and does not lock in 2D bounding box, orientation, and dimension errors. While we still use geometry to generate a proposal, errors can be corrected in a later regression stage. Shape Reconstruction Large-scale synthetic datasets such as ShapeNet <ref type="bibr" target="#b2">[3]</ref> have allowed deep neural networks to be trained on the task of shape reconstruction from single images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38]</ref>. While shape reconstruction can be considered a separate problem, understanding shape can be beneficial for 3D object detection. Chabot et al. <ref type="bibr" target="#b1">[2]</ref> use a network to output a 2D bounding box, vehicle part coordinates, and 3D box dimensions. They then match the dimensions to a CAD model and estimate pose using the matched model and the predicted vehicle parts. Kundu et al. <ref type="bibr" target="#b20">[21]</ref> match CAD models using the shape space created by a set of CAD models, and train the network with a render-and-compare loss. While these methods have been shown to improve pose estimation, they require annotated datasets of 3D models for training, and can introduce error from CAD model mismatch. We instead devise a more flexible, class-agnostic solution that works with object point clouds directly and automatically generates relevant shape data from real-world LiDAR data to facilitate local shape learning.</p><p>Moreover, methods involving shape completion <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">32]</ref> demonstrate the importance of enforcing consistency between the 3D estimations and 2D observations. Examples of differentiable 2D-3D consistency constraints for training deep networks are introduced in <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref>. The monocular 3D object detection methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref> loosely capture 2D-3D consistency when using geometric constraints with 2D boxes and the corners of 3D boxes. However, they ignore the shape of the object within the box. We, on the other hand, use the 3D point cloud of the object, and enforce 2D-3D consistency through a differentiable pixelwise projection alignment loss.</p><p>Depth Prediction Recently, deep learning methods have shown significant improvement on the task of monocular depth prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. MultiFusion <ref type="bibr" target="#b40">[40]</ref> uses depth prediction outputs from MonoDepth <ref type="bibr" target="#b14">[15]</ref> and fuses this information with the corresponding RGB image to produce 3D bounding box estimates through a modified Faster R-CNN network. As opposed to predicting the depth of the entire scene, we use an instance-centric focus to make the task easier by avoiding regressing large depth ranges. We capture both shape and depth information in our formulation which predicts a point cloud in a local object frame then transforms it into the camera coordinate frame. In addition, we take a multi-task learning approach by sharing feature extractor weights used for box regression and point cloud estimation, and we combine our depth estimates with a well informed 2D box prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Monocular 3D Detection Framework</head><p>Given an M ? N image I, the objective is to classify and localize objects of interest by fitting a 3D bounding box parameterized by its class</p><formula xml:id="formula_0">C, centroid T = (t x , t y , t z ), di- mensions D = (d x , d y , d z ), and orientation O = (?, ?, ?).</formula><p>The core idea of our method is to reduce the search space by using a single high-quality proposal per object and to leverage shape reconstruction for accurate localization.</p><p>We take advantage of the robust performance of existing 2D detectors to generate classified 2D bounding boxes. Using these boxes, image crops are passed through an encoder to output a feature map shared by the three downstream modules shown in <ref type="figure" target="#fig_1">Fig. 2</ref>: Proposal Generation, Proposal Refinement, and Instance Reconstruction. Full image features are included to provide additional scene context, and further improves results as shown in Sec. 5.2.</p><p>The objective of the Proposal Generation module is to generate high quality 3D proposals parameterized by their centroids P = (p x , p y , p z ). The Proposal Refinement module regresses these proposals and outputs amodal, oriented 3D bounding boxes. The Instance Reconstruction module estimates a point cloud per instance in a local object coordinate frame, then transforms it into the camera coordinate frame using the centroid regression from the Proposal Refinement module. These modules are jointly optimized with a projection alignment loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposal Generation</head><p>Features are extracted from the image using two encoders. The first encoder extracts features from the resized RGB crops of each instance detected by the 2D detector. The second encoder extracts features from the full image resized to half of its original size, and then RoI pooling <ref type="bibr" target="#b13">[14]</ref> is used to crop a feature map for each instance. A shared feature map is generated through the depth-wise concatenation of both feature maps.</p><p>Using the shared feature map, a proposal is generated per detected 2D bounding box as follows. For each 2D box, the orientation and dimensions are first estimated. The proposal depth is initialized from the perspective transformation relation of the object height in 3D and its projected height in image space. Lastly, the vertical and horizontal location are predicted, which are a function of the initialized depth and the horizontal and vertical viewing angles, ? h and ? v , which are the rotations between the camera principle axis [0, 0, 1] T and the ray passing through the center of the 2D box.</p><p>Orientation and Dimensions In the KITTI benchmark implementation of this network, only the observation angle ? is estimated, which is the sum of the viewing angle ? h and object yaw ?. As explained in <ref type="bibr" target="#b23">[24]</ref>, the estimation of an object's observation angle, instead of yaw, accounts for the change in appearance based on the viewing angle to the object. Both ? and ? are assumed to be zero in the KITTI benchmark, although the network can be extended to estimate them. The observation angle is estimated using discrete bins as in <ref type="bibr" target="#b24">[25]</ref>. Dimensions offsets, (?d x , ?d y , ?d z ), are predicted from the mean class sizes. Using discrete bins and predicting offsets from mean sizes facilitates orientation and dimension learning by restricting values to be within a smaller range. These estimations are predicted early in the network to be made available for proposal initialization and refinement. Proposal Depth The depth of an object is the most challenging parameter to estimate due to the large range in expected values, which can range from 5 m to 80 m on the KITTI dataset, and the fact that this information is lost during perspective projection. The classical pinhole camera model provides a relation between object height h, depth from the image plane t z , focal length f , and projected height on the image plane? through similar triangles,</p><formula xml:id="formula_1">t z = f ? h .</formula><p>(1)</p><p>We can use this relation with object height estimates to predict the proposal depth p z . It is important to note that the height, h, and the actual object height, d y , are rarely equivalent due to perspective projection and camera viewpoint. However, for a camera with a viewpoint approximately parallel to the ground using this approximation provides a reasonable initial depth estimation. The depth of the proposal is initialized to the value calculated from Eq. 1, and we show in Sec. 5 that this initialization provides a more accurate estimate of an object's depth compared to a direct estimation from the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposal Refinement</head><p>The Proposal Refinement module further regresses the proposal location initialized in the previous stage of the network. Mousavian et al. <ref type="bibr" target="#b23">[24]</ref> choose to regress box dimensions D, rather than the translations, T , because there is smaller variance in box dimensions, which improves regression performance. We also make the learning task easier by regressing values within a small range, but since we generate an accurate proposal in the previous step, we formulate the refinement regression as an offset from the proposal, which we show provides better results than directly estimating depth in Sec. 5. The proposal depth error can be modeled as a function s in,</p><formula xml:id="formula_2">t z = f d y b h + s(?, ?, ?, D, ? v , ? h )<label>(2)</label></formula><p>where s is dependent on the rotation of the object, its dimensions, and the viewing angles ? v , ? h . The first term of Eq. 2 is the proposal depth p z which uses object height d y as h and the 2D bounding box height b h as?. We also confirm that learning the regression from the proposal, as opposed to a direct estimation, provides a more stable error in depth in Sec. 5.</p><p>In the network, the shared feature maps from the feature extractor are flattened and concatenated with the proposed centroid, 2D box size, dimensions, and orientation. This feature vector is then passed through two sets of fully connected layers that output the translation regression targets (?t y , ?t z ). Instead of regressing ?t x , which would be overconstraining the problem, t x is recovered using the ray from the camera center to the estimated 3D centroid of the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Instance Reconstruction</head><p>The Instance Reconstruction module takes advantage of the available LiDAR data during training to learn shape and scale information. We encode shape and scale information through a predicted local point cloud p O = {p i ? R 3 , i = 1, . . . , K}, with K as the number of points, and facilitate the learning task by using an object coordinate frame as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Using local coordinate frames have recently been shown to be effective in applications of 3D scene understanding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>. In contrast to MultiFusion <ref type="bibr" target="#b40">[40]</ref> where a global depth map is estimated up to 70 m, the point cloud in the object coordinate frame only contains values up to the size of the objects. Moreover, the rich point cloud representation is able to estimate instance shape and size while allowing for variations in 3D shape.</p><p>We next note that the tasks of point cloud estimation and object localization are closely related. The estimated centroid and horizontal viewing angle are used to transform the predicted point cloud into the camera coordinate system. The predictions of both the local point cloud and the object position should be consistent with the object's appearance in image space. It is therefore intuitive to optimize these tasks jointly. This is achieved through a Z-channel loss and a projection alignment error loss, which penalizes the misalignment of the instance point cloud projected back into the image.</p><p>Generating 3D Instance Training Data The sparse Li-DAR scan is first interpolated using <ref type="bibr" target="#b18">[19]</ref> and converted into a 3 channel tensor J using the provided camera calibration, with each pixel representing a point (x, y, z). Points within ground truth boxes are considered as the set of points for an instance. These points are used to generate three additional sets of ground truth. First, the projection of these points in image space provides instance segmentation masks. Second, the third channel of J corresponds to the instance depth map, which is masked and resized to the RGB crop size. Third, the local point cloud is generated. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the rotation matrix R ? h ? SO(3) is calculated using the horizontal viewing angle, and an object's point cloud in the camera coordinate frame is calculated in homogeneous coordinates as p C = T CO * p O , where T CO is the transformation matrix</p><formula xml:id="formula_3">T CO = R ? h T 0 1 ,<label>(3)</label></formula><p>and T is the ground truth object centroid. The local point cloud is calculated by reversing this transformation. Note that we resize and encode the local and global instance point clouds as three channel tensors L ? W ? 3. Note, the segmented LiDAR input is not required during inference and is only used to generate training data.</p><p>Point Cloud Estimation The shared feature maps are fed into a small decoder network that produces the local L ? W ? 3 point clouds, with each grid element representing a point in the object coordinate system. Valid points are obtained by applying the automatically generated instance segmentation mask corresponding to the visible portion of the object. The predicted point clouds provide a rich form of 3D scene understanding not captured by methods that only output 3D bounding boxes, and avoids the inflexibility of a CAD dataset that certain methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> require. This more flexible formulation should allow for generalization to a wide variety of objects. The network can also be trained to additionally output an L ? W segmentation mask to allow the object point clouds to be used as signals for other tasks such as geometric appearance tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses</head><p>The network is trained using a multi-task loss defined by</p><formula xml:id="formula_4">L total = L t + L ? + L dim + L c .<label>(4)</label></formula><p>where L t is the regression loss for the difference between the true centroid and proposal centroid, L ? is for the object's orientation, L dim is the offset regression loss for the bounding box dimensions, and L c represents the Instance Reconstruction losses described in the next section. The orientation loss, L ? , is formulated as in <ref type="bibr" target="#b24">[25]</ref> with a classification loss for the discrete angle bins and a regression loss for the angle bin offsets. All regression losses use the smooth L1 loss and all classification losses use the softmax loss. Each loss is weighed such that validation losses converge to values with approximately the same magnitude. <ref type="figure">Fig. 4</ref> shows the three losses used in the Instance Reconstruction module. The instance point cloud is represented as an L ? W ? 3 grid of points. Valid points, selected from the generated segmentation masks, are trained with a smooth L1 loss. The joint localization loss is composed of two parts, a Z-channel loss and a projection alignment loss. The local point cloud is transformed from its object coordinate frame to the camera coordinate frame using the transform T CO from Eq. 3. The last channel of the transformed point cloud corresponds to the scene's depth map located at the instance, and is trained with a smooth L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Instance Reconstruction Losses</head><p>The global instance point cloud can be projected into the image. From the generation process explained in Sec. 3.3 each projected point has a known image coordinate location within the 2D bounding box. The projected image coordinates H are calculated using H = ?p C where ? is the camera projection matrix. The projection alignment error is calculated with the expected coordinates G as E proj = |G ? H|, and trained using a smooth L1 loss with regression targets of 0 at valid pixel locations. The projection error values are normalized by the width and height of the 2D bounding box. This loss penalizes the misalignment when the point cloud is projected to the image, which enforces the consistency between the 3D estimation and 2D appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>We use MS-CNN <ref type="bibr" target="#b0">[1]</ref> as our 2D detector for fair comparison with <ref type="bibr" target="#b23">[24]</ref>. To facilitate faster convergence, the full scene and instance convolution encoders are initialized with ResNet-101 weights before conv5 1 pre-trained on the task of 2D object detection on KITTI. The full image is resized to 160 ? 608, and each instance is cropped and resized into a 48 ? 48 ? 3 image from the RGB image. The feature extractor output stride is set to 4, resulting in a final layer feature map with resolution 12 ? 12. A flattened version of this map is passed into two fully connected layers for box regression and orientation estimation. The local point cloud is generated from the small decoder network consisting of repeated upsampling and convolutional layers, resulting in a feature map of the original 48 ? 48 resolution, after which a 48 ? 48 ? 3 point cloud p l is outputted through a 3 ? 3 convolutional layer. The network is trained using an Adam optimizer for 100K iterations with an initial learning rate of 0.0008 and decay factor of 0.8 every 10K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present results on the challenging KITTI 3D Object Detection benchmark where we compare with the current state-of-the-art monocular methods, validate design choices through ablation studies, and present qualitative results. <ref type="figure">Figure 4</ref>. Instance Reconstruction Losses: Losses for the corresponding predictions (red) and ground truth (green). All penalties use the smooth L1 loss at valid pixel locations using automatically generated segmentation masks. First, the point cloud loss penalizes the instance point cloud along each channel (x, y, z). The point cloud is then placed at its estimated location in the camera coordinate frame using TCO, the transformation between object and camera coordinate frames, and penalized in the last channel z. Finally, the point cloud is projected into image space with ?, the camera projection matrix. A projection alignment loss penalizes points projected into the wrong image pixel location.</p><p>Two validation splits are used to compare against previous methods. The first split val1 follows <ref type="bibr" target="#b5">[6]</ref> and the second split val2 follows <ref type="bibr" target="#b39">[39]</ref>. The ablation studies were performed on val1. As with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, a separate training split is used for better generalization on the test results. Training is done with a batch of up to 32 ground truth 2D boxes from the same image, while all inference and evaluation is done using the 2D detection boxes from MS-CNN <ref type="bibr" target="#b0">[1]</ref>. The only data augmentation used during training is 2D box jittering to simulate 2D detections. Specifically, Gaussian noise was added to the dimensions and center of the 2D box, scaled by the size of the box, while keeping a minimum 0.7 IoU overlap with the original ground truth box. For evaluation, the KITTI easy, moderate, and hard difficulty classifications are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">AP Comparison with State-of-the-Art Methods</head><p>We compare our approach with previous monocular state-of-the-art methods on the tasks of 3D localization and 3D detection, using the Bird's Eye View (BEV) and 3D AP metrics, on the KITTI validation splits in Tab. 1 and Tab. 2, respectively. The results of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> are taken from <ref type="bibr" target="#b40">[40]</ref>. We also submit our detections to the KITTI test server for evaluation, with the results shown in Tab. 3. The results demonstrate that our method outperforms the previous state-of- the-art by a significant margin while maintaining efficient runtime. The total inference time for the network is 120ms on a Titan X GPU, which is in addition to the 2D detector which takes 80ms. We are also the first monocular 3D object detection method to publish pedestrian and cyclist results on the KITTI 3D Object Detection benchmark. Highly promising results on the test and validation sets are shown in Tab. 4 and Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effect of Proposals and Full Image Features</head><p>Tab. 6 investigates the effect of the proposal formulation described in Sec. 3.1 and the importance of full image features for learning scene context. The focus of the proposal is to improve depth estimation, so the metrics used are average depth error and standard deviation.</p><p>To show the viability of using a single proposal per detection, we train a model to directly estimate the object depth with full image features appended, and two models that predict regressions from proposals, one that regresses using only instance features and the other with full image features appended. The model trained for direct depth estimation performs worse than the proposal regression networks. The first two rows of Tab. 6 show that proposal regression leads to more accurate centroid depth than the direct estimation method. The use of our proposals is further justified as they lead to a more stable error with lower standard deviation. The last row shows that the use of full image features provides useful cues for centroid depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Instance Reconstruction Analysis</head><p>We analyze the effect of the Instance Reconstruction module in Tab. 7. The baseline network is taken as the proposal regression network using the full image from Sec. 5.2.  We train three additional networks, and show the effect of each additional loss. The results show that the estimation of a local point cloud and its depth map are useful tasks for 3D object detection. The final row shows that the joint optimization of the losses through the projection alignment further helps the learning procedure, increasing AP 3D performance at 0.5 IoU by 7.2% over the baseline. <ref type="figure" target="#fig_4">Fig. 5</ref> shows qualitative detection results on scenes from the KITTI dataset. It can be noted that the projection of the 3D boxes of cars and cyclists often match the 2D detection boxes closely. However, for pedestrians, the projection of the 3D boxes varies greatly from the 2D box, and constraining the 3D box to fit the 2D box would result in poor localization. In our formulation, the less restrictive proposal regression method allows for accurate localization of different objects in the scene, including pedestrians. In addition,  <ref type="table">Table 7</ref>. Instance Reconstruction Analysis: The effect of the local point cloud estimation and the depth and projection losses. Results are evaluated for AP3D at 0.5 IoU on the moderate car category. the estimated instance point clouds shown in the 3D view appear consistent with their appearance in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>To conclude, this work presents a monocular 3D object detection method that uses accurate proposals to reduce the search space and leverages shape reconstruction through a point cloud. Object centroid estimation is formulated as an offset regression from proposals generated by a well informed 2D bounding box prior, and object scale and shape is encoded through a predicted point cloud in a canonical object coordinate frame. Accurate localization is achieved through joint optimization of these tasks through a depth map and projection alignment loss. These innovations lead to state-of-the-art results on the KITTI 3D Object Detection benchmark while maintaining efficient runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Additional Qualitative Results</head><p>In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show additional detection results on several scenes in the KITTI <ref type="bibr" target="#b11">[12]</ref> validation split, val1 <ref type="bibr" target="#b5">[6]</ref>. <ref type="figure" target="#fig_6">Fig. 7</ref> shows common failure cases for the network which are noted when there is heavy occlusion or truncation of the object. Additional results on several KITTI driving sequences can be found at https://youtu.be/ iJpEpXB7j4  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Pipeline for 3D Object Detection and Instance Point Cloud Estimation: Our network takes an image with 2D bounding boxes and regresses instance-centric 3D proposals to produce 3D bounding boxes. Simultaneously, instance point clouds are estimated to recover local shape and scale, and to enforce 2D-3D consistency. The proposal regression and point cloud estimation are trained jointly in the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Network Architecture: The network produces a feature map using an image crop of an object and global context features as inputs. From this feature map three tasks are performed a) the dimensions and orientation are predicted to estimate a proposal b) offsets for the proposals are regressed c) local point clouds are predicted and transformed into the global frame for auxiliary loss calculations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposal</head><label></label><figDesc>Vertical and Horizontal Location The horizontal and vertical location (p x , p y ) of the proposal is determined by re-projecting the center coordinate of the 2D box (u c , v c ) into 3D space at depth z c = p z using the camera calibration. The corresponding 3D point in camera coordinates is (x c , y c , z c ) where x c and y c are the horizontal and vertical locations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Object Coordinate System: The instance point cloud is predicted within an object coordinate system created by translating the origin by the object centroid and rotating by the horizontal viewing angle ? h .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative detection results on several scenes in the KITTI dataset. 2D detections (top) are shown in orange. 3D detections in green are shown projected into the image (top) and in the 3D scene (bottom). Ground truth 3D boxes (bottom) are shown in red. Points within the detection boxes are the estimated point clouds from the network, while the background points are taken from the colorized interpolated LiDAR scan. Note that for pedestrians in particular, the projected 3D boxes do not fit tightly within their 2D box, so constraining the 3D box with the 2D box is not ideal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Additional Qualitative Results: 2D detections (top) are shown in orange. 3D detections in green are shown projected into the image (top) and in the 3D scene (bottom). Ground truth 3D boxes (bottom) are shown in red. Points within the detection boxes are the estimated point clouds from the network, while the background points are taken from the colorized interpolated LiDAR scan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Common Failure Cases: Truncation and occlusion are two common causes of localization errors. Truncation is evident for the far left car in the first image, and occlusion is common in large groups of pedestrians walking together as shown in the second image. 2D detections (top) are shown in orange. 3D detections in green are shown projected into the image (top) and in the 3D scene (bottom). Ground truth 3D boxes (bottom) are shown in red. Points within the detection boxes are the estimated point clouds from the network, while the background points are taken from the colorized interpolated LiDAR scan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>/ 54.18 36.73 / 38.06 31.27 / 31.46 22.03 / 19.20 13.63 / 12.17 11.60 / 10.89 Ours 56.97 / 55.45 43.39 / 43.31 36.00 / 35.47 20.63 / 21.52 18.67 / 18.90 14.45 / 14.94 Table 1. 3D Localization: APBEV on KITTI val1/val2 sets. / 48.89 41.71 / 40.93 29.95 / 33.43 12.75 / 13.94 11.48 / 12.24 8.59 / 10.77 Table 2. 3D Detection: AP3D on KITTI val1/val2 sets. 3D Car Localization and Detection: APBEV and AP3D AP on KITTI test set. 3D Pedestrian and Cyclist Detection: APBEV and AP3D for the pedestrian and cyclist classes on the KITTI test split. No other published method currently has results on the test server.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.5 IoU</cell><cell></cell><cell></cell><cell>0.7 IoU</cell></row><row><cell></cell><cell></cell><cell>Easy</cell><cell cols="3">Moderate</cell><cell>Hard</cell><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>Mono3D [4]</cell><cell></cell><cell>30.50 / -</cell><cell cols="3">22.39 / -</cell><cell>19.16 / -</cell><cell></cell><cell>5.22 / -</cell><cell>5.19 / -</cell><cell>4.13 / -</cell></row><row><cell>Deep3DBox [24]</cell><cell></cell><cell cols="2">-/ 30.02</cell><cell cols="2">-/ 23.77</cell><cell cols="2">-/ 18.83</cell><cell>-/ 9.99</cell><cell>-/ 7.71</cell><cell>-/ 5.30</cell></row><row><cell cols="2">A3DODWTDA [16]</cell><cell>45.46 / -</cell><cell cols="3">33.83 / -</cell><cell>31.78 / -</cell><cell></cell><cell>15.64 / -</cell><cell>12.90 / -</cell><cell>12.30 / -</cell></row><row><cell cols="3">MultiFusion [40] 55.02 Method</cell><cell></cell><cell></cell><cell>0.5 IoU</cell><cell></cell><cell></cell><cell>0.7 IoU</cell></row><row><cell></cell><cell></cell><cell>Easy</cell><cell></cell><cell cols="2">Moderate</cell><cell>Hard</cell><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>Mono3D [4]</cell><cell></cell><cell>25.19 / -</cell><cell cols="3">18.20 / -</cell><cell cols="2">15.52 / -</cell><cell>2.53 / -</cell><cell>2.31 / -</cell><cell>2.31 / -</cell></row><row><cell>Deep3DBox [24]</cell><cell></cell><cell cols="2">-/ 27.04</cell><cell cols="2">-/ 20.55</cell><cell cols="2">-/ 15.88</cell><cell>-/ 5.85</cell><cell>-/ 4.10</cell><cell>-/ 3.84</cell></row><row><cell cols="2">A3DODWTDA [16]</cell><cell>40.31 / -</cell><cell cols="3">30.77 / -</cell><cell cols="2">26.55 / -</cell><cell>10.13 / -</cell><cell>8.32 / -</cell><cell>8.20 / -</cell></row><row><cell>MultiFusion [40]</cell><cell></cell><cell cols="6">47.88 / 44.57 29.48 / 30.03 26.44 / 23.95</cell><cell>10.53 / 7.85</cell><cell>5.69 / 5.39</cell><cell>5.39 / 4.73</cell></row><row><cell>Ours</cell><cell cols="2">49.65 Method</cell><cell></cell><cell></cell><cell></cell><cell>BEV AP</cell><cell></cell><cell>3D AP</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Runtime Easy Moderate Hard</cell><cell>Easy Moderate Hard</cell></row><row><cell cols="3">MultiFusion [40]</cell><cell cols="2">0.12</cell><cell>13.73</cell><cell>9.62</cell><cell cols="2">8.22</cell><cell>7.08</cell><cell>5.18</cell><cell>4.68</cell></row><row><cell cols="3">A3DODWTDA [16]</cell><cell cols="2">0.80</cell><cell>10.21</cell><cell>10.61</cell><cell cols="2">8.64</cell><cell>6.76</cell><cell>6.45</cell><cell>4.87</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell cols="2">0.20</cell><cell>20.25</cell><cell>17.66</cell><cell cols="2">15.78</cell><cell>12.57</cell><cell>10.85</cell><cell>9.06</cell></row><row><cell></cell><cell></cell><cell>Metric</cell><cell></cell><cell></cell><cell>Pedestrians</cell><cell></cell><cell></cell><cell>Cyclists</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Easy Moderate Hard</cell><cell cols="2">Easy Moderate Hard</cell></row><row><cell></cell><cell></cell><cell>BEV AP</cell><cell>14.27</cell><cell></cell><cell>11.22</cell><cell>10.54</cell><cell>14.75</cell><cell>12.17</cell><cell>11.35</cell></row><row><cell></cell><cell></cell><cell>3D AP</cell><cell>12.65</cell><cell></cell><cell>10.66</cell><cell>10.08</cell><cell>13.43</cell><cell>11.01</cell><cell>9.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 5 .</head><label>5</label><figDesc>3D Pedestrian and Cyclist Detection: APBEV and AP3D for the pedestrian and cyclist classes on KITTI val1 set.</figDesc><table><row><cell>Class</cell><cell>Metric</cell><cell>IoU</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>Ped.</cell><cell>BEV AP 3D</cell><cell>0.25 0.5 0.25 0.5</cell><cell>32.54 11.68 31.89 10.64</cell><cell>28.92 10.05 28.23 8.18</cell><cell>24.32 8.14 23.36 7.18</cell></row><row><cell>Cyc.</cell><cell>BEV AP 3D AP</cell><cell>0.25 0.5 0.25 0.5</cell><cell>24.79 11.18 23.77 10.88</cell><cell>17.53 10.18 17.24 9.93</cell><cell>16.96 10.03 16.45 9.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 .</head><label>6</label><figDesc>Proposal Representation and Full Image Features. Error in meters for centroid depth estimation (average absolute error / standard deviation) for the hard car category on the KITTI val1 set. C = features from the RGB crop, D = the estimated 2D and 3D box dimensions, F = features from the full image. Networks here do not include the Instance Reconstruction module.</figDesc><table><row><cell>Version</cell><cell>Input</cell><cell>Depth Error</cell></row><row><cell>Proposal Only</cell><cell>C+D</cell><cell>1.43 / 2.08</cell></row><row><cell>Proposal Regression</cell><cell>C+D</cell><cell>1.31 / 1.93</cell></row><row><cell>Direct Estimation</cell><cell>C+F+D</cell><cell>1.48 / 2.28</cell></row><row><cell cols="2">Proposal Regression C+F+D</cell><cell>1.22 / 1.84</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rogerio Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teulire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, ICCV &apos;15</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Brostow. Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automotive 3d object detection without target domain annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Linder-Norn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for singleview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1966" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<title level="m">defense of classical image processing: Fast depth completion on the cpu. CRV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A learning framework for generating region proposals with mid-level cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosecka</surname></persName>
		</author>
		<title level="m">3d bounding box estimation using deep learning and geometry. CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5632" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omnisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>C. Cortes, N. D. Lawrence, D. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
	<note>JunYoung Gwak, Daeyun Shin, and Derek Hoiem</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synthesizing 3d shapes via modeling multi-view depth maps and silhouettes with deep generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Amir Arsalan Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Marrnet: 3d shape reconstruction via 2.5 d sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="540" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning singleview 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
