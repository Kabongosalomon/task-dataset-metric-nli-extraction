<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quality Assessment of In-the-Wild Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-21">2019. October 21-25, 2019. October 21-25, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
						</author>
						<title level="a" type="main">Quality Assessment of In-the-Wild Videos</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM &apos;19)</title>
						<meeting>the 27th ACM International Conference on Multimedia (MM &apos;19) <address><addrLine>Nice, France</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2019-10-21">2019. October 21-25, 2019. October 21-25, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3351028</idno>
					<note>, France. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quality assessment of in-the-wild videos is a challenging problem because of the absence of reference videos and shooting distortions. Knowledge of the human visual system can help establish methods for objective quality assessment of in-the-wild videos. In this work, we show two eminent effects of the human visual system, namely, content-dependency and temporal-memory effects, could be used for this purpose. We propose an objective no-reference video quality assessment method by integrating both effects into a deep neural network. For content-dependency, we extract features from a pre-trained image classification neural network for its inherent content-aware property. For temporal-memory effects, long-term dependencies, especially the temporal hysteresis, are integrated into the network with a gated recurrent unit and a subjectivelyinspired temporal pooling layer. To validate the performance of our method, experiments are conducted on three publicly available inthe-wild video quality assessment databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm, respectively. Experimental results demonstrate that our proposed method outperforms five state-of-the-art methods by a large margin, specifically, 12.39%, 15.71%, 15.45%, and 18.09% overall performance improvements over the second-best method VBLIINDS, in terms of SROCC, KROCC, PLCC and RMSE, respectively. Moreover, the ablation study verifies the crucial role of both the content-aware features and the modeling of temporalmemory effects. The PyTorch implementation of our method is released at https://github.com/lidq92/VSFA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KEYWORDS</head><p>video quality assessment; human visual system; content dependency; temporal-memory effects; in-the-wild videos ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: [Best viewed when zoomed in] Human judgments of visual quality are content-dependent. The first/second row shows a pair of in-focus/out-of-focus images. Every two images in a pair are taken in the same shooting condition, and they only differ in image content. However, user study shows that humans consistently prefer the left ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, most videos are captured in the wild by users with diverse portable mobile devices, which may contain annoying distortions due to out of focus, object motion, camera shake, or under/over exposure. Thus, it is highly desirable to automatically identify and cull low-quality videos, prevent their occurrence by quality monitoring processes during acquisition, or repair/enhance them with the quality-aware loss. To achieve this goal, quality assessment of in-the-wild videos is a precondition. However, this is a challenging problem due to the fact that the "perfect" source videos are not available and the shooting distortions are unknown. There is an essential difference between in-the-wild videos and synthetically-distorted videos, i.e., the former contains a mass of content and may suffer from complex mixed real-world distortions that are temporally heterogeneous. On account of this, current state-of-the-art video quality assessment (VQA) methods (e.g., VBLIINDS <ref type="bibr" target="#b34">[35]</ref> and VIIDEO <ref type="bibr" target="#b27">[28]</ref>) validated on traditional synthetic VQA databases <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref> fail in predicting the quality of in-the-wild videos <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>This work focuses on the problem "quality assessment of inthe-wild videos". Since humans are the end-users, we believe that knowledge of the human visual system (HVS) can help establish objective methods for our problem. Specifically, two eminent effects of HVS are incorporated into our method.</p><p>Human judgments of visual image/video quality depend on content, which is well known in many subjective experiments <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref>. For images, <ref type="bibr">Siahaan et al.</ref> show that scene and object categories influence human judgments of visual quality for JPEG compressed and blurred images <ref type="bibr" target="#b40">[41]</ref>. Two compressed images with the same compression ratio may have different subjective quality if they contain different scenes <ref type="bibr" target="#b42">[43]</ref>, since the scene content can have different impact on the compression operations and the visibility of artifacts. For videos, similar content dependency can be found in compressed video quality assessment <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref> and quality-of-experience of streaming videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Unlike quality assessment of synthetically-distorted images/videos, quality assessment of in-the-wild images/videos essentially requires to compare cross-content image/video pairs (i.e., the pair from different reference images/videos) <ref type="bibr" target="#b24">[25]</ref>, which may be more strongly affected by content. To verify the correctness of this effect on our problem, we collect data and conduct a user study. We ask 10 human subjects to do the cross-content pairwise comparison for 201 image pairs. More than 7 of 10 subjects prefer one image to the other image in 82 image pairs. For illustration, two pairs of in-the-wild images are shown in <ref type="figure">Figure 1</ref>. Each image pair is taken in the same shooting conditions (e.g., focus length, object distance). For the in-focus image pair in the first row, 9 of 10 subjects prefer the left one. For the out-of-focus image pair in the second row, 8 of 10 subjects prefer the left one to the right one. The only difference within a pair is the image content, so from our user study, we can infer that image content can affect human perception on quality assessment of in-the-wild images. We also conduct a user study for 43 video pairs, where every two videos in a pair are taken in similar settings. Similar results are found that video content could have impacts on judgments of visual quality for in-the-wild videos. In the supplemental material, we provide a video pair, for which all 10 subjects prefer the same video. Thus, we consider content-aware features in our problem to address the content dependency.</p><p>Human judgments of video quality are affected by their temporal memory. Temporal-memory effects indicate that human judgments of current frame rely on the current frame and information from previous frames. And this implies that long-term dependencies exist in the VQA problem. More specifically, humans remember poor quality frames in the past and lower the perceived quality scores for following frames, even when the frame quality has returned to acceptable levels <ref type="bibr" target="#b36">[37]</ref>. This is called the temporal hysteresis effect. It indicates that the simple average pooling strategy overestimates the quality of videos with fluctuating frame-wise quality scores. Since the in-the-wild video contains more temporally-heterogeneous distortions than the syntheticallydistorted video, human judgments of its visual quality reflect stronger hysteresis effects. Therefore, in our problem, modeling of temporalmemory effects should be taken into account.</p><p>In light of the two effects, we propose a simple yet effective no-reference (NR) VQA method with content-aware features and modeling of temporal-memory effects. To begin with, our method extracts content-aware features from deep convolutional neural networks (CNN) pre-trained on image classification tasks, for they are able to discriminate abundant content information. After that, it includes a gated recurrent unit (GRU) for modeling long-term dependencies and predicting frame quality. Finally, to take the temporal hysteresis effects into account, we introduce a differentiable subjectively-inspired temporal pooling model, and embed it as a layer into the network to output the overall video quality.</p><p>To demonstrate the performance of our method, we conduct experiments on three publicly available databases, i.e., KoNViD-1k <ref type="bibr" target="#b11">[12]</ref>, LIVE-Qualcomm <ref type="bibr" target="#b9">[10]</ref> and CVD2014 <ref type="bibr" target="#b30">[31]</ref>. Our method is compared with five state-of-the-art methods, and its superior performance is proved by the experimental results. Moreover, the ablation study verifies the key role of each component in our method. This suggests that incorporating the knowledge of HVS could make objective methods more consistent with human perception.</p><p>The main contributions of this work are as follows:</p><p>? An objective NR-VQA method and the first deep learningbased model is proposed for in-the-wild videos. ? To our best knowledge, it is the first time that a GRU network is applied to model the long-term dependencies for quality assessment of in-the-wild videos and a differentiable temporal pooling model is put forward to account for the hysteresis effect. ? The proposed method outperforms the state-of-the-art methods by large margins, which is demonstrated by experiments on three large-scale in-the-wild VQA databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Video Quality Assessment</head><p>Traditional VQA methods consider structures <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, gradients <ref type="bibr" target="#b20">[21]</ref>, motion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, energy <ref type="bibr" target="#b17">[18]</ref>, saliency <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref>, or natural video statistics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b56">57]</ref>. Besides, quality assessment can be achieved by fusion of primary features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, four deep learningbased VQA methods are proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. Kim et al. <ref type="bibr" target="#b14">[15]</ref> utilize CNN models to learn the spatio-temporal sensitivity maps.</p><p>Liu et al. <ref type="bibr" target="#b19">[20]</ref> exploit the 3D-CNN model for codec classification and quality assessment of compressed videos. Zhang et al. <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> apply the transfer learning technique with CNN for video quality assessment. However, all these methods are trained, validated, and tested on synthetically distorted videos. Streaming video qualityof-experience is relevant to video quality but beyond the scope of this paper, and an interested reader can refer to the good surveys <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>. Quality assessment of in-the-wild videos is a quite new topic in recent years <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>. Four relevant databases have been constructed and corresponding subjective studies have been conducted. Overall, CVD2014 <ref type="bibr" target="#b30">[31]</ref>, KoNViD-1k <ref type="bibr" target="#b11">[12]</ref>, and LIVE-Qualcomm <ref type="bibr" target="#b9">[10]</ref> are publicly available, while LIVE-VQC <ref type="bibr" target="#b41">[42]</ref> will be available soon. Due to the fact that we cannot access the pristine reference videos in this situation, only NR-VQA methods are applicable. Unfortunately, the evaluation of current state-of-the-art NR-VQA methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref> on these video databases shows a poor performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>. Existing deep learning-based VQA models are unfeasible in our problem since they either need the reference information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> or only suit for compression artifacts <ref type="bibr" target="#b19">[20]</ref>. Thus, this motivates us to propose the first deep learning-based model that is capable of predicting the quality of in-the-wild videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-Aware Features</head><p>Content-aware features can help addressing content-dependency on the predicted image/video quality, so as to improve the performance of objective models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. Jaramillo et al. <ref type="bibr" target="#b12">[13]</ref>     <ref type="figure">Figure 2</ref>: The overall framework of the proposed method. It mainly consists of two modules. The first module "content-aware feature extraction" is a pre-trained CNN with effective global pooling (GP) serving as a feature extractor. The second module "modeling of temporal-memory effects" includes two sub-modules: one is a GRU network for modeling long-term dependencies; the other is a subjectively-inspired temporal pooling layer accounting for the temporal hysteresis effects. Note that the GRU network is the unrolled version of one GRU and the parallel CNNs/FCs share weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling of Temporal-Memory Effects Content-Aware Feature Extraction</head><p>extract handcrafted content-relevant features to tune existing quality measures. Siahaan et al. <ref type="bibr" target="#b40">[41]</ref> and Wu et al. <ref type="bibr" target="#b48">[49]</ref> utilize semantic information from the top layer of pre-trained image classification networks to incorporate with traditional quality features. Li et al. <ref type="bibr" target="#b16">[17]</ref> exploit the deep semantic feature aggregation of multiple patches for image quality assessment. It is shown that these deep semantic features alleviate the impact of content on the quality assessment task. Inspired by their work, we consider using pretrained image classification networks for content-aware feature extraction as well. Unlike the work in <ref type="bibr" target="#b16">[17]</ref>, to get the features, we directly feed the whole frame into the network and apply not only global average pooling but also global standard deviation pooling to the output semantic feature maps. Since our work aims at the VQA task, we further put forward a new module for modeling temporal characteristics of human behaviors when rating video quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Temporal Modeling</head><p>The temporal modeling in the VQA field can be viewed in two aspects, i.e., feature aggregation and quality pooling.</p><p>In the feature aggregation aspect, most methods aggregate framelevel features to video-level features by averaging them over the temporal axis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b34">35]</ref>. Li et al. <ref type="bibr" target="#b18">[19]</ref> adopt a 1D convolutional neural network to aggregate the primary features for a time interval. Unlike the previous methods, we consider using GRU network to model the long-term dependencies for feature integration.</p><p>In the quality pooling aspect, the simple average pooling strategy is adopted by many methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>. Several pooling strategies considering the recency effect or the worst quality section influence are discussed in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref>. Kim et al. <ref type="bibr" target="#b14">[15]</ref> adopt a convolutional neural aggregation network (CNAN) for learning frame weights, then the overall video quality is calculated by the weighted average of frame quality scores. Seshadrinathan and Bovik <ref type="bibr" target="#b36">[37]</ref> notice the temporal hysteresis effect in the subjective experiments, and propose a temporal hysteresis pooling strategy for quality assessment. The effectiveness of this strategy has been verified in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>. We also take account of the temporal hysteresis effects. However, the temporal pooling model in <ref type="bibr" target="#b36">[37]</ref> is not differentiable. So we introduce a new one with subjectively-inspired weights which can be embedded into the neural network and be trained with back propagation as well. In the experimental part, we will show that this new temporal pooling model with subjectivelyinspired weights is better than the CNAN temporal pooling <ref type="bibr" target="#b14">[15]</ref> with learned weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>In this section, we introduce a novel NR-VQA method by integrating knowledge of the human visual system into a deep neural network. The framework of the proposed method is shown in <ref type="figure">Figure 2</ref>. It extracts content-aware features from a modified pre-trained CNN with global pooling (GP) for each video frame. Then the extracted frame-level features are sent to a fully-connected (FC) layer for dimensional reduction followed by a GRU network for long-term dependencies modeling. In the meantime, the GRU outputs the frame-wise quality scores. Lastly, to account for the temporal hysteresis effect, the overall video quality is pooled from these frame quality scores by a subjectively-inspired temporal pooling layer. We will detail each part in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content-Aware Feature Extraction</head><p>For in-the-wild videos, the perceived video quality strongly depends on the video content as described in Section 1. This can be attributed to the fact that, the complexity of distortions, the human tolerance thresholds for distortions, and the human preferences could vary for different video content/scenes.</p><p>To evaluate the perceived quality of in-the-wild videos, the above observation motivates us to extract features that are not only perceptual (distortion-sensitive) but also content-aware. The image classification models pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref> using CNN possess the discriminatory power of different content information. Thus, the deep features extracted from these models (e.g. ResNet <ref type="bibr" target="#b10">[11]</ref>) are expected to be content-aware. Meanwhile, the deep features are distortion-sensitive <ref type="bibr" target="#b4">[5]</ref>. So it is reasonable to extract content-aware perceptual features from pre-trained image classification models.</p><p>Firstly, assuming the video has T frames, we feed the video frame I t (t = 1, 2, . . . ,T ) into a pre-trained CNN model and output the deep semantic feature maps M t from its top convolutional layer:</p><formula xml:id="formula_0">M t = CNN(I t ).</formula><p>(1)</p><p>M t contains a total of C feature maps. Then, we apply spatial GP for each feature map of M t . Applying the spatial global average pooling operation (GP mean ) to M t discards much information of M t . We further consider the spatial global standard deviation pooling operation (GP std ) to preserve the variation information in M t . The output feature vectors of GP mean , GP std are f mean</p><formula xml:id="formula_1">t , f std t respectively. f mean t = GP mean (M t ), f std t = GP std (M t ).</formula><p>(2)</p><p>After that, f mean t and f std t are concatenated to serve as the contentaware perceptual features f t :</p><formula xml:id="formula_2">f t = f mean t ? f std t ,<label>(3)</label></formula><p>where ? is the concatenation operator and the length of f t is 2C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling of Temporal-Memory Effects</head><p>Temporal modeling is another important clue for designing objective VQA models. We model the temporal-memory effects in two aspects. In the feature integration aspect, we adopt a GRU network for modeling the long-term dependencies in our method. In the quality pooling aspect, we propose a subjectively-inspired temporal pooling model and embed it into the network. Long-term dependencies modeling. Existing NR-VQA methods cannot well model the long-term dependencies in the VQA task. To handle this issue, we resort to GRU <ref type="bibr" target="#b1">[2]</ref>. It is a recurrent neural network model with gates control which is capable of both integrating features and learning long-term dependencies. Specifically, in this paper, we consider using GRU to integrate the content-aware perceptual features and predict the frame-wise quality scores.</p><p>The extracted content-aware features are of high dimension, which is not easy for training GRU. Therefore, it is better to perform dimension reduction before feeding them into GRU. It could be beneficial by performing dimension reduction with other steps in the optimization process jointly. In this regard, we perform dimension reduction using a single FC layer, that is:</p><formula xml:id="formula_3">x t = W f x f t + b f x ,<label>(4)</label></formula><p>where W f x and b f x are the parameters in the single FC layer. Without the bias term, it acts as a linear dimension reduction model. After dimension reduction, the reduced features x t (t = 1, ? ? ? ,T ) are sent to GRU. We consider the hidden states of GRU as the integrated features, whose initial values are h 0 . The current hidden state h t is calculated from the current input x t and the previous hidden state h t ?1 , that is:</p><formula xml:id="formula_4">h t = GRU(x t , h t ?1 ).<label>(5)</label></formula><p>With the integrated features h t , we can predict the frame quality score q t by adding a single FC layer:</p><formula xml:id="formula_5">q t = W hq h t + b hq ,<label>(6)</label></formula><p>where W hq and b hq are the weight and bias parameters. Subjectively-inspired temporal pooling. In subjective experiments, subjects are intolerant of poor quality video events <ref type="bibr" target="#b31">[32]</ref>. More specifically, temporal hysteresis effect is found in the subjective experiments, i.e., subjects react sharply to drops in video quality and provide poor quality for such time interval, but react dully to improvements in video quality thereon <ref type="bibr" target="#b36">[37]</ref>. A temporal pooling model is adopted in <ref type="bibr" target="#b36">[37]</ref> to account for the hysteresis effect. Specifically, a memory quality element is defined as the minimum of the quality scores over the previous frames; a current quality element is defined as a sort-order-based weighted average of the quality scores over the next frames; the approximate score is calculated as the weighted average of the memory and current elements; the video quality is computed as the temporal average pooling of the approximate scores. However, there are some limitations on directly applying this model to the NR quality assessment of in-the-wild videos. First, this model requires the reliable frame quality scores as input, which cannot be provided in our task. Second, the model in <ref type="bibr" target="#b36">[37]</ref> is not differentiable due to the sort-order-based weights in the definition of the current quality element. Thus it cannot be embedded into the neural network. In our problem, since we only have access to the overall subjective video quality, we need to learn the neural network without framelevel supervision. Thus, to connect the predicted frame quality score q t to the video quality Q, we put forward a new differentiable temporal pooling model by replacing the sort-order-based weight function in <ref type="bibr" target="#b36">[37]</ref> with a differentiable weight function, and embed it into the network. Details are as follow.</p><p>To mimic the human's intolerance to poor quality events, we define a memory quality element l t at the t-th frame as the minimum of quality scores over the previous several frames:</p><formula xml:id="formula_6">l t = q t , for t = 1, l t = min k ?V pr ev q k , for t &gt; 1,<label>(7)</label></formula><p>where V pr ev = {max (1, t ? ? ), ? ? ? , t ? 2, t ? 1} is the index set of the considered frames, and ? is a hyper-parameter relating to the temporal duration. Accounting for the fact that subjects react sharply to the drops in quality but react dully to the improvements in quality, we construct a current quality element m t at the t-th frame, using the weighted quality scores over the next several frames, where larger weights are assigned for worse quality frames. Specifically, we define the weights w k t by a differentiable softmin function (a composition of the negative linear function and the softmax function).</p><formula xml:id="formula_7">m t = k ?V ne x t q k w k t , w k t = e ?q k j ?V ne x t e ?q j , k ? V nex t ,<label>(8)</label></formula><p>where V nex t = {t, t + 1, ? ? ? , min (t + ? ,T )} is the index set of the related frames.</p><p>In the end, we approximate the subjective frame quality scores by linearly combining the memory quality and current quality elements. The overall video quality Q is then calculated by temporal global average pooling (GAP) of the approximate scores:</p><formula xml:id="formula_8">q ? t = ?l t + (1 ? ? )m t ,<label>(9)</label></formula><formula xml:id="formula_9">Q = 1 T T t =1 q ? t ,<label>(10)</label></formula><p>where ? is a hyper-parameter to balance the contributions of memory and current elements to the approximate score. Note that we model the temporal-memory effects with both a global module (i.e., GRU) and a local module (i.e., subjectivelyinspired temporal pooling with a window size of 2? + 1). The longterm dependency is always considered by GRU, no matter which value of ? in the temporal pooling is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>We choose ResNet-50 <ref type="bibr" target="#b10">[11]</ref> pre-trained on ImageNet <ref type="bibr" target="#b3">[4]</ref> for the content-aware feature extraction, and the feature maps are extracted from its 'res5c' layer. In this instance, the dimension of f t is 4096. The long-term dependencies part is a single FC layer that reduces the feature dimension from 4096 to 128, followed by a single-layer GRU network whose hidden size is set as 32. The subjectively-inspired temporal pooling layer contains two hyperparameters, ? and ? , which are set as 12 and 0.5, respectively. We fix the parameters in the pre-trained ResNet-50 to ensure that the content-aware property is not altered, and we train the whole network in an end-to-end manner. The proposed model is implemented with PyTorch <ref type="bibr" target="#b32">[33]</ref>. The L 1 loss and Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with an initial learning rate 0.00001 and training batch size 16 are used for training our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first describe the experimental settings, including the databases, compared methods and basic evaluation criteria. Next, we carry out the performance comparison and result analysis of our method with five state-of-the-art methods. After that, an ablation study is conducted. Then, we show results of different choices of feature extractor and temporal pooling strategy. Finally, the adding value of motion information and computational efficiency are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Databases. There are four databases constructed for our problem: LIVE Video Quality Challenge Database (LIVE-VQC) <ref type="bibr" target="#b41">[42]</ref>, Konstanz Natural Video Database (KoNViD-1k) <ref type="bibr" target="#b11">[12]</ref>, LIVE-Qualcomm Mobile In-Capture Video Quality Database (LIVE-Qualcomm) <ref type="bibr" target="#b9">[10]</ref>, and Camera Video Database (CVD2014) <ref type="bibr" target="#b30">[31]</ref>. The latter three are now publicly available, while the first one is not accessible now. So we conduct experiments on KoNViD-1k, LIVE-Qualcomm and CVD2014. Subjective quality scores are provided in the form of mean opinion score (MOS).</p><p>KoNViD-1k <ref type="bibr" target="#b11">[12]</ref> aims at natural distortions. To guarantee the video content diversity, it comprises a total of 1,200 videos of resolution 960?540 that are fairly sampled from a large public video dataset, YFCC100M. The videos are 8s with 24/25/30fps. The MOS ranges from 1.22 to 4.64.</p><p>LIVE-Qualcomm <ref type="bibr" target="#b9">[10]</ref> aims at in-capture video distortions during video acquisition. It includes 208 videos of resolution 1920?1080 captured by 8 different smart-phones and models 6 in-capture distortions (artifacts, color, exposure, focus, sharpness and stabilization). The videos are 15s with 30fps. The realignment MOS ranges from 16.5621 to 73.6428.</p><p>CVD2014 <ref type="bibr" target="#b30">[31]</ref> also aims at complex distortions introduced during video acquisition. It contains 234 videos of resolution 640?480 or 1280?720 recorded by 78 different cameras. The videos are 10-25s with 11-31fps, which are a wide range of time span and fps. The realignment MOS ranges from -6.50 to 93.38.</p><p>Compared methods. Because only NR methods are applicable for quality assessment of in-the-wild videos, we choose five stateof-the-art NR methods (whose original codes are released by the authors) for comparison: VBLIINDS <ref type="bibr" target="#b34">[35]</ref>, VIIDEO <ref type="bibr" target="#b27">[28]</ref>, BRISQUE [27] 1 , NIQE <ref type="bibr" target="#b28">[29]</ref>, and CORNIA <ref type="bibr" target="#b50">[51]</ref>. Note that we cannot compare with the three recent deep learning-based general VQA methods, since <ref type="bibr" target="#b54">[55]</ref> needs scores of full-reference methods and <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b55">56]</ref> are fullreference methods, which are unfeasible for our problem.</p><p>Basic evaluation criteria. Spearman's rank-order correlation coefficient (SROCC), Kendall's rank-order correlation coefficient (KROCC), Pearson's linear correlation coefficient (PLCC) and root mean square error (RMSE) are the four performance criteria of VQA methods. SROCC and KROCC indicate the prediction monotonicity, while PLCC and RMSE measure the prediction accuracy. Better VQA methods should have larger SROCC/KROCC/PLCC and smaller RMSE. When the objective scores (i.e., the quality scores predicted by a VQA method) are not the same scale as the subjective scores, we refer to the suggestion of Video Quality Experts Group (VQEG) <ref type="bibr" target="#b43">[44]</ref> before calculating PLCC and RMSE values, and adopt a four-parameter logistic function for mapping the objective score o to the subjective score s:</p><formula xml:id="formula_10">f (o) = ? 1 ? ? 2 1 + e ? o?? 3 ? 4 + ? 2 ,<label>(11)</label></formula><p>where ? 1 to ? 4 are fitting parameters initialized with ? 1 = max(s), ? 2 = min(s), ? 3 = mean(o), ? 4 = std(o)/4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>For each database, 60%, 20%, and 20% data are used for training, validation, and testing, respectively. There is no overlap among these three parts. This procedure is repeated 10 times and the mean and standard deviation of performance values are reported in <ref type="table" target="#tab_0">Table 1</ref>. For VBLIINDS, BRISQUE and our method, we choose the models with the highest SROCC values on the validation set during the training phase. NIQE, CORNIA, and VIIDEO are tested on the same 20% testing data after the parameters in Eqn. <ref type="bibr" target="#b10">(11)</ref> are optimized with the training and validation data. <ref type="table" target="#tab_0">Table 1</ref> summarizes the performance values on the three databases, and the overall performance values (indicated by the weighted performance values) as well. Our method achieves the best overall performance in terms of both the prediction monotonicity (SROCC, KROCC) and the prediction accuracy (PLCC, RMSE), and have a  <ref type="bibr" target="#b9">10</ref>.759 (? 0.939) VIIDEO <ref type="bibr" target="#b27">[28]</ref> 0.237 (? 0.073) 0.164 (? 0.050) 0.218 (? 0.070) 5.115 (? 0.285) 0.127 (? 0.137) 9.77E-11 0.082 (? 0.099) -0.001 (? 0.106) 12.308 (? 0.881) VBLIINDS <ref type="bibr" target="#b34">[35]</ref>  large gain over the second-best method VBLIINDS. VIIDEO fails because it is based only on temporal scene statistics and cannot model the complex distortions. For all individual databases, our method outperforms the other compared methods by a large margin. For example, compared to the second-best method VBLIINDS, in terms of SROCC, our method achieves 30.21% improvements on LIVE-Qualcomm, 8.63% improvements on KoNViD-1k and 17.96% improvements on CVD2014. Among the three databases, LIVE-Qualcomm is the most challenging one for the compared methods and our method-not only mean performance values are small but also standard deviation values for all methods are large. This verifies the statement in <ref type="bibr" target="#b9">[10]</ref> that videos in LIVE-Qualcomm challenge both human viewers and objective VQA models. Statistical significance. We further carry out the statistical significance test to see whether the results shown in <ref type="table" target="#tab_0">Table 1</ref> are statistical significant or not. On each database, the paired t-test is conducted at 5% significance level using the SROCC values (in 10 runs) of our method and of the compared one. The p-values are shown in <ref type="table" target="#tab_0">Table 1</ref>. All are smaller than 0.05 and prove our method is significantly better than all the other five state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To demonstrate the importance of each module in our framework, we conduct an ablation study. The overall 10-run-results are shown in the form of box plots in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>Content-aware features. We first show the performance drop due to the removal of the content-aware features. When we remove the content-aware features extracted from CNN, we use BRISQUE <ref type="bibr" target="#b26">[27]</ref> features instead (red). The removal of the contentaware features causes significant performance drop in all three databases. p-values are 1.10E-05, 1.76E-08, 2.47E-06, and 14.57%, 30.00%, 26.87% decrease in terms of SROCC are found on KoNViD-1k, CVD2014 and LIVE-Qualcomm respectively. Content-aware perceptual features contribute most to our method, which verifies that content-aware perceptual features are crucial for assessing the perceived quality of in-the-wild videos.</p><p>Modeling of temporal-memory effects. To verify the effectiveness of modeling of temporal-memory effects, we compare the full version of our proposed method (blue) with the whole temporal modeling module removed (green). Temporal modeling provides 7.70%, 4.14%, 12.01% SROCC gains on KoNViD-1k, CVD2014 and LIVE-Qualcomm respectively, where the p-values are 4.00E-04, 1.11E-04, and 8.49E-03. In view of PLCC, it leads to 5.98%, 4.00%, 10.41% performance improvements on KoNViD-1k, CVD2014 and LIVE-Qualcomm respectively. We further do the ablation study on KoNViD-1k for the two individual temporal sub-modules separately. Removal of long-term dependencies modeling leads to 2.12% decrease in terms of SROCC, while removal of subjectively-inspired temporal pooling leads to 2.68% decrease in terms of SROCC. This indicates the two temporal sub-modules (one is global and the other is local) are complementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Choice of Feature Extractor</head><p>There are many choices for content-aware feature extraction. In the following, we mainly consider the pre-trained image classification models and the global standard deviation (std) pooling.</p><p>Pre-trained image classification models. In our implementation, we choose ResNet-50 as the content-aware feature extractor. It is interesting to explore other pre-trained image classification models for feature extraction. The results in <ref type="table" target="#tab_1">Table 2</ref> show that VGG16 have similar performance with ResNet-50 (p-values of paired ttest using SROCC values are greater than 0.05, actually 0.1011). However, ResNet-50 has less parameters than AlexNet and VGG16. Global std pooling. When the global std pooling is removed, the performance on KoNViD-1k drops as shown in <ref type="figure" target="#fig_5">Figure 4</ref>. mean SROCC drops from 0.755 to 0.701, while mean PLCC drops significantly from 0.744 to 0.672. This verifies that global std pooling preserves more information and thus results in good performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Choices of Temporal Pooling Strategy</head><p>Here, we explore different choices of temporal pooling strategy.</p><p>Hyper-parameters in subjectively-inspired temporal pooling. The subjectively-inspired temporal pooling contains two hyperparameters, ? and ? . <ref type="figure" target="#fig_6">Figure 5</ref> shows results of different choices of the two parameters. In the left figure, ? is fixed to 12, and ? varies from 0.1 to 0.9 with a step size 0.1. SROCC fluctuates up and down around 0.75, and achieves the best with ? = 0.5. This is because smaller ? overlooks the memory quality while larger ? overlooks the current quality. In the right figure, ? is fixed to 0.5, and ? varies from 6 to 30 with a step size 6. The highest SROCC value is obtained with ? = 12, which suggests temporal hysteresis effect may lasts about one second for videos with a frame rate of 25fps. Pooling in subjective-inspired temporal pooling. To verify the effectiveness of min pooling, we compare it with average pooling. The results on KoNViD-1k are shown in <ref type="table" target="#tab_2">Table 3</ref>. And we can see that average pooling is statistically worse than min pooling (p-value is 3.04E-04). This makes sense since min pooling accounts for "humans are quick to criticize and slow to forgive". Handcrafted weights vs. learned weights. Our subjectivelyinspired temporal pooling can be regarded as a weighted average pooling strategy, where the weights are designed by hand (see Eqn. <ref type="formula" target="#formula_6">(7)</ref>, <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_8">(9)</ref>) to mimic the temporal-memory effects. One interesting question is whether the performance can be further improved by making the weights learnable. One possible way is using a temporal CNN (TCNN) to learn the approximate scores q ? from the frame quality scores q, i.e.,</p><formula xml:id="formula_11">q ? = TCNN(q, kernel_size = 2? + 1) = w ? q,</formula><p>where ? means the convolutional operator, and w is the learnable weights of TCNN with length 2? + 1 (the same size as ours).</p><p>Another way is by the convolutional neural aggregation network (CNAN) introduced in <ref type="bibr" target="#b14">[15]</ref>. It is formulated as follow:</p><formula xml:id="formula_12">? = softmax(w m ? q), Q = ? T q,</formula><p>where w m is a memory kernel, ? is the learned frame weights normalized by a softmax function and Q is the overall video quality.</p><p>In <ref type="figure" target="#fig_7">Figure 6</ref>, we report the mean and standard deviation of SROCC values among these three temporal pooling models (including ours) on the three databases. It can be seen that the two models with the learned weights (TCNN and CNAN) underperform the model with handcrafted weights (Ours). This may be explained by the fact that the handcrafted weights are manually designed to mimic the temporal hysteresis effects, while the learned weights do not capture the patterns well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Motion information</head><p>Motion information is important for video processing. In this subsection, we would like to see whether the performance can be further improved with the motion information added. We extract the optical flow using the initialized TVNet <ref type="bibr" target="#b6">[7]</ref> without finetuning, and calculate the optical flow statistics as described in <ref type="bibr" target="#b21">[22]</ref>, then concatenate the statistics to the content-aware features. The performance comparison of our model with/without motion information on KoNViD-1k is shown in <ref type="figure" target="#fig_8">Figure 7</ref>. Motion information can further improve the performance a little. However, we should note that optical flow computation is very expensive, which makes the small improvements seem unnecessary. It is desired to explore effective and efficient motion-aware features in the VQA task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Computational efficiency</head><p>Besides the performance, computational efficiency is also crucial for NR-VQA methods. To provide a fair comparison for the computational efficiency of different methods, all tests are carried out on a desktop computer with Intel Core i7-6700K CPU@4.00 GHz, 12G NVIDIA TITAN Xp GPU and 64 GB RAM. The operating system is Ubuntu 14.04. The compared methods are implemented with MAT-LAB R2016b while our method is implemented with Python 3.6. The default settings of the original codes are used without any modification. From the three databases, we select four videos with different lengths and different resolutions for test. We repeat the tests ten times and the average computation time (seconds) for each method is shown in <ref type="table" target="#tab_3">Table 4</ref>. Our method is faster than VBLIINDS-the method with the second-best performance. It is worth mentioning that our method can be accelerated to 30x faster or more (The larger resolution is, the faster acceleration is.) by simply switching the CPU mode to the GPU mode. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a novel NR-VQA method for in-the-wild videos by incorporating two eminent effects of HVS, i.e., contentdependency and temporal-memory effects. Our proposed method is compared with five state-of-the-art methods on three publicly available in-the-wild VQA databases (KoNViD-1k, CVD2014, and LIVE-Qualcomm), and achieves 30.21%, 8.63%, and 17.96% SROCC improvements on LIVE-Qualcomm, KoNViD-1k, and CVD2014, respectively. Experiments also show that content-aware perceptual features and modeling of temporal-memory effects are of importance for in-the-wild video quality assessment. However, the correlation values of the best method are still less than 0.76 on KoNViD-1k and LIVE-Qualcomm. This indicates that there is ample room for developing an objective model which correlates well with human perception. In the further study, we will consider embedding the spatio-temporal attention models into our framework since they could provide information about when and where the video is important for the VQA problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0.686 (? 0.035) 0.503 (? 0.032) 0.660 (? 0.037) 3.753 (? 0.365) 0.566 (? 0(? 0.028) 0.582 (? 0.029) 0.762 (? 0.031) 3.074 (? 0.448) 0.737 (? 0.045) -0.552 (? 0.047) 0.732 (? 0.0360) 8.863 (? 1(? 0.042) 6.00E-06 0.473 (? 0.034) 0.626 (? 0.041) 0.507 (? 0.031) 0.709 (? 0.067) 7.03E-07 0.518 (? 0.060) 0.715 (? 0.048) 15.197 (? 1.325) NIQE [29] 0.544 (? 0.040) 7.31E-11 0.379 (? 0.029) 0.546 (? 0.038) 0.536 (? 0.010) 0.489 (? 0.091) 1.73E-10 0.358 (? 0.064) 0.593 (? 0.065) 17.168 (? 1.318) CORNIA [51] 0.610 (? 0.034) 6.77E-09 0.436 (? 0.029) 0.608 (? 0.032) 0.509 (? 0.014) 0.614 (? 0.075) 5.69E-09 0.441 (? 0.058) 0.618 (? 0.079) 16.871 (? 1.200) VIIDEO [28] 0.298 (? 0.052) 4.22E-15 0.207 (? 0.035) 0.303 (? 0.049) 0.610 (? 0.012) 0.023 (? 0.122) 3.02E-14 0.021 (? 0.081) -0.025 (? 0.144) 21.822 (? 1.152) VBLIINDS [35] 0.695 (? 0.024) 6.75E-05 0.509 (? 0.020) 0.658 (? 0.025) 0.483 (? 0.011) 0.746 (? 0.056) 2.94E-06 0.562 (? 0.0570) 0.753 (? 0.053) 14.292 (? 1.413) Ours 0.755 (? 0.025) -0.562 (? 0.022) 0.744 (? 0.029) 0.469 (? 0.054) 0.880 (? 0.030) -0.705 (? 0.044) 0.885 (? 0.031) 11.287 (? 1.943)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Box plots of the ablation study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Effectiveness of global std pooling on KoNViD-1k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Performance on KoNViD-1k of different hyperparameters in subjectively-inspired temporal pooling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>SROCC comparison between temporal pooling models with learned weights or handcrafted weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The performance comparison of our model with/without motion information on KoNViD-1k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on the three VQA databases. Mean and standard deviation (std) of the performance values in 10 runs are reported, i.e., mean (? std). 'Overall Performance' shows the weighted-average performance values over all three databases, where weights are proportional to database-sizes. In each column, the best and second-best values are marked in boldface and underlined, respectively.</figDesc><table><row><cell>Method</cell><cell>SROCC?</cell><cell>Overall Performance KROCC? PLCC?</cell><cell>RMSE?</cell><cell>SROCC?</cell><cell>p-value (&lt;0.05)</cell><cell cols="2">LIVE-Qualcomm [10] KROCC?</cell><cell>PLCC?</cell><cell>RMSE?</cell></row><row><cell>BRISQUE [27]</cell><cell cols="4">0.643 (? 0.059) 0.465 (? 0.047) 0.625 (? 0.053) 3.895 (? 0.380) 0.504 (? 0.147)</cell><cell>1.21E-04</cell><cell>0.365 (? 0.111)</cell><cell cols="2">0.516 (? 0.127)</cell><cell>10.731 (? 1.335)</cell></row><row><cell>NIQE [29]</cell><cell cols="4">0.526 (? 0.055) 0.369 (? 0.041) 0.542 (? 0.054) 4.214 (? 0.323) 0.463 (? 0.105)</cell><cell>5.28E-07</cell><cell>0.328 (? 0.088)</cell><cell cols="2">0.464 (? 0.136)</cell><cell>10.858 (? 1.013)</cell></row><row><cell>CORNIA [51]</cell><cell cols="4">0.591 (? 0.052) 0.423 (? 0.043) 0.595 (? 0.051) 4.139 (? 0.300) 0.460 (? 0.130)</cell><cell>4.98E-06</cell><cell>0.324 (? 0.104)</cell><cell cols="2">0.494 (? 0.133)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of different pre-trained image classification models on KoNViD-1k.</figDesc><table><row><cell>Pre-trained model</cell><cell>SROCC?</cell><cell>KROCC?</cell><cell>PLCC?</cell></row><row><cell>ResNet-50</cell><cell cols="3">0.755 (?0.025) 0.562 (?0.022) 0.744 (?0.029)</cell></row><row><cell>AlexNet</cell><cell cols="3">0.732 (?0.040) 0.540 (?0.036) 0.731 (?0.035)</cell></row><row><cell>VGG16</cell><cell cols="3">0.745 (?0.024) 0.554 (?0.023) 0.747 (?0.022)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness of min pooling in subjective-inspired temporal pooling on KoNViD-1k.</figDesc><table><row><cell>pooling</cell><cell>SROCC?</cell><cell>p-value</cell><cell>KROCC?</cell><cell>PLCC?</cell></row><row><cell>min</cell><cell>0.755 (?0.025)</cell><cell>-</cell><cell cols="2">0.562 (?0.022) 0.744 (?0.029)</cell></row><row><cell cols="5">average 0.736 (?0.031) 3.04E-4 0.543 (?0.027) 0.740 (?0.027)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The average computation time (seconds) for four videos selected from the original databases. {xxx}frs@{yyy}p indicates the video frame length and the resolution.</figDesc><table><row><cell>Method</cell><cell cols="4">240frs@540p 364frs@480p 467frs@720p 450frs@1080p</cell></row><row><cell>BRISQUE [27]</cell><cell>12.6931</cell><cell>12.3405</cell><cell>41.2220</cell><cell>79.8119</cell></row><row><cell>NIQE [29]</cell><cell>45.6477</cell><cell>41.9705</cell><cell>155.9052</cell><cell>351.8327</cell></row><row><cell>CORNIA [51]</cell><cell>225.2185</cell><cell>325.5718</cell><cell>494.2449</cell><cell>616.4856</cell></row><row><cell>VIIDEO [28]</cell><cell>137.0538</cell><cell>128.0868</cell><cell>465.2284</cell><cell>1024.5400</cell></row><row><cell>VBLIINDS [35]</cell><cell>382.0657</cell><cell>361.3868</cell><cell>1390.9999</cell><cell>3037.2960</cell></row><row><cell>Ours</cell><cell>269.8371</cell><cell>249.2085</cell><cell>936.8452</cell><cell>2081.8400</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Video-level features of BRISQUE are the average pooling of its frame-level features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the National Basic Research Program of China (973 Program) under contract 2015CB351803, the Natural Science Foundation of China under contracts 61572042, 61520106004, and 61527804. We acknowledge the High-Performance Computing Platform of Peking University for providing computational resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Study of temporal effects on subjective video quality of experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><forename type="middle">George</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><forename type="middle">Krishna</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5217" to="5231" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video quality assessment accounting for temporal visual masking of local flicker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Lark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIC</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="182" to="198" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding how image quality affects deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<editor>QoMEX. IEEE</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quality-of-experience of adaptive video streaming: Exploring the space of adaptations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM MM. ACM</title>
		<imprint>
			<biblScope unit="page" from="1752" to="1760" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Motion Representation for Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">Ermon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using multiple spatio-temporal features to estimate video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Garcia</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Welington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myl?ne Cq</forename><surname>Akamine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIC</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A noreference video quality predictor for compression and scaling artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasi</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3445" to="3449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In-Capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanjit</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Chieh</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Konstanz natural video database (KoNViD-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
		<editor>QoMEX. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-aware objective video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><forename type="middle">Oswaldo</forename><surname>Benhur Ortiz Jaramillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ljiljana</forename><surname>Ni?o-Casta?eda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Plati?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">13011</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measurement of quality of experience of video-on-demand services: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Juluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Tamarapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deep</forename><surname>Medhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surv. Tutor</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="418" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep Video Quality Assessor: From Spatio-temporal Visual Sensitivity to A Convolutional Neural Aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woojae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Which Has Better Visual Quality: The Clear Blue Sky or a Blurry Animal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1221" to="1234" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment with 3D shearlet transform and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai-Man</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ho</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwok-Wai</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1044" to="1057" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-End Blind Quality Assessment of Compressed Videos Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A spatiotemporal model of video quality assessment via 3D gradient differencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An optical flow-based noreference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sumohana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2400" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Empirical evaluation of noreference VQA methods on a natural video quality database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
		<editor>QoMEX. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Spatiotemporal Feature Combination Model for No-Reference Video Quality Assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Saupe</surname></persName>
		</author>
		<editor>QoMEX. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Psychometric scaling of TID2013 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Mikhailiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><surname>P?rez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Mantiuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In QoMEX</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating the role of content in subjective video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Mirkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Vrgovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dubravko</forename><surname>Culibrk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Darko Stefanovic, and Andras Anderla</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Noreference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Anush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video quality assessment on mobile devices: Subjective, behavioral and objective studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lark</forename><forename type="middle">Kwon</forename><surname>Anush Krishna Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">De</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JSTSP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="652" to="671" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CVD2014-A database for evaluating noreference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Vuori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
	<note>Pirkko Oittinen, and Jukka H?kkinen</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jincheol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpana</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Influence of temporal pooling method on the objective video quality evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Snjezana Rimac-Drlje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago</forename><surname>Vranjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMSB. IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Motion tuned spatiotemporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpana</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpana</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpana</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on quality of experience of HTTP adaptive streaming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Slanina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ho?feld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuoc</forename><surname>Tran-Gia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Surv. Tutor</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="469" to="492" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">To pool or not to pool&quot;: A comparison of temporal pooling methods for HTTP adaptive video streaming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Slanina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meik</forename><surname>Kottkamp</surname></persName>
		</author>
		<editor>QoMEX. IEEE</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic-aware blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernestasia</forename><surname>Siahaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">A</forename><surname>Redi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIC</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="237" to="252" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeina</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image quality comparison between JPEG and JPEG2000. II. Scene dependency, scene analysis, and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Triantaphillidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIST</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="259" to="270" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Final report from the Video Quality Experts Group on the validation of objective models of video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ViS 3 : An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Phong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon M</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">13016</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VideoSet: A large-scale compressed video quality dataset based on JND measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawmin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-On</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="292" to="302" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Novel spatio-temporal structural information based video quality metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="989" to="998" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video quality assessment based on structural distortion measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIC</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Blind image quality assessment with hierarchy: Degradation from local structure to deep semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Peng Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention driven foveated video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Touradj</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Perkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Study of saliency in objective video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1275" to="1288" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Blind Video Quality Assessment with Weakly Supervised Learning and Resampling Strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihuo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Objective Video Quality Assessment Combining Transfer Learning With CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihuo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Blind video quality assessment based on spatio-temporal internal generative mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="305" to="309" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
