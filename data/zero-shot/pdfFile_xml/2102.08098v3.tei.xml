<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
							<email>chenzhu@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renkun</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
							<email>xuzheng@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<email>kong@cs.umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.</p><p>Recently, Dauphin and Schoenholz [16] proposed a task-agnostic and automatic initialization method, MetaInit, for any neural network achitecture. MetaInit optimized the norms of weight tensors to minimize the "gradient quotient", which measures the effect of curvature near the initial parameters,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The initialization of network parameters has a strong impact on the training stability and performance of deep neural networks. Initializations that prevent gradient explosion/vanishing in back propagation played a key role in early successes with feed-forward networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Even with cleverly designed initialization rules, complex models with many layers or multiple branches can still suffer from instability. For example, the original Transformer model <ref type="bibr" target="#b2">[3]</ref> does not converge without learning rate warmup using the default initialization <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>; RoBERTa <ref type="bibr" target="#b6">[7]</ref> and GPT-3 <ref type="bibr" target="#b7">[8]</ref> have to tune the ? 2 parameter of Adam for stability when the batch size is large. Recent innovations have shown that architecture-specific initializations, which are carefully derived to maintain stability, can promote convergence without needing normalization layers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Unfortunately, the reliance on analytically derived initializations makes it difficult to realize the benefits of these methods when performing architecture search, training networks with branched or heterogeneous components, or proposing altogether new architectures.</p><p>In this work, we propose a simple method for learning the initialization of a network with any architecture. Typically, initialization schemes draw parameters independently from a zero-mean distribution, with the variance of each distribution set to pre-determined values depending on the 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2102.08098v3 <ref type="bibr">[cs.</ref>LG] 24 Nov 2021 dimensions of the layers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Rather than deriving a closed-form expression for the these distribution parameters, our method re-scales each random weight tensor (e.g. convolution kernels) directly by a learned scalar coefficient. This small set of coefficients is optimized to make the first step of a stochastic optimizer (e.g. SGD or Adam) as effective as possible at minimizing the training loss, while preventing the initial gradient norm from exploding. In addition, this process is designed to take into account the direction, step size, and stochasticity of the optimizer. Finally, after the variance has been learned for each parameter tensor, the random network parameters are re-scaled and optimization proceeds as normal. We empirically find that our methods can make the initialization fall into a smooth loss region, reduce the inter-sample gradient variance, and accelerates training.</p><p>Our proposed method, GradInit, is architecture agnostic, and works with both Adam and SGD optimizers. In the vision domain, we show it accelerates the convergence and test performance of a variety of deep architectures, from the vanilla feed-forward VGG net to ResNet, with or without Batch Normalization. It is efficient and scalable, finding good initializations using less than 1% of the total training time in our experiments, and it improves the initialization of ResNet-50 on ImageNet to obtain better final test accuracy. In the language domain, GradInit enables training the original Transformer model <ref type="bibr" target="#b2">[3]</ref> using either Adam or SGD without learning rate warmup for machine translation, which is commonly acknowledged to be difficult <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. As an extreme example of the capabilities of GradInit, we use it to initialize and train a 1202-layer ResNet that achieves significantly higher test accuracy than ResNet-110, which other initialization methods have failed to achieve.</p><p>Finally, by visualizing the initial norms and gradient variances of the weights before and after GradInit is applied, we show that GradInit is a useful tool for identifying potential causes for instability at initialization, such as those imposed by normalization layers, and we summarize interesting scale patterns learned by GradInit that can be helpful for designing better initialization rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Controlling the norms of network parameters at initialization has proven to be an effective approach for speeding up and stabilizing training. Glorot and Bengio <ref type="bibr" target="#b0">[1]</ref> studied how the variance of features evolves with depth in feed-forward linear neural networks by assuming both activations and weight tensors are independent and identical random variables. They developed a technique in which the variance of each filter scales with its fan-in (the number of input neurons). This style of analysis was later generalized to the case of ReLU networks <ref type="bibr" target="#b1">[2]</ref>. These two analyses are most effective for feed-forward networks without skip connections or normalization layers. Based on the orthogonal initialization scheme <ref type="bibr" target="#b13">[14]</ref>, Mishkin and Matas <ref type="bibr" target="#b14">[15]</ref> proposed an iterative procedure to rescale the orthogonally initialized weights of each layer in feedforward networks so that the activations of that layer have unit variance. However, this method fails to prevent the blowup of activations with depth for ResNets <ref type="bibr" target="#b15">[16]</ref>. Recently, Gurbuzbalaban and Hu <ref type="bibr" target="#b16">[17]</ref> proposed initialization schemes such that the network can provably preserve any given moment of order s ? (0, 2] for the output of each layer. The motivation is that the stochastic gradient updates can result in heavy-tailedness in the distribution of the network weights with a potentially infinite variance, but finite s-order moment <ref type="bibr" target="#b17">[18]</ref>. Again, these initialization schemes can only be applied for feed-forward neural networks.</p><p>For more complex architectures, normalization layers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and skip connections <ref type="bibr" target="#b20">[21]</ref> stabilized training dynamics and improved the state-of-the-art. Similarly, learning rate warmup is a common trick for training large Transformers <ref type="bibr" target="#b2">[3]</ref>. These methods make training tractable for some models, but do not eliminate the high initial gradient variance that destabilizes training when the network is deep <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> or when the normalization layers are not carefully positioned <ref type="bibr" target="#b3">[4]</ref>.</p><p>Several authors have proposed better initializations for networks with skip connections. This is often achieved by replacing the normalization layers with simpler scaling or bias operations, and scaling the weight matrices in each layer so that the variance of activations does not increase with depth <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Similar analysis has been applied to self attention in Transformers <ref type="bibr" target="#b4">[5]</ref>. Without removing the normalization layers, it is possbile to stabilize the initial parameter updates by introducing carefully initialized learnable scale factors to the skip connections <ref type="bibr" target="#b5">[6]</ref> or the residual branches <ref type="bibr" target="#b21">[22]</ref>. However, such techniques are often restricted to one specific architecture such as ResNets. on minibatches of random Gaussian samples. However, as training data is usually accessible for most tasks of interest, it is simpler and potentially more efficient to use the training data for initialization. MetaInit also involves the gradient of a Hessian-vector product that requires computing a "gradient of the gradient" multiple times in tandem, which is very computationally intensive. Our proposed method distinguishes itself from MetaInit in the following ways: (i) Our method is more computationally efficient. MetaInit involves computing third-order derivatives, results in long computing times and high memory usage. The memory overhead of MetaInit is more of an issue for networks with normalization layers. For the relatively small-scale CIFAR-10 problem with batch size 64, MetaInit requires three GPUs (RTX 2080Ti), while the proposed GradInit needs just one. (ii) Our method takes the stochasticity of minibatches into consideration. MetaInit uses the local curvature evaluated on a single minibatch, which fails to capture the variance of the loss/gradient between two different stochastic minibatches. (iii) Our method considers the training dynamics of different optimization algorithms including the learning rate and the direction of the gradient step, and effectively handles different optimizers including SGD and Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We aim to develop an initialization scheme applicable to arbitrary network architectures. Since previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> have shown that the initial weight norms effectively control the initial gradient norm on average, our method rescales the randomly initialized weight matrices using learnable scale factors. <ref type="bibr" target="#b0">1</ref> Using a small number of gradient descent steps on these scale factors, the proposed GradInit method chooses the initialization scalars so that the loss after the first gradient step taken by a stochastic optimizer (SGD or Adam) is as low as possible. The process of learning initialization coefficients accounts for the chosen learning rate, optimizer, and other parameters. To prevent gradient explosion, our method enforces a constraint that the gradient norm is no larger than a constant ?.</p><p>Note that for scale-invariant weights, e.g., convolution kernels before BN layers, rescaling still changes their learning dynamics by changing their effective learning rate <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Empirically, GradInit goes beyond simply preventing exploding or vanishing gradients; it also reduces the gradient variance, making the initialization fall into a smooth loss region with small gradient variance so that training is fast, see discussion about <ref type="figure" target="#fig_1">Figure 1</ref> and comparisons in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficient Learning-based Initialization via Constrained Optimization</head><p>We begin by filling all the weight matrices {W 1 , . . . , W M } of the network with values drawn from independent zero-mean Gaussian distributions, except for the scales and biases of the normalization layers (if any), which are initialized to 1 and 0 respectively. During the initialization process, we keep {W 1 , . . . , W M } constant, but we multiply each W i with a learnable non-negative scale factor ? i (initialized to 1). After initialization, we rescale the weights by the learned scale factors, and start training without the learnable scale factors just as normal. We use m = {? 1 , . . . , ? M } to denote the set of scale factors, and ? m = {? 1 W 1 , . . . , ? M W M } is the set of rescaled weight matrices. Let L(S; ?) = 1 |S| x?S (x; ?) be the average loss of the model parameterized by ? on a minibatch of samples S, where |S| is the number of samples in the minibatch. We use g S,? = ? ? L(S; ?) as a shorthand for the gradient of ?. During standard training, this gradient is preprocessed/preconditioned by the optimization algorithm A, and then used to update the network parameters. GradInit solves the following constrained optimization problem:</p><formula xml:id="formula_0">minimize m L(S; ? m ? ?A [g S,?m ]), subject to g S,?m p A ? ?,<label>(1)</label></formula><p>where S andS are two different minibatches, ? is a prescribed learning rate for the optimization algorithm A, p A is the p -norm associated with A, and ? is the upper bound for the norm. For the first gradient step, Adam uses A[g S,?m ] = sign(g S,?m ) <ref type="bibr" target="#b24">[25]</ref>, while SGD uses A[g(S; ? m )] = ?g(S; ? m )/ g(S; ? m ) 2 . We show how to choose ? and p A without tuning in Section 3.3. We discuss the formulation of this problem and how to solve it below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Solving the Constrained Problem</head><p>The problem (1) is solved using a stochastic gradient descent method in which we sample new mini-batches on each iteration. Since the proposed method uses gradient updates to compute the initialization, we dub it GradInit. We propose a simple solver to optimize objective (1) in Algorithm 1.</p><p>A key feature of our method is that is makes a simple approximation: after g S,?m is computed on the forward pass of an iteration, we treat A[g S,?m ] as a constant and do not back-propagate through A[g S,?m ] on the backward pass. We make this choice to keep computing costs low, and because it is not possible to back-propagate through the non-differentiable sign function for Adam. Sample St from training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Lt ? 1</p><formula xml:id="formula_1">|S t | x k ?S t (x k ; ?m t ), gt ? ? ? Lt 6: if gt p A &gt; ? then 7: mt+1 ? mt ? ? ?m t gt p A 8: else 9:</formula><p>SampleSt from training set.</p><formula xml:id="formula_2">10:Lt+1 ? 1 |S t | x k ?S t (x k ; ?m t ? ?A[gt]) 11: mt+1 ? mt ? ? ?m tL t+1 12:</formula><p>Clamp mt+1 using ?</p><p>To enforce the constraint in (1), we test whether the constraint is satisfied after computing g(S; ? m ).</p><p>If not, we take a gradient descent step to minimize g(S; ? m ) p A , which involves computing second order derivatives. If the constraint is satisfied, then we instead compute a gradient descent step for the loss. In addition, we set a lower bound ? = 0.01 for all ? i . We find that this prevents scalars from landing on small values during minimization and keeps the GradInit optimizer stable. In our experiments, we find the only layer that ever hit this lower bound is the final FC layer on some networks (see the figures in Section 4.1). We find this procedure converges reliably within 2000 iterations for ImageNet, and fewer than 400 iterations for CIFAR-10, taking less than 1% of the total training time on both problems. We also find it works well to set the step size ? to values within the range between 10 ?3 and 10 ?1 . During initialization, the gradient norm constraint is satisfied for the majority of iterations. The choice of ?, p A will be discussed in Section 3.3. Stochasticity of mini-batching. The objective in (1) uses two different mini-batches; S is used to compute the gradient, andS is used to compute the loss. Ideally, S and S should be independently sampled from the training set to capture the randomness of the stochastic optimizer. However, when the network has large initial gradient variance, the gradients on S andS usually differ a lot, and forS, the gradient update step ? m ? ?A [g S,?m ] becomes more similar to adding random perturbations to the parameters. We find our objective less effective at accelerating convergence in this case, as shown by the first-epoch accuracy (Acc 1 ) in <ref type="table" target="#tab_0">Table 1</ref>. On the other hand, the randomness is not captured if S =S, and we find empirically that ? m can exploit the loss by increasing the gradient norm and destabilize training in this case (see <ref type="table">Table 8</ref>). Without excessive tuning, we find that we get more reliable behavior for different architectures whenS is a mixture of 50% samples from S and 50% re-sampled training data, and use this setting by default unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Setting and Enforcing the Constraint</head><p>The constraint in (1) is included to prevent the network from minimizing the loss in a trivial way by blowing up the initial gradient. In other words, we want the optimizer to achieve small loss by choosing an effective search direction rather than by taking an extremely large step in a sub-optimal direction.</p><p>Setting p A and ? through first-order approximation. We show that p A and ? can be set easily with a rule of thumb and without a parameter search. From the first-order approximation, we expect the first gradient step to result in a change in the loss on S as following:</p><formula xml:id="formula_3">L(S; ? m ??A[g S,?m ])?L(S; ? m ) ? ??A[g S,?m ] T g S,?m = ?? g S,?m 2 2 , if A is SGD, ?? g S,?m 1 , if A is Adam.<label>(2)</label></formula><p>To effectively bound the approximated change in Eq. 2, we choose p A to be the 2 and 1 norm for SGD and Adam respectively, so when the constraint is satisifed, the maximum change in the loss, according to our local approximation, is ?? 2 for SGD and ?? for Adam. We recommend setting ? such that ?? 2 = 0.1 for SGD and ?? = 0.1 for Adam. According to the linear approximations, this limits the gradient magnitude so that the first step of SGD can decrease the loss by at most 0.1. This simple rule was used across all vision and language experiments.</p><p>Why a constraint and not a penalty? Instead of formulating GradInit as a constrained optimization, one can alternatively formulate it as minimizing the objective with a gradient penalty: minimize m L(S; ? m ? ?A [g S,?m ]) + ? g S;?m p A , where ? &gt; 0 is the penalty strength. The penalized objective has two drawbacks compared to the constrained one in Eq. 1. First, every gradient descent step on the penalized objective involves second-order gradients due to the gradient regularization, while the constrained form does not need second-order gradients when the constraint is satisfied. Second, it is difficult to choose a good ? that works well for all architectures. By contrast, we set ? by analyzing the first-order approximation mentioned above, and find the same ? works well for different architectures. The results supporting these two points are given in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate GradInit on benchmark datasets for image classification and machine translation tasks. For image classification, five different architectures are evaluated for CIFAR10 <ref type="bibr" target="#b25">[26]</ref>, and ResNet-50 is evaluated for ImageNet <ref type="bibr" target="#b26">[27]</ref>. For machine translation, we use GradInit to find good initializations for a Post-LN Transformer without any change to its original architecture on IWSLT-14 De-En <ref type="bibr" target="#b27">[28]</ref>. We observe that the method can remove the necessity of any form of learning rate warmup for both Adam and SGD.</p><p>We conduct our experiments in PyTorch. We use the fairseq library for machine translation <ref type="bibr" target="#b28">[29]</ref>. All the experiments on CIFAR-10 and IWSLT-14 DE-EN can run with one single NVIDIA RTX 2080 Ti GPU with 11GB of RAM.</p><p>GradInit first initializes the weights using Kaiming initialization <ref type="bibr" target="#b1">[2]</ref> for all the Conv and FC layers for image classification. For machine translation, we use the default Xavier initialization <ref type="bibr" target="#b0">[1]</ref>. We optimize the scale factors {? i } with Adam <ref type="bibr" target="#b29">[30]</ref> using the default momentum parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Datasets with Various Architectures</head><p>The introduction of Batch Normalization (BN) <ref type="bibr" target="#b18">[19]</ref> and skip connections makes it relatively easy to train common CNNs for image classification to achieve high accuracy. Despite this, we show that when the network is very deep, the network is unstable even when both BN and skip connections are used, and GradInit can significantly improve the stability. The results on CIFAR-10 are given in <ref type="table" target="#tab_2">Table 3</ref> and results on ImageNet are given in <ref type="table" target="#tab_5">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Settings</head><p>Architectures. On CIFAR-10, we focus on the feedforward VGG net and the prevalent and powerful ResNet, with and without BN layers. For networks without BN, we use learnable biases in all layers. For ResNet, we additionally evaluate a deep 1202-layer version. We give results for other architectures (Wide ResNet, DenseNet) in Appendix E due to space limits. We compare with four different methods/settings: 1) Kaiming Initialization <ref type="bibr" target="#b1">[2]</ref>; 2) First train the network for one epoch with a constant learning rate equal to the starting learning rate, labelled as "+1 epoch (Const. LR)" in <ref type="table" target="#tab_2">Table 3</ref>; 3) First train the network for one epoch with a linear warmup learning rate, labbeled as "+1 epoch (Warmup)" in <ref type="table" target="#tab_2">Table 3</ref>; 4) MetaInit <ref type="bibr" target="#b15">[16]</ref>.</p><p>On ImageNet, we use the ResNet-50 model <ref type="bibr" target="#b20">[21]</ref>. We compare with Kaiming Initialization, FixUp initialization <ref type="bibr" target="#b8">[9]</ref> and MetaInit. For the ResNet-50 without BN, we follow the architecture of FixUp for fair comparisons, but we still use the original Kaiming initialization as the starting point of GradInit.</p><p>Hyperparameters. We set A to SGD and ? = 0.1 (the same as the base learning rate) for GradInit in all image classification experiments. On CIFAR-10, we train networks with a batch size of 128. We find MetaInit often takes 2 to 3 times as much memory as GradInit. We run GradInit or MetaInit for one epoch on the data, which takes less than 1% of the total training time. For GradInit, according to our analysis in Section 3.3, we fix the gradient norm constraint ? = 1 in all these experiments. Therefore, as in MetaInit, the only hyperparameter that needs to be tuned is the learning rate ? of the scale factors. We do a grid search on ? in the range [10 ?3 , 10 ?1 ], and report the results with the best average final test accuracy on 4 runs. After GradInit initialization, we use a learning rate of 0.1 and the cosine annealing learning rate schedule without restart <ref type="bibr" target="#b30">[31]</ref> to train the model for 200 epochs, where the learning rate decays after each iteration and decays to 0 in the last iteration. Due to their high initial gradient variance (see <ref type="figure" target="#fig_9">Figure 6</ref>), we have applied gradient clipping (maximum norm is 1) to all non-BN networks so that they converge without GradInit under the same schedule.</p><p>On ImageNet, we train the ResNet-50 model for 90 epochs with a total batch size of 256 on 4 GPUs. Due to the difference in the library for training and the number of GPUs used, which affects the BN statistics, our baseline top-1 accuracy of ResNet-50 (w/ BN) on ImageNet is 0.79% lower than <ref type="bibr" target="#b31">[32]</ref>. We use SGD with a starting learning rate of 0.1 and decay the learning rate by 10 after the 30th and 60th epoch. We provide additional details in Appendix A.   eliminate the instability of Kaiming initialization. As shown in <ref type="figure">Figure 4</ref>, its initial gradient variance is still relatively high compared with GradInit. BN could magnify the gradient variance when the variance of its input features (in the forward pass) is smaller than 1 (see Appendix C). GradInit reduces the gradient variance by 4 orders of magnitude compared to Kaiming initialization , resulting in significantly higher test accuracy after the first epoch (47.79% vs. 12.57%), which also has an impact on the final test accuracy (95.13% vs. 94.41%). The reduction in gradient variance is achieved mainly by scaling down the weights of the final FC layer and the last 2 BN layers, so that the variance of the activations is reduced in the forward pass. This learned behavior is consistent with the strategy of FixUp, where the final FC layer is initialized to 0. Another source of gradient variance reduction is achieved by increasing the weight norms of the remaining Conv and BN layers, so that the variance of the inputs to the BN layers is increased and the gradient magnifying effect of BN is alleviated in the backward pass. This reduced the ratio ?(g 1 )/?(g 16 ) from 204.9 to 164.8 for the Conv layers in <ref type="figure">Figure 4</ref>. By contrast, FixUp only reduces the weight norms, which may not always be the best solution for networks with normalization layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results and Analysis</head><p>Deep residual networks still need better initializations. We also gain significant improvements from GradInit for ResNet-110 and ResNet-1202. In ResNets, the skip connections cause the variance of activations to accumulate as the ResNet goes deeper, even for the version with BN <ref type="bibr" target="#b9">[10]</ref>. This issue is more significant when the ResNet scales to 1202 layers, from which we can see that with Kaiming initialization, the first-epoch accuracy of ResNet-1202 is quite low, and the final test accuracy is even worse than the shallower ResNet-110, matching the observations of He et al. <ref type="bibr" target="#b20">[21]</ref>. Warmup is even more effective than MetaInit at accelerating the convergence and improving the final test accuracy of ResNet-1202, but GradInit still outperforms its final test accuracy by 0.8%, and the resulting ResNet-1202 finally achieved higher accuracy than ResNet-110.</p><p>The learned layer-wise rescaling patterns of GradInit are even more interesting for ResNets with BN. For ResNets with BN, recall that we have two Conv layers and two BN layers in each residual block. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, GradInit learns to increase the weight norms of all the linear layers except for the final FC layer, instead of decreasing as for the case without BN (see <ref type="figure" target="#fig_9">Figure 6</ref>). A more unique pattern is the collaborative behavior of the BN weights, where the second BN in each residual block is usually scaled down while the first BN is always scaled up. In deeper layers, the joint effect of these two BN weights is to downscale the activations and reduce their variance in the forward pass, with a more significant reducing effect as the layers get deeper. Intuitively, the marginal utility of adding a new layer decreases with depth. Therefore, for deeper layers, GradInit learns to further downscale the residual branch, and prevents the variance from increasing too much in the forward pass. Inside each residual block, increasing the scale factors of the first BN helps to reduce the magnification effect of the second BN on the gradient; forcing the input activations to the second convolution to have variance larger than 1 ensures its variance after the following convolution layer does not go below 1, avoiding the magnification effect that the second BN has on the gradient variance. See Appendix C for more discussions about the magnifying effect.  Generalizing to Adam. Models in previous experiments are trained with SGD. We also consider the case when A is Adam and use AdamW <ref type="bibr" target="#b32">[33]</ref> to train the ResNet-110 (w/ BN) model on CIFAR-10.</p><p>Following <ref type="bibr" target="#b33">[34]</ref>, we use a cosine annealing learning rate schedule with initial learning rate 3 ? 10 ?3 and weight decay 0.2. For GradInit, we set ? = 25. The Acc 1 and Acc best of Kaiming initialization and GradInit are (36.6 ? 4.7, 94.9 ? 0.1) and (40.2 ? 0.2, 95.3 ? 0.1), respectively. We also show the per-epoch test accuracy in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>The importance of rescaling BN layers. The scale parameters of BN layers usually controls the variance of activations and gradients in the forward and backward passes, while the linear layers right before the BN layers are scale-invariant. Although changing the magnitudes of the scale-invariant layers affect their learning dynamics <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, we find it important for GradInit to rescale both BN and other linear layers, as shown in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>The importance of GradInit's objective. GradInit is designed to rescale the layers to solve the constrained optimization problem in Eq. 1. Simply letting the model to learn to rescale the layers cannot improve the results, and sometimes further causes instability, as shown in <ref type="table" target="#tab_4">Table 5</ref>. We hypothesize that the bad results with VGG are due to a mismatch between the scales/norms of the gradients of the scalars and the weights. To make this alternative work, we may need to set different learning rates for the scalars and the weights, which adds to the difficulty of hyperparameter tuning. Note we do not learn the scalars when training networks initialized by GradInit. GradInit scales to ImageNet. As shown in <ref type="table" target="#tab_5">Table 6</ref>, GradInit also accelerates convergence and improves test accuracy of ResNet-50 on ImageNet, with or without BN layers, despite having to use a smaller batch size for GradInit than training due to our GPU memory limit. The acceleration achieved by GradInit is even more significant than FixUp, even on the network with the architecture designed for the initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training the Original Transformer Model without Warmup</head><p>For a Transformer model to converge, either an explicit or implicit learning rate warmup stage is needed, especially for the original Transformer architecture. It is observed that this Post-LN architecture tends to outperform the Pre-LN model <ref type="bibr" target="#b5">[6]</ref> while having higher gradient variance at initialization <ref type="bibr" target="#b3">[4]</ref>. Is it believed that this high variance makes a warmup stage inevitable. Previous works that removes the warmup stage often involves architectural changes, e.g., removing Layer Normalizations, since it can surprisingly cause instability <ref type="bibr" target="#b3">[4]</ref>. Here, we show that with a proper initialization, we can do away with the warmup stage for the original Post-LN Transformer without any modification to the architecture. <ref type="table" target="#tab_6">Table 7</ref> summarizes the architectural changes and best results of methods for improving the initialization of Post-LN Transformers. We compare the stability of the GradInit and Admin initialization methods without warmup in <ref type="figure" target="#fig_3">Figure 3</ref>.  <ref type="bibr" target="#b2">[3]</ref>, which is a Post-LN Transformer placing its Layer Normalization after the summation of the skip connection and the residual branch. It has a 512dimensional word embedding layer and 1024 dimensions in its hidden FFN layer. We also apply GradInit to the variant from Admin <ref type="bibr" target="#b5">[6]</ref>, where a learnable vector w skip is element-wise multiplied with each dimension of the skip connection, but we initialize it to 1 for GradInit. Please refer to <ref type="bibr" target="#b5">[6]</ref> for how Admin initializes these weights. Following <ref type="bibr" target="#b5">[6]</ref>, we use a linearly decaying learning rate schedule that decays from the maximum learning rate ? max to 0 as the model trains for 100K iterations. For training with SGD, we set the prescribed learning rate ? max = 0.15, and use ? = 0.15, ? = 1 for GradInit. We do a grid search on ? max for Admin and report its best result in <ref type="table" target="#tab_6">Table 7</ref>. For training with Adam, we set ? = 5 ? 10 ?4 , ? = 10 3 for the objective of GradInit, so that ?? is O(10 ?1 ) as discussed in Section 3.3. We train the initialized model ? max and ? 2 as listed in <ref type="figure" target="#fig_3">Figure 3</ref>. We evaluate the BLEU score every epoch, and report the best BLEU scores throughout training for each run. For GradInit, we set the maximum number of iterations T to 780. By comparison, the warmup stage usually takes 4000 iterations, and we find that if we use 780 steps for warmup, the model does not converge with ? max ? 3 ? 10 ?4 . For ? max = 2 ? 10 ?4 with 780-step warmup, the BLEU score is 35.4, worse than GradInit's 36.0, showing the advantage of GradInit against warmup.</p><p>Stability after removing warmup for Adam. In <ref type="figure" target="#fig_3">Figure 3</ref>, the training process becomes more unstable as ? 2 grows larger. From the analysis of RAdam <ref type="bibr" target="#b35">[35]</ref>, this is because the variance of the gradient has a stronger impact on the adaptive learning rate when ? 2 is closer to 1. Therefore, the largest ? 2 &lt; 1 that maintains the performance of the trained model reflects the stability of the initialization. We can see GradInit results in more stable models than Admin in general, though their best performance numbers are almost the same. In addition, we find w skip can help stabilize training in extreme hyper parameter settings, e.g., at ? max = 5?10 ?4 and ? 2 = 0.995 in <ref type="figure" target="#fig_3">Figure 3</ref>, GradInit with  We also find the network is unable to be trained without learning rate warmup if we just fix w skip to its initial value given by Admin and leave the initialization of other parameters unchanged. Nevertheless, with GradInit, we do not need to modify the architecture of Post-LN Transformer to obtain the same good result as Admin. For a closer look at the stabilization mechanism, we show the weight norms and gradient variance at initialization of the original Post-LN architecture using GradInit and Xavier initialization in <ref type="figure" target="#fig_13">Figure 9</ref> of the Appendix. For Xavier initialization, the gradient variance is relatively higher for all encoder layers, so Gra-dInit downscales the encoder layer weights more in general. For the LN weights, GradInit only downscales the final LN of both the encoder and decoder, which reduces the variance of the encoder and decoder during the forward pass. Another strategy GradInit learns is to downscale the weights of the output projection and the FFN layers, so that the residual branch is relatively down-weighted compared with the skip connection, similar to Admin.</p><p>Removing warmup without architectural change. Another widely observed phenomenon is that adaptive methods such as Adam seem to be much better than SGD for training Transformer-based language models <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_6">Table 7</ref> shows that, with GradInit, we can find a good initialization for the Post-LN Transformer on IWSLT-14 DE-EN that trains using SGD without learning rate warmup nor gradient clipping, and achieves performance close to Adam trained using the same type of learning rate schedule. By comparison, Admin also makes the Transformer trainable with SGD, but the BLEU score is lower than the one initialized with GradInit. By comparing <ref type="figure" target="#fig_1">Figures 9 and 10</ref> in the Appendix, we find GradInit for Adam and SGD adopts different rescaling patterns, with the Adam version depending more on downscaling the residual branches through the FFN and output projection layers than the SGD version, and the SGD version downscaling more in the final FFN block of the decoder. This highlights the importance of considering the optimization algorithm A in GradInit, and also indicates the presence of different ways to reduce the initial gradient variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose GradInit, a gradient-based initialization scheme for any architecture. GradInit reinitializes a network by learning a scale factor for each randomly initialized parameter block of a network, so that the training loss evaluated on a different minibatch after one gradient step of a specific stochastic optimizer is minimized. Such a design takes the stochasticity, the learning rate, and the direction of the optimizer into account, allowing us to find better initializations tailored for the optimizer. The initialization learned by GradInit often decreases the gradient variance for most of the parameter blocks. We show that GradInit accelerates the convergence and improves the test performance of a variety of architectures on image classification. It also enables training the Post-LN Transformer without any form of learning rate warmup, even for SGD. GradInit can be a useful tool in the future discovery of better neural architectures that are otherwise discarded due to poor initializations. By analyzing the learned scaling coefficients and their impact on gradient variance, it can also serve a guide to design better initialization schemes for complex architectures to shorten the training schedule and save energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This project was supported by the Office of Naval Research, AFOSR MURI program, the DARPA Young Faculty Award, and the National Science Foundation Division of Mathematical Sciences. Additional support was provided by Capital One Bank and JP Morgan Chase. Architectures. The base architectures include a popular variant of VGG-19 <ref type="bibr" target="#b36">[36]</ref> with BN for CIFAR-10, which includes all the sixteen convolutional layers but only one fully connected layer; a ResNet-110 <ref type="bibr" target="#b20">[21]</ref> with base width 16 and two Conv layers in each residual block, as well as its 1202-layer verison with the same depth configurations as FixUp; a 28-layer Wide ResNet <ref type="bibr" target="#b37">[37]</ref> with Widen Factor 10 (WRN-28-10) ; and a DenseNet-100 <ref type="bibr" target="#b38">[38]</ref>. To isolate the effect of BN, we also consider removing the BN layers from these three networks and adding learnable bias parameters in their place. To compare with a strong initialization scheme that is tailor-made for an architecture family, we consider a 110-layer FixUpResNet <ref type="bibr" target="#b8">[9]</ref>. FixUpResNet removes the BN from ResNet, replacing it with bias parameters and a learnable scale parameter after the second convolutional layer of each block. FixUp initializes the weights of the second convolutional layer in each residual block, and of the final fully connected layer, to zero. It also scales the first convolutional layer in each residual block by 1/ ? M . This causes the gradient to be zero in the first step for all layers except for the final FC layer. When testing GradInit on this architecture, we adopt the non-zero Kaiming initialization to all convolutional and FC layers. The results are given in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>Additional Training Hyerparameters. We use batch size 128 to train all models, except for DenseNet-100, where the recommended batch size is 64. <ref type="bibr" target="#b1">2</ref> We use random cropping, random flipping and cutout <ref type="bibr" target="#b39">[39]</ref> for data augmentation. We do not use dropout in any of our experiments. We set weight decay to 10 ?4 in all cases.</p><p>Configurations for GradInit. As in Algorithm 1, each scale factor is initialized to 1 and we set lower bounds ? = 0.01. For each architecture, we try ? from {10 ?3 , 2 ? 10 ?3 , 5 ? 10 ?3 , 10 ?2 , 2 ? 10 ?2 , 5?10 ?2 , 10 ?1 }, and report the results of 4 runs with the best ? . We find the best ? for VGG-19 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 On ImageNet</head><p>We use random cropping and flipping as data augmentation. For experiments without BN, we additionally apply MixUp <ref type="bibr" target="#b40">[40]</ref> with ? = 0.7 for all models, for fair comparisons FixUp. We train the models for 90 epochs and decay the learning rate by a factor of 10 every 30 epochs. To fit into the memory, we use a batch size of 128 for GradInit.</p><p>We simply run GradInit for 2000 iterations, which is less than half an epoch. Considering ImageNet and CIFAR-10 has 1000 and 10 classes respectively, the cross entropy loss of a random guess on ImageNet is 3 times as large as the loss on CIFAR-10, so a proper initial gradient norm for ImageNet may be 3 times as large as that for CIFAR-10. Therefore, we set ? = 3 for ImageNet. Since ? = 10 ?2 worked the best for ResNet-110 (w/ BN) on CIFAR-10, we tried ? ? {1 ? 10 ?3 , 3 ? 10 ?3 , 5 ? 10 ?3 , 10 ?2 } on ImageNet, and chose ? = 3 ? 10 ?3 , which maximizes the test accuracy of first epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 On Machine Translation</head><p>For training with SGD, we fix the momentum to 0.9, and did a grid search fo the prescribed learning rate ? max from 0.05 to 0.2 just to present its best result. During this grid search process, we set the ? of GradInit to ? = ? max We find using ? max = 0.15 gives the best results, though the model with ? max obtained a similar BLEU of 35.4. We also set ? = 0.15, ? = 1 for GradInit in this case. We did a grid search on learning rates from {0.01, 0.025, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1} for Admin. We find it achieves the best result with learning rate 0.06, and diverges when ? max &gt; 0.06. For training with Adam, we set ? = 5 ? 10 ?4 for the objective of GradInit, and tried ? max and ? 2 as listed in <ref type="figure" target="#fig_3">Figure 3</ref>. We evaluate the BLEU score every epoch, and report the best BLEU scores throughout training for each method. For GradInit, we set the maximum number of iterations T to 780. By comparison, the warmup stage usually takes 4000 iterations. As discussed in Section 3.3, we also set ? = 10 3 .  <ref type="table">Table 8</ref>: Using GradInit without the gradient norm constraint with different overlapping ratios r to initialize and train a VGG-19 (w/ BN). For both r = 0.5 and r = 1, we tried ? from the range of 1 ? 10 ?4 to 2 ? 10 ?2 . The first two rows show the results with the best final test accuracy Acc best among different ? 's, while the last row shows using a larger ? for r = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Mini</head><p>The choice ofS, the minibatch on which the objective L(S; ? ? ?A[g(S; g)]) is evaluated, has great influence on the results. We have chosenS to have 50% of its samples from S to reduce the variance of the objective. IfS is a completely new batch, i.e., the overlapping ratio r = |S ? S|/|S| is 0, then it becomes difficult for GradInit to work with some models with high initial gradient variance. On the other hand, when we use the same minibatch (S = S), the objective does not capture the stochasticity of the optimizer A and can cause undesirable results in some cases. We study the effect of different choices ofS through VGG-19 networks on CIFAR-10. We consider two settings.</p><p>In the first setting, we evaluate VGG-19 (w/o BN) initialized with GradInit using different overlapping ratios of S andS. The results are given in <ref type="table">Table 8</ref>. As we have shown in <ref type="figure" target="#fig_9">Figure 6</ref>, VGG-19 (w/o BN) has high initial gradient variance. When r = 0, sometimes the test accuracy after the first epoch is only 10%, which is worse than the baseline without GradInit. This indicates when r = 0, the high variance of L(S; ? ? ?A[g(S; g)]) hinders the effectiveness of GradInit. When r = 1, GradInit does effectively reduce the initial gradient variance, achieving lower variance in the first-epoch test accuracy (Acc 0 ) and higher final test accuracy (A test ) than the baseline (Kaiming Initialization in <ref type="table" target="#tab_2">Table 3</ref>), but the result is not as ideal as using r = 0.5. We leave more fine-grained evaluation on the choice of overlapping ratio r as future work.</p><p>In the second setting, we consider removing the gradient norm constraint of GradInit (by setting ? to ?) while using overlapping ratios r = 1 and r = 0.5 respectively for a VGG-19 (w/ BN) model. We remove the gradient norm constraint to highlight the different degrees of reliance of the two approaches on the gradient constraint. As shown in <ref type="table">Table 8</ref>, when r = 1, we have to use the smallest ? , which results in minimum change to the scale factors, to obtain results that are not significantly worse than the baseline (Kaiming initialization listed in <ref type="table" target="#tab_2">Table 3</ref>). It is easy for these large over-parameterized models to overfit a single minibatch with the scale factors. When r = 1, GradInit learns a greedy strategy, which increases the gradient as much as possible to enable a steeper descent that sometimes can reduce the loss on the same minibatch by more than 50% in just one iteration. The greedy strategy tends to blow up of the gradient norm at initialization, which hinders convergence and results in a higher dependence on the gradient norm constraint ?. However, when we use ? = 0.5, GradInit is able to improve the baseline without any gradient norm constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Magnification Effect of BN</head><p>Intuitively, if we stop the gradient passing through the mean and bias of the BN layer during backpropagation, the BN layer will magnify the gradient variance when the variance of its input features is smaller than 1 in the forward pass. Here we show its magnification effect analytically for the practical case where the gradient is not stopped for the mean and bias of the BN layer. From the input to the output, the layers are usually ordered as Linear, BN, Activation. Without loss of generality, we assume the linear layer before BN is X = ZW + b, where the output features X = [x 1 , ..., x n ] T ? R n?d , the input activations Z ? R n?k , n is the number of samples and d, k are the dimension of each feature vector. Batch Normalization normalizes each activation vector x i as following</p><formula xml:id="formula_4">y i = ? x i ? ? ? ? 2 + + ?,<label>(3)</label></formula><p>where all operators are element-wise, ?, ? ? R d are learnable parameters usually initialized to 1 and 0 respectively, &gt; 0 is a small constant for numerical stability, and </p><formula xml:id="formula_5">? 2 = 1 n n i=1 (x i ? ?) 2 , ? = 1 n n i=1 x i .<label>(4)</label></formula><formula xml:id="formula_6">?L ?(?x i ) = ? n ? ? 2 ? 2 + ? ? n ?L ?y i ? n j=1 ?L ?y j ? y i ? ? ? n j=1 ?L ?y j ? y j ? ? ? ? ? ,<label>(5)</label></formula><p>where, again, all operations are element-wise. Therefore, when ? is larger, the variance of the input feature ? 2 ? 2 is larger, and the gradient variance becomes smaller after propagated through this BN layer. Since Z remains the same, Var ?L ?W becomes smaller. This explains why GradInit learns to enlarge the weights of Conv layers in the VGG-19 (w/ BN) experiments. Further, to simplify the analysis and show its magnification effect on gradient variance when ? 2 ? 2 &lt; 1, let ? = 1, ? = 0, and we assume each dimension of ?L ?yi is i.i.d., and y i is independent from ?L ?yi , which is not necessarily a stronger assumption than <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, then</p><formula xml:id="formula_7">Var ?L ?(?x i ) = 1 n 2 (? 2 ? 2 + ) Var ? ? n ?L ?y i ? n j=1 ?L ?y j ? y i n j=1 ?L ?y j ? y j ? ? = 1 n 2 (? 2 ? 2 + ) Var ? ? (n ? 1 ? y 2 i ) ?L ?y i ? n j=1,j =i (1 + y i y j ) ?L ?y j ? ? ? 1 n 2 (? 2 ? 2 + ) ? ? ? (n ? 1) 2 Var ?L ?y i + n j=1,j =i Var ?L ?y j ? ? ? = n(n ? 1) n 2 (? 2 ? 2 + ) Var ?L ?y i ,<label>(6)</label></formula><p>where the inequality comes from the assumption that y i is independent from ?L ?yi and the fact that</p><formula xml:id="formula_8">Var[(X + a)Y ] ? Var[X] + a 2 Var[Y ]</formula><p>(a is a constant ) when X, Y are independent, and the last equality comes from the i.i.d. assumption. Therefore, if is ignorable and ? 2 ? 2 &lt; n(n?1) n 2 , we will have</p><formula xml:id="formula_9">Var ?L ?(?x i ) &gt; Var ?L ?y i ,<label>(7)</label></formula><p>i.e., the BN layer magnifies the gradient variance when ? 2 ? 2 is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Improved Implementation of MetaInit</head><p>MetaInit was originally designed to be task-agnostic, and learns to initialize the network with random samples as inputs. Here, for fair comparisons, we also feed training data to MetaInit, as this should intuitively improve MetaInit for the specific task, and use Adam with the same gradient clipping to optimize the weight norms for MetaInit. Originally, MetaInit <ref type="bibr" target="#b15">[16]</ref> uses signSGD with momentum <ref type="bibr" target="#b41">[41]</ref>, but we found using Adam with the hyperparameters above can give better results for MetaInit. <ref type="table" target="#tab_11">Table 9</ref> shows the comparison before and after the changes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head><p>We give additional results on WRN-28-10, FixUpResNet and DensetNet-100 on CIFAR-10 in <ref type="table" target="#tab_0">Table 10</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Weight norms and gradient variances</head><p>In this section, we give weight norms and gradient variances before and after GradInit is applied on various datasets and networks. We consider DenseNet-100 (w/o BN) and DenseNet-100 (w/ BN) on CIFAR-10 in <ref type="figure" target="#fig_11">Figure 7</ref> and <ref type="figure" target="#fig_12">Figure 8</ref>, as well as ResNet-50 on ImageNet in <ref type="figure" target="#fig_2">Figure 2</ref>. We also compare the weight norms and gradient variances of the Post-LN Transformer model initialized using GradInit with A set to Adam and SGD respectively in <ref type="figure" target="#fig_13">Figure 9</ref> and <ref type="figure" target="#fig_1">Figure 10</ref>.   <ref type="figure">Figure 4</ref>: Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (?(gi)) for each layer i of the VGG-19 (w/ BN) on CIFAR-10. The ratio between the weight magnitudes of GradInit and Kaiming Initialization is the learned scale factor of GradInit in each layer. The standard deviation is computed over the minibatches, with a batch size of 128, with the BN in its training mode. This VGG-19 on CIFAR-10 has only one FC layer, but it has the same number of convolutional layers <ref type="bibr" target="#b15">(16)</ref> as its ImageNet version. All the weights are indexed from shallow to deep, so the first 16 entries of the Linear Weights are of Conv layers, while the 17th is the FC layer. Due to the magnification effect of BN, ?(g1)/?(g16) for the Conv layers is higher than it is in VGG-19 without BN, shown in <ref type="figure" target="#fig_9">Figure 6</ref>. GradInit learns to reduce the magnification effect of BN layers by scaling up all the Conv layers and most of the BN layers, given it has greatly down scaled the last two BN layers and the final FC layer to reduce the variance in the forward pass.  GradInit finds a combination of weight norms where the gradient variance is reduced for all layers. Specifically, it learns to further scale down the second BN layer of each residual block in deeper layers, which is a useful strategy, as deeper layers should have less marginal utility for the feature representations, and scaling down those layers helps to alleviate the growth in variance in the forward pass <ref type="bibr" target="#b9">[10]</ref>. GradInit also learns to scale up weights of the first BN layer and all the Conv layers in each residual block, which alleviates the magnification effect of the BN layers on the gradient variance during backpropagation, happening if their input features in the forward pass have small variances. The jump on the curves occur when the dimension of the convolutional filters changes.    ((?(gi))) of the VGG-19 (left two) and ResNet-110 (right two) without BN on CIFAR-10, evaluated with a batch size of 128. For VGG-19 (w/o BN), ?(gi) increases at Conv layers with different input and output dimensions during backpropagation. For ResNet-110 without GradInit, the gradient variance is very high due to the cumulative effect of skip connections during the forward pass. In this scenario, to reduce the gradient variance, there is no reason to increase the weights, so GradInit downscales the weights for all layers in both architectures, unlike the case with BN.   <ref type="figure" target="#fig_9">Figure 6</ref>, DenseNet-100 does not significantly increase the gradient variance during backpropagation. The standard deviation of the gradient is reduced by around 10 6 with GradInit, which explains why it is possible to train DenseNet-100 (w/o BN) without gradient clipping after using GradInit. The major source of gradient reduction of GradInit comes from reducing the weights in each layer.   Transformer blocks in its encoder and decoder. In each plot, we first list the values for weights in the encoder, and then those in the decoder. Inside each encoder, we first list the weights from the self attention layers and then the those from the FFN layers. Inside each decoder, we first list the weights in the order of self attention, encoder attention and FFN. In general, GradInit reduces the variance for all the weights, except for some of the Query-Projection and Key-Projection weights in the decoder, which are inside the softmax operations in the self attention blocks. The major source of gradient variance reduction comes from downscaling the final LN weights of the decoder, as well as the linear layers of each residual branch (Out-Projection and Value-Projection weights, FFN.FC1 and FFN.FC2 weights) in each block. Here, GradInit sets A to SGD. The Transformer model and the way each weight is permuted are the same as in <ref type="figure" target="#fig_13">Figure 9</ref>. Again, in general, GradInit reduces the variance for most of the weights, except for some of the Query-Projection and Key-Projection weights in the decoder, which are inside the softmax operations in the self attention blocks. Different from the patterns in the Adam version, which downscale all the weights in every layer except for the Query-Projection and Key-Projection weights, the SGD version of GradInit mostly reduces the weights in the final Transformer block of the decoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 1 :</head><label>11</label><figDesc>GradInit for learning the initialization of neural networks. Input: Target optimization algorithm A and learning rate ? for model training, initial model parameters ?0, learning rate ? of the GradInit scales m, total iterations T , upper bound of the gradient ?, lower bound for the initialization scalars ? = 0.01. 2: m1 ? 1 3: for t = 1 to T do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Top row: results of ResNet-110 on CIFAR-10. Bottom row: results of ResNet-50 on ImageNet. Left two columns: compare the relative cross-batch gradient variance on the training set for the BN and Conv/FC layers before and after GradInit. Right two columns: weight norms before and after GradInit. Ratio between points in the same layer reflects the scale factor. Note each of the residual blocks has 2 and 3 Conv and BN layers for the ResNet-110 and ResNet-50, respectively. The initial relative gradient variance are reduced for all layers except the final linear layer in both settings. The strategies are similar on two different datasets. Within each residual block, the last BN layer has the smallest scaling factors, and the scales of all Conv layers are surprisingly increased. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparing the convergence of Kaiming Initialization and GradInit on CIFAR-10, for models trained with SGD (left three) and Adam (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>BLEU scores for the Post-LN Transformer without learning rate warmup using Adam on IWSLT-14 DE-EN under different learning rates ?max (y axis) and ?2 (x axis). Each result is averaged over 4 experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(w/o BN), VGG-19 (w/ BN), ResNet-110 (w/o BN), ResNet-110 (w/ BN), FixUpResNet, DenseNet-100 (w/o BN), DenseNet-100 (w/ BN) are 10 ?2 , 10 ?1 , 5 ? 10 ?2 , 5 ? 10 ?3 , 2 ? 10 ?2 , 5 ? 10 ?3 , 10 ?2 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Averaged per-dimension weight magnitude ( Wi /di) and standard deviation of their gradient ((?(gi))) of the Batch Normalization (BN) layers and the linear layers of the ResNet-110 on CIFAR-10. All the layers are indexed from shallow to deep. The linear layers include all Conv layers (2 for each of the residual blocks) and the final FC layer. The ratio between the weight magnitudes of GradInit and Kaiming Initialization is the learned scale factor of GradInit in each layer. The gradient variance is computed with a batch size of 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>) of ResNet-110 (No BN) Linear weights Kaiming's GradInit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Averaged per-dimension weight magnitude ( Wi /di) and standard deviation of their gradient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>) of DensetNet-100 (No BN) Linear weights Kaiming's GradInit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (?(gi)) for each linear layer i in DenseNet-100 (w/o BN). All the layers are indexed from shallow to deep. The linear layers include all convolutional layers and the final fully connected layer. Inside each dense block, each layer concatenates all the preceding features, so their input dimension increases, the weight dimension increases and the weight norm increases. Compared with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (?(gi)) for each (BN or linear) layer i in the DenseNet-100 (w/ BN). All the layers are indexed from shallow to deep. The linear layers include all convolutional layers and the final fully connected layer. The major source of variance reduction comes from down-scaling the final FC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>||Wi||1/di of Transformer FFN.FC1 weights (Adam) ) of Transformer FFN.FC1 weights (Adam) ||Wi||1/di of Transformer FFN.FC2 weights (Adam) ) of Transformer FFN.FC2 weights (Adam) ) of Transformer Value-Projections weights (Adam) Xavier GradInit Weight norm and averaged per-dimension standard deviation of each weight of the normalization layers and linear layers in the Post-LN Transformer. Here, GradInit sets A to Adam. The Transformer has 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>of Transformer Out-Projections weights (SGD) ||Wi||1/di of Transformer FFN.FC1 weights (SGD) ) of Transformer FFN.FC1 weights (SGD) ||Wi||1/di of Transformer FFN.FC2 weights (SGD) ) of Transformer FFN.FC2 weights (SGD) of Transformer Query-Projection weights (SGD) of Transformer Key-Projections weights (SGD) of Transformer Value-Projections weights (SGD) ) of Transformer Value-Projections weights (SGD) Xavier GradInit Weight norm and averaged per-dimension standard deviation of each weight of the normalization layers and linear layers in the Post-LN Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Accuracies on CIFAR-10 using</cell></row><row><cell cols="4">different overlapping ratios ofS and S for</cell></row><row><cell>GradInit.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>|S?S| |S|</cell><cell>Acc 1</cell><cell>Acc best</cell></row><row><cell>VGG-19 w/o BN (20.03 M)</cell><cell>0 0.5 1</cell><cell cols="2">21.9 ? 4.4 94.5 ? 0.1 29.3 ? 0.6 94.7 ? 0.02 28.7 ? 1.0 94.5 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Time cost and accuracy (average of 4 runs) for</cell></row><row><cell cols="5">running one epoch of regularization/constrained form of</cell></row><row><cell>GradInit.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>VGG-19 w/o BN</cell><cell>VGG-19 w/ BN</cell><cell>ResNet-110 w/o BN</cell><cell>ResNet-110 w/ BN</cell></row><row><cell cols="5">Time (s) 82 vs. 56 100 vs. 62 169 vs. 103 269 vs. 195</cell></row><row><cell cols="5">? = 10 ?4 32.3, 94.6 10.6, 93.1 33.7, 93.9 32.4, 95.2 ? = 10 ?2 30.4, 94.5 10.</cell></row></table><note>4, 93.0 36.7, 94.1 32.6, 95.3 ? = 1 18.2, 74.7 38.5, 95.1 30.7, 94.2 36.5, 95.3 ? = 1 29.3, 94.7 47.8, 95.1 36.2, 94.6 38.2, 95.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Acc best 94.5 ? 0.1 94.4 ? 0.1 94.2 ? 0.Acc best 94.7 ? 0.1 95.1 ? 0.1 94.6 ? 0.1 95.4 ? 0.1 96.2 ? 0.(gi)/|E[gi]| (gi)/|E[gi]| of ResNet-110 Linear weights (gi)/|E[gi]| (gi)/|E[gi]| of ResNet-110 BN weights</figDesc><table><row><cell>Model</cell><cell></cell><cell>VGG-19</cell><cell>VGG-19</cell><cell>ResNet-110</cell><cell>ResNet-110</cell><cell>ResNet-1202</cell></row><row><cell></cell><cell></cell><cell>w/o BN</cell><cell>w/ BN</cell><cell>w/o BN</cell><cell>w/ BN</cell><cell>w/ BN</cell></row><row><cell cols="2">(# Params)</cell><cell>(20.03M)</cell><cell>(20.04M)</cell><cell>(1.72M)</cell><cell>(1.73M)</cell><cell>(19.42M)</cell></row><row><cell>Kaiming +1 epoch (Const. LR) +1 epoch (Warmup) MetaInit GradInit</cell><cell cols="4">Acc 1 Acc 1 37.2 ? 1.1 19.6 ? 4.0 21.0 ? 3.8 Acc best 94.4 ? 0.1 94.5 ? 0.1 93.9 ? 0.4 Acc 1 37.4 ? 1.2 53.5 ? 2.9 19.8 ? 0.5 Acc best 94.4 ? 0.1 94.7 ? 0.1 94.1 ? 0.1 Acc 1 30.5 ? 0.9 35.1 ? 0.6 14.6 ? 2.2 Acc best 94.6 ? 0.1 94.6 ? 0.1 94.2 ? 0.1 Acc 1 29.3 ? 0.6 47.8 ? 1.8 36.2 ? 0.8</cell><cell>95.0 ? 0.2 32.5 ? 3.8 94.7 ? 0.3 48.7 ? 1.1 95.1 ? 0.1 29.0 ? 1.5 94.8 ? 0.1 38.2 ? 0.9</cell><cell>94.4 ? 0.6 12.6 ? 2.8 94.0 ? 0.4 28.1 ? 1.3 95.4 ? 0.2 11.7 ? 1.6 95.0 ? 0.5 29.0 ? 1.1</cell></row></table><note>First epoch (Acc1) and best test accuracy over all epochs (Acc best ) for models on CIFAR-10. We report the mean and standard error of the test accuracies in 4 experiments with different random seeds. Best results in each group are in bold.1 29.1 ? 1.5 12.6 ? 0.6 16.1 ? 2.1 23.2 ? 0.9 12.9 ? 2.81 GradInit further stabilizes feedforward nets with BN. BN does stabilize VGG-19 and allows training without gradient clipping, but with an average first-epoch test accuracy of only 12.57 and an average final test accuracy lower than the version without BN (see Table 3), it does not seem to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparing the results of GradInit with fixed BN scale parameters (Fix BN) and only rescale the BN parameters (Only BN). ResNet-110 (w/ BN) 23.2 ? 0.9 95.0 ? 0.2 38.2 ? 0.9 95.4 ? 0.1 24.7 ? 3.1 94.7 ? 0.3 25.4 ? 3.1 94.6 ? 0.3</figDesc><table><row><cell></cell><cell cols="2">Kaiming</cell><cell cols="2">GradInit</cell><cell cols="2">GradInit (Fix BN)</cell><cell cols="2">GradInit (Only BN)</cell></row><row><cell>Model</cell><cell>Acc 0</cell><cell>Acc best</cell><cell>Acc 0</cell><cell>Acc best</cell><cell>Acc 0</cell><cell>Acc best</cell><cell>Acc 0</cell><cell>Acc</cell></row></table><note>best VGG-19 (w/ BN) 12.6 ? 0.6 94.4 ? 0.1 47.8 ? 1.8 95.1 ? 0.1 13.1 ? 0.9 94.6 ? 0.1 14.4 ? 2.1 94.4 ? 0.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparing the results with multiplying each weight matrix with a learnable scaler (Learning Scalars) on CIFAR10. The VGG-19 model is not able to converge unless we reduce the initial learning rate to 0.01, which obtained worse final accuracy. The ResNet-110 model's Acc0 was 10% for 2 of the 4 runs.</figDesc><table><row><cell>Model</cell><cell cols="2">Learning Scalars</cell><cell cols="2">GradInit</cell></row><row><cell></cell><cell>Acc 0</cell><cell>Acc best</cell><cell>Acc 0</cell><cell>Acc</cell></row></table><note>best VGG-19 (w/ BN, lr=0.1) 10.0 ? 0.0 10.0 ? 0.0 47.8 ? 1.8 95.1 ? 0.1 VGG-19 (w/ BN, lr=0.01) 50.6 ? 0.8 93.4 ? 0.1 - - ResNet-110 (w/ BN) 21.5 ? 6.9 94.7 ? 0.1 38.2 ? 0.9 95.4 ? 0.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Acc1/Acc best of ResNet-50 models on ImageNet. Result of MetaInit comes from Dauphin and Schoenholz<ref type="bibr" target="#b15">[16]</ref> and we reimplemented the rest.</figDesc><table><row><cell></cell><cell>Kaiming</cell><cell>FixUp</cell><cell cols="2">MetaInit GradInit</cell></row><row><cell>w/ BN</cell><cell>14.6/75.9</cell><cell>-</cell><cell>-</cell><cell>19.2/76.2</cell></row><row><cell>w/o BN</cell><cell>-</cell><cell>18.0/75.7</cell><cell>-/75.4</cell><cell>19.2/75.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">Remove LN w skip Warmup Optimizer BLEU</cell></row><row><cell>Standard [6]</cell><cell>RAdam</cell><cell>35.6</cell></row><row><cell>FixUp [9]</cell><cell>Adam</cell><cell>34.5</cell></row><row><cell>T-FixUp [5]</cell><cell>Adam</cell><cell>35.5</cell></row><row><cell>Admin [6]</cell><cell>RAdam</cell><cell>35.7</cell></row><row><cell>Admin</cell><cell>Adam</cell><cell>36.1</cell></row><row><cell>Admin</cell><cell>SGD</cell><cell>33.7</cell></row><row><cell>GradInit</cell><cell>Adam</cell><cell>36.0</cell></row><row><cell>GradInit</cell><cell>Adam</cell><cell>36.1</cell></row><row><cell>GradInit</cell><cell>SGD</cell><cell>35.6</cell></row><row><cell cols="2">Dataset, Architecture, &amp; Hyperparameters.</cell><cell></cell></row></table><note>A comparison of GradInit with with the results from the papers (top 4 rows), and our reimplementation of Admin for training the Post-LN Transformer model on the IWSLT-14 De-EN dataset. "Standard" refers to training with standard initialization and warmup.IWSLT'14 DE-EN [28] is a German to English translation dataset that has 160k training examples. Our Transformer model is inherited from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>w skip obtains a good average BLEU score of 36.0, while without w skip only succeeded in obtaining a BLEU score &gt; 35 for one out of four experiments, resulting in an average BLEU score of 8.9.</figDesc><table><row><cell></cell><cell cols="3">Admin (w/ wskip)</cell><cell cols="3">GradInit (w/ wskip)</cell><cell cols="3">GradInit (w/o wskip)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.0</cell></row><row><cell>2 ? 10 ?4</cell><cell>35.4</cell><cell>35.4</cell><cell>35.5</cell><cell>35.3</cell><cell>35.3</cell><cell>35.3</cell><cell>35.3</cell><cell>35.5</cell><cell>35.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.5</cell></row><row><cell>3 ? 10 ?4</cell><cell>35.8</cell><cell>35.9</cell><cell>35.9</cell><cell>35.7</cell><cell>35.8</cell><cell>35.7</cell><cell>35.8</cell><cell>35.9</cell><cell>35.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.0</cell></row><row><cell>4 ? 10 ?4</cell><cell>36.1</cell><cell>36.1</cell><cell>27.1</cell><cell>36.0</cell><cell>36.0</cell><cell>35.9</cell><cell>35.9</cell><cell>36.0</cell><cell>36.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.5</cell></row><row><cell>5 ? 10 ?4</cell><cell>0.4</cell><cell>0.4</cell><cell>0.2</cell><cell>35.9</cell><cell>36.0</cell><cell>36.0</cell><cell>35.6</cell><cell>36.1</cell><cell>8.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34.0</cell></row><row><cell></cell><cell>0.98</cell><cell>0.99</cell><cell>0.995</cell><cell>0.98</cell><cell>0.99</cell><cell>0.995</cell><cell>0.98</cell><cell>0.99</cell><cell>0.995</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>For most initialization schemes, b is initialized to 0. is often small and ignorable. Under these two assumptions, each y i is invariant to the rescaling of W . Rescaling W changes the scale of x i , ? and ? homogeneously. Therefore, among all the parameters of the network, if we only change W by rescaling it into ?W (? &gt; 0), then y i does not change, and consequently, Var[y i ], ?L ?yi and Var[ ?L ?yi ] do not change, but ? 2 becomes ? 2 ? 2 . To see the magnification effect on the gradient variance during backward propagation, we first find the relation between ?L ?yi</figDesc><table><row><cell>and ?L ?(?xi) under different scales ?. In</cell></row><row><cell>fact,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Acc 1 , Acc best for different versions of MetaInit (4 runs). "rand.": using random data. "real": using real data.</figDesc><table><row><cell>config</cell><cell cols="4">vgg19 w/o BN vgg19 w/ BN res.110 w/o BN res.110 w/ BN</cell></row><row><cell>rand. + signSGD</cell><cell>29.08, 94.36</cell><cell>15.62, 94.53</cell><cell>15.91, 93.91</cell><cell>24.47, 94.93</cell></row><row><cell>real + signSGD</cell><cell>30.89, 94.41</cell><cell>16.58, 94.46</cell><cell>16.21, 94.29</cell><cell>26.28, 94.95</cell></row><row><cell>real + Adam</cell><cell>30.48, 94.62</cell><cell>35.09, 94.64</cell><cell>14.55, 94.19</cell><cell>29.00, 94.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>First epoch (Acc1) and best test accuracy over all epochs (Acc best ) for models on CIFAR-10. We report the mean and standard error of the test accuracies in 4 experiments with different random seeds. Best results in each group are in bold. For WRN, we have additionally used MixUp during training to enhance the results, but we do not consider mixup for GradInit to test its transferability to different training regularizations. Its result with MetaInit comes from the MetaInit paper.</figDesc><table><row><cell cols="2">Model</cell><cell>WRN-28-10</cell><cell>FixUpResNet</cell><cell>DenseNet-100</cell><cell>DenseNet-100</cell></row><row><cell></cell><cell></cell><cell>w/ BN</cell><cell>N/A</cell><cell>w/o BN</cell><cell>w/ BN</cell></row><row><cell cols="2">(# Params)</cell><cell>(36.49M)</cell><cell>(1.72M)</cell><cell>(0.75M)</cell><cell>(0.77M)</cell></row><row><cell>Kaiming MetaInit GradInit</cell><cell>Acc 1 Acc best Acc 1 Acc best Acc 1 Acc best</cell><cell>43.1 ? 2.7 97.2 ? 0.1 -97.1 46.3 ? 0.4 97.3 ? 0.1</cell><cell>38.2 ? 0.8 95.4 ? 0.1 21.5 ? 0.6 95.0 ? 0.1 35.0 ? 0.7 95.4 ? 0.1</cell><cell>35.5 ? 0.6 94.0 ? 0.1 35.1 ? 0.2 94.4 ? 0.1 37.2 ? 1.1 94.9 ? 0.1</cell><cell>51.2 ? 1.5 95.5 ? 0.1 46.7 ? 4.0 95.5 ? 0.1 58.2 ? 0.9 95.5 ? 0.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For convenience, we refer to weight vectors/matrices/tensors as "weight matrices", which includes the scale vectors of the normalization layers, the bias vectors, the weight matrices of the fully connected layers, and the convolution kernels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/gpleiss/efficient_densenet_pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shi Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel L</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">High-performance largescale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Why are adaptive methods good for attention models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sai Praneeth Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metainit: Initializing learning by learning to initialize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12645" to="12657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fractional moment-preserving initialization schemes for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Gurbuzbalaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhan</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Traditional and heavy-tailed self regularization in neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08276</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanru</forename><forename type="middle">Henry</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Theoretical analysis of auto rate-tuning by batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruosi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08419</idno>
		<title level="m">Spherical motion dynamics of deep neural networks with batch normalization and weight decay</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dissecting adam: The sign, magnitude and variance of stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maxva: Fast adaptation of step sizes by maximizing observed variance of gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">signsgd: Compressed optimisation for non-convex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
