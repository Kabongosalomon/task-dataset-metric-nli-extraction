<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Twenty-thousand Classes using Image-level Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Twenty-thousand Classes using Image-level Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at https://github.com/facebookresearch/Detic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection consists of two sub-problems -finding the object (localization) and naming it (classification). Traditional methods tightly couple these two subproblems and thus rely on box labels for all classes. Despite many data collection efforts, detection datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> are much smaller in overall size and vocabularies than classification datasets <ref type="bibr" target="#b9">[10]</ref>. For example, the recent LVIS detection dataset <ref type="bibr" target="#b17">[18]</ref> has 1000+ classes with 120K images; OpenImages <ref type="bibr" target="#b27">[28]</ref> has 500 classes in 1.8M images. Moreover, not all classes contain sufficient annotations to train a robust detector (see <ref type="figure" target="#fig_0">Figure 1</ref> Top). In classification, even the ten-yearold ImageNet <ref type="bibr" target="#b9">[10]</ref> has 21K classes and 14M images <ref type="figure" target="#fig_0">(Figure 1</ref> Bottom).</p><p>In this paper, we propose Detector with image classes (Detic) that uses image-level supervision in addition to detection supervision. We observe that the localization and classification sub-problems can be decoupled. Modern region proposal networks already localize many 'new' objects using existing detection supervision. Thus, we focus on the classification sub-problem and use image-level labels to train the classifier and broaden the vocabulary of the detector. We propose a simple classification loss that applies the image-level supervision to the proposal with the largest size, and do not supervise other outputs for imagelabeled data. This is easy to implement and massively expands the vocabulary.</p><p>Most existing weakly-supervised detection techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b66">67]</ref> use the weakly labeled data to supervise both the localization and classification sub-problems of detection. Since image-classification data has no box labels, these methods develop various label-to-box assignment techniques based on model predictions to obtain supervision. For example, YOLO9000 <ref type="bibr" target="#b44">[45]</ref> and DLWL <ref type="bibr" target="#b43">[44]</ref> assign the image label to proposals that have high prediction scores on the labeled class. Unfortunately, this prediction-based assignment requires good initial detections which leads to a chicken-and-egg problem-we need a good detector for good label assignment, but we need many boxes to train a good detector. Our method completely side-steps the prediction-based label assignment process by supervising the classification sub-problem alone when using classification data. This also enables our method to learn detectors for new classes which would have been impossible to predict and assign.</p><p>Experiments on the open-vocabulary LVIS <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and the open-vocabulary COCO <ref type="bibr" target="#b1">[2]</ref> benchmarks show that our method can significantly improve over a strong box-supervised baseline, on both novel and base classes. With imagelevel supervision from ImageNet-21K <ref type="bibr" target="#b9">[10]</ref>, our model trained without novel class detection annotations improves the baseline by 8. Center: Existing prediction-based weakly supervised detection methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> use image-level labels by assigning them to the detector's predicted boxes (proposals). Unfortunately, this assignment is error-prone, especially for large vocabulary detection. Right: Detic simply assigns the image-labels to the max-size proposal. We show that this loss is both simpler and performs better than prior work.</p><p>outperforms the previous state-of-the-art OVR-CNN <ref type="bibr" target="#b71">[72]</ref> by 5 point with the same detector and data. Finally, we train a detector using the full ImageNet-21K with more than twenty-thousand classes. Our detector generalizes much better to new datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Weakly-supervised object detection (WSOD) trains object detector using image-level labels. Many works use only image-level labels without any box supervision <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70]</ref>. WSDDN <ref type="bibr" target="#b2">[3]</ref> and OIRC <ref type="bibr" target="#b59">[60]</ref> use a subnetwork to predict per-proposal weighting and sum up proposal scores into a single image scores. PCL <ref type="bibr" target="#b58">[59]</ref> first clusters proposals and then assign image labels at the cluster level. CASD <ref type="bibr" target="#b21">[22]</ref> further introduces feature-level attention and self-distillation. As no bounding box supervision is used in training, these methods rely on low-level region proposal techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b61">62]</ref>, which leads to reduced localization quality. Another line of WSOD work uses bounding box supervision together with image labels, known as semi-supervised WSOD <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b74">75]</ref>. YOLO9000 <ref type="bibr" target="#b44">[45]</ref> mixes detection data and classification data in the same minibatch, and assigns classification labels to anchors with the highest predicted scores. DLWL <ref type="bibr" target="#b43">[44]</ref> combines self-training and clustering-based WSOD <ref type="bibr" target="#b58">[59]</ref>, and again assigns image labels to max-scored proposals. MosaicOS <ref type="bibr" target="#b72">[73]</ref> handles domain differences between detection and image datasets by mosaic augmentation <ref type="bibr" target="#b3">[4]</ref> and proposed a three-stage self-training and finetuning framework. In segmentation, Pinheiro et al. <ref type="bibr" target="#b40">[41]</ref> use a log-sum-exponential function to aggregate pixels scores into a global classification. Our work belongs to semi-supervised WSOD. Unlike prior work, we use a simple image-supervised loss. Besides image labels, researchers have also studied complementary methods for weak localization supervision like points <ref type="bibr" target="#b6">[7]</ref> or scribles <ref type="bibr" target="#b46">[47]</ref>. Open-vocabulary object detection, or also named zero-shot object detection, aims to detect objects outside of the training vocabulary. The basic solution <ref type="bibr" target="#b1">[2]</ref> is to replace the last classification layer with language embeddings (e.g., GloVe <ref type="bibr" target="#b39">[40]</ref>) of the class names. Rahman et al. <ref type="bibr" target="#b42">[43]</ref> and Li et al. <ref type="bibr" target="#b32">[33]</ref> improve the classifier embedding using external text information. OVR-CNN <ref type="bibr" target="#b71">[72]</ref> pretrains the detector on image-text pairs. ViLD <ref type="bibr" target="#b16">[17]</ref>, OpenSeg <ref type="bibr" target="#b15">[16]</ref> and langSeg <ref type="bibr" target="#b28">[29]</ref> upgrade the language embedding to CLIP <ref type="bibr" target="#b41">[42]</ref>. ViLD further distills region features from CLIP image features. We use CLIP <ref type="bibr" target="#b41">[42]</ref> classifier as well, but do not use distillation. Instead, we use additional image-labeled data for co-training. Large-vocabulary object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b68">69]</ref> requires detecting 1000+ classes. Many existing works focus on handling the long-tail problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b73">74]</ref>. Equalization losses <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref> and SeeSaw loss <ref type="bibr" target="#b63">[64]</ref> reweights the per-class loss by balancing the gradients <ref type="bibr" target="#b54">[55]</ref> or number of samples <ref type="bibr" target="#b63">[64]</ref>. Federated Loss <ref type="bibr" target="#b75">[76]</ref> subsamples classes per-iteration to mimic the federated annotation <ref type="bibr" target="#b17">[18]</ref>. Yang et al. <ref type="bibr" target="#b68">[69]</ref> detects 11K classes with a label hierarchy. Our method builds on these advances, and we tackle the problem from a different aspect: using additional image-labeled data. Proposal Network Generalization. ViLD <ref type="bibr" target="#b16">[17]</ref> reports that region proposal networks have certain generalization abilities for new classes by default. Dave et al. <ref type="bibr" target="#b8">[9]</ref> shows segmentation and localization generalizes across classes. Kim et al. <ref type="bibr" target="#b24">[25]</ref> further improves proposal generalization with a localization quality estimator. In our experiments, we found proposals to generalize well enough (see Appendix A), as also observed in ViLD <ref type="bibr" target="#b16">[17]</ref>. Further improvements to RPNs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> can hopefully lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We train object detectors using both object detection and image classification datasets. We propose a simple way to leverage image supervision to learn object detectors, including for classes without box labels. We first describe the object detection problem and then detail our approach. Problem setup. Given an image I ? R 3?h?w , object detection solves the two subproblems of (1) localization: find all objects with their location, represented as a box b j ? R 4 and (2) classification: assign a class label c j ? C test to the j-th object. Here C test is the class vocabulary provided by the user at test time. During training, we use a detection dataset</p><formula xml:id="formula_0">D det = {(I, {(b, c) k }) i } |D det | i=1</formula><p>with vocabulary C det that has both class and box labels. We also use an image classification dataset</p><formula xml:id="formula_1">D cls = {(I, {c k }) i } |D cls | i=1</formula><p>with vocabulary C cls that only has image-level class labels. The vocabularies C test , C det , C cls may or may not overlap. Traditional Object detection considers C test = C det and D cls = ?. Predominant object detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46]</ref>   called the region proposal network (RPN), takes the image I and produces a set</p><formula xml:id="formula_2">of object proposals {(b, f , o) j }, where f j ? R D is a D-dimensional region feature</formula><p>and o ? R is the objectness score. The second stage takes the object feature and outputs a classification score and a refined box location for each object,</p><formula xml:id="formula_3">s j = Wf j , b j = Bf j + b j , where W ? R |C det |?D and B ? R 4?D</formula><p>are the learned weights of the classification layer and the regression layer, respectively. 1 Our work focuses on improving classification in the second stage. In our experiments, the proposal network and the bounding box regressors are not the current performance bottleneck, as modern detectors use an over-sufficient number of proposals in testing (1K proposals for &lt; 20 objects per image. see Appendix A for more details).</p><p>Open-vocabulary object detection allows C test = C det . Simply replacing the classification weights W with fixed language embeddings of class names converts a traditional detector to an open-vocabulary detector <ref type="bibr" target="#b1">[2]</ref>. The region features are trained to match the fixed language embeddings. We follow Gu et al. <ref type="bibr" target="#b16">[17]</ref> to use the CLIP embeddings <ref type="bibr" target="#b41">[42]</ref> as the classification weights. In theory, this open-vocabulary detector can detect any object class. However, in practice, it yields unsatisfying results as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Our method uses image-level supervision to improve object detection including in the open-vocabulary setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detic: Detector with Image Classes</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, our method leverages the box labels from detection datasets D det and image-level labels from classification datasets D cls . During training, we compose a mini-batch using images from both types of datasets. For images with box labels, we follow the standard two-stage detector training <ref type="bibr" target="#b45">[46]</ref>. For image-level labeled images, we only train the features from a fixed region proposal for classification. Thus, we only compute the localization losses (RPN loss and bounding box regression loss) on images with ground truth box labels. Below we describe our modified classification loss for image-level labels. A sample from the weakly labeled dataset D cls contains an image I and a set of K labels {c k } K k=1 . We use the region proposal network to extract N object features</p><formula xml:id="formula_4">{(b, f , o) j } N j=1 .</formula><p>Prediction-based methods try to assign image labels to regions, and aim to train both localization and classification abilities. Instead, we propose simple ways to use the image labels {c k } K k=1 and only improve classification. Our key idea is to use a fixed way to assign image labels to regions, and side-step a complex prediction-based assignment. We allow the fixed assignment schemes miss certain objects, as long as they miss fewer objects than the prediction-based counterparts, thus leading to better performance. Non-prediction-based losses. We now describe a variety of simple ways to use image labels and evaluate them empirically in <ref type="table" target="#tab_3">Table 1</ref>. Our first idea is to use the whole image as a new "proposal" box. We call this loss image-box. We ignore all proposals from the RPN, and instead use an injected box of the whole image b = (0, 0, w, h). We then apply the classification loss to its RoI features f for all classes c ? {c k } K k=1 :</p><formula xml:id="formula_5">L image-box = BCE(Wf , c) where BCE(s, c) = ?log?(s c ) ? k =c log(1 ? ?(s k )</formula><p>) is the binary cross-entropy loss, and ? is the sigmoid activation. Thus, our loss uses the features from the same 'proposal' for solving the classification problem for all the classes {c k }.</p><p>In practice, the image-box can be replaced by smaller boxes. We introduce two alternatives: the proposal with the max object score or the proposal with the max size:</p><formula xml:id="formula_6">L max-object-score = BCE(Wf j , c), j = argmax j o j L max-size = BCE(Wf j , c), j = argmax j (size(b j ))</formula><p>We show that all these three losses can effectively leverage the image-level supervision, while the max-size loss performs the best. We thus use the max-size loss by default for image-supervised data. We also note that the classification parameters W are shared across both detection and classification data, which greatly improves detection performance. The overall training objective is</p><formula xml:id="formula_7">L(I) = L rpn + L reg + L cls , if I ? D det ?L max-size , if I ? D cls</formula><p>where L rpn , L reg , L cls are standard losses in a two-stage detector, and ? = 0.1 is the weight of our loss. Relation to prediction-based assignments. In traditional weakly-supervised detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>, a popular idea is to assign the image to the proposals based on model prediction. Let F = (f 1 , . . . , f N ) be the stacked feature of all object proposals and S = WF be their classification scores.</p><formula xml:id="formula_8">For each c ? {c k } K k=1 , L = BCE(S j , c), j = F(S, c),</formula><p>where F is the label-to-box assignment process. In most methods, F is a function of the prediction S. For example, F selects the proposal with max score on c. Our key insight is that F should not depend on the prediction S. In large-vocabulary detection, the initial recognition ability of rare or novel classes is low, making the label assignment process inaccurate. Our method side-steps this prediction-and-assignment process entirely and relies on a fixed supervision criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate Detic on the large-vocabulary object detection dataset LVIS <ref type="bibr" target="#b17">[18]</ref>. We mainly use the open-vocabulary setting proposed by Gu et al. <ref type="bibr" target="#b16">[17]</ref>, and also report results on the standard LVIS setting. We describe our experiment setup below. LVIS. The LVIS <ref type="bibr" target="#b17">[18]</ref> dataset has object detection and instance segmentation labels for 1203 classes with 100K images. The classes are divided into three groups -frequent, common, rare based on the number of training images. We refer to this standard LVIS training set as LVIS-all. Following ViLD <ref type="bibr" target="#b16">[17]</ref>, we remove the labels of 337 rare-class from training and consider them as novel classes in testing. We refer to this partial training set with only frequent and common classes as LVIS-base. We report mask mAP which is the official metric for LVIS. While our model is developed for box detection, we use a standard class-agnostic mask head <ref type="bibr" target="#b19">[20]</ref> to produce segmentation masks for boxes. We train the mask head only on detection data. Image-supervised data. We use two sources of image-supervised data: ImageNet-21K <ref type="bibr" target="#b9">[10]</ref> and Conceptual Captions <ref type="bibr" target="#b49">[50]</ref>. ImageNet-21K (IN-21K) contains 14M images for 21K classes. For ease of training and evaluation, most of our experiments use the 997 classes that overlap with the LVIS vocabulary and denote this subset as IN-L. Conceptual Captions <ref type="bibr" target="#b49">[50]</ref> (CC) is an image captioning dataset containing 3M images. We extract image labels from the captions using exact text-matching and keep images whose captions mention at least one LVIS class. See Appendix B for results of directly using captions. The resulting dataset contains 1.5M images with 992 LVIS classes. We summarize the datasets used below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>Box-Supervised: a strong LVIS baseline. We first establish a strong baseline on LVIS to demonstrate that our improvements are orthogonal to recent advances in object detection. The baseline only uses the supervised bounding box labels. We use the CenterNet2 <ref type="bibr" target="#b75">[76]</ref> detector with ResNet50 <ref type="bibr" target="#b20">[21]</ref> backbone. We use Federated Loss <ref type="bibr" target="#b75">[76]</ref> and repeat factor sampling <ref type="bibr" target="#b17">[18]</ref>. We use large scale jittering <ref type="bibr" target="#b14">[15]</ref> with input resolution 640?640 and train for a 4? (?48 LVIS epochs) schedule. To show our method is compatible with better pretraining, we use ImageNet-21k pretrained backbone weights <ref type="bibr" target="#b47">[48]</ref>. As described in ? 3, we use the CLIP <ref type="bibr" target="#b41">[42]</ref> embedding as the classifier. Our baseline is 9.1 mAP higher than the detectron2 baseline [66] (31.5 vs. 22.4 mAP mask ) and trains in a similar time (17 vs. 12 hours on 8 V100 GPUs). See Appendix C for more details.</p><p>Resolution change for image-labeled images. ImageNet images are inherently smaller and more object-focused than LVIS images <ref type="bibr" target="#b72">[73]</ref>. In practice, we observe it is important to use smaller image resolution for ImageNet images.</p><p>Using smaller resolution in addition allows us to increase the batch-size with the same computation. In our implementation, we use 320?320 for ImageNet and CC and ablate this in Appendix D.</p><p>Multi-dataset training. We sample detection and classification mini-batches in a 1 : 1 ratio, regardless of the original dataset size. We group images from the same dataset on the same GPU to improve training efficiency <ref type="bibr" target="#b76">[77]</ref>.</p><p>Training schedules. To shorten the experimental cycle and have a good initialization for prediction-based WSOD losses <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>  <ref type="table" target="#tab_3">Table 1</ref> shows the results of the box-supervised baseline, existing predictionbased methods, and our proposed non-prediction-based methods. The baseline (Box-Supervised) is trained without access to novel class bounding box labels. It uses the CLIP classifier <ref type="bibr" target="#b16">[17]</ref> and has open-vocabulary capabilities with 16.3 mAP novel . In order to leverage additional image-labeled data like ImageNet or CC, we use prior prediction-based methods or our non-prediction-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prediction-based vs non-prediction-based methods</head><p>We compare a few prediction-based methods that assign image labels to proposals based on predictions. Self-training assigns predictions of Box-Supervised as pseudo-labels offline with a fixed score threshold (0.5). The other predictionbased methods use different losses to assign predictions to image labels online. See Appendix E for implementation details. For DLWL <ref type="bibr" target="#b43">[44]</ref>, we implement a simplified version that does not include bootstrapping and refer to it as DLWL*. . This baseline is trained on boxes from the base classes and has non-zero novel-class mAP as it uses the CLIP classifier. All models in the following rows are finetuned from the baseline model and leverage image-labeled data.</p><p>We repeat experiments for 3 runs and report mean/ std. All variants of our proposed non-prediction-based losses outperform existing prediction-based counterparts.</p><p>generalize to different types of image-labeled data. Overall, the results from <ref type="table" target="#tab_3">Table 1</ref> suggest that complex prediction-based methods that overly rely on model prediction scores do not perform well for open-vocabulary detection. Amongst our non-prediction-based variants, the max-size loss consistently performs the best, and is the default for Detic in our following experiments. Why does max-size work? Intuitively, our simpler non-prediction methods outperform the complex prediction-based method by side-stepping a hard assignment problem. Prediction-based methods rely on strong initial detections to assign image-level labels to predicted boxes. When the initial predictions are reliable, prediction-based methods are ideal. However, in open-vocabulary scenarios, such strong initial predictions are absent, which explains the limited performance of prediction-based methods. Detic's simpler assignment does not rely on strong predictions and is more robust under the challenges of open-vocabulary setting. We now study two additional advantages of the Detic max-size variant over prediction-based methods that may contribute to improved performance: 1) the selected max-size proposal can safely cover the target object; 2) the selected max-size proposal is consistent during different training iterations. <ref type="figure">Figure 4</ref> provides typical qualitative examples of the assigned region for the prediction-based method and our max-size variant. On an annotated subset of IN-L, Detic max-size covers 92.8% target objects, vs. 69.0% for the prediction-based method. Overall, unlike prediction-based methods, Detic's simpler assignment yields boxes that are more likely to contain the object. Indeed, Detic may miss  <ref type="figure">Fig. 4</ref>: Visualization of the assigned boxes during training. We show all boxes with score &gt; 0.5 in blue and the assigned (selected) box in red. Top: The predictionbased method selects different boxes across training, and the selected box may not cover the objects in the image. Bottom: Our simpler max-size variant selects a box that covers the objects and is more consistent across training.</p><p>certain objects (especially small objects) or supervise to a loose region. However, in order for Detic to yield a good detector, the selected box need not be perfect, it just needs to 1) provide meaningful training signal (cover the objects and be consistent during training); 2) be 'more correct' than the box selected by the prediction-based method. We provide details about our metrics, more quantitative evaluation, and more discussions in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with a fully-supervised detector</head><p>In   <ref type="table">Table 3</ref>: Open-vocabulary COCO <ref type="bibr" target="#b1">[2]</ref>. We compare Detic using the same training data and architecture from OVR-CNN <ref type="bibr" target="#b71">[72]</ref>. We report box mAP at IoU threshold 0.5 using Faster R-CNN with ResNet50-C4 backbone. Detic builds upon the CLIP baseline (second row) and shows significant improvements over prior work. ?: results quoted from OVR-CNN <ref type="bibr" target="#b71">[72]</ref> paper or code. ?: results quoted from the original publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the state-of-the-art</head><p>We In each case, we strictly follow the architecture and setup from prior work to ensure fair comparisons.</p><p>Open-vocabulary LVIS. We compare to ViLD <ref type="bibr" target="#b16">[17]</ref>, which first uses CLIP embeddings <ref type="bibr" target="#b41">[42]</ref> for open-vocabulary detection. We strictly follow their training setup and model architecture (Appendix G) and report results in <ref type="table" target="#tab_6">Table 2</ref>. Here ViLD-text is exactly our Box-Supervised baseline. Detic provides a gain of 7.7 points on mAP novel . Compared to ViLD-text, ViLD, which uses knowledge distillation from the CLIP visual backbone, improves mAP novel at the cost of hurting overall mAP. Ensembling the two models, ViLD-ens provides improvements for both metrics. On the other hand, Detic uses a single model which improves both novel and overall mAP, and outperforms the ViLD ensemble. Open-vocabulary COCO. Next, we compare with prior works on the popular open-vocabulary COCO benchmark <ref type="bibr" target="#b1">[2]</ref> (see benchmark and implementation details in Appendix H). We strictly follow OVR-CNN <ref type="bibr" target="#b71">[72]</ref> to use Faster R-CNN with ResNet50-C4 backbone and do not use any improvements from ? 5.1. Following <ref type="bibr" target="#b71">[72]</ref>, we use COCO captions as the image-supervised data. We extract nouns from the captions and use both the image labels and captions as supervision. <ref type="table">Table 3</ref> summarizes our results. As the training set contains only 48 base classes, the base-class only model (second row) yields low mAP on novel classes. Detic improves the baseline and outperforms OVR-CNN <ref type="bibr" target="#b71">[72]</ref> by a large margin, using exactly the same model, training recipe, and data.</p><p>Additionally, similar to <ref type="table" target="#tab_3">Table 1</ref>, we compare to prior prediction-based methods on the open-vocabulary COCO benchmark in Appendix H. In this setting too, Detic improves over prior work providing significant gains on novel class detection and overall detection performance.</p><p>Objects365 <ref type="bibr" target="#b48">[49]</ref> OpenImages <ref type="bibr" target="#b27">[28]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Detecting 21K classes across datasets without finetuning</head><p>Next, we train a detector with the full 21K classes of ImageNet. We use our strong recipe with Swin-B <ref type="bibr" target="#b36">[37]</ref> backbone. In practice, training a classification layer of 21K classes is computationally involved. <ref type="bibr" target="#b1">2</ref> We adopt a modified Federated Loss <ref type="bibr" target="#b75">[76]</ref> that uniformly samples 50 classes from the vocabulary at every iteration. We only compute classification scores and back-propagate on the sampled classes. As there are no direct benchmark to evaluate detectors with such large vocabulary, we evaluate our detectors on new datasets without finetuning. We evaluate on two large-scale object detection datasets: Objects365v2 <ref type="bibr" target="#b48">[49]</ref> and OpenImages <ref type="bibr" target="#b27">[28]</ref>, both with around 1.8M training images. We follow LVIS to split <ref type="bibr" target="#b0">1</ref> 3 of classes with the fewest training images as rare classes. <ref type="table" target="#tab_8">Table 4</ref> shows the results. On both datasets, Detic improves the Box-Supervised baseline by a large margin, especially on classes with fewer annotations. Using all the 21k classes   further improves performance owing to the large vocabulary. Our single model significantly reduces the gap towards the dataset-specific oracles and reaches 70%-80% of their performance without using the corresponding 1.8M detection annotations. See <ref type="figure" target="#fig_5">Figure 5</ref> for qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation studies</head><p>We now ablate our key components under the open-vocabulary LVIS setting with IN-L as the image-classification data. We use our strong training recipe as described in ? 5.1 for all these experiments. Classifier weights. We study the effect of different classifier weights W.</p><p>While our main open-vocabulary experiments use CLIP <ref type="bibr" target="#b41">[42]</ref>, we show the gain of Detic is independent of CLIP. We train Box-Supervised and Detic with different classifiers, including a standard random initialized and trained classifier, and other fixed language models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> The results are shown in <ref type="table" target="#tab_10">Table 5</ref>. By default, a trained classifier cannot recognize novel classes. However, Detic enables novel class recognition ability even in this setting (17.4 mAP novel for classes without detection labels). Using language models such as FastText <ref type="bibr" target="#b23">[24]</ref> or an open-source version of CLIP <ref type="bibr" target="#b22">[23]</ref> leads to better novel class performance. CLIP <ref type="bibr" target="#b41">[42]</ref> performs the best among them. Effect of Pretraining. Many existing methods use additional data only for pretraining <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73]</ref>, while we use image-labeled data for co-training. We present results of Detic with different types of pretraining in <ref type="table" target="#tab_12">Table 6</ref>. Detic provides similar gains across different types of pretraining, suggesting that our   <ref type="table">Table 7</ref>: Standard LVIS. We evaluate our baseline (Box-Supervised) and Detic using different backbones on the LVIS dataset. We report the mask mAP. We also report prior work on LVIS using large backbone networks (single-scale testing) for references (not for apple-to-apple comparison). ?: detectors using additional data. Detic improves over the baseline with increased gains for the rare classes.</p><p>gains are orthogonal to advances in pretraining. We believe that this is because pretraining improves the overall features, while Detic uses co-training which improves both the features and the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">The standard LVIS benchmark</head><p>Finally, we evaluate Detic on the standard LVIS benchmark <ref type="bibr" target="#b17">[18]</ref>. In this setting, the baseline (Box-Supervised) is trained with box and mask labels for all classes while Detic uses additional image-level labels from IN-L. We train Detic with the same recipe in ? 5.1 and use a strong Swin-B <ref type="bibr" target="#b36">[37]</ref> backbone and 896 ? 896 input size. We report the mask mAP across all classes and also split into rare, common, and frequent classes. Notably, Detic achieves 41.7 mAP and 41.7 mAP r , closing the gap between the overall mAP and the rare mAP. This suggests Detic effectively uses image-level labels to improve the performance of classes with very few boxes labels. Appendix I provides more comparisons to prior work <ref type="bibr" target="#b72">[73]</ref> on LVIS. Appendix J shows Detic generalizes to DETR-based <ref type="bibr" target="#b78">[79]</ref> detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and Conclusions</head><p>We present Detic which is a simple way to use image supervision in largevocabulary object detection. While Detic is simpler than prior assignment-based weakly-supervised detection methods, it supervises all image labels to the same region and does not consider overall dataset statistics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Region proposal quality</head><p>In this section, we show the region proposal network trained on LVIS <ref type="bibr" target="#b17">[18]</ref> is satisfactory and generalizes well to new classes by default. We experiment under our strong baseline in ? 5.1. <ref type="table" target="#tab_14">Table 8a</ref> shows the proposal recalls with or without rare classes in training. First, we observe the recall gaps between the two models on rare classes are small (79.7 vs. 78.5); second, the gaps between rare classes and all classes are small (79.7 vs. 80.9); third, the absolute recall is relatively high (? 80%, note recall at IoU threshold 0.5 can be translated into oracle mAP-pool <ref type="bibr" target="#b7">[8]</ref> given perfect classifier and regressor). All observations indicate the proposals have good generalization abilities to new classes even though they are supervised to background during training. We consider the proposal generalization is currently not the performance bottleneck in open-vocabulary detection. This especially the case as modern detectors use an over-sufficient number of proposals in testing (1K proposals for &lt; 20 objects per image). Our observations are consistent with ViLD <ref type="bibr" target="#b16">[17]</ref>. We in addition evaluate a more strict setting, where we uniformly split LVIS classes into two halves. I.e., we use classes ID 1, 3, 5, ? ? ? as the first half, and the rest as the second half. These two subsets have completely different definitions of "objects". We then train a proposal network on each of them, and evaluate on both subsets. As shown in <ref type="table" target="#tab_14">Table 8b</ref>, the proposal networks give non-trivial recalls at the complementary other half (69.6% over 82.2% percent of the full generalizability). This again supports proposal networks trained on a diverse vocabulary learned a general concept of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Direct captions supervision</head><p>As we are using a language model CLIP <ref type="bibr" target="#b41">[42]</ref>   </p><formula xml:id="formula_9">= L({t i } B i=1</formula><p>). For the i-th image in the minibatch, its "classification" label is the i-th text, and other texts are negative samples. We use the injected whole image box to extract RoI feature f i for image i. We use the same binary cross entropy loss as classifying image labels:</p><formula xml:id="formula_10">L cap = B i=1 BCE( Wf i , i)</formula><p>We do not back-propagate into the language encoder.</p><p>We evaluate the effectiveness of the caption loss in <ref type="table" target="#tab_16">Table 9</ref> on both openvocabulary LVIS and COCO (see dataset details in Appendix H). We compare individually applying the max-size loss for image labels and the caption loss, and applying both of them. Both image labels and captions can improve both overall mAP and novel class mAP. Combining both losses gives a more significant improvement. Our open-vocabulary COCO results in <ref type="table">Table 3</ref> uses both the max-size loss and the caption loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LVIS baseline details</head><p>We first describe the standard LVIS baseline from the detectron2 model zoo <ref type="bibr" target="#b2">3</ref>   The bounding box regression head and the mask head are class-specific. <ref type="table" target="#tab_3">Table 10</ref> shows the roadmap from the detectron2 baseline to our baseline ( ? 5.1). First, we prepare the model for new classes by making the box and mask heads class-agnostic. This slightly hurts performance. We then use Federated loss <ref type="bibr" target="#b75">[76]</ref> and upgrade the detector to CenterNet2 <ref type="bibr" target="#b75">[76]</ref> (i.e., replacing RPN with CenterNet and multiplying proposal score to classification score). Both modifications improve mAP and mAP r significantly, and CenterNet2 slightly increases the training time.</p><p>Next, we use the EfficientDet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b57">58]</ref> style large-scale jittering and train a longer schedule (4?). To balance the training time, we also reduce the training image size to 640 ? 640 (the testing size is unchanged at 800 ? 1333) and increase batch-size to 64 (with the learning rate scaled up to 0.08). The resulting augmentation and schedule is slightly better than the default multi-scale training, with 30% more training time. A longer schedule is beneficial when using more data, and can be improved by larger resolution.</p><p>Next, we switch in the CLIP classifier <ref type="bibr" target="#b41">[42]</ref>. We follow ViLD <ref type="bibr" target="#b16">[17]</ref> to L2 normalize the embedding and RoI feature before dot-product. Note CenterNet2 uses a cascade classifier <ref type="bibr" target="#b4">[5]</ref>. We use CLIP for all of them. Using CLIP classifier improves rare class mAP.</p><p>Finally, we use an ImageNet-21k pretrained ResNet-50 model from Ridnik et al. <ref type="bibr" target="#b47">[48]</ref>. We remark the ImageNet-21k pretrained model requires using Adam optimizer (with learning rate 2e-4  . We use this model as our baseline in the main paper. Increasing the training resolution or using a larger backbone <ref type="bibr" target="#b36">[37]</ref> can further increase performance significantly, at a cost of longer training time. We use the large models only when compared to the state-of-the-art models. <ref type="table" target="#tab_3">Table 11</ref> ablates the resolution change in ? 5.1. Using a smaller input resolution improves ? 1 point for both mAP and mAP novel with ImageNet, but does not impact much with CC. Using more batches for the weak datasets is slightly better than a 1 : 1 ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Resolution change for classification data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Prediction-based losses implementation details</head><p>Following the notations in ? 4, we implement the prediction-based weaklysupervised detection losses as below: WSDDN [3] learns a soft weight on the proposals to weight-sum the proposal classification scores into a single image classification score:</p><formula xml:id="formula_11">L WSDDN = BCE( j (softmax(W F) j * S j ), c)</formula><p>where W is a learnable network parameter. Predicted <ref type="bibr" target="#b44">[45]</ref> selects the proposal with the max predicted score on class c: L Predicted = BCE(S j , c), j = argmax j S jc DLWL* <ref type="bibr" target="#b43">[44]</ref> first runs a clustering algorithm with IoU threshold 0.5. Let J be the set of peaks of each cluster (i.e., the proposal within the cluster and has the max predicted score on class c), We then select the top N c = 3 peaks with the highest prediction scores on class c.</p><formula xml:id="formula_12">L DLWL* = 1 N c Nc t=1 BCE(S jt , c), j t = argmax j?J ,j ={j1,...,jt?1} S jc</formula><p>The original DLWL <ref type="bibr" target="#b43">[44]</ref> in addition upgrades S using an IoU-based assignment matrix from self-training and bootstrapping (See their Section 3.2). In our implementation, we did not include this part, as our goal is to only compare the training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More comparison between prediction-based and non-prediction-based methods</head><p>Our non-prediction-based losses perform significantly better than predictionbased losses as is shown in <ref type="table" target="#tab_3">Table 1</ref>. In this section, we take the max-size loss and the predicted-loss as the representitives and conduct more detailed comparisons between them. A straightforward reason is that the predicted loss requires a good initial prediction to guide the pseudo-label-based training. However in the open-vocabulary detection setting the initial predictions are inherently flawed. To verify this, in <ref type="table" target="#tab_3">Table 12a</ref>, we show both improving the backbone and including rare classes in training can narrow the gap. However in the current performance regime, our max-size loss performs better. We highlight two additional advantages of the max-size loss that may contribute to the good performance: (1) the max-size loss is a safe approximation of object regions; (2) the max-size loss is consistent during training. <ref type="figure">Figure 4</ref> provides qualitative examples of the assigned region for the predicted loss and the max-size loss. First, we observe that while being coarse at the boundary, the max-size loss can cover the target object in most cases. Second, the assigned regions of the predicted loss are usually different across training iterations, especially in the early phase where the model predictions are unstable. On the contrary, max-size loss supervises consistent regions across training iterations. <ref type="table" target="#tab_3">Table 12b</ref> quantitatively evaluates these two properties. We use the ground truth box annotation in the full COCO detection dataset and a subset of ImageNet with bounding box annotation 5 to evaluate the cover rate. We define cover rate as the ratio of image labels whose ground-truth box has &gt; 0.5 intersection-overarea with the assigned region. We define the consistency metric as the average assigned-region IoU of the same image between the 1/2 schedule and the final schedule. <ref type="table" target="#tab_3">Table 12b</ref> shows max-size loss is more favorable than predicted loss on these two metrics. However we highlight that these two metrics alone do not always correlate to the final performance, as the image-box loss is perfect on both metrics but underperforms max-size loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ViLD baseline details</head><p>The baseline in ViLD <ref type="bibr" target="#b16">[17]</ref> is very different from detectron2. They use MaskRCNN detector <ref type="bibr" target="#b19">[20]</ref> with Res50-FPN backbone, but trains the network from scratch without ImageNet pretraining. They use large-scale jittering <ref type="bibr" target="#b14">[15]</ref> with input resolution 1024 ? 1024 and train a 32? schedule. The optimizer is SGD with batch size 256 and learning rate 0.32. We first reproduce their baselines (both the oracle detector and ViLD-text) under the same setting. We observe half of their schedule (16?) is sufficient to closely match their numbers. The half training schedule takes 4 days on 4 nodes (each with 8 V100 GPUs). We then finetune another 16? schedule using ImageNet data with our max-size loss.  The training set is the same as the full COCO, but only images containing at least one base class are used. During testing, we report results under the "generalized zero-shot detection" setting <ref type="bibr" target="#b1">[2]</ref>, where all COCO validation images are used. We strictly follow the literatures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b71">72</ref>] to use FasterRCNN <ref type="bibr" target="#b45">[46]</ref> with ResNet50-C4 backbone and the 1? training schedule (90k iterations). We use horizontal flip as the only data augmentation in training and keep the input resolution fixed to 800 ? 1333 in both training and testing. We use SGD optimizer with a learning rate 0.02 (dropped by 10? at 60k and 80k iteration) and batch size 16. The evaluation metric on open-vocabulary COCO is box mAP at IoU threshold 0.5. Our reproduced baseline matches OVR-CNN <ref type="bibr" target="#b71">[72]</ref>. Our model is finetuned on the baseline model with another 1? schedule. We sample detection data and image-supervised data in a 1 : 1 ratio. <ref type="table" target="#tab_3">Table 13</ref> repeats the experiments in <ref type="table" target="#tab_3">Table 1</ref> on open-vocabulary COCO. The observations are consistent: our proposed non-prediction-based methods outperform existing prediction-based counterparts, and the max-size loss performs the best among our variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Open-vocabulary COCO benchmark details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Compare to MosaicOS [73]</head><p>MosaicOS <ref type="bibr" target="#b72">[73]</ref> first uses image-level annotations to improve LVIS detectors. We compare to MosaicOS <ref type="bibr" target="#b72">[73]</ref> by strictly following their baseline setup (without any improvements in ? 5.1). The detailed hyper-parameters follow the detectron2 baseline as described in Appendix C. We finetune on the Box-supervised model with an additional 2? schedule with Adam optimizer. <ref type="table" target="#tab_3">Table 14</ref> shows our re-trained baseline exactly matches their reported results from the paper. Our method is developed based on the CLIP classifier, and we also report our baseline with CLIP. The baseline has slightly lower mAP and higher mAP r .   outperforms MosaicOS <ref type="bibr" target="#b72">[73]</ref> in mAP and mAP r , without using their multi-stage training and mosaic augmentation. Our relative improvements over the baseline are slightly higher than MosiacOS <ref type="bibr" target="#b72">[73]</ref>. We highlight our training framework is simpler and we use less additional training data (Google-searched images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Generalization to Deformable-DETR.</head><p>We apply Detic to the recent Transformer based Deformable-DETR <ref type="bibr" target="#b78">[79]</ref> to study its generalization. We use their default training recipe, Federated Loss <ref type="bibr" target="#b75">[76]</ref> and train for a 4? schedule (? 48 LVIS epochs). We apply the image supervision to the query from the encoder with the max predicted size.   K Improvements breakdown to classes <ref type="table" target="#tab_3">Table 16</ref> shows mAP breakdown into classes with and without image labels for both the Box-Supervised baseline and Detic. As expected, most of the improvements are from classes with image-level labels. On ImageNet, Detic also improves classes without image labels thanks to the CLIP classifier which leverages interclass relations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L mAP Fixed evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M Image Attributions</head><p>License for the images from OpenImages in <ref type="figure" target="#fig_5">Figure 5</ref>: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Work done during an internship at Meta. arXiv:2201.02605v3 [cs.CV] 29 Jul 2022 Top: Typical detection results from a strong open-vocabulary LVIS detector. The detector misses objects of "common" classes. Bottom: Number of images in LVIS, ImageNet, and Conceptual Captions per class (smoothed by averaging 100 neighboring classes). Classification datasets have a much larger vocabulary than detection datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>3 point and matches the performance of using full class annotations in training. With the standard LVIS annotations, our model reaches 41.7 mAP and 41.7 mAP rare , closing the gap between rare classes and all classes. On open-vocabulary COCO, our method Our non-prediction-based loss Left: Standard detection requires ground-truth labeled boxes and cannot leverage image-level labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>follow a two-stage framework. The first stage,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Approach Overview. We mix train on detection data and image-labeled data. When using detection data, our model uses the standard detection losses to train the classifier (W) and the box prediction branch (B) of a detector. When using image-labeled data, we only train the classifier using our modified classification loss. Our loss trains the features extracted from the largest-sized proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>compare Detic's open-vocabulary object detectors with state-of-the-art methods on the open-vocabulary LVIS and the open-vocabulary COCO benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative results of our 21k-class detector. We show random samples from images containing novel classes in OpenImages (top) and Objects365 (bottom) validation sets. We use the CLIP embedding of the corresponding vocabularies. We show LVIS classes in purple and novel classes in green. We use a score threshold of 0.5 and show the most confident class for each box. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>-</head><label></label><figDesc>"Oyster": Photo by The Local People Photo Archive (CC BY 2.0) -"Cheetah": Photo by Michael Gil (CC BY 2.0) -"Harbor seal": Photo by Alden Chadwick (CC BY 2.0) -"Dinosaur": Photo by Paxson Woelber (CC BY 2.0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we always first train a converged base-class-only model (4? schedule) and finetune on it with additional image-labeled data for another 4? schedule. We confirm finetuning the model using only box supervision does not improve the performance. The 4? schedule for our joint training consists of ?24 LVIS epochs plus ?4.8 ImageNet epochs or ?3.8 CC epochs. Training our ResNet50 model takes ? 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in ? 24 hours on 32 GPUs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 (</head><label>1</label><figDesc>third block) shows the results of our non-prediction-based methods in ? 4. All variants of our proposed simpler method outperform the complex prediction-based counterparts, with both image-supervised datasets. On the novel classes, Detic provides a significant gain of ? 4.2 points with ImageNet over the best prediction-based methods. Using non-object centric images from Conceptual Captions. ImageNet images typically have a single large object<ref type="bibr" target="#b17">[18]</ref>. Thus, our non-prediction-based methods, for example image-box which considers the entire image as a bounding box, are well suited for ImageNet. To test whether our losses work with different image distributions with multiple objects, we test it with the Conceptual Captions (CC) dataset. Even on this challenging dataset with multiple objects/labels per image, Detic provides a gain of ? 2.6 points on novel class detection over the best prediction-based methods. This suggests that our simpler Detic method can IN-L (object-centric) CC (non object-centric) mAP mask mAP mask novel mAP mask mAP mask novel</figDesc><table><row><cell>Box-Supervised (baseline)</cell><cell>30.0?0.4</cell><cell>16.3?0.7</cell><cell>30.0?0.4</cell><cell>16.3?0.7</cell></row><row><cell>Prediction-based methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-training [54]</cell><cell>30.3?0.0</cell><cell>15.6?0.1</cell><cell>30.1?0.2</cell><cell>15.9?0.8</cell></row><row><cell>WSDDN [3]</cell><cell>29.8?0.2</cell><cell>15.6?0.3</cell><cell>30.0?0.1</cell><cell>16.5?0.8</cell></row><row><cell>DLWL* [44]</cell><cell>30.6?0.1</cell><cell>18.2?0.2</cell><cell>29.7?0.3</cell><cell>16.9?0.6</cell></row><row><cell>YOLO9000 [45]</cell><cell>31.2?0.3</cell><cell>20.4?0.9</cell><cell>29.4?0.1</cell><cell>15.9?0.6</cell></row><row><cell cols="2">Non-prediction-based methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Detic (Max-object-score)</cell><cell>32.2?0.1</cell><cell>24.4?0.3</cell><cell>29.8?0.1</cell><cell>18.2?0.6</cell></row><row><cell>Detic (Image-box)</cell><cell cols="2">32.4?0.1 23.8?0.5</cell><cell>30.9?0.1</cell><cell>19.5?0.5</cell></row><row><cell>Detic (Max-size)</cell><cell cols="2">32.4?0.1 24.6?0.3</cell><cell>30.9?0.2</cell><cell>19.5?0.3</cell></row><row><cell cols="2">Fully-supervised (all classes) 31.1?0.4</cell><cell>25.5?0.7</cell><cell>31.1?0.4</cell><cell>25.5?0.7</cell></row></table><note>Table 1: Prediction-based vs non-prediction-based methods. We show overall and novel-class mAP on open-vocabulary LVIS [17] (with 866 base classes and 337 novel classes) with different image-labeled datasets (IN-L or CC). The models are trained using our strong baseline ? 5.1 (top row)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 ,</head><label>1</label><figDesc>compared with the strong baseline Box-Supervised, Detic improves the detection performance by 2.4 mAP and 8.3 mAP novel . Thus, Detic with image-level labels leads to strong open-vocabulary detection performance and can provide orthogonal gains to existing open-vocabulary detectors [2]. To further understand the open-vocabulary capabilities of Detic, we also report the top-line results trained with box labels for all classes(Table 1last row). Despite not using box labels for the novel classes, Detic with ImageNet performs favorably compared to the fully-supervised detector. This result also suggests that bounding box annotations may not be required for new classes. Detic combined with large image classification datasets is a simple and effective alternative for increasing detector vocabulary.</figDesc><table><row><cell></cell><cell>mAP mask</cell><cell>mAP mask novel</cell><cell>mAP mask c</cell><cell>mAP mask f</cell></row><row><cell>ViLD-text [17]</cell><cell>24.9</cell><cell>10.1</cell><cell>23.9</cell><cell>32.5</cell></row><row><cell>ViLD [17]</cell><cell>22.5</cell><cell>16.1</cell><cell>20.0</cell><cell>28.3</cell></row><row><cell>ViLD-ensemble [17]</cell><cell>25.5</cell><cell>16.6</cell><cell>24.6</cell><cell>30.3</cell></row><row><cell>Detic</cell><cell>26.8</cell><cell>17.8</cell><cell>26.3</cell><cell>31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Open-vocabulary LVIS compared to ViLD<ref type="bibr" target="#b16">[17]</ref>. We train our model using their training settings and architecture (MaskRCNN-ResNet50, training from scratch). We report mask mAP and its breakdown to novel (rare), common, and frequent classes. Variants of ViLD use distillation (ViLD) or ensembling (ViLD-ensemble.). Detic (with IN-L) uses a single model and improves both mAP and mAP novel .</figDesc><table><row><cell></cell><cell>mAP50 box all</cell><cell>mAP50 box novel</cell><cell>mAP50 box base</cell></row><row><cell>Base-only ?</cell><cell>39.9</cell><cell>0</cell><cell>49.9</cell></row><row><cell>Base-only (CLIP)</cell><cell>39.3</cell><cell>1.3</cell><cell>48.7</cell></row><row><cell>WSDDN [3] ?</cell><cell>24.6</cell><cell>20.5</cell><cell>23.4</cell></row><row><cell>Cap2Det [71] ?</cell><cell>20.1</cell><cell>20.3</cell><cell>20.1</cell></row><row><cell>SB [2] ?</cell><cell>24.9</cell><cell>0.31</cell><cell>29.2</cell></row><row><cell>DELO [78] ?</cell><cell>13.0</cell><cell>3.41</cell><cell>13.8</cell></row><row><cell>PL [43] ?</cell><cell>27.9</cell><cell>4.12</cell><cell>35.9</cell></row><row><cell>OVR-CNN [72] ?</cell><cell>39.9</cell><cell>22.8</cell><cell>46.0</cell></row><row><cell>Detic</cell><cell>45.0</cell><cell>27.8</cell><cell>47.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Detecting 21K classes across datasets. We use Detic to train a detector and evaluate it on multiple datasets without retraining.</figDesc><table><row><cell>We report the bounding box mAP</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Detic with different classifiers. We vary the classifier used with Detic and observe that it works well with different choices. While CLIP embeddings give the best performance (* indicates our default), all classifiers benefit from our Detic.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Detic with different pretraining data. Top: our method using ImageNet-1K as pretraining and ImageNet-21K as co-training; Bottom: using ImageNet-21K for both pretraining and co-training. Co-training helps pretraining in both cases.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell cols="2">mAP mask mAP mask r</cell><cell>mAP mask c</cell><cell>mAP mask f</cell></row><row><cell cols="2">MosaicOS ? [73] ResNeXt-101</cell><cell>28.3</cell><cell>21.7</cell><cell>27.3</cell><cell>32.4</cell></row><row><cell cols="2">CenterNet2 [76] ResNeXt-101</cell><cell>34.9</cell><cell>24.6</cell><cell>34.7</cell><cell>42.5</cell></row><row><cell cols="2">AsyncSLL ? [19] ResNeSt-269</cell><cell>36.0</cell><cell>27.8</cell><cell>36.7</cell><cell>39.6</cell></row><row><cell cols="2">SeesawLoss [64] ResNeSt-200</cell><cell>37.3</cell><cell>26.4</cell><cell>36.3</cell><cell>43.1</cell></row><row><cell cols="3">Copy-paste [15] EfficientNet-B7 38.1</cell><cell>32.1</cell><cell>37.1</cell><cell>41.9</cell></row><row><cell>Tan et al. [57]</cell><cell>ResNeSt-269</cell><cell>38.8</cell><cell>28.5</cell><cell>39.5</cell><cell>42.7</cell></row><row><cell>Baseline</cell><cell>Swin-B</cell><cell>40.7</cell><cell>35.9</cell><cell>40.5</cell><cell>43.1</cell></row><row><cell>Detic ?</cell><cell>Swin-B</cell><cell>41.7</cell><cell>41.7</cell><cell>40.8</cell><cell>42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>We leave incorporating such information for future work. Moreover, open vocabulary generalization has no guarantees on extreme domains. Our experiments show Detic improves large-vocabulary detection with various weak data sources, classifiers, detector architectures, and training recipes. Proposal networks trained with (top) and without (bottom) rare classes. We report recalls on rare classes and all classes at IoU threshold 0.5 with different number of proposals. Proposal networks trained without rare classes can generalize to rare classes in testing. Proposal networks trained on half of the LVIS classes. We report recalls at IoU threshold 0.5 on the other half classes. Proposal networks produce non-trivial recalls on novel classes.</figDesc><table><row><cell cols="5">ARr50@100 ARr50@300 ARr50@1k AR50@1k</cell></row><row><cell>LVIS-all</cell><cell>63.3</cell><cell>76.3</cell><cell>79.7</cell><cell>80.9</cell></row><row><cell>LVIS-base</cell><cell>62.2</cell><cell>76.2</cell><cell>78.5</cell><cell>81.0</cell></row><row><cell cols="5">(a) AR half-1st 50@1k AR half-2nd 50@1k</cell></row><row><cell cols="2">LVIS-half-1st</cell><cell>80.8</cell><cell>69.6</cell><cell></cell></row><row><cell cols="2">LVIS-half-2nd</cell><cell>62.9</cell><cell>82.2</cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Proposal network generalization ability evaluation. (a): Generalize from 866 LVIS base classes to the 337 rare classes; (b): Generalize from uniformly sampled half LVIS classes (601/ 602 classes) to the other half.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>as the classifier, our framework can seamlessly incorporate the free-form caption text as image-supervision. Using Supervision mAP mask mAP mask novel</figDesc><table><row><cell>Box-Supervised</cell><cell>-</cell><cell>30.2</cell><cell>16.4</cell></row><row><cell>Detic w. CC</cell><cell>Image label</cell><cell>31.0</cell><cell>19.8</cell></row><row><cell>Detic w. CC</cell><cell>Caption</cell><cell>30.4</cell><cell>17.4</cell></row><row><cell>Detic w. CC</cell><cell>Both</cell><cell>31.0</cell><cell>21.3</cell></row><row><cell></cell><cell></cell><cell cols="2">mAP50 box all mAP50 box novel</cell></row><row><cell>Box-Supervised</cell><cell>-</cell><cell>39.3</cell><cell>1.3</cell></row><row><cell cols="2">Detic w. COCO-cap. Image label</cell><cell>44.7</cell><cell>24.1</cell></row><row><cell cols="2">Detic w. COCO-cap. Caption</cell><cell>43.8</cell><cell>21.0</cell></row><row><cell cols="2">Detic w. COCO-cap. Both</cell><cell>45.0</cell><cell>27.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table /><note>Direct caption supervision. Top: Open-vocabulary LVIS with Conceptual Caption as weakly-labeled data; Bottom block: Open-vocabulary COCO with COCO- caption as weakly-labeled data. Directly using caption embeddings as a classifier is helpful on both benchmarks; the improvements are complementary to Detic.the notations in ? 4, here D cls = {(I, t) i } where t is a free-form text. In our open-vocabulary detection formulation, text t can natrually be converted to an embedding by the CLIP [42] language encoder L: w = L(t). Given a minibatch of B samples {(I, t) i } B i=1 , we compose a dynamic classification layer by stacking all caption features within the batch W</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>.</head><label></label><figDesc>This baseline uses ResNet-50 FPN backbone and a 2? training schedule (180k</figDesc><table><row><cell></cell><cell cols="2">mAP box mAP box r</cell><cell cols="2">mAP mask mAP mask r</cell><cell>T</cell></row><row><cell>D2 baseline [66]</cell><cell>22.9</cell><cell>11.3</cell><cell>22.4</cell><cell cols="2">11.6 12h</cell></row><row><cell>+Class-agnostic box&amp;mask</cell><cell>22.3</cell><cell>10.1</cell><cell>21.2</cell><cell cols="2">10.1 12h</cell></row><row><cell>+Federated loss [76]</cell><cell>27.0</cell><cell>20.2</cell><cell>24.6</cell><cell cols="2">18.2 12h</cell></row><row><cell>+CenterNet2 [76]</cell><cell>30.7</cell><cell>22.9</cell><cell>26.8</cell><cell cols="2">19.4 13h</cell></row><row><cell cols="2">+LSJ 640?640, 4? sched. [15] 31.0</cell><cell>21.6</cell><cell>27.2</cell><cell cols="2">20.1 17h</cell></row><row><cell>+CLIP classifier [42]</cell><cell>31.5</cell><cell>24.2</cell><cell>28</cell><cell cols="2">22.5 17h</cell></row><row><cell cols="2">+Adam optimizer, lr2e-4 [26] 30.4</cell><cell>23.6</cell><cell>26.9</cell><cell cols="2">21.4 17h</cell></row><row><cell>+IN-21k pretrain [48]*</cell><cell>35.3</cell><cell>28.2</cell><cell>31.5</cell><cell cols="2">25.6 17h</cell></row><row><cell>+Input size 896?896</cell><cell>37.1</cell><cell>29.5</cell><cell>33.2</cell><cell cols="2">26.9 25h</cell></row><row><cell>+Swin-B backbone [37]</cell><cell>45.4</cell><cell>39.9</cell><cell>40.7</cell><cell cols="2">35.9 43h</cell></row><row><cell>*Remove rare class ann.[17]</cell><cell>33.8</cell><cell>17.6</cell><cell>30.2</cell><cell cols="2">16.4 17h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>LVIS baseline evolution. First row: the configuration from the detectron2 model zoo. The following rows change components one by one. Last row: removing rare classes from the "+IN-21k pretrain*" row. The two gray-filled rows are the baselines in our main paper, for full LVIS and open-vocabulary LVIS, respectively. We show rough wall-clock training times (T ) on our machine with 8 V100 GPUs in the last column. iterations with batch-size 16) 4 . Data augmentation includes horizontal flip and random resize short side [640, 800], long side &lt; 1333. The baseline uses SGD optimizer with a learning rate 0.02 (dropped by 10? at 120k and 160k iteration).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>). Combing all the improvements results in Ratio Size mAP mask mAP mask novel</figDesc><table><row><cell cols="2">Bos-Supervised 1: 0</cell><cell>-</cell><cell>30.2</cell><cell>16.4</cell></row><row><cell>Detic w. IN-L</cell><cell>1: 1</cell><cell>640</cell><cell>30.9</cell><cell>23.3</cell></row><row><cell>Detic w. IN-L</cell><cell>1: 1</cell><cell>320</cell><cell>32.0</cell><cell>24.0</cell></row><row><cell>Detic w. IN-L</cell><cell>1: 4</cell><cell>640</cell><cell>31.1</cell><cell>23.5</cell></row><row><cell>Detic w. IN-L</cell><cell>1: 4</cell><cell>320</cell><cell>32.4</cell><cell>24.9</cell></row><row><cell>Detic w. CC</cell><cell>1: 1</cell><cell>640</cell><cell>30.8</cell><cell>21.6</cell></row><row><cell>Detic w. CC</cell><cell>1: 1</cell><cell>320</cell><cell>30.8</cell><cell>21.5</cell></row><row><cell>Detic w. CC</cell><cell>1: 4</cell><cell>640</cell><cell>30.7</cell><cell>21.0</cell></row><row><cell>Detic w. CC</cell><cell>1: 4</cell><cell>320</cell><cell>31.1</cell><cell>21.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Ablations of the resolution change. We report mask mAP on the openvocabulary LVIS following the setting of Table 1. Top: ImageNet as the image-labeled data. Bottom: CC as the image-labeled data.35.3 mAP box and 31.5 mAP mask , and trains in a favorable time (17h on 8 V100 GPUs)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Dataset Backbone mAP mask mAP mask novel Predicted loss and max-size loss with different prediction qualities. We show the mask mAP of the box-supervised baseline, Predicted loss<ref type="bibr" target="#b44">[45]</ref>, and our max-size loss. We show the delta between max-size loss and predicted loss in green. Improving the backbone and including rare classes in training can both narrow the gap. Max-size consistently performs better. Assigned proposal cover rate and consistency. Left: ratio of assigned proposal covering the ground truth both. We evaluate on an ImageNet subset that has box ground truth and the annotated COCO training set; Right: average assigned bounding box IoU of between the final model and the half-schedule model.</figDesc><table><row><cell>Box-Supervised</cell><cell></cell><cell></cell><cell></cell><cell>30.2</cell><cell>16.4</cell></row><row><cell>Predicted</cell><cell cols="3">LVIS-base Res50</cell><cell>31.2</cell><cell>20.4</cell></row><row><cell>Max-size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">32.4 (+1.2) 24.6 (+4.2)</cell></row><row><cell>Box-Supervised</cell><cell></cell><cell></cell><cell></cell><cell>38.4</cell><cell>21.9</cell></row><row><cell>Predicted</cell><cell cols="3">LVIS-base SwinB</cell><cell>40.0</cell><cell>31.7</cell></row><row><cell>Max-size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">40.7 (+0.7) 33.8 (+2.1)</cell></row><row><cell>Box-Supervised</cell><cell></cell><cell></cell><cell></cell><cell>31.5</cell><cell>25.6</cell></row><row><cell>Predicted</cell><cell cols="2">LVIS-all</cell><cell>Res50</cell><cell>32.5</cell><cell>28.4</cell></row><row><cell>Max-size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">33.2 (+0.7) 29.7 (+1.3)</cell></row><row><cell>Box-Supervised</cell><cell></cell><cell></cell><cell></cell><cell>40.7</cell><cell>35.9</cell></row><row><cell>Predicted</cell><cell cols="3">LVIS-all SwinB</cell><cell>40.6</cell><cell>39.8</cell></row><row><cell>Max-size</cell><cell></cell><cell></cell><cell></cell><cell cols="2">41.3 (+0.7) 40.9 (+1.1)</cell></row><row><cell cols="4">(a) Cover rate</cell><cell cols="2">Consistency</cell></row><row><cell></cell><cell cols="5">IN-L COCO IN-L CC COCO</cell></row><row><cell cols="2">Predicted 69.0</cell><cell>73.8</cell><cell cols="2">71.5 30.0</cell><cell>57.7</cell></row><row><cell>Max-size</cell><cell>92.8</cell><cell>80.0</cell><cell cols="2">87.9 73.0</cell><cell>62.8</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Comparison between predicted loss and and max-size loss. (a): comparison under different baselines. (b): comparison in customized metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Open-vocabulary COCO is proposed by Bansal et al.<ref type="bibr" target="#b1">[2]</ref>. They manually select 48 classes from the 80 COCO classes as base classes, and 17 classes as novel classes.</figDesc><table><row><cell></cell><cell>mAP50 box all</cell><cell>mAP50 box novel</cell></row><row><cell>Box-Supervised (base cls)</cell><cell>39.3</cell><cell>1.3</cell></row><row><cell>Self-training [54]</cell><cell>39.5</cell><cell>1.8</cell></row><row><cell>WSDDN [3]</cell><cell>39.9</cell><cell>5.9</cell></row><row><cell>DLWL* [44]</cell><cell>42.9</cell><cell>19.6</cell></row><row><cell>Predicted [45]</cell><cell>41.9</cell><cell>18.7</cell></row><row><cell>Detic (Max-object-score)</cell><cell>43.3</cell><cell>20.4</cell></row><row><cell>Detic (Image-box)</cell><cell>43.4</cell><cell>21.0</cell></row><row><cell>Detic (Max-size)</cell><cell>44.7</cell><cell>24.1</cell></row><row><cell>Box-Supervised (all cls)</cell><cell>54.9</cell><cell>60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Different ways to use image supervision on open-vocabulary COCO. The models are trained using the OVR-CNN [72] recipe with ResNet50-C4 [2] backbone. We follow setups in Table 1. The observations are consistent with LVIS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>MosaicOS uses IN-L and additional web-search images as image-supervised data. Detic</figDesc><table><row><cell></cell><cell>mAP mask</cell><cell>mAP mask r</cell></row><row><cell>Box-Supervised [73]</cell><cell>22.6</cell><cell>12.3</cell></row><row><cell>MosaicOS [73]</cell><cell>24.5 (+1.9)</cell><cell>18.3 (+6.0)</cell></row><row><cell>Box-Supervised (Reproduced)</cell><cell>22.6</cell><cell>12.3</cell></row><row><cell>Detic (default classifier)</cell><cell>25.1 (+2.5)</cell><cell>18.6 (+6.3)</cell></row><row><cell>Box-Supervised (CLIP classifier)</cell><cell>22.3</cell><cell>14.1</cell></row><row><cell>Detic (CLIP classifier)</cell><cell cols="2">24.9 (+2.6) 20.7 (+6.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 14 :</head><label>14</label><figDesc>Standard LVIS compared to MosiacOS [73]. Top block: results quoted from MosiacOS paper; Middle block: Detic with the default random intialized and trained classifier; Bottom block: Detic with CLIP classifier.</figDesc><table><row><cell></cell><cell cols="2">mAP box mAP box r</cell><cell>mAP box c</cell><cell>mAP box f</cell></row><row><cell cols="2">Box-Supervised 31.7</cell><cell>21.4</cell><cell>30.7</cell><cell>37.5</cell></row><row><cell>Detic</cell><cell>32.5</cell><cell>26.2</cell><cell>31.3</cell><cell>36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 15 :</head><label>15</label><figDesc>Detic applied to Deformable-DETR<ref type="bibr" target="#b78">[79]</ref>. We report Box mAP on full LVIS. Our method improves Deformable-DETR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 15</head><label>15</label><figDesc>shows that Detic improves over the baseline (+0.8 mAP and +4.8 mAP r ) and generalizes to Transformer based detectors.</figDesc><table><row><cell></cell><cell>mAP mask</cell><cell>mAP mask IN-L</cell><cell>mAP mask non-IN-L</cell></row><row><cell>Box-Supervised</cell><cell>30.2</cell><cell>30.6</cell><cell>27.6</cell></row><row><cell>Max-size</cell><cell>32.4</cell><cell>33.5</cell><cell>28.1</cell></row><row><cell></cell><cell>mAP mask</cell><cell>mAP mask CC</cell><cell>mAP mask non-CC</cell></row><row><cell>Box-Supervised</cell><cell>30.2</cell><cell>30.1</cell><cell>29.5</cell></row><row><cell>Max-size</cell><cell>30.9</cell><cell>31.7</cell><cell>28.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 16 :</head><label>16</label><figDesc>mAP breakdown into classes with and without image labels. Top: Detic trained on ImageNet. Bottom: Detic trained on CC. Most of the improvements are from classes with image-level labels. On ImageNet Detic also improves classes without image labels thanks to the CLIP classifier. Datasets mAP box mAP box novel mAP Fixed mAP Fixed novel (+2.2) 24.9 (+8.5) 33.4 (+2.3) 26.7 (+8.5)</figDesc><table><row><cell>Box-Supervised</cell><cell>30.2</cell><cell>16.4</cell><cell>31.2</cell><cell>18.2</cell></row><row><cell>Detic</cell><cell>32.4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 17 :</head><label>17</label><figDesc>mAP Fixed evaluation. Middle: the original box mAP metric used in the main paper. Right: the new box mAP Fix metric. Our improvements are consistent under the new metric.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 17</head><label>17</label><figDesc>compares our improvements under the new mAP fix proposed in Dave et al. [8]. Our improvements are consistent under the new metric.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We omit the two linear layers and the bias in the second stage for notation simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is more pronounced in detection than classification, as the "batch-size" for the classification layer is 512? image-batch-size, where 512 is #RoIs per image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/detectron2/blob/main/configs/ LVISv1-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We are aware different projects use different notations of a 1? schedule. In this paper we always refer 1? schedule to 16 ? 90k images</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://image-net.org/download-bboxes.php. 213K of the 1.2M IN-L images have bounding box annotations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Bowen Cheng and Ross Girshick for helpful discussions and feedback. This material is in part based upon work supported by the National Science Foundation under Grant No. IIS-1845485 and IIS-2006820. Xingyi is supported by a Facebook PhD Fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2016) 3, 6, 9</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagelevel or object-level? a tale of two resampling strategies for long-tailed detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Points as queries: Weakly semi-supervised object detection by points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating largevocabulary object detectors: The devil is in the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01066</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards segmenting anything that moves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">VirTex: Learning Visual Representations from Textual Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting weakly supervised object detection via learning bounding box adjusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wssod: A new pipeline for weakly-and semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11293</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring classification equilibrium in long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2021)</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 7, 14</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12143</idno>
		<title level="m">Open-vocabulary image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Joint coco and lvis workshop at eccv 2020: Lvis challenge track technical report: Asynchronous semi-supervised learning for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comprehensive attention selfdistillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5143773</idno>
		<ptr target="https://doi.org/10.5281/zenodo.514377313" />
		<imprint>
			<date type="published" when="2021-07" />
			<publisher>Openclip</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning open-world object proposals without learning to classify</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06753</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Extending one-stage detection with open-world proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Konan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02302</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<title level="m">The open images dataset v4. IJCV (2020)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language-driven semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Weakly supervised object detection with segmentation collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixed supervised object detection with robust objectness transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanhere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixed supervised object detection by transferringmask prior and semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multimodal transformers excel at class-agnostic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11430</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On model calibration for long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Weakly supervised semantic segmentation with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved visual-semantic alignment for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dlwl: Improving detection for lowshot classes with weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2020) 2, 3, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ufo 2 : A unified framework towards omni-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACL</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Enabling deep residual networks for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">R-fcn-3000 at 30fps: Decoupling detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A simple semisupervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01559</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>1st place solution of lvis challenge 2020: A good box is</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pcl: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Revisiting knowledge transfer for training object class detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">C-mil:continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised object detection with expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08740</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Detecting 11k classes: Large scale object detection without fine-grained bounding boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Towards precise end-to-end weakly supervised object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Cap2det: Learning to amplify weak caption supervision for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 3, 4, 11</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Mosaicos: A simple and effective use of object-centric images for long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note>ICCV (2021) 3, 8, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Distribution alignment: A unified framework for long-tail visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Boosting weakly supervised object detection with progressive knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<title level="m">Probabilistic two-stage detection</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Simple multi-dataset detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
