<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Rough Differential Equations for Long Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Salvi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
						</author>
						<title level="a" type="main">Neural Rough Differential Equations for Long Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural controlled differential equations (CDEs) are the continuous-time analogue of recurrent neural networks, as Neural ODEs are to residual networks, and offer a memory-efficient continuoustime way to model functions of potentially irregular time series. Existing methods for computing the forward pass of a Neural CDE involve embedding the incoming time series into path space, often via interpolation, and using evaluations of this path to drive the hidden state. Here, we use rough path theory to extend this formulation. Instead of directly embedding into path space, we instead represent the input signal over small time intervals through its log-signature, which are statistics describing how the signal drives a CDE. This is the approach for solving rough differential equations (RDEs), and correspondingly we describe our main contribution as the introduction of Neural RDEs. This extension has a purpose: by generalising the Neural CDE approach to a broader class of driving signals, we demonstrate particular advantages for tackling long time series. In this regime, we demonstrate efficacy on problems of length up to 17k observations and observe significant training speed-ups, improvements in model performance, and reduced memory requirements compared to existing approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural controlled differential equations (CDEs) <ref type="bibr" target="#b19">(Kidger et al., 2020)</ref> are the continuous-time analogue to recurrent neural networks (RNNs) and provide a natural method for modelling temporal dynamics with neural networks.</p><p>Neural CDEs are similar to neural ordinary differential equations (ODEs), as popularised by . A Neural ODE is determined by its initial condition, without a 1 Mathematical Institute, University of Oxford, UK 2 The Alan Turing Institute, British Library, UK. Correspondence to: James Morrill &lt;morrill@maths.ox.ac.uk&gt;.</p><p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). direct way to modify the trajectory given subsequent observations. In contrast, the vector field of a Neural CDE depends upon the time-varying data, so that the trajectory of the system is driven by a sequence of observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Controlled Differential Equations</head><p>Let a, b ? R with a &lt; b, and let v, w ? N. Let ? ? R w . Let X : [a, b] ? R v be a continuous function of bounded variation (which is for example implied by it being Lipschitz), and let f : R w ? R w?v be continuous.</p><p>Then we may define Z : [a, b] ? R w as the unique solution to the controlled differential equation</p><formula xml:id="formula_0">Z a = ?, Z t = Z a + t a f (Z s ) dX s for t ? (a, b]. (1)</formula><p>The notation "f (Z s )dX s " denotes a matrix-vector product. "dX s " itself denotes a Riemann-Stieltjes integral: if X is differentiable then If in equation <ref type="formula" target="#formula_20">(1)</ref>, dX s was replaced with ds, then the equation would just be an ODE. Using dX s causes the solution to depend continuously on the evolution of X. We say that the solution is "driven by the control X".</p><p>Next, we recall the definition of a Neural CDE as introduced in <ref type="bibr" target="#b19">Kidger et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Neural Controlled Differential Equations</head><p>Consider a time series x as a collection of points x i ? R v?1 with corresponding time-stamps t i ? R such that x = ((t 0 , x 0 ), (t 1 , x 1 ), ..., (t n , x n )), and t 0 &lt; ... &lt; t n .</p><p>Let X : [t 0 , t n ] ? R v be some interpolation of the data such that X ti = (t i , x i ). In <ref type="bibr" target="#b19">Kidger et al. (2020)</ref> the authors use natural cubic splines to ensure differentiability of the control X, so as to treat the term "dX s " in equation <ref type="formula" target="#formula_20">(1)</ref> as "? s ds". output dimension q ? N. Here ? is used to denote dependence on learnable parameters.</p><p>We define Z as the hidden state and Y as the output of a Neural CDE driven by X if</p><formula xml:id="formula_1">Z t0 = ? ? (t 0 , x 0 ), with Z t = Z t0 + t t0 f ? (Z s ) dX s , and Y t = ? (Z t ) for t ? (t 0 , t n ] (3)</formula><p>That is -just like an RNN -we have an evolving hidden state Z, which is fed into a linear map to produce an output Y . This formulation is a universal approximator <ref type="bibr">(Kidger et al., 2020, Appendix B)</ref>. The output may be either the timeevolving Y t or just the final Y tn . This is then fed into a loss function (L 2 , cross entropy, . . . ) and trained via stochastic gradient descent in the usual way.</p><p>To compute the integral of equation <ref type="formula">(3)</ref> in <ref type="bibr" target="#b19">Kidger et al. (2020)</ref>, X is assumed differentiable and the CDE is simply rewritten as an ODE of the form</p><formula xml:id="formula_2">Z t = Z t0 + t t0 g ?,X (Z s , s) ds,<label>(4)</label></formula><p>where</p><formula xml:id="formula_3">g ?,X (Z, s) = f ? (Z)? s .<label>(5)</label></formula><p>This simple observation allows for incorporating the timevarying data X driving the CDE into the vector field g ?,X of the equivalent ODE (4). In doing so existing tools for Neural ODEs can be used to carry out the forward pass and backpropagate via adjoint methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Contributions</head><p>Neural CDEs, as with RNNs, begin to break down for long time series. Loss/accuracy worsens, and training time becomes prohibitive due to the sheer number of forward operations within each training epoch.</p><p>Meanwhile, and at first glance tangentially, it is known in the field of rough path theory <ref type="bibr">(Lyons, 1998;</ref><ref type="bibr" target="#b23">Lyons et al., 2004;</ref><ref type="bibr" target="#b11">Friz &amp; Victoir, 2010)</ref> that it is possible to numerically solve CDEs not by pointwise evaluations of the control path (as in the existing Neural CDE approach), but by using a specific summarisation -known as the log-signature -of the control path over short time intervals. See <ref type="figure" target="#fig_1">Figure 1</ref>. A CDE treated in this way is termed a rough differential equation, and the numerical method is termed the log-ODE method.</p><p>The central contribution of this paper is to observe that this latter technique actually offers a way to solve the former problem. The log-ODE method offers a way to update the hidden state of a Neural CDE over large intervals -much larger than would be expected given the sampling rate or length of the data. This dramatically reduces the effective length of the time series. Log-signatures represents a CDEspecific choice of summarisation, which works because closely-spaced samples are often strongly correlated. Additionally, this approach no longer requires differentiability of the control path.</p><p>In line with the usual mathematical terminology, we refer to our approach as neural rough differential equations (Neural RDEs). Moreover, Neural RDEs are still able to exploit memory-efficient continuous-time adjoint backpropagation. This is of additional benefit as memory pressure becomes increasingly relevant for long time series -indeed many of our experiments could not have been ran without it.</p><p>With Neural RDEs, we demonstrate improvements experimentally on real-world problems of length up to 17 000. We report substantial improvements in model performance (by as much as 17% on some classification tasks, reflecting the difficulty inherent in long time series), speed (by roughly a factor of 10), and memory usage (by roughly a factor of 100 compared to models not using the adjoint method).</p><formula xml:id="formula_4">Data, x X 1 X 2 ?X 1 ?X 2 A? A+ Path, X X 1 X 2 = ?X 1 = ?X 2 = A + ? A ? . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log-signature</head><p>Depth <ref type="formula" target="#formula_20">1</ref> Depth 2</p><p>Higher order <ref type="figure" target="#fig_4">Figure 2</ref>. Geometric intuition for the first two levels of the log-signature for a 2-dimensional path. The depth 1 terms correspond to the change in each of the coordinates over the interval. The depth 2 term corresponds to the L?vy area of the path, this being the signed area between the curve and the chord joining its start and endpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theory</head><p>We begin with an exposition on the motivating theory. Our description here will focus on the high-level intuitions. For a full technical description we refer to the appendices; see also section 7.1 of <ref type="bibr" target="#b11">(Friz &amp; Victoir, 2010)</ref>.</p><p>Readers primarily interested in practical applications should feel free to skip to section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Signatures and Log-signatures</head><p>The signature transform is a map from paths to a vector of real values, specifying a collection of statistics about the path. It is a central component of the theory of controlled differential equations since these statistics describe how the data interacts with dynamical systems. The log-signature is then formed by representing the same information in a compressed format.</p><p>Signature transform Let X = (X 1 , ..., X d ) : [0, T ] ? R d be continuous and piecewise differentiable. 1 Letting 2</p><formula xml:id="formula_5">S i1,...i k a,b (X) = ... a&lt;t 1 &lt;...&lt;t k &lt;b k j=1 dX ij dt (t j )dt j ,<label>(6)</label></formula><p>then the depth-N signature transform of X is given by</p><formula xml:id="formula_6">Sig N a,b (X) = S i a,b (X) (i) d i=1 , S i,j a,b (X) d i,j=1 , . . . , S i1,...,i N a,b (X) d i1,...,i N =1 . (7)</formula><p>This definition is independent of the choice of T and t i , by change of variables in equation <ref type="formula" target="#formula_5">(6)</ref>.</p><p>We see that the signature is a collection of integrals, with each integral defining a real value. It is a graded sequence of 1 For our purposes later it will typically be a linear interpolation of a time series.</p><p>2 This is a slightly simplified definition, and the signature is often instead defined using the notation of stochastic calculus; for completeness see Definition <ref type="bibr">A.2.</ref> statistics that characterise the input time series. In particular, <ref type="bibr" target="#b15">(Hambly &amp; Lyons, 2010)</ref> show that under mild conditions, Sig ? (X) completely determines X up to translation, provided time is included as a channel in X.</p><p>Log-signature transform The signature transform has some redundancy: a little algebra shows that for example</p><formula xml:id="formula_7">S 1,2 a,b (X) + S 2,1 a,b (X) = S 1 a,b (X)S 2 a,b (X)</formula><p>, so that we already know S 2,1 a,b (X) provided we know the other three quantities. The log-signature transform is then essentially obtained by computing the signature transform, and throwing out redundant terms, to obtain some (nonunique) minimal collection.</p><p>Starting from the depth-N signature transform and removing some fixed set of redundancies produces the depth-N log-signature transform. We fix some set of redundancies throughout (essentially corresponding to a choice of basis), and denote this LogSig N a,b . This is a map from Lipschitz continuous paths [a, b] ? R v into R ?(v,N ) , where ?(v, N ) denotes the dimension of the log-signature (see Appendix A).</p><p>Geometric intuition In figure 2 we provide a geometric intuition for the first two levels of the log-signature, which have natural geometric interpretations. The depth 1 terms correspond to the changes in each channel over the interval; this is ?X 1 , ?X 2 in the figure. The depth 2 term corresponds to the signed area in between the chord joining the endpoints and the path itself; this corresponds to A + ? A ? in the figure. Higher order terms correspond to higher order integrals and iterated areas in higher dimensional spaces, and become a little more difficult to visualise.</p><p>(Log-)Signatures and CDEs In <ref type="figure" target="#fig_2">Figure 3</ref> we give the equations for how log-signatures arise in the solution of CDEs. Begin by letting D f denote the Jacobian of a function f . Now expand equation (1) by linearising the vector field f and neglecting higher order terms. The action of the vector field f on the depth-N signature is a matrix-vector product and is fully described, for any N, in <ref type="bibr" target="#b3">(Boutaib et al., 2014)</ref>. This is simply the Taylor Expansion of the CDE. The Taylor coefficients are precisely these signature terms, thus demonstrating how signatures are intrinsically linked to the solutions of CDEs. Higher order Taylor expansions results in corrections using higher order signature terms.</p><formula xml:id="formula_8">Z t ? Z a + t a f (Z a ) + D f (Z a )(Z s ? Z a ) dX dt (s)ds = Z a + t a f (Z a ) dX dt (s) ds + t a D f (Z a ) s a f (Z u ) dX dt (u) du dX dt (s) ds ? Z a + f (Z a ) t a dX dt (s) ds + D f (Z a )f (Z a ) t a s a dX dt (u) du dX dt (s)ds = Z a + f (Z a ) S(X) (i) } d i=1 + D f (Z a )f (Z a ) S(X) (i,j) d i,j=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Log-ODE Method</head><p>Recall for X :</p><formula xml:id="formula_9">[a, b] ? R v that LogSig N a,b (X) ? R ?(v,N ) . The log-ODE method states that Z b ? Z b where Z u = Z a + u a f ( Z s ) LogSig N a,b (X) b ? a ds for u ? (a, b],<label>(8)</label></formula><p>and Z a = Z a . Here Z is the same as in equation <ref type="formula">(3)</ref>, and the relationship between f to f is given in Appendix A.</p><p>That is, the solution of the CDE may be approximated by the solution to an ODE. This is typically applied locally: pick some points r i such that a = r 0 &lt; r 1 &lt; ? ? ? &lt; r m = b, split up the CDE of equation <ref type="formula" target="#formula_20">(1)</ref> into an integral over [r 0 , r 1 ], an integral over [r 1 , r 2 ], and so on, and apply the log-ODE method to each interval separately. A CDE treated in this way is, for the purposes of this exposition, termed a rough differential equation.</p><p>See Appendix A for the precise details and Appendix B for a proof of convergence. For the reader familiar with the Magnus expansion for linear differential equations <ref type="bibr" target="#b2">(Blanes et al., 2009)</ref>, then the log-ODE method is a generalisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We move on to introducing the neural rough differential equation.</p><p>Recall that we observe some time series x = ((t 0 , x 0 ), (t 1 , x 1 ), ..., (t n , x n )), and have constructed a piecewise linear interpolation X : [t 0 , t n ] ? R v such that X ti = (t i , x i ).</p><p>We now pick points r i such that t 0 = r 0 &lt; r 1 &lt; ? ? ? &lt; r m = t n . In principle these can be variably spaced but in practice we will typically space them equally far apart. The total number of points m should be much smaller than n. The choice and spacing of r i will be a hyperparameter.</p><p>We also pick a depth hyperparameter N ? 1. In section 2 we introduced the depth-N log-signature transform. For X : [t 0 , t n ] ? R v and t 0 ? r i &lt; r i+1 ? t n the logsignature of X over the interval [r i , r i+1 ] was defined to be a particular collection of statistics LogSig N ri,ri+1 (X) ? R ?(v,N ) ; specifically those statistics that best describe how X drives the CDE equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Rough Hidden State Update</head><p>Recall how the Neural CDE formulation of equation <ref type="formula">(3)</ref> was solved via equations (4), (5). For the rough approach we begin by replacing (5) with the piecewise</p><formula xml:id="formula_10">g ?,X (Z, s) = f ? (Z) LogSig N ri,ri+1 (X) r i+1 ? r i for s ? [r i , r i+1 ), (9) where f ? : R w ? R w??(v,N )</formula><p>is an arbitrary neural network, and the right hand side denotes a matrix-vector product between f ? and the log-signature. Equation (4) then becomes</p><formula xml:id="formula_11">Z t = Z t0 + t t0 g ?,X (Z s , s)ds.<label>(10)</label></formula><p>This may now be solved as a (neural) ODE using standard ODE solvers.</p><p>We give an overview of this process in <ref type="figure">figure 4</ref>. The left hand side represents a single step method, as in the existing approach to Neural CDEs. The right hand side depicts a rough approach that takes steps larger than the discretisation of the data in exchange for additional terms of the logsignature.</p><formula xml:id="formula_12">t 0 tm ? ? ? Time Data x Path X Hidden state Zt Integration steps r0 r1 r2 r3 r m?2 r m?1 rm ? ? ? ? ? ? Time Data x Path X LogSig r i ,r i+1 (X)</formula><p>Log-signature path Hidden state Zt Integration steps <ref type="figure">Figure 4</ref>. An overview of the log-ODE method applied to Neural RDEs. Left: A single step (CDE or RDE) model. The path X is quickly varying, meaning a lot of integration steps are needed to resolve it. Right: The Neural RDE utilising the log-ODE method with integration steps larger than the discretisation of the data. The path of log-signatures is more slowly varying (in a higher dimensional space), and needs fewer integration steps to resolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neural RDEs Generalise Neural CDEs</head><p>Suppose we happened to choose r i = t i and r i+1 = t i+1 . Then the log-signature term is</p><formula xml:id="formula_13">LogSig N ti,ti+1 (X) t i+1 ? t i</formula><p>Recall that the depth 1 log-signature is just the increment of the path over the interval. So this becomes</p><formula xml:id="formula_14">?X [ti,ti+1] t i+1 ? t i = dX linear dt (s) for s ? [t i , t i+1 ),</formula><p>that is to say the same as obtained via the original method if using linear interpolation. In this way the Neural RDE approach generalises the existing Neural CDE approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>Length/Channel Trade-Off The sequence of logsignatures is now of length m, which was chosen to be much smaller than n. As such, it is much more slowly varying over the interval [t 0 , t n ] than the original data, which was of length n. The differential equation it drives is better behaved, and so larger integration steps may be used in the numerical solver. This is the source of the speed-ups of this method; we observe typical speed-ups by a factor of about 10.</p><p>Memory Efficiency Long sequences need large amounts of memory to perform backpropagation-through-time (BPTT). As with the original Neural CDEs, the log-ODE approach supports memory-efficient backpropagation via the adjoint equations. If the vector field f ? requires O(H) memory, and the time series is of total length T , then backpropagating through the solver requires O(HT ) memory whilst the adjoint method requires only O(H + T ); see <ref type="bibr" target="#b19">Kidger et al. (2020)</ref>.</p><p>The Log-signature as a Preprocessing</p><p>Step When training a model in practice, the log-signatures need only be computed once and thus the computation can be performed as part of data preprocessing. Log-signatures can also be easily computed in an online fashion, making the model suitable for such problems.</p><p>Structure of f The description here aligns with the log-ODE scheme described in equation <ref type="formula" target="#formula_9">(8)</ref>. There is one discrepancy: we do not attempt to model the specific structure of f . This is in principle possible, but is computationally expensive. Instead, we model f as a neural network directly. This need not necessarily exhibit the requisite structure, but as neural networks are universal approximators <ref type="bibr" target="#b24">(Pinkus, 1999;</ref><ref type="bibr" target="#b17">Kidger &amp; Lyons, 2020a</ref>) then this approach is at least as general from a modelling perspective.</p><p>Ease of Implementation This method is straightforward to implement using pre-existing tools.</p><p>There are standard libraries available for computing the logsignature transform: we use Signatory <ref type="bibr" target="#b18">(Kidger &amp; Lyons, 2020b)</ref>. As equation <ref type="formula" target="#formula_11">(10)</ref> is an ODE, it may be solved directly using tools such as torchdiffeq .</p><p>As an alternative, we note that the form of equation <ref type="formula">(9)</ref> is that of equation <ref type="formula" target="#formula_3">(5)</ref>, with the driving path taken to be piecewise linear in log-signature space. Computation of the log-signatures can therefore be considered as a preprocessing step, producing a sequence of log-signatures. From this we may construct a path in log-signature space, and apply existing tools for neural CDEs. (Rather than tools for neural ODEs.) This idea is summarised in figure 4. We make this approach available in the <ref type="bibr">[redacted]</ref> open source project.</p><p>Applications In principle, a Neural RDE may be applied to solve any Neural CDE problem. However, we typically observe limited benefit on relatively short time series: the original Neural CDE formulation works well enough, and there is little room to see either speed or loss/accuracy improvements via this approach.</p><p>The situation changes for long time series. Here, the existing approach struggles as the length of the time series grows. Performance worsens, and speed drops due to the sheer number of forward evaluations. This is the same behaviour as for RNNs. Now, the reduction in length (from n to m n) is highly beneficial. Moreover, the compression performed by the log-signature is also of benefit: closely-sampled points will be typically be strongly correlated, and there is little to be gained by treating them all individually.</p><p>In addition, there are two advantages shared by both Neural CDEs and Neural RDEs, that make them suitable for long time series. The first is the sharply reduced memory requirements of the adjoint method. For example (chosen arbitrarily without cherry-picking) in one experiment we see a reduction in memory usage from 3.6GB to just 47MB.</p><p>The second is that as both operate in continuous time, the steps in the numerical solver may be decoupled from the sampling rate of the data: steps are taken with respect to the complexity of the data, not just its sampling rate. In particular a slowly-varying but densely-sampled path would still be fast without requiring many integration steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Depth and</head><p>Step Hyperparameters To solve a Neural RDE accurately via the log-ODE method, we should be prepared to take the depth N suitably large, or the intervals r i+1 ? r i suitably small. Accomplishing this would often require that they are taken relatively large or relatively small, respectively. Instead, we treat these as hyperparameters. This makes use of the log-ODE method a modelling choice rather than an implementation detail.</p><p>Increasing step size will lead to faster (but less informative) training by reducing the number of operations in the forward pass. Increasing depth will lead to slower (but more informative) training, as more information about each local interval is used in each update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We run experiments applying Neural RDEs to four realworld datasets. Every problem was chosen for its long length. The lengths are sufficiently long that adjoint-based backpropagation  was often needed simply to avoid running out of memory at any reasonable batch size. Every problem is regularly sampled, so we take t i = i.</p><p>Recall that the Neural RDE approach features two hyperparameters, corresponding to log-signature depth and step size. Good choices will turn out to have a dramatic positive effect on performance. Accordingly for every experiment we run Neural RDEs for all depths in N = 2, 3 and all step sizes in 2, <ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024</ref>. Depth 1 and step 1 are not considered as both reduce onto the Neural CDE model, as discussed in section 3.2. In practice, when choosing a final model, one would choose that with depth and step values that minimise the validation loss, as in any hyperparamter value selection.</p><p>We compare against two baseline models. The first is a Neural CDE; as the model we are extending then comparisons to this are our primary concern. For context we also additionally include a baseline against the ODE-RNN introduced in <ref type="bibr" target="#b26">Rubanova et al. (2019)</ref>. For both of these models, we also run experiments on the full range of step sizes described above.</p><p>For the Neural CDE model, increased step sizes correspond to na?ve subsampling of the data (in accordance with section 3.2). For the ODE-RNN model, we instead fold the time dimension into the feature dimension, so that at each step the ODE-RNN model sees several adjacent time points. This represents an alternate technique for dealing with long time series, so as to provide a reasonable benchmark.</p><p>For each model, and each hyperparameter combination, we run the experiment three times and report the mean and standard deviation of the test metrics. We additionally report mean training times and memory usages.</p><p>Precise details of hyperparameter selection, optimisers, normalisation, and so on can be found in Appendix C. For brevity, we provide results for only some of the step sizes here. The full results are described in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classifying EigenWorms</head><p>Our first example uses the EigenWorms dataset from the UEA archive from <ref type="bibr" target="#b0">Bagnall et al. (2017)</ref>. This consists of time series of length 17 984 and 6 channels (including time), corresponding to the movement of a roundworm. The goal is to classify each worm as either wild-type or one of four mutant-type classes.</p><p>Results are shown in <ref type="table">Table 1</ref>. We begin by seeing that the step-1 Neural CDE model takes roughly a day to train. Switching to Neural RDEs speeds this up by an order of magnitude, to roughly two hours. Moreover doing so dramatically improves accuracy, by up to 17%, reflecting the classical difficulty of learning from long time series.</p><p>Meanwhile na?ve subsampling approaches for the Neural CDE method only achieve speed-ups without performance improvements. The folded ODE-RNN model performs poorly, attaining the worst score for any step size whilst imposing a significantly higher memory burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Step Accuracy (%) Time (Hrs) Mem (Mb) Results across all step sizes may be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Estimating Vitals Signs from PPG and ECG data</head><p>Next we consider three separate problems, using data from the TSR archive <ref type="bibr" target="#b29">(Tan et al., 2020)</ref>, coming originally from the Beth Israel Deaconess Medical Centre (BIDMC).</p><p>We aim to predict a person's respiratory rate (RR), their heart rate (HR), or their oxygen saturation (SpO2) at the end of the sample, having observed PPG and ECG data over the length of the sample. The data is sampled at 125Hz and each series has length 4 000. There are 3 channels (including time). We evaluate performance with the L 2 loss.</p><p>The results are shown in table 2.</p><p>We find that the depth 3 Neural RDE is the top performer for every task at every step size, reducing test loss by 30-59% versus the Neural CDE. Moreover, it does so with roughly an order of magnitude less training time.</p><p>We attribute the improved test loss to the Neural RDE model being better able to learn long-term dependencies due to the reduced sequence length. Note that the performance of the rough models actually improves as the step size is increased. This is in contrast to Neural CDE, which sees a degradation in performance.</p><p>The ODE-RNN model, besides using significantly more memory, struggles to train effectively when the sequence length is long. Training improves as the sequence size is shortened, but still produces results substantially worse than those achieved by the Neural RDE.</p><p>As a visual summary of these results, including the full range of step sizes, we also provide heatmaps in <ref type="figure">Figure 5</ref>.</p><p>The full results across the full range of step sizes may be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>Number of hyperparameters Two new hyperparameters -truncation depth and step size -with substantial effects on training time and memory usage must now also be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of input channels</head><p>The log-ODE method is most feasible with few input channels, as the number of logsignature channels ?(v, N ) grows exponentially in v. For larger v then the available parallelism may become saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>CNNs and Transformers have been shown to offer improvements over RNNs for modelling long-term dependencies <ref type="bibr" target="#b1">(Bai et al., 2018;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>, although the latter in particular have typically focused on language modelling. On a more practical note, Transformers are famously O(L 2 ) in the length of the time series L. Several approaches have been introduced to reduce this, for example <ref type="bibr" target="#b22">Li et al. (2019)</ref> reduce this to O(L(log L) 2 ). Extensions specifically to long sequences do exist <ref type="bibr" target="#b28">(Sourkov, 2018)</ref>, but again these typically focus on language modelling rather than multivariate time series data.</p><p>There has also been some work on long time series for classic RNN (GRU/LSTM) models. One meaningful comparison is to hierarchical subsampling as in Graves <ref type="formula" target="#formula_17">(2012)</ref>; <ref type="bibr" target="#b9">De Mulder et al. (2015)</ref>. There the data is split into windows, an RNN is run over each window, and then an additional RNN is run over the first RNN's (Recalling that the NCDE is a depth-1 NRDE.) '-' denotes that the model could not be run within GPU memory. Bold denotes the best model score for a given step size, and * denotes that the score was the best achieved over all models and step sizes. <ref type="figure">Figure 5</ref>. Heatmap depicting normalised losses on the three BIDMC datasets for differing step sizes and depths. We can see that the point of lowest MSE (deepest red) has step &gt; 1 and depth &gt; 1, and that performance worsens for very long steps. This represents the depth/step tradeoff for long length time series.</p><p>outputs; we may describe this as an RNN/RNN pair. Liao et al. (2019) then perform the equivalent operation with a log-signature/RNN pair. In this context, our use of log-ODE method is analogous to an log-signature/NCDE pair.</p><p>In comparison to <ref type="bibr">Liao et al. (2019)</ref>, this means moving from an inspired choice of pre-processing to an actual implementation of the log-ODE method. In doing so the differential equation structure is preserved. Moreover this takes advantage of the synergy between log-signatures (which extract statistics on how data drives differential equations), and the controlled differential equation it then drives. Broadly speaking these connections are natural: at least within the signature/CDE/rough path community, it is a well-known but poorly-published fact that RNNs, (log-)signatures, and ODEs and approximation theory, with the goal of improving the long-term memory capacity of RNNs. Given the differential equation structure both they and we consider, a hybridisation of these techniques seems like a promising line of future inquiry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have introduced neural rough differential equations as an approach to continuous-time time series modelling. These extend Neural CDEs, driving the hidden state not by point evaluations but by interval summarisations of the underlying time series or control path. Neural RDEs may still be solved via ODE methods, and thus retain both adjoint backpropagation and continuous dynamics. As they additionally reduce the effective length of the control path, we observe substantial practical benefits in applying Neural RDEs to long time series. In this regime we report significant training speed-ups, model performance improvements, and reduced memory requirements, on problems of length up to 17 000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>In sections A and B, we give a more thorough introduction to solving CDEs via the log-ODE method.</p><p>In section C we discuss the experimental details such as the choice of network structure, computing infrastructure and hyperparameter selection approach.</p><p>In section D we give a full breakdown of every experimental result.</p><p>A. An introduction to the log-ODE method for controlled differential equations</p><p>The log-ODE method is an effective method for approximating the controlled differential equation:</p><formula xml:id="formula_15">dY t = f (Y t ) dX t ,<label>(11)</label></formula><formula xml:id="formula_16">Y 0 = ?,</formula><p>where X : [0, T ] ? R d has finite length, ? ? R n and f : R n ? L(R d , R n ) is a function with certain smoothness assumptions so that the CDE (11) is well posed. Throughout these appendices, L(U, V ) denotes the space of linear maps between the vector spaces U and V . In rough path theory, the function f is referred to as the "vector field" of (11) and usually assumed to have Lip(?) regularity (see definition 10.2 in <ref type="bibr" target="#b11">Friz &amp; Victoir (2010)</ref>). In this section, we assume one of the below conditions on the vector field:</p><p>1. f is bounded and has N bounded derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">f is linear.</head><p>In order to define the log-ODE method, we will first consider the tensor algebra and path signature.</p><p>Definition A.1 We say that T R d := R ? R d ? (R d ) ?2 ? ? ? ? is the tensor algebra of R d and T R d := a = a 0 , a 1 , ? ? ? : a k ? R d ?k ?k ? 0 is the set of formal series of tensors of R d . Moreover, T R d and T R d can be endowed with the operations of addition and multiplication. Given a = (a 0 , a 1 , ? ? ? ) and b = (b 0 , b 1 , ? ? ? ), we have</p><formula xml:id="formula_17">a + b = a 0 + b 0 , a 1 + b 1 , ? ? ? ,<label>(12)</label></formula><formula xml:id="formula_18">a ? b = c 0 , c 1 , c 2 , ? ? ? ,<label>(13)</label></formula><p>where for n ? 0, the n-th term c n ? R d ?n can be written as</p><formula xml:id="formula_19">c n := n k=0 a k ? b n?k .<label>(14)</label></formula><p>The use of ? in equation <ref type="formula" target="#formula_2">(14)</ref> denotes the usual tensor product. The use of ? in equation <ref type="formula" target="#formula_18">(13)</ref> is also referred to as the "tensor product": when precisely one a i and precisely one b i are nonzero then it reduces to the usual tensor product; equation <ref type="formula" target="#formula_18">(13)</ref> is a generalisation. </p><p>s,t , X</p><p>(2) s,t , X</p><p>(3)</p><formula xml:id="formula_21">s,t , . . . ? T R d ,<label>(15)</label></formula><p>where for n ? 1,</p><formula xml:id="formula_22">X (n) s,t := ? ? ? s&lt;u1&lt;???&lt;un&lt;t dX u1 ? ? ? ? ? dX un ? R d ?n .</formula><p>Similarly, we can define the depth-N (or truncated) signature of the path X on [s, t] as S N s,t X := 1 , X</p><p>(1) s,t , X</p><p>(2)</p><formula xml:id="formula_23">s,t , . . . , X (N ) s,t ? T N R d ,<label>(16)</label></formula><p>where</p><formula xml:id="formula_24">T N R d := R ? R d ? (R d ) ?2 ? ? ? ? ? (R d ) ?N denotes the truncated tensor algebra.</formula><p>The (truncated) signature provides a natural feature set that describes the effects a path X has on systems that can be modelled by <ref type="formula" target="#formula_15">(11)</ref>. That said, defining the log-ODE method actually requires the so-called "log-signature" which efficiently encodes the same integral information as the signature. The log-signature is obtained from the path's signature by removing certain algebraic redundancies, such as</p><formula xml:id="formula_25">t 0 s 0 dX i u dX j s + t 0 s 0 dX j u dX i s = X i t X j t ,</formula><p>for i, j ? {1, ? ? ? , d}, which follows by the integration-by-parts formula. To this end, we will define the logarithm map on the depth-N truncated tensor algebra</p><formula xml:id="formula_26">T N R d := R ? R d ? ? ? ? ? (R d ) ?N .</formula><p>Definition A.3 (The logarithm of a formal series) For a = (a 0 , a 1 , ? ? ? ) ? T R d with a 0 &gt; 0, define log(a) to be the element of T R d given by the following series:</p><formula xml:id="formula_27">log(a) := log(a 0 ) + ? n=1 (?1) n n 1 ? a a 0 ?n ,<label>(17)</label></formula><p>where 1 = (1, 0, ? ? ? ) is the unit element of T R d and log(a 0 ) is viewed as log(a 0 )1.</p><p>Definition A.4 (The logarithm of a truncated series) For a = (a 0 , a 1 , ? ? ? , a N ) ? T R d with a 0 &gt; 0, define log N (a) to be the element of T N R d defined from the logarithm map <ref type="formula" target="#formula_20">(17)</ref> as</p><formula xml:id="formula_28">log N (a) := P N log( a) ,<label>(18)</label></formula><p>where a := (a 0 , a 1 , ? ? ? , a N , 0, ? ? ? ) ? T R d and P N denotes the standard projection map from T R d onto T N R d .</p><p>Definition A.5 The log-signature of a finite length path X : [0, T ] ? R d over the interval [s, t] is defined as LogSig s,t (X) := log(S s,t (X)), where S s,t (X) denotes the path signature of X given by Definition A.2. Likewise, the depth-N (or truncated) log-signature of X is defined for each N ? 1 as LogSig N s,t (X) := log N (S N s,t (X)).</p><p>In this section, we view each LogSig N s,t (X) as an element of T N R d to simplify the definition of the log-ODE method. That said, this is equivalent to the definition used in the main body of the paper, which defines the log-signature as a map from X : [0, T ] ? R d to R ? <ref type="bibr">(d,N )</ref> . This corresponds to the interpretation of a log-signature as an element of a certain free Lie algebra (see, for example, <ref type="bibr">Lyons et al. (2007)</ref>; <ref type="bibr" target="#b25">Reizenstein (2017)</ref>  The final ingredient we use to define the log-ODE method are the derivatives of the vector field f . It is worth noting that these derivatives also naturally appear in the Taylor expansion of (11). </p><formula xml:id="formula_29">Y s , X : [s, t] ? R d Y Taylor t := Y s +f (Y s )S N s,t (X) z =f (z)LogSig N s,t (X) z(0) = Y s Y Log t := z(1) Log-ODE method</formula><p>? Action of f on signature of X Action of f on log-signature of X Solve ODE on [0, 1] <ref type="figure" target="#fig_6">Figure 6</ref>. Illustration of the log-ODE and Taylor methods for controlled differential equations.</p><p>Using these definitions, we can describe two closely related numerical methods for the CDE (11).</p><p>Definition A.7 (The Taylor method) Given the CDE (11), we can use the path signature of X to approximate the solution Y on an interval [s, t] via its truncated Taylor expansion. That is, we use</p><formula xml:id="formula_30">Taylor(Y s , f, S N s,t (X)) := N k=0 f ?k (Y s )? k S N s,t (X) ,<label>(19)</label></formula><p>as an approximation for Y t where each ? k : T N (R d ) ? (R d ) ?k is the projection map onto R d ?k .</p><p>Definition A.8 (The Log-ODE method) Using the Taylor method (19), we can define the function f : R n ? L(T N (R d ), R n ) by f (z) := Taylor(z, f, ?). By applying f to the truncated log-signature of the path X over an interval [s, t], we can define the following ODE on [0, 1]</p><formula xml:id="formula_31">dz du = f (z)LogSig N s,t (X),<label>(20)</label></formula><formula xml:id="formula_32">z(0) = Y s .</formula><p>Then the log-ODE approximation of Y t (given Y s and LogSig N s,t (X)) is defined as</p><formula xml:id="formula_33">LogODE(Y s , f, LogSig N s,t (X)) := z(1).<label>(21)</label></formula><p>Remark A.9 Our assumptions of f ensure that z ? f (z)LogSig N s,t (X) is either globally bounded and Lipschitz continuous or linear. Hence both the Taylor and log-ODE methods are well defined.</p><p>Remark A.10 It is well known that the log-signature of a path X lies in a certain free Lie algebra (this is detailed in section 2.2.4 of <ref type="bibr">Lyons et al. (2007)</ref>). Furthermore, it is also a theorem that the Lie bracket of two vector fields is itself a vector field which doesn't depend on choices of basis. By expressing LogSig N s,t (X) using a basis of the free Lie algebra, it can be shown that only the vector field f and its (iterated) Lie brackets are required to construct the log-ODE vector field f (z)LogSig N s,t (X). In particular, this leads to our construction of the log-ODE (8) using the Lyndon basis of the free Lie algebra (see <ref type="bibr" target="#b25">(Reizenstein, 2017</ref>) for a precise description of the Lyndon basis). We direct the reader to <ref type="bibr" target="#b3">Lyons (2014)</ref> and <ref type="bibr" target="#b3">Boutaib et al. (2014)</ref> for further details on this Lie theory.</p><p>To illustrate the log-ODE method, we give two examples:</p><p>Example A.11 (The "increment-only" log-ODE method) When N = 1, the ODE <ref type="formula" target="#formula_31">(20)</ref> becomes</p><formula xml:id="formula_34">dz du = f (z)X s,t , z(0) = Y s .</formula><p>Therefore we see that this "increment-only" log-ODE method is equivalent to driving the original CDE (11) by a piecewise linear approximation of the control path X. This is a classical approach for stochastic differential equations (i.e. when X t = (t, W t ) with W denoting a Brownian motion) and is an example of a Wong-Zakai approximation (see <ref type="bibr" target="#b32">Wong &amp; Zakai (1965)</ref> for further details).</p><p>Example A.12 (An application for SDE simulation) Consider the following affine SDE,</p><formula xml:id="formula_35">dY t = a(b ? y t ) dt + ? y t ? dW t ,<label>(22)</label></formula><formula xml:id="formula_36">y(0) = y 0 ? R ?0 ,</formula><p>where a, b ? 0 are the mean reversion parameters, ? ? 0 is the volatility and W denotes a standard real-valued Brownian motion. The ? means that this SDE is understood in the Stratonovich sense. The SDE <ref type="formula" target="#formula_35">(22)</ref> is known in the literature as Inhomogeneous Geometric Brownian Motion (or IGBM). Using the control path X = {(t, W t )} t?0 and setting N = 3, the log-ODE <ref type="formula" target="#formula_31">(20)</ref> becomes</p><formula xml:id="formula_37">dz du = a(b ? z u )h + ? z u W s,t ? ab?A s,t + ab? 2 L (1) s,t + a 2 b?L (2) s,t , z(0) = Y s .</formula><p>where h := t ? s denotes the step size and the random variables A s,t , L In <ref type="bibr" target="#b10">Foster et al. (2020)</ref>, the depth-3 log-signature of X = {(t, W t )} t?0 was approximated so that the above log-ODE method became practical and this numerical scheme exhibited state-of-the-art convergence rates. For example, the approximation error produced by 25 steps of the high order log-ODE method was similar to the error of the "increment only" log-ODE method with 1000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence of the log-ODE method for rough differential equations</head><p>In this section, we shall present "rough path" error estimates for the log-ODE method. In addition, we will discuss the case when the vector fields governing the rough differential equation are linear. We begin by stating the main result of <ref type="bibr" target="#b3">Boutaib et al. (2014)</ref> which quantifies the approximation error of the log-ODE method in terms of the regularity of the systems vector field f and control path X. Since this section uses a number of technical definitions from rough path theory, we recommend <ref type="bibr">Lyons et al. (2007)</ref> as an introduction to the subject.</p><p>For T &gt; 0, we will use the notation T := {(s, t) ? [0, T ] 2 : s &lt; t} to denote a rescaled 2-simplex.</p><p>Theorem B.1 (Lemma 15 in <ref type="bibr" target="#b3">Boutaib et al. (2014)</ref>) Consider the rough differential equation</p><formula xml:id="formula_38">dY t = f (Y t ) dX t ,<label>(23)</label></formula><formula xml:id="formula_39">Y 0 = ?,</formula><p>where we make the following assumptions:</p><formula xml:id="formula_40">? X is a geometric p-rough path in R d , that is X : T ? T p (R d ) is a continuous path in the tensor algebra T p (R d ) := R ? R d ? R d ?2 ? ? ? ? ? R d ? p with increments X s,t = 1, X (1) s,t , X (2) s,t , ? ? ? , X ( p ) s,t ,<label>(24)</label></formula><formula xml:id="formula_41">X (k) s,t := ? k X s,t ,</formula><p>where ? k : T p R d ? R d ?k is the projection map onto R d ?k , such that there exists a sequence of continuous finite variation paths x n : [0, T ] ? R d whose truncated signatures converge to X in the p-variation metric:</p><formula xml:id="formula_42">d p S p (x n ), X ? 0,<label>(25)</label></formula><p>as n ? ?, where the p-variation between two continuous paths Z 1 and Z 2 in T p (R d ) is</p><formula xml:id="formula_43">d p Z 1 , Z 2 := max 1?k? p sup D ti?D ? k Z 1 ti,ti+1 ? ? k Z 2 ti,ti+1 p k k p ,<label>(26)</label></formula><p>where the supremum is taken over all partitions D of [0, T ] and the norms ? must satisfy (up to some constant)</p><formula xml:id="formula_44">a ? b ? a b ,</formula><p>for a ? (R d ) ?n and b ? (R d ) ?m . For example, we can take ? to be the projective or injective tensor norms (see Propositions 2.1 and 3.1 in <ref type="bibr" target="#b27">Ryan (2002)</ref>).</p><p>? The solution Y and its initial value ? both take their values in R n .</p><p>? The collection of vector fields {f 1 , ? ? ? , f d } on R n are denoted by f :</p><formula xml:id="formula_45">R n ? L(R n , R d ), where L(R n , R d )</formula><p>is the space of linear maps from R n to R d . We will assume that f has Lip(?) regularity with ? &gt; p. That is, f it is bounded with ? bounded derivatives, the last being H?lder continuous with exponent (? ? ? ). Hence the following norm is finite:</p><formula xml:id="formula_46">f Lip(?) := max 0?k? ? D k f ? ? D ? f (?? ? )?H?l ,<label>(27)</label></formula><p>where D k f is the k-th (Fr?chet) derivative of f and ? ?-H?l is the standard ?-H?lder norm with ? ? (0, 1).</p><p>? The RDE (23) is defined in the Lyon's sense. Therefore by the Universal Limit Theorem (see Theorem 5.3 in <ref type="bibr">Lyons et al. (2007)</ref>), there exists a unique solution Y : [0, T ] ? R n .</p><p>We define the log-ODE for approximating the solution Y over an interval [s, t] ? [0, T ] as follows:</p><p>1. Compute the depth-? log-signature of the control path X over <ref type="bibr">[s, t]</ref>. That is, we obtain LogSig</p><formula xml:id="formula_47">? s,t (X) := log ? S ? s,t (X) ? T ? (R d ),</formula><p>where log ? (?) is defined by projecting the standard tensor logarithm map onto {a ? T ? (R d ) : ? 0 (a) &gt; 0}.</p><p>2. Construct the following (well-posed) ODE on the interval [0, 1],</p><formula xml:id="formula_48">dz s,t du = F z s,t ,<label>(28)</label></formula><formula xml:id="formula_49">z s,t 0 = Y s ,</formula><p>where the vector field F : R n ? R n is defined from the log-signature as</p><formula xml:id="formula_50">F (z) := ? k=1 f ?k (z)? k LogSig ? s,t (X) .<label>(29)</label></formula><p>Recall that f ?k : R n ? L((R d ) ?k , R n ) was defined previously in Definition A.6.</p><p>Then we can approximate Y t using the u = 1 solution of (28). Moreover, there exists a universal constant C p,? depending only on p and ? such that</p><formula xml:id="formula_51">Y t ? z s,t 1 ? C p,? f ? Lip(?) X ? p-var;[s,t] ,<label>(30)</label></formula><p>where ? p-var; <ref type="bibr">[s,t]</ref> is the p-variation norm defined for paths in T p (R d ) by</p><formula xml:id="formula_52">X p-var;[s,t] := max 1?k? p sup D ti?D X k ti,ti+1 p k k p ,<label>(31)</label></formula><p>with the supremum taken over all partitions D of [s, t].</p><p>Remark B.2 If the vector fields {f 1 , ? ? ? , f d } are linear, then it immediately follows that F is linear.</p><p>Although the above theorem requires some sophisticated theory, it has a simple conclusion -namely that log-ODEs can approximate controlled differential equations. That said, the estimate (30) does not directly apply when the vector fields {f i } are linear as they would be unbounded. Fortunately, it is well known that linear RDEs are well posed and the growth of their solutions can be estimated. </p><formula xml:id="formula_53">dY t = f (Y t ) dX t , Y 0 = ?,</formula><p>where X is a geometric p-rough path in R d , ? ? R n and the vector fields {f i } 1?i?d take the form f i (y) = A i y + B where {A i } and {B i } are n ? n matrices. Let K denote an upper bound on max i ( A i + B i ). Then a unique solution Y : [0, T ] ? R n exists. Moreover, it is bounded and there exists a constant C p depending only on p such that</p><formula xml:id="formula_54">Y t ? Y s ? C p 1 + ? K X p-var;[s,t] exp C p K p X p p-var;[s,t] ,<label>(32)</label></formula><formula xml:id="formula_55">for all 0 ? s ? t ? T .</formula><p>When the vector fields of the RDE (23) are linear, then the log-ODE (28) also becomes linear. Therefore, the log-ODE solution exists and is explicitly given as the exponential of the matrix F . </p><formula xml:id="formula_56">dY t = f (Y t ) dX t , Y 0 = ?.</formula><p>Then the log-ODE vector field F given by <ref type="formula" target="#formula_50">(29)</ref> is linear and the solution of the associated ODE (28) exists and satisfies</p><formula xml:id="formula_57">z s,t u ? Y s exp ? m=1 K m ? m LogSig ? s,t (X) ,<label>(33)</label></formula><p>for u ? [0, 1] and all 0 ? s ? t ? T .</p><p>Proof B.5 Since F is a linear vector field on R n , we can view it as an n ? n matrix and so for u ? [0, 1],</p><formula xml:id="formula_58">z s,t u = exp(uF )z s,t 0 ,</formula><p>where exp denotes the matrix exponential. The result now follows by the standard estimate exp(F ) ? exp( F ).</p><p>Remark B.6 Due to the boundedness of linear RDEs (32) and log-ODEs (33), the arguments that established Theorem B.1 will hold in the linear setting as f Lip(?) would be finite when defined on the domains that the solutions Y and z lie in.</p><p>Given the local error estimate (30) for the log-ODE method, we can now consider the approximation error that is exhibited by a log-ODE numerical solution to the RDE (23). Thankfully, the analysis required to derive such global error estimates was developed by Greg Gyurk? in his PhD thesis. Thus the following result is a straightforward application of Theorem 3.2.1 from <ref type="bibr" target="#b14">Gyurk? (2008)</ref>.</p><p>Theorem B.7 Let X, f and Y satisfy the assumptions given by Theorem B.1 and suppose that {0 = t 0 &lt; t 1 &lt; ? ? ? &lt; t N = T } is a partition of [0, T ] with max k X p-var;[t k ,t k+1 ] sufficiently small. We can construct a numerical solution {Y log k } 0?k?N of (23) by setting Y log 0 := Y 0 and for each k ? {0, 1, ? ? ? , N ? 1}, defining Y log k+1 to be the solution at u = 1 of the following ODE:</p><formula xml:id="formula_59">dz t k ,t k+1 du := F z t k ,t k+1 ,<label>(34)</label></formula><p>z t k ,t k+1 0 := Y log k , where the vector field F is constructed from the log-signature of X over the interval [t k , t k+1 ] according to <ref type="bibr">(29)</ref>. Then there exists a constant C depending only on p, ? and f Lip(?) such that</p><formula xml:id="formula_60">Y t k ? Y log k ? C k?1 i=0 X ? p-var;[ti,ti+1] ,<label>(35)</label></formula><p>for 0 ? k ? N .</p><p>Remark B.8 The above error estimate also holds when the vector field f is linear (by Remark B.6)).</p><p>Since ? is the truncation depth of the log-signatures used to construct each log-ODE vector field, we see that high convergence rates can be achieved through using more terms in each log-signature. It is also unsurprising that the error estimate (35) increases with the "roughness" of the control path. So just as in our experiments, we see that the performance of the log-ODE method can be improved by choosing an appropriate step size and depth of log-signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental details</head><p>Code The code to reproduce the experiments is available at https://github.com/jambo6/neuralRDEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data splits</head><p>Each dataset was split into a training, validation, and testing dataset with relative sizes 70%/15%/15%.</p><p>Hyperparameter selection Hyperparameters were selected for the Neural CDE model by performing a grid search, with a step size chosen so that the length of the sequence was 500 steps. This was found to create a reasonable balance between training time and sequence length. We additionally performed a separate hyperparameter selection for the ODE-RNN model. The Neural RDE models then use the same hyperparameters as the Neural CDE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalisation</head><p>The training splits of each dataset were normalised to zero mean and unit variance. The statistics from the training set were then used to normalise the validation and testing datasets.</p><p>Architecture We give a graphical description of the architecture used for updating the Neural CDE hidden state in <ref type="figure">figure  7</ref>. The input is first run through a multilayer perceptron with n layers of size h, with with n, h being hyperparameters. ReLU nonlinearities are used at each layer except the final one, where we instead use a tanh nonlinearity. The goal of this is to help prevent term blow-up over the long sequences.</p><p>Note that this is a small inconsistency between this work and the original model proposed in <ref type="bibr" target="#b19">Kidger et al. (2020)</ref>. Here, we applied the tanh function as the final hidden layer nonlinearity, whilst in the original paper the tanh nonlinearity is applied after the final linear map. Both methods are used to constrain the rate of change of the hidden state; we do not know of a reason to prefer one over the other.</p><p>Note that the final linear layer in the multilayer perceptron is reshaped to produce a matrix-valued output, of shape v ? p.</p><p>(As f ? is matrix-valued.) A matrix-vector multiplication with the log-signature then produces the vector field for the ODE solver.</p><p>ODE Solver All problems used the 'rk4' solver as implemented by torchdiffeq (Chen, 2018) version 0.0.1.</p><p>Computing infrastructure All EigenWorms experiments were run on a computer equipped with three GeForce RTX 2080 Ti's. All BIDMC experiments were run on a computed with two GeForce RTX 2080 Ti's and two Quadro GP100's.</p><p>Input, Z r i Hidden layer 1 Hidden layer n f ? (Z r i ) LogSig</p><formula xml:id="formula_61">r i ,r i+1 Output, Z r i+1 v ? 1 h ? 1 h ? 1 v ? p p ? 1 v ? 1 . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logsig factor</head><p>ReLU ReLU Tanh Linear + reshape</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODE Solve</head><p>Matrix multiplication n layer? f ? <ref type="figure">Figure 7</ref>. Overview of the hidden state update network structure. We give the dimensions at each layer in the top right hand corner of each box.</p><p>Optimiser All experiments used the Adam optimiser. The learning rate was initialised at 0.032 divided by batch size. The batch size used was 1024 for EigenWorms and 512 for the BIDMC problems. If the validation loss failed to decrease after 15 epochs the learning rate was reduced by a factor of 10. If the validation loss did not decrease after 60 epochs, training was terminated and the model was rolled back to the point at which it achieved the lowest loss on the validation set.</p><p>Hyperparameter selection Hyperparameters were selected to optimise the score of the NCDE 1 model on the validation set. For each dataset the search was performed with a step size that meant the total number of hidden state updates was equal to 500, as this represented a good balance between length and speed that allowed us to complete the search in a reasonable time-frame. In particular, this was short enough that we could train using the non-adjoint training method which helped to speed this section up. The hyperparameters that were considered were:</p><p>? Hidden dimension: <ref type="bibr">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">64</ref>] -The dimension of the hidden state Z t .</p><p>? Number of layers: [2, 3, 4] -The number of hidden state layers.</p><p>? Hidden hidden multiplier: [1, 2, 3] -Multiplication factor for the hidden hidden state, this being the 'Hidden layer k' in figure 7. The dimension of each of these 'hidden hidden' layers with be this value multiplied by 'Hidden dimension'.</p><p>We ran each of these 27 total combinations for every dataset and the parameters that corresponded were used as the parameters when training over the full depth and step grid. The full results from the hyperparameter search are listed in tables (3, 5) with bolded values to show which values were eventually selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>Here we include the full breakdown of all experimental results. Tables 7 and 8 include all results from the EigenWorms and BIDMC datasets respectively.  <ref type="table">Table 7</ref>. Mean and standard deviation of test set accuracy (in %) over three repeats, as well as memory usage and training time, on the EigenWorms dataset for depths 1-3 and a small selection of step sizes. The bold values denote that the model was the top performer for that step size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Let ? ? : R v ? R w and f ? : R w ? R w?v be two neural networks and let ? : R w ? R q be a linear map, for some arXiv:2009.08295v4 [cs.LG] 21 Jun 2021 Here we give a high level comparison of the CDE and the RDE formulations. Left: The original CDE formulation where the data is smoothly interpolated and pointwise derivative information is used to drive the CDE. Right: The corresponding rough approach. Local interval summarisations of the data are computed and used to drive the response over the interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Signature (Taylor) expansion of a CDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(Neural) CDEs are all related; see for example Kidger et al. (2020) for a little exposition on this. De Brouwer et al. (2019); Lechner &amp; Hasani (2020) amongst others consider continuous time modifications to GRUs and LSTMs, improving the learning of long-term dependencies. Voelker et al. (2019); Gu et al. (2020) consider links with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition A. 2</head><label>2</label><figDesc>The signature of a finite length path X : [0, T ] ? R d over the interval [s, t] is defined as the following collection of iterated (Riemann-Stieltjes) integrals: S s,t X := 1 , X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>for details). The exact form of ?(d, N ) is given by ?M?bius function. The precise order of this remains an open question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Definition A. 6 (</head><label>6</label><figDesc>Vector field derivatives) We define f ?k : R n ? L((R d ) ?k , R n ) recursively by f ?(0) (y) := y, f ?(1) (y) := f (y), f ?(k+1) (y) := D f ?k (y)f (y), for y ? R n , where D f ?k denotes the Fr?chet derivative of f ?k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Theorem B.3 (Theorem 10.57 in Friz &amp; Victoir (2010)) Consider the linear RDE on [0, T ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table</figDesc><table><row><cell></cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ODE-RNN</cell><cell>4</cell><cell>35.0 ? 1.5</cell><cell>0.8</cell><cell>3629.3</cell></row><row><cell>(folded)</cell><cell>32</cell><cell>32.5 ? 1.5</cell><cell>0.1</cell><cell>532.2</cell></row><row><cell></cell><cell>128</cell><cell>47.9 ? 5.3</cell><cell>0.0</cell><cell>200.8</cell></row><row><cell></cell><cell>1</cell><cell>62.4 ? 12.1</cell><cell>22.0</cell><cell>176.5</cell></row><row><cell>NCDE</cell><cell>4 32</cell><cell>66.7 ? 11.8 64.1 ? 14.3</cell><cell>5.5 0.5</cell><cell>46.6 8.0</cell></row><row><cell></cell><cell>128</cell><cell>48.7 ? 2.6</cell><cell>0.1</cell><cell>3.9</cell></row><row><cell>NRDE (depth 2)</cell><cell>4 32 128</cell><cell>83.8 ? 3.0  *  67.5 ? 12.1 76.1 ? 5.9</cell><cell>2.4 0.7 0.2</cell><cell>180.0 28.1 7.8</cell></row><row><cell>NRDE (depth 3)</cell><cell>4 32 128</cell><cell>76.9 ? 9.2 75.2 ? 3.0 68.4 ? 8.2</cell><cell>2.8 0.6 0.1</cell><cell>856.8 134.7 53.3</cell></row></table><note>1. EigenWorms dataset: mean ? standard deviation of test set accuracy measured over three repeats. Also reported are the mean memory usage and training time. For all models a variety of step sizes are considered. For the Neural RDE we additionally investigate varying depths. (Recalling that the NCDE is a depth-1 NRDE.) '-' denotes that the model could not be run within GPU memory. Bold denotes the best model score for a given step size, and* denotes that the score was the best achieved over all models and step sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b31">Wisdom et al. (2016)</ref>;<ref type="bibr" target="#b16">Jing et al. (2019)</ref> show that unitary or orthogonal RNNs can mitigate the vanishing/exploding gradients problem. However, they are expensive to train due to the need to compute a matrix inversion at each training step.<ref type="bibr" target="#b5">Chang et al. (2017)</ref> introduce dilated RNNs with skip connections between RNN states, which help improve training speed and learning of long-term dependencies.<ref type="bibr" target="#b4">Campos et al. (2017)</ref> introduce the 'Skip-RNN' model, which extend the RNN by adding an additional learnt component that skips state updates.<ref type="bibr" target="#b21">Li et al. (2018)</ref> introduce the 'IndRNN' model, with particular structure tailored to learning long time series.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Step</cell><cell></cell><cell>L 2</cell><cell></cell><cell></cell><cell cols="2">Time (Hrs)</cell><cell>Memory (Mb)</cell></row><row><cell></cell><cell></cell><cell>RR</cell><cell>HR</cell><cell>SpO2</cell><cell>RR</cell><cell>HR</cell><cell>SpO2</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>-</cell><cell>13.06 ? 0.0</cell><cell>-</cell><cell>-</cell><cell>10.5</cell><cell>-</cell><cell>3653.0</cell></row><row><cell>ODE-RNN (folded)</cell><cell>8</cell><cell>2.47 ? 0.35</cell><cell>13.06 ? 0.00</cell><cell>3.3 ? 0.00</cell><cell>1.5</cell><cell>1.2</cell><cell>0.9</cell><cell>917.2</cell></row><row><cell></cell><cell>128</cell><cell>1.62 ? 0.07</cell><cell>13.06 ? 0.00</cell><cell>3.3 ? 0.00</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>81.9</cell></row><row><cell></cell><cell>512</cell><cell>1.66 ? 0.06</cell><cell>6.75 ? 0.9</cell><cell>1.98 ? 0.31</cell><cell>0.0</cell><cell>0.1</cell><cell>0.1</cell><cell>40.4</cell></row><row><cell></cell><cell>1</cell><cell>2.79 ? 0.04</cell><cell>9.82 ? 0.34</cell><cell>2.83 ? 0.27</cell><cell cols="2">23.8 22.1</cell><cell>28.1</cell><cell>56.5</cell></row><row><cell>NCDE</cell><cell>8 128</cell><cell>2.80 ? 0.06 2.64 ? 0.18</cell><cell>10.72 ? 0.24 11.98 ? 0.37</cell><cell>3.43 ? 0.17 2.86 ? 0.04</cell><cell>3.0 0.2</cell><cell>2.6 0.2</cell><cell>4.8 0.3</cell><cell>14.3 8.7</cell></row><row><cell></cell><cell>512</cell><cell>2.53 ? 0.03</cell><cell>12.22 ? 0.11</cell><cell>2.98 ? 0.04</cell><cell>0.1</cell><cell>0.0</cell><cell>0.1</cell><cell>8.4</cell></row><row><cell></cell><cell>8</cell><cell>2.63 ? 0.12</cell><cell>8.63 ? 0.24</cell><cell>2.88 ? 0.15</cell><cell>2.1</cell><cell>3.4</cell><cell>3.3</cell><cell>21.8</cell></row><row><cell>NRDE (depth 2)</cell><cell>128</cell><cell>1.86 ? 0.03</cell><cell>6.77 ? 0.42</cell><cell>1.95 ? 0.18</cell><cell>0.3</cell><cell>0.4</cell><cell>0.7</cell><cell>10.9</cell></row><row><cell></cell><cell>512</cell><cell>1.81 ? 0.02</cell><cell>5.05 ? 0.23</cell><cell>2.17 ? 0.18</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>10.3</cell></row><row><cell></cell><cell>8</cell><cell>2.42 ? 0.19</cell><cell>7.67 ? 0.40</cell><cell>2.55 ? 0.13</cell><cell>2.9</cell><cell>3.2</cell><cell>3.1</cell><cell>43.3</cell></row><row><cell>NRDE (depth 3)</cell><cell>128</cell><cell>1.51 ? 0.08</cell><cell>2.97 ? 0.45  *</cell><cell>1.37 ? 0.22</cell><cell>0.5</cell><cell>1.7</cell><cell>1.7</cell><cell>17.3</cell></row><row><cell></cell><cell>512</cell><cell>1.49 ? 0.08  *</cell><cell>3.46 ? 0.13</cell><cell>1.29 ? 0.15  *</cell><cell>0.3</cell><cell>0.4</cell><cell>0.4</cell><cell>15.4</cell></row></table><note>. The three experiments on BIDMC datasets: mean ? standard deviation of test set L 2 loss, measured over three repeats, over each of three different vital signs prediction tasks (RR, HR, SpO2). Also reported are the memory usage and training time. Only mean times are shown for space. For all models a variety of step sizes are considered. For the Neural RDE we additionally investigate varying depths.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Liao, S.,Lyons, T., Yang, W., and Ni, H. Learning stochastic differential equations using RNN with log signature features. arXiv preprint arXiv:1908.08286, 2019. Lyons, T. Rough paths, signatures and the modelling of functions on streams. Proceedings of the International Congress of Mathematicians, 4, 2014.</figDesc><table><row><cell>Lyons, T., Michael, C., and Thierry, L. Differential equa-</cell></row><row><cell>tions driven by rough paths. In?cole d'?t? de proba-</cell></row><row><cell>bilit?s de Saint-Flour XXXIV-2004, edited by J. Picard</cell></row><row><cell>in Volume 1908 of Lecture Notes in Mathematics, Berlin,</cell></row><row><cell>Springer, 2007.</cell></row><row><cell>Lyons, T. J. Differential equations driven by rough sig-</cell></row><row><cell>nals. Revista Matem?tica Iberoamericana, 14(2):215-</cell></row><row><cell>310, 1998.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Theorem B.4 Consider the same linear RDE on [0, T ] as in Theorem B.3,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Hyperparameter selection results for the folded ODE-RNN model on the BIDMC problem. Bold values indicate selected hyperparamter values. The ODE-RNN model failed to train effectively for the HR and SpO2 problems which is why the validation losses are the same (to 2dp).</figDesc><table><row><cell>Model</cell><cell cols="2">Step Test Accuracy</cell><cell>Time (Hrs)</cell><cell>Memory (Mb)</cell></row><row><cell></cell><cell>1</cell><cell cols="3">Memory Error Memory Error Memory Error</cell></row><row><cell></cell><cell>2</cell><cell>36.8 ? 1.5</cell><cell>1.6</cell><cell>7170.1</cell></row><row><cell></cell><cell>4</cell><cell>35.0 ? 1.5</cell><cell>0.8</cell><cell>3629.3</cell></row><row><cell></cell><cell>6</cell><cell>36.8 ? 1.5</cell><cell>0.5</cell><cell>2448.6</cell></row><row><cell></cell><cell>8</cell><cell>36.8 ? 1.5</cell><cell>0.4</cell><cell>1858.8</cell></row><row><cell></cell><cell>16</cell><cell>32.5 ? 3.0</cell><cell>0.2</cell><cell>973.5</cell></row><row><cell>ODE-RNN</cell><cell>32</cell><cell>32.5 ? 1.5</cell><cell>0.1</cell><cell>532.2</cell></row><row><cell>(folded)</cell><cell>64</cell><cell>41.0 ? 4.4</cell><cell>0.1</cell><cell>311.2</cell></row><row><cell></cell><cell>128</cell><cell>47.9 ? 5.3</cell><cell>0.0</cell><cell>200.8</cell></row><row><cell></cell><cell>256</cell><cell>46.2 ? 0.0</cell><cell>0.0</cell><cell>147.0</cell></row><row><cell></cell><cell>512</cell><cell>47.9 ? 10.4</cell><cell>0.0</cell><cell>124.5</cell></row><row><cell></cell><cell>1024</cell><cell>44.4 ? 7.4</cell><cell>0.0</cell><cell>122.4</cell></row><row><cell></cell><cell>2048</cell><cell>48.7 ? 6.8</cell><cell>0.0</cell><cell>137.2</cell></row><row><cell></cell><cell>1</cell><cell>62.4 ? 12.1</cell><cell>22.0</cell><cell>176.5</cell></row><row><cell></cell><cell>2</cell><cell>69.2 ? 4.4</cell><cell>14.6</cell><cell>90.6</cell></row><row><cell></cell><cell>4</cell><cell>66.7 ? 11.8</cell><cell>5.5</cell><cell>46.6</cell></row><row><cell></cell><cell>6</cell><cell>65.8 ? 12.9</cell><cell>2.6</cell><cell>31.5</cell></row><row><cell></cell><cell>8</cell><cell>64.1 ? 13.3</cell><cell>3.1</cell><cell>24.3</cell></row><row><cell></cell><cell>16</cell><cell>64.1 ? 16.8</cell><cell>1.5</cell><cell>13.4</cell></row><row><cell>NCDE</cell><cell>32 64</cell><cell>64.1 ? 14.3 56.4 ? 6.8</cell><cell>0.5 0.4</cell><cell>8.0 5.2</cell></row><row><cell></cell><cell>128</cell><cell>48.7 ? 2.6</cell><cell>0.1</cell><cell>3.9</cell></row><row><cell></cell><cell>256</cell><cell>42.7 ? 3.0</cell><cell>0.1</cell><cell>3.2</cell></row><row><cell></cell><cell>512</cell><cell>44.4 ? 5.3</cell><cell>0.0</cell><cell>2.9</cell></row><row><cell></cell><cell>1024</cell><cell>41.9 ? 14.6</cell><cell>0.0</cell><cell>2.7</cell></row><row><cell></cell><cell>2048</cell><cell>38.5 ? 5.1</cell><cell>0.0</cell><cell>2.6</cell></row><row><cell></cell><cell>2</cell><cell>76.1 ? 13.2</cell><cell>9.8</cell><cell>354.3</cell></row><row><cell></cell><cell>4</cell><cell>83.8 ? 3.0</cell><cell>2.4</cell><cell>180.0</cell></row><row><cell></cell><cell>6</cell><cell>76.9 ? 6.8</cell><cell>2.0</cell><cell>82.2</cell></row><row><cell></cell><cell>8</cell><cell>77.8 ? 5.9</cell><cell>2.1</cell><cell>94.2</cell></row><row><cell></cell><cell>16</cell><cell>78.6 ? 3.9</cell><cell>1.3</cell><cell>50.2</cell></row><row><cell>NRDE2</cell><cell>32</cell><cell>67.5 ? 12.1</cell><cell>0.7</cell><cell>28.1</cell></row><row><cell></cell><cell>64</cell><cell>73.5 ? 7.8</cell><cell>0.4</cell><cell>17.2</cell></row><row><cell></cell><cell>128</cell><cell>76.1 ? 5.9</cell><cell>0.2</cell><cell>7.8</cell></row><row><cell></cell><cell>256</cell><cell>72.6 ? 12.1</cell><cell>0.1</cell><cell>8.9</cell></row><row><cell></cell><cell>512</cell><cell>69.2 ? 11.8</cell><cell>0.0</cell><cell>7.6</cell></row><row><cell></cell><cell>1024</cell><cell>65.0 ? 7.4</cell><cell>0.0</cell><cell>6.9</cell></row><row><cell></cell><cell>2048</cell><cell>67.5 ? 3.9</cell><cell>0.0</cell><cell>6.5</cell></row><row><cell></cell><cell>2</cell><cell>66.7 ? 4.4</cell><cell>7.4</cell><cell>1766.2</cell></row><row><cell></cell><cell>4</cell><cell>76.9 ? 9.2</cell><cell>2.8</cell><cell>856.8</cell></row><row><cell></cell><cell>6</cell><cell>70.9 ? 1.5</cell><cell>1.4</cell><cell>606.1</cell></row><row><cell></cell><cell>8</cell><cell>70.1 ? 6.5</cell><cell>1.3</cell><cell>460.7</cell></row><row><cell></cell><cell>16</cell><cell>73.5 ? 3.0</cell><cell>1.4</cell><cell>243.7</cell></row><row><cell>NRDE3</cell><cell>32</cell><cell>75.2 ? 3.0</cell><cell>0.6</cell><cell>134.7</cell></row><row><cell></cell><cell>64</cell><cell>74.4 ? 11.8</cell><cell>0.3</cell><cell>81.0</cell></row><row><cell></cell><cell>128</cell><cell>68.4 ? 8.2</cell><cell>0.1</cell><cell>53.3</cell></row><row><cell></cell><cell>256</cell><cell>60.7 ? 8.2</cell><cell>0.1</cell><cell>40.2</cell></row><row><cell></cell><cell>512</cell><cell>62.4 ? 10.4</cell><cell>0.0</cell><cell>33.1</cell></row><row><cell></cell><cell>1024</cell><cell>59.8 ? 3.9</cell><cell>0.0</cell><cell>29.6</cell></row><row><cell></cell><cell>2048</cell><cell>61.5 ? 4.4</cell><cell>0.0</cell><cell>27.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>JM was supported by the EPSRC grant EP/L015803/1 in collaboration with Iterex Therapuetics. PK was supported by the EPSRC grant EP/L015811/1. CS was supported by the EPSRC grant EP/R513295/1. JF was supported by the EPSRC grant EP/N509711/1. JM, CS, PK, JF were supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 8</ref><p>. Mean and standard deviation of the L 2 losses on the test set for each of the vitals signs prediction tasks (RR, HR, SpO2) on the BIDMC dataset, across three repeats. Only mean times are shown for space. The memory usage is given as the mean over all three of the tasks as it was approximately the same for any task for a given depth and step. Error denotes that the model could not be run within GPU memory. The bold values denote the algorithm with the lowest test set loss for a fixed step size for each task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="606" to="660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Magnus expansion and some of its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blanes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-A</forename><surname>Oteo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ros</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">470</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="151" to="238" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimension-free Euler estimates of rough differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boutaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gyurk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revue Roumaine de Mathmatiques Pures et Appliques</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gir?-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torchdiffeq</surname></persName>
		</author>
		<ptr target="https://github.com/rtqichen/torchdiffeq" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on the application of recurrent neural networks to statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Mulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="98" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An optimal polynomial approximation of Brownian motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Oberhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1393" to="1421" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multidimensional stochastic processes as rough paths: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Friz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Victoir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">120</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised sequence labelling with recurrent neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hippo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07669</idno>
		<title level="m">Recurrent Memory with Optimal Polynomial Projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Numerical methods for approximating solutions to rough differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Gyurk?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">DPhil thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uniqueness for the signature of a path of bounded variation and the reduced path group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hambly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated orthogonal recurrent units: On learning to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soljacic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="783" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Universal Approximation with Deep Narrow Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00706</idno>
		<ptr target="https://github.com/patrick-kidger/signatory" />
		<title level="m">Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies in irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04418</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5243" to="5253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Differential equations driven by rough paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>L?vy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="93" />
		</imprint>
		<respStmt>
			<orgName>Ecole d&apos;?t? de Probabilit?s de Saint-Flour</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximation theory of the MLP model in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="195" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Calculation of Iterated-Integral Signatures and Log Signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02757</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Latent odes for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03907</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to Tensor Products of Banach Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Monographs in Mathematics</title>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sourkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03402</idno>
		<title level="m">Slicing the features space to represent sequences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monash university, uea, ucr time series regression archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<ptr target="http://timeseriesregression.org/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the Convergence of Ordinary Integrals to Stochastic Integrals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1560" to="1564" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
