<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Error-Bounded Correction of Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Goswami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Error-Bounded Correction of Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice. In this paper, we provide the first theoretical explanation for these methods. We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean. Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are consistent with the true Bayesian optimal classifier with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Label noise is ubiquitous in real world data. It may be caused by unintentional mistakes of manual or automatic annotators <ref type="bibr" target="#b42">(Yan et al., 2014;</ref><ref type="bibr" target="#b0">Andreas et al., 2017)</ref>. It may also be introduced by malicious attackers <ref type="bibr" target="#b16">(Jacob et al., 2017)</ref>. Noisy labels impair the performance of a model <ref type="bibr" target="#b31">(Smyth et al., 1994;</ref><ref type="bibr" target="#b2">Brodley &amp; Friedl, 1999)</ref>, especially a deep neural network, which tends to have strong memorization power <ref type="bibr" target="#b10">(Fr?nay &amp; Verleysen, 2014;</ref><ref type="bibr" target="#b43">Zhang et al., 2017)</ref>. Improving the robustness of a model trained with noisy labels is a crucial yet challenging task in many applications <ref type="bibr">(Volodymyr</ref> To adapt to heterogeneous noise pattern and to fully exploit the power of deep neural networks, data-re-calibrating methods have been proposed to focus on individual data instead of an overall model adjustment <ref type="bibr" target="#b23">(Malach &amp; Shalev-Shwartz, 2017;</ref><ref type="bibr" target="#b17">Jiang et al., 2018;</ref><ref type="bibr" target="#b14">Han et al., 2018;</ref><ref type="bibr" target="#b32">Tanaka et al., 2018;</ref><ref type="bibr" target="#b29">Ren et al., 2018;</ref><ref type="bibr" target="#b5">Cheng et al., 2020)</ref>. These methods learn to re-calibrate the model on each individual datum depending on its own context. They gradually collect clean data whose labels are trustworthy. As more clean data are collected, the quality of the trained models improves. These methods slowly accumulate useful/trustworthy information and eventually attain state-of-the-art quality models.</p><p>Despite the success of data-re-calibrating methods, their underlying mechanism remains elusive. It is unclear why the neural nets trained on noisy labels can help select clean data. A theoretical underpinning will not only explain the phenomenon, but also advance the methodology. One major challenge for these methods is to control the data recalibration quality. It is hard to monitor the model's recalibrating decision on individual data. An aggressive selection of clean data can unknowingly accumulate irreversible errors. On the other hand, an overly-conservative strategy can be very slow in training, or stops with insufficient clean data and mediocre models. A theoretical guarantee will help develop models with self-assurance that the decision on each datum is reasonably close to the truth.</p><p>In this paper, we provide the first theoretical explanation for data-re-calibrating methods. Our main theorem states that a noisy classifier (i.e., one trained on noisy labels) can identify whether a label has been corrupted. In particular, we prove that when the noisy classifier has low confidence on the label of a datum, such label is likely corrupted. In fact, we can quantify the threshold of confidence, below which the label is likely to be corrupted, and above which is it likely to be not. We also empirically show that the bound in our theorem is tight.</p><p>Our theoretical result not only explains existing data-recalibrating methods, but also suggests a new solution for the problem. As a second contribution of this paper, we propose a novel method for noisy-labeled data. Based on our theorem and statistical principles, we verify the purity of a label through a likelihood ratio test w.r.t. the prediction of a noisy classifier, and the threshold value of confidence. The label is corrected or left intact depending on the test result. We prove that this simple label-correction algorithm has a guaranteed success rate and will recover the true labels with high probability. We incorporate the label-correction algorithm into the training of deep neural networks. We validate our method on different datasets with various noise patterns and levels. Our theoretically-founded method outperforms state-of-the-arts due to its simplicity and due to its principled design.</p><p>Our paper shows that a theorem that is well-grounded in applications will inspire elegant and powerful algorithms even in deep learning settings. Our contribution is two-fold:</p><p>? We provide a theorem quantifying how a noisy classifier's prediction correlates to the purity of a datum's label. This provides theoretical explanation for data-recalibrating methods for noisy labels. ? Inspired by the theorem, we propose a new labelcorrection algorithm with guaranteed success rate. We train neural networks using the new algorithm and achieve superior performance.</p><p>The code of this paper can be found in https://github. com/pingqingsheng/LRT.git.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>One representative strategy for handling label noise is to model and employ noise transition matrix to correct the loss. For example, <ref type="bibr" target="#b25">Patrini et al. (2017)</ref> propose to correct the loss function with estimated noise pattern. The resulting loss is an unbiased estimator of the ground truth loss, and enables the trained model to achieve better performance. However, such an estimator relies on strong assumptions and could be inaccurate in certain scenarios. <ref type="bibr" target="#b28">Reed et al. (2014)</ref> consider modeling the noise pattern with a hidden layer. The learning of this hidden layer is regularized with a feature reconstruction loss, yet without a guarantee that the true label distribution is learned. Another method mentioned in their work is to minimize the entropy of neural network output; however, this method tends to predict a single class. To address this weakness, <ref type="bibr" target="#b8">Dan et al. (2019)</ref> propose to utilize a small number of trusted, clean data to pre-train a network and estimate the noise pattern. However, such clean data may not always be available in practice.</p><p>Alternatively, another direction proposes to design models that are intrinsically robust to noisy data. <ref type="bibr" target="#b7">Crammer et al. (2009)</ref> introduce a regularized confidence weighting learning algorithm (AROW), which attempts to preserve the weight distribution as much as possible while requiring the model to maintain discrimination ability. The follow-up work <ref type="bibr" target="#b6">(Crammer &amp; Lee 2010)</ref> improves this algorithm by herding the updating direction via specific velocity field (NHERD), and achieves better performance. Both of these works impose constraints on parameters, which, however, could prevent classifiers from adapting to complex datasets. Another similar strategy proposes to assume Gaussian distribution for features, and models the data with a robust generative classifier <ref type="bibr" target="#b20">(Lee et al., 2019)</ref>. However, such an assumption may not generalize to other complex scenarios. <ref type="bibr" target="#b9">Devansh et al. (2017)</ref> show that deep neural networks tend to learn meaningful patterns before they over-fit to noisy ones. Based on this observation, they propose to add Gaussian or adversarial noise to input when training with noisy labels, and empirically show that such data perturbation is able to make the resulting model more robust. Other commonly adopted techniques, such as weight decay and dropout, are also shown to be effective in increasing the robustness of trained classifier <ref type="bibr" target="#b9">(Devansh et al. 2017;</ref><ref type="bibr" target="#b43">Zhang et al. 2017</ref>). However, the intrinsic reasons for this phenomenon still remain unclear and overfitting to noisy label is extremely likely. Data-re-calibrating methods select clean data while eliminating noisy ones during training. For example, <ref type="bibr" target="#b23">Malach &amp; Shalev-Shwartz (2017)</ref> and <ref type="bibr" target="#b14">Han et al. (2018)</ref> train two networks simultaneously, and update the networks only with samples that are considered clean by both networks. Similarly, <ref type="bibr" target="#b17">Jiang et al. (2018)</ref> also use two networks: the first one is pre-trained to learn a curriculum, and then utilized to select clean samples for training the second network. These methods deliver promising results but lack control of the quality of the collected clean data.</p><p>Finally, beyond deep learning framework, there are several theoretic works that demonstrate the robustness of a variety of losses to label noise <ref type="bibr" target="#b22">(Long &amp; Servedio 2010;</ref><ref type="bibr" target="#b24">Nagarajan et al. 2013;</ref><ref type="bibr" target="#b12">Ghosh et al. 2015;</ref><ref type="bibr" target="#b35">Van Rooyen et al. 2015)</ref>. Following the work of (Wang &amp; Chaudhuri 2018), <ref type="bibr" target="#b11">Gao et al. (2016)</ref> propose an algorithm that can converge to the Bayesian optimal classifier under different noise settings. Moreover, they provide in-depth discussion regarding the performance of k-nearest neighbor (KNN) classifiers. However, the problem with KNN is that it is computationally intensive and difficult to be incorporated into a learning context. Within the framework of deep learning, there are more efforts that need to be made to bridge theory and practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Main Theorem: Probing Label Purity Using the Noisy Classifier</head><p>Our main theorem answers the following question: without knowing the ground truth, how to decide whether a label is corrupted or not. During training, the only information one can rely on is a noisy classifier, i.e., one that is trained on the corrupted labels. Data-re-calibrating methods use the noisy classifier to decide whether a datum is clean-labeled. However, these methods lack a theoretical justification.</p><p>We establish the relationship between a noisy classifier and the purity of a label. We prove that if the classifier has low confidence on a datum with regard to its current label, then this label is likely corrupted. This result provides the first theoretical explanation of why noisy classifiers can be used to determine the purity of labels in previous methods.</p><p>This section is organized as follows. We start by providing basic notations and assumptions. Next, we state the main theorem for binary classification and then extend it to the multiclass setting. We also use experiments on synthetic data and CIFAR10 to validate the tightness of our bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries and Assumptions</head><p>We first focus on binary classification. Later the result will be extended to multiclass setting. Let X be the feature space, Y = {0, 1} be the label space. The joint probability distribution, D, can be factored as D(x, y) = Pr(y|x) Pr(x). We denote by ?(x) = Pr(y = 1|x) the true conditional probability. The risk of a binary classifier h : X ? Y is R(D, h) = Pr (x,y)?D [h(x) = y]. A Bayes optimal classifier is the minimizer of the risk over all possible hypotheses, i.e., h * = arg min h R(D, h). It can be calculated using the true conditional probability, ?,</p><formula xml:id="formula_0">h * (x) = 1 {?(x)&gt; 1 2 } (x) := 1 ?(x) &gt; 1 2 0 otherwise .</formula><p>We assume ? satisfies the Tsybakov condition <ref type="bibr" target="#b34">(Tsybakov, 2004)</ref>. This condition, also called margin assumption, stipulates that the uncertainty of ? is bounded. In other words, the margin region close to the decision boundary, {x ? X | ?(x) = 1/2}, has a bounded volume. Assumption 1 <ref type="bibr">(Tsybakov Condition)</ref>. There exist constants C, ? &gt; 0, and t 0 ? (0, 1 2 ], such that for all t ? t 0 ,</p><formula xml:id="formula_1">Pr ?(x) ? 1 2 ? t ? Ct ? .</formula><p>This assumption is adopted in previous works such as <ref type="bibr" target="#b3">(Chaudhuri &amp; Dasgupta, 2014;</ref><ref type="bibr" target="#b1">Belkin et al., 2018;</ref><ref type="bibr" target="#b27">Qiao et al., 2019)</ref>. However, we have not seen any empirical verification of the condition in real datasets. In this paper, we conduct experiments to verify this condition and provide empirical estimation of the constants C and ?. Our experiments indicate that this condition holds with moderate values of the constants C and ?.</p><p>The noisy label setting. Instead of samples from D, we are given a sample set with noisy labels S = {(x, y)}, where y is the possibly corrupted label based on the true label y. We assume a transition probability ? i?j = Pr( y = j|y = i), i.e., the chance a true label y is flipped from class i to class j. For simplicity, we denote ? ij = ? i?j . The transition probabilities ? 01 and ? 10 are independent of the true joint distribution D and the feature x. We denote the conditional probability of the noisy labels as ?(x) = Pr( y = 1|x). We call ? the noisy conditional probability. It is easy to verify that ? is linear to the true conditional probability, ?:</p><formula xml:id="formula_2">?(x) = (1 ? ? 10 )?(x) + ? 01 [1 ? ?(x)] = (1 ? ? 01 ? ? 10 )?(x) + ? 01 .</formula><p>We intend to learn a classifier whose prediction is consistent with the Bayes optimal classifier h * . Therefore, we call the prediction of h * the correct label.</p><p>Definition 1 (Correct Label). Given x, its correct label is the Bayes optimal classifier prediction h * (x).</p><p>The correct label, h * (x), is subtly different from the true label, y. In particular, h * (x) is uniquely decided by ?(x), whereas y is a sample from ?. Since h * is our final goal, we focus on recovering the correct label, h * (x), instead of y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Main Theorem</head><p>Our main theorem connects a noisy classifier f : X ? Y with the chance of a noisy label y being correct. We assume f is trained on the noisy labels and is trained well enough, i.e., -close to the noisy conditional probability, ?. For convenience, we denote by f y the classifier prediction of label being y, formally,</p><formula xml:id="formula_3">f y (x) = f (x) if y = 1, and 1 ? f (x) otherwise. Define the estimation error := f ? ? ? . Theorem 1. Assume ?(x) satisfies the Tsybakov condition with constants C, ? &gt; 0, and t 0 ? (0, 1 2 ]. Assume ? t 0 (1 ? ? 10 ? ? 01 ). For ? = 1?|?10??01| 2</formula><p>, we have:</p><formula xml:id="formula_4">Pr (x,y)?D y = h * (x), f y (x) &lt; ? ? C O( ) ? .</formula><p>Implication of the theorem. Intuitively, the theorem states that a noisy label y has bounded probability to be correct if it has a low vote-of-confidence by f . The upper bound of the probability is controlled by , the approximation error of f . In other words, the better f approximates ?, the tighter the bound is. This justifies the usage of a good-quality f to determine if y is trustworthy. Later we will show is reasonably small in deep learning setting and the bound is tight in practice.</p><p>We remark that the constant ? and the constant hidden inside the big-O in the theorem depend on ? ij 's, which are unknown in practice. Based on this theorem, we will propose a new label-correction algorithm that determines ? robustly in practice without knowing ? ij 's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">PROOF OF THEOREM 1</head><p>Preliminary Lemmata. To prove this theorem, we need to first prove two lemmata. Lemma 1 will show that if a classifier g is a linear transformation of ?, when the value g y is below a certain threshold, y is unlikely to be consistent with the true Bayesian optimal decision, h * . Next, Lemma 2 states that since ?(x) is a linear transformation of ?(x), Lemma 1 will apply to ?(x) and ? can be set accordingly. Finally, based on the conclusion of Lemma 2 and the Tsybakov condition, we can upperbound</p><formula xml:id="formula_5">Pr [ y = h * (x), f y (x) &lt; ?] if f is -close to ?. Lemma 1. If a classifier g depends linearly on ?, i.e., g(x) = a?(x) + b with a, b &gt; 0. Set ? = min a 2 + b, 1 ? b ? a 2 . We have Pr (x,y)?D y = h * (x), g y (x) &lt; ? = 0 (1)</formula><p>Proof. To calculate Pr (x,y)?D y = h * (x), g y (x) &lt; ? , we enumerate two cases:</p><formula xml:id="formula_6">Case 1: y = 1. Observe h * (x) = 1 iff ?(x) &gt; 1/2; g y (x) = g(x) = a?(x) + b &lt; ? iff ?(x) &lt; ??b a .</formula><p>We have:</p><formula xml:id="formula_7">Pr y = h * (x), g y (x) &lt; ? = Pr 1 2 &lt; ?(x) &lt; ? ? b a .</formula><p>(2) We next show that this probability is 0 for the chosen ? = min a</p><formula xml:id="formula_8">2 + b, 1 ? b ? a 2 . If ? = a 2 + b, the probability is zero as ??b a = 1 2 . Otherwise, ? = 1 ? b ? a 2 . We know that 1 ? b ? a 2 &lt; a 2 + b. Therefore, 1 ? 2b &lt; a. In this case, ? ? b a = 1 ? 2b a ? 1 2 &lt; 1 ? 1 2 = 1 2 . Thus we have Pr 1 2 &lt; ?(x) &lt; ??b a = 0. Case 2: y = 0. Observe that h * (x) = 0 iff ?(x) ? 1/2; g y (x) = 1 ? g(x) = 1 ? [a?(x) + b] &lt; ? iff ?(x) &gt; L := 1?b?? a</formula><p>, we have:</p><formula xml:id="formula_9">Pr y = h * (x), g y (x) &lt; ? = Pr L &lt; ?(x) &lt; 1 2 .</formula><p>Similar to Case 1, by checking when ? = a 2 + b and when ? = 1 ? b ? a 2 , we can verify that <ref type="formula">(1)</ref> and completes the proof.</p><formula xml:id="formula_10">Pr 1?b?? a &lt; ?(x) &lt; 1 2 = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This proves Equation</head><formula xml:id="formula_11">Lemma 2. Let ? = 1?|?10??01| 2 . Let ? 1 = ? and ? 0 = 1 ? ?. Pr (x,y)?D y = h * (x), ? y (x) &lt; ? = 0.</formula><p>(3)</p><p>Proof. Recall ?(x) = (1 ? ? 01 ? ? 10 )?(x) + ? 01 , in which ? 01 and ? 10 are transition probabilities. We can directly prove this lemma using Lemma 1 by setting g = ? with a = 1 ? ? 01 ? ? 10 and b = ? 01 .</p><p>Proof of Theorem 1 using the Lemmata.</p><formula xml:id="formula_12">Proof. When y = 1, f y (x) = f (x) ? ?(x) ? . Pr [ y = h * (x), f y (x) &lt; ?] ? Pr [ y = h * (x), ?(x) ? &lt; ?]</formula><p>Substituting ? with ? + into equation <ref type="formula">(2)</ref>, we have:</p><formula xml:id="formula_13">Pr [ y = h * (x) = 1, ?(x) ? &lt; ?] = Pr [ y = h * (x) = 1, ?(x) &lt; ? + ] = Pr 1 2 &lt; ?(x) &lt; ? + ? ? 01 1 ? ? 01 ? ? 10</formula><p>Similar to Lemma 1, by discussing the cases when ? = 1+?10??01 2 and when ? = 1+?01??10 2 , we can show that</p><formula xml:id="formula_14">???01 1??01??10 &lt; 1 2 .</formula><p>Based on the Tsybakov condition, we have</p><formula xml:id="formula_15">Pr 1 2 &lt; ?(x) &lt; ? ? ?01 1 ? ?01 ? ?10 + 1 ? ?01 ? ?10 ? Pr 1 2 &lt; ?(x) &lt; 1 2 + 1 ? ?01 ? ?10 ? C 1 ? ?01 ? ?10 ?</formula><p>This implies that:</p><formula xml:id="formula_16">Pr [ y = h * (x) = 1, f y (x) &lt; ?] ? C 1 ? ? 01 ? ? 10 ?</formula><p>Similar to case 1 of Lemma 1, by using equation <ref type="formula">(3)</ref> for the case when y = 0, we can prove that Combining the two cases ( y = 1) and ( y = 0) completes the proof.</p><formula xml:id="formula_17">Pr y = h * (x) = 0, f y(x) &lt; ? ? Pr [ y = h * (x) = 0, 1 ? ?(x) ? &lt; ?] = Pr 1 ? ? 01 ? ? 1 ? ? 10 ? ? 01 ? 1 ? ? 10 ? ? 01 &lt; ?(x) &lt; 1 2 ? Pr 1 2 ? 1 ? ? 01 ? ? 10 &lt; ?(x) &lt; 1 2 ?C 1 ? ? 01 ? ? 10 ? (a) (b) (c)</formula><p>Remark 1. Indeed, we can also prove a bound for the opposite case: when f y is highly confident, y is correct with high probability. In this paper, we only focus on the bound in theorem 1 as we only want to identify incorrect labels and fix them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multiclass Setting</head><p>Theorem 1 can be generalized to a multiclass setting. Let y be the observed (possibly) corrupted label,</p><formula xml:id="formula_18">? i (x) = Pr(y = i | x) and ? i (x) = Pr( y = i | x). Recall f i (x)</formula><p>is the classifier's prediction on label i. Define N c to be the number of total classes and [N c ] = {1, 2, ? ? ? , N c }.</p><p>First we extend the Tsybakov condition to multiclass scenario <ref type="bibr" target="#b4">(Chen &amp; Sun, 2006)</ref>. Denote by u x the Bayes optimal classifier prediction, or say the class predicted by ?(x), formally u x := h * (x) = arg max i ? i (x). Denote by s x the second best prediction, s x := arg max i =ux ? i (x). The difference between their corresponding true conditional probability is a non-negative function, whose zero level set {x|? ux (x) ? ? sx (x) = 0} is the decision boundary of h * . We assume the Tsybakov condition around the margin of this decision boundary: ?C, ? &gt; 0 and ?t 0 ? (0, 1], such that for all t ? t 0 ,</p><formula xml:id="formula_19">Pr ? ux (x) ? ? sx (x) ? t ? Ct ?<label>(4)</label></formula><p>For any pair of labels i, j ? [N c ], we have the lin-</p><formula xml:id="formula_20">ear relationship ? i (x) = j?[Nc] ? ji ? j (x). Define m x := arg max i f i (x). Define the estimation error := max x,i |f i (x) ? ? i (x)|.</formula><p>Theorem 2. Assume ?(x) fulfills multi-class Tsybakov condition for constants C, ? &gt; 0 and t 0 ? (0, 1]. Assume that</p><formula xml:id="formula_21">? t 0 min i ? i,i . For ? = min 1, min x [? y, y ? sx (x) + j = y ? j, y ? j (x)] : Pr (x,y)?D y = h * (x), f y (x) &lt; ? ? C [O( )] ?</formula><p>The proof of Theorem 2 will be provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Empirical Validation of the Bound</head><p>To better understand the Tsybakov condition assumption and the bound in our theorem, we conduct the following experiment. On the CIFAR10 dataset, we train deep neural networks to approximate relevant functions. We use these functions to estimate the constants C and ? in the Tsybakov condition. Using these constants, we calculate the bound in Theorem 2 as a function of and check if it is tight.</p><p>To estimate C and ?, we approximate the true conditional probability ? using a deep neural network trained on the original clean-labeled CIFAR10 data. We densely sample t between 0 and 0.9. For each t, we empirically evaluate the left hand side (LHS) probability of Equation <ref type="formula" target="#formula_19">(4)</ref> and then use these values to estimate C and ? via regression.</p><p>In particular, for each t we calculate LHS of Equation <ref type="formula" target="#formula_19">(4)</ref> using the frequency p t = 1</p><formula xml:id="formula_22">n n i=1 1 {?m x (x)??s x (x)?t} (x)</formula><p>, in which n is the number of data. If the RHS bound is tight, we can use log p t to approximate log(Ct ? ). log(Ct ? ) = log C + ? log t. As shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, we plot all (log t, log(Ct ? )) pairs as blue dots and estimate C and ? via linear regression (red line). We observe that the samples are quite close to linear. Indeed, we could get ordinary least square (OLS) estimator of constant C and ? with high confidence (determinant coefficient R 2 = 0.99, p-value &lt; 1e ? 53). The estimated C and ? are 0.23 and 1.04 respectively.</p><p>Next, we verify our bound in Theorem 2. Using the estimated C and ?, we can calculate the bound (RHS of Equation (4)) as a function of (the constant in the big-O is  provided in the supplemental material). In <ref type="figure" target="#fig_0">Figure 1(b)</ref>, we plot the bound function in green curve. We compare this bound with the LHS of Equation <ref type="formula" target="#formula_19">(4)</ref> which we can empirically evaluate. In particular, we train a noisy classifier f by training a neural network on noisy labels (symmetric noise level 20%, see Section 4 for details). Using f , we can count the number of data points which has f y ? ? and meanwhile y is equal to h * (x) (calculated using ?: the clean-label-trained neural network). This gives us the LHS of Equation <ref type="formula" target="#formula_19">(4)</ref>, which is the probability of a label being correct when f has low confidence (blue line in <ref type="figure" target="#fig_0">Figure 1(b)</ref>). Similarly, we can calculate the probability of a label being correct when f has high confidence (orange line in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). We also carry out the same experiment on a different noise setting (asymmetric noise level 20%, see Sec. 4 for details).</p><p>Discussion. On CIFAR10 dataset, we estimated the constants of Tsybakov condition to be C = 0.23 and ? = 1.04 with high confidence. This means our bound (Equation <ref type="formula" target="#formula_19">(4)</ref> is almost linear. As observed in <ref type="figure" target="#fig_0">Figure 1(b)</ref> and (c), the bound is rather small (only up to 0.2 when the approximation error of the classifier, , is below 0.4). Furthermore, the empirically evaluated chance of y being correct when f has low confidence (blue lines <ref type="figure" target="#fig_0">Figure 1</ref>) is almost zero, well below the curve of the bound. In <ref type="figure" target="#fig_0">Figure 1(b)</ref> The fact that the blue and green line intersects at = 0.06 implies that can be as small as 0.06. Similarly, <ref type="figure" target="#fig_0">Figure 1</ref>(c) implies can be as small as 0.12. Finally, we note that the orange lines are well above the blue ones. This means when f has high confidence on y, there is a high chance y is correct. In other words, by comparing f y with a properly chosen constant ?, we can identify most data with corrupted labels.</p><p>We also conduct experiments on synthetic data (generated using multivariate normal distribution). In such case, we can calculate ? and ? exactly. The estimated C and ? are 0.6 and 1.3 respectively. More details about the synthetic experiments can be found in the supplemental material.</p><p>In conclusion, experiments on synthetic and on CIFAR10 datasets show that the constants in Tsybakov condition are rather small and the bound in our theorem is almost linear to . We also note the bound is generally small/tight even in deep learning setting. Thresholding f 's confidence does detect corrupted labels accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Algorithm: Likelihood Ratio Test for Label Correction</head><p>Our theoretical insight inspires a new algorithm for label correction. We propose to directly test the confidence level of the noisy classifier to determine whether a label is correct.</p><p>One additional requirement is that if we decide that a label is incorrect, we also need to decide what is the correct label. Therefore, instead of checking the confidence level, we check the likelihood ratio between f 's confidence on y and its confidence on its own label prediction, i.e., m x . Specifically, we check the likelihood ratio</p><formula xml:id="formula_23">LR(f, x, y) = f y (x)/f mx (x).</formula><p>We compare this likelihood ratio with a predetermined threshold ?. The value of ? is given in the next theorem. This is essentially a hypothesis testing on the null hypothesis H 0 : y = h * (x). If LR(f, x, y) &lt; ?, we reject the null hypothesis and flip the label y new = m x . Otherwise, the label remains unchanged, y new = y. If y = m x then the likelihood ratio is 1, y new = m x = y. Detailed algorithm is provided in Procedure 1. See <ref type="figure" target="#fig_2">Figure 2</ref> for an illustration of the algorithm in a binary classification case. We will show in the following theorem that the LRT correction algorithm is guaranteed to make proper correction and clean most of the corrupted labels. In particular, we show that in practice if we have a reasonable approximation ? to the theoretically optimal ?, the algorithm flips y to the correct label (the Bayes optimal prediction, h * (x)) with a good chance. Recall the approximation error of the classifier is :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure 1 LRT-Correction</head><formula xml:id="formula_24">= max x,i |f i (x) ? ? i (x)|.</formula><p>We consider two cases: (1) the label being flipped y new = m x ; and (2) the label remaining the same y new = y. Each case has its own ideal ?. We bound the probability of obtaining a correct label with and ?. Here ? is the difference between the chosen? and the ideal ?. We also introduce an additional term, ?, denoting the probability that the true label is neither y nor m x , formally, ? = Pr (x,y)?D [u x / ? {m x , y}]. Theorem 3. ?i, j ? [N c ], assume ?(x) fulfills multi-class Tsybakov condition for constants C &gt; 0, ? &gt; 0, t 0 ? (0, 1].</p><p>Case 1 (Label flipped by LRT-Corr((x, y),f (x),?)):</p><formula xml:id="formula_25">let ? 1 = min x 1 fm x (x) ? y, y ? sx (x) + j = y ? j, y ? j (x)</formula><p>and ? 1 := |? ? ? 1 |.</p><p>Assume ? 1 ? ? 1 and</p><formula xml:id="formula_26">? min t0? 2 1 mini ?ii?? 2 1 ??1 ? 2 1 , (t 0 ? ? 1 ) min i ? ii . Then: Pr (x,y)?D [ y new = h * (x), y is flipped ] is at least 1 ? C [O(max( , ?1))] ? ? ?.</formula><p>Case 2 (Label preserved by LRT- <ref type="figure">Corr((x, y)</ref>,f (x),?)):</p><formula xml:id="formula_27">let ? 2 = max x f y (x) ?m x ,mx ?s x (x)+ j =mx ?j,m x ?j (x) and ? 2 := |? ? ? 2 |. Assume ? 2 ? ? 2 and ? min t0? 2 2 mini ?ii?? 2 2 ??2 ? 2 2 , (t 0 ? ? 2 ) min i ? ii . Then: Pr (x,y)?D [ y new = h * (x), y isn't flipped] is at least 1 ? C [O(max( , ?))] ? ? ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Deep Nets with LRT-Correction</head><p>We incorporate the proposed label-correction into the training of deep neural networks. Similar to other data-re-calibrating methods, our training algorithm continuously trains a deep neural network while correcting the noisy labels. Procedure 2 is the pseudocode of the training method, called AdaCorr. It trains a neural network model iteratively. Each iteration includes both label correction and model training steps. In label correction step, the prediction of the current neural network, f , is used to run LRT test on all training data, and to correct their labels according to the test result. Since f is used to approximate the conditional probability ?, we use the softmax layer output of the neural network as f . After the labels of all training data are updated, we use them to train the neural network incrementally. We continue this iterative procedure until the training converges.</p><p>We also have a burn-in stage in which we train the network using the original noisy labels for m epochs. During the burn-in stage, we use the original cross-entropy loss, L CE . Afterwards, we add an additional retroactive loss, with the intention of stabilizing the network and avoiding overfitting.</p><p>After the burn-in stage, we want to avoid overfitting of the neural network, so that its output better approximates ?. To achieve this goal, we introduce a retroactive loss term L retro (f (x), y). The idea is to enforce the consistency between f and the prediction of the model at a previous epoch, f r . It has been observed that a neural network at earlier training stage tends to learn the true pattern rather than to overfit the noise <ref type="bibr" target="#b9">(Devansh et al., 2017)</ref>. Formally, the loss can be written as Train using L retro + L CE , with f r and y 14: end for</p><p>In the experiment we evaluate our method on 4 public datasets: CIFAR10, CIFAR100, MNIST and ModelNet40 (see Section 4 for more details). Based on previous observations <ref type="bibr" target="#b9">(Devansh et al., 2017)</ref>, on CIFAR10 and CIFAR100 datasets, a neural network takes about 30 epochs to fit the true pattern before overfitting the noise. We use this number as the burn-in stage length m. For easier datasets like MNIST and ModelNet40, we set m to be slightly smaller (25). As for ?, setting ? to be slightly smaller than 1 seems sufficient. Our Theorem 3 guarantees that the bound is affected almost linearly (as ? ? 1 per Section 2.4) to the error of the manually picked ? from the optimal one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we empirically evaluate our proposed method with several datasets, where noisy labels are injected according to specified noise transition matrices.</p><p>Datasets. We use the following datasets: <ref type="bibr">MNIST (LeCun et al. 1998</ref>), CIFAR10 <ref type="bibr" target="#b18">(Krizhevsky et al. 2009</ref>), CIFAR100 <ref type="bibr" target="#b18">(Krizhevsky et al. 2009</ref>) and ModelNet40 <ref type="bibr" target="#b40">(Wu et al. 2015)</ref>. MNIST consists of 28 ? 28 grayscale images with 10 categories. It contains 60,000 images, and we use 45,000 for training, 5,000 for validation and 10,000 for testing. CI-FAR10 and CIFAR100 consist of the same 60,000 images whose size is 32 ? 32 ? 3. CIFAR10 has 10 classes while CIFAR100 has 100 fine-grained classes. Similar to MNIST, we split 90% and 10% data from the official training set for the training and validation, respectively, and use the official test set for testing. ModelNet40 contains 12,311 CAD models from 40 categories, where 8,859 are used for training, 984 for validation and the remaining 2,468 for testing. We follow the protocol of <ref type="bibr" target="#b26">(Qi et al., 2017)</ref> to convert the CAD models into point clouds by uniformly sampling 1,024 points from the triangular mesh and normalizing them within a unit ball. In all experiments, we use early stopping on validation set to tune hyperparameters and report the performance on test set.</p><p>Baselines. We compare the proposed method with the following methods: (1) Standard, which trains the network in a standard manner, without any label resistance technique; (2) Forward Correction <ref type="bibr" target="#b25">(Patrini et al. 2017)</ref>, which explicitly estimates the noise transition matrix to correct the training loss; (3) Decoupling <ref type="bibr" target="#b23">(Malach &amp; Shalev-Shwartz 2017)</ref>, which trains two networks simultaneously and updates the parameters on selected data whose labels are possibly clean; (4) Coteaching <ref type="bibr" target="#b14">(Han et al. 2018)</ref>, which also trains two networks but exchanges their error information for network updating; (5) MentorNet <ref type="bibr" target="#b17">(Jiang et al. 2018)</ref>, which learns a curriculum to filter out noisy data; (6) Forgetting <ref type="bibr" target="#b9">(Devansh et al., 2017)</ref>, which uses dropout to help deep models resist label noise. (7) Abstention <ref type="bibr" target="#b33">(Thulasidasan et al. 2019)</ref>, which regularizes the network with abstention loss to ensure model robustness under label noise.</p><p>Experimental setup. For the classification of MNIST, CI-FAR10 and CIFAR100, we use preactive ResNet-34 <ref type="bibr" target="#b15">(He et al. 2016)</ref> as the backbone for all the methods. On Mod-elNet40, we use PointNet. We train the models for 180 epochs to ensure that all the methods have converged. We utilize RAdam <ref type="bibr" target="#b21">(Liu et al. 2019)</ref> for the network optimization, and adopt batch size 128 for all the datasets. We use an initial learning rate of 0.001, which is decayed by 0.5 very 60 epochs. We also update f r to f once at epoch m + 40 to reflect better predictive power of network after several epochs. The experimental results are listed in <ref type="table" target="#tab_1">Table 2</ref>. As is shown, our method overall achieves the best performance across the datasets under different noise settings.</p><p>Clothing 1M. We also evaluate our method on a large scale Clothing 1M dataset , which consists of 1M images with real-world noisy labels. We use pre-trained ResNet-50 and train the model using SGD for 20 epochs. Our method achieves accuracy 71.47%. It outperforms Standard (68.94%), Forward Correction (69.84%) and Backward Correction <ref type="bibr" target="#b25">(Patrini et al., 2017)</ref> (69.13%), where we take the number from the original paper directly. Note that other baselines (Forgetting, Decoupling, MentorNet, Coteaching and Abstention) did not report results on this dataset. Discussion. Our method outperform state-of-the-arts over a broad spectrum of noise patterns and levels. This is due to the relatively simple procedure our theoretically guaranteed algorithm. Looking closely, in <ref type="figure" target="#fig_5">Figure 3</ref>, we draw convergence curves on CIFAR10 with 0.4 uniform noise. On the left, we show the curves of our proposed AdaCorr method. The model continues to flip labels to correct ones. Meanwhile, it fits with the corrected labels y new and the test accuracy on clean labels does not drop. This shows that the model and the label correction are improving in a harmonic fashion and do not collapse. On the right, we show the curves of the Standard method. Without label correction, the model overfits with noisy labels and the performance on test data degrades catastrophically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We prove theoretical guarantees for data-re-calibrating methods for noisy labels. Based on the result, we propose a label correction algorithm to combat label noise. Our method can produce models robust to different noise patterns. Experiments on various datasets show that our method outperforms many recently proposed methods.  In this section, we provide additional experiments on a synthetic data set generated using a mixture-of-Gaussians distribution. In this ideal setting, we know ?, ? 01 , ? 10 , ? exactly. We can a) use ? as the classifier and b) evaluate the constants in Tsybakov condition for ? in order to evaluate the upper bound in Theorem 1.</p><p>Estimation of Tsybakov condition constants. We let Pr(x) be a mixture of Gaussian distribution in a 10 dimensional feature space, x ? 1 2 N (0, I 10?10 ) + 1 2 N (1, I 10?10 ). We sample from the two components with equal probability. If x comes from component N (0, I 10?10 ), it is given label 0. Otherwise, if x comes from component N (1, I 10?10 ), it is given</p><formula xml:id="formula_28">label 1. The true conditional distribution is ?(x) = exp{? 1 2 ||x?1|| 2 } exp{? 1 2 ||x|| 2 }+exp{? 1 2 ||x?1|| 2 } .</formula><p>Following the idea of our experiment on CIFAR10 in the manuscript (Section 2.4), we estimate Pr |?(x) ? 1 2 | ? t for values of t sampled between 0 and 0.9 using the empirical frequency p t = 1 n n i=1 1 {|?(x)?1/2|?t} (x). Note that if the Tsybakov condition is tight, log(p t ) approximates log(Ct ? ). The samples for log(t) and correspondingly, log(Ct ? ) ? log(p t ) are drawn as blue dots in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. The ordinary least square (OLS) linear regression results is drawn as a red line. We found the estimated values of C and ? to be 0.58 and 1.27 respectively. The estimation is high is confidence: the determinant coefficient R 2 equals 0.904, and we have a p-value which is less than 10 ?4 .</p><p>Estimation of the error bound, and its tightness. We also introduce label noise using predefined transition probability ? 01 and ? 10 . We can estimate C and ? as mentioned above, and know ? 01 , ? 10 , ?(x), and thus, ?(x). Therefore we can evaluate the error bound in Theorem 1. We plot the error bound as a function of in <ref type="figure" target="#fig_0">Figures 1(b)</ref> and (c) (drawn green curves).</p><p>Finally, we assume a perfect noisy classifier f = ?. In other words, = 0. We empirically show that when f (x) &lt; ?, the probability of y being correct (i.e., y = h * (x)) is zero (blue lines in <ref type="figure" target="#fig_0">Figures 1(b)</ref> and <ref type="formula">(c)</ref>).</p><p>Validation of the label-correction algorithm. To the same synthetic dataset, we also apply our LRT-Correction algorithm and validate the bound in Corollary 1. Since we know ?(x), ? 01 and ? 10 , we calculate the correction error bound of Corollary 1 in closed form. We draw the bound w.r.t. the error in orange curves in <ref type="figure" target="#fig_2">Figure 2</ref>. Finally, we run our label correction algorithm using the perfect noisy classifier f = ? and validate that the corrected labels are very close to clean (the success rate is limited by the asymmetry level of the noise pattern). See blue lines in <ref type="figure" target="#fig_2">Figure 2</ref>. For multi-class scenario, we know ?i ? [N c ], ? i (x) = j? <ref type="bibr">[Nc]</ref> ? ji ? j (x). We also restate the multi-class Tsybakov condition here:</p><p>Assumption 1 (Multi-class Tsybakov Condition). ?C, ? &gt; 0 and t 0 ? (0, 1] such that for all t ? t 0 , Then if we substitute ? in (1) with ? y, y ? sx (x) + j = y ? j, y ? j (x), continuing the derivation of (1), we will end up with:</p><formula xml:id="formula_29">Pr [|? ux (x) ? ? sx (x)| ? t] ?</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Synthetic experiment using CIFAR10 at noise level 20%. (a): Check of Tsybakov condition using linear regression. Where y-axis is the proportion of data points at distance t from decision boundary. (b): Proportion of labels that are not correct (not consistent with Bayes optimal decision rule) and the proposed upper bound. (c). Same as (b) but labels are corrupted with asymmetric noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Noisy labels and f .(b) Corrected labels and ?.(c) LR for y = 1.(d) LR for y = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of the label correction algorithm. ? is set to 1. (a): a corrupted sample and its corresponding classifier prediction f . (b): after correction, the labels are consistent with the true conditional probability, ?. (c): likelihood ratio for y = 1. Data with x &lt; 0 are corrected to ?new = 0 as LR(x) are below ? = 1. (d): likelihood ratio for y = 0. Data with x &gt; 0 are corrected to ?new = 1 as LR(x) are below ? = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Input: (x, y), f (x), ?. Output: y new 1: m x := arg max i f i (x) 2: LR(f, x, y) := f y (x)/f mx (x) 3: if LR(f, x, y) &lt; ? then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>log f c (x), in which N c is the number of possible label classes. The training loss is the sum of the retroactive loss and the cross-entropy loss:L(f (x), y, f r ) = L retro (f (x), f r (x)) + L CE (f (x), y) = Nc c=1 f r c (x) log f c (x) + Nc c=1y c log f c (x).Procedure 2 AdaCorrInput: S = {(x, y)}, ?, m, T 1: for epoch=1 to m do 2:Train neural network with L CE 3: end for 4: f r = current model prediction 5: for epoch=m + 1 to T do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Convergence curves for CIFAR10 with 40% uniform noise. Left: AdaCorr -training accuracy evaluated against the corrected label (ynew) (cyan), testing accuracy against clean label (orange), and the proportion of correct label (green). Right: Standard -training accuracy against noisy label ( y) and testing accuracy against clean label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .Figure 2 .Figure 3 .</head><label>123</label><figDesc>Synthetic experiment using Mixture of Gaussian at noise level 20%. (a): Check of Tsybakov condition using linear regression, where y-axis is the proportion of data points at distance t from decision boundary. (b): Proportion of labels that are not correct (not consistent with Bayes optimal decision rule) and the proposed upper bound. (c): Same as (b) but labels are corrupted with aysmmetric noise. (d): t-SNE of the clean data. (e): t-SNE of the data with symmetric noise. (f): t-SNE of the data with asymmetric noise. Performance of LRT algorithm given ?(x) v.s the proposed upper bound. (a): Symmetric noise (?10 = ?01 = 0.3). (b): Asymmetric noise (?10 = 0.2, ?01 = 0.3). (c): Asymmetric noise (?10 = 0.1, ?01 = 0.3). (d): Asymmetric noise (?10 = 0.3, ?01 = 0) Label Correction Result Using LRT-Correct. (a): Clean data as it in Fig 1d. (b): Labels after correction for data in Fig 1e. (c): Labels after correction for data in Fig 1f.2. Proof of Theorem 2 Define m x := arg max i f i (x), u x := arg max i ? i (x) and s x := arg max i =ux ? i (x). Let [N c] := {1, 2, ? ? ? , N c }. Finally, define i (x) := |f i (x) ? ? i (x)| and := max x,i i (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Ct ? Theorem 2 .</head><label>2</label><figDesc>Assume ?(x) fulfills multi-class Tsybakov condition for constant C, ? &gt; 0 and t 0 ? (0, 1]. Assume that? t 0 min i ? i,i . For ? = min 1, min x [? y, y ? sx (x) + j = y ? j, y ? j (x)] : Pr (x,y)?D y = h * (x), f y (x) &lt; ? ? C [O( )] ? Proof. Pr [ y = h * (x), f y (x) &lt; ?] = Pr [? y (x) ? ? sx (x), f y (x) &lt; ?] ? Pr [? y (x) ? ? sx (x), ? y (x) &lt; ? + y ] ? Pr [? y (x) ? ? sx (x),? y (x) &lt; ? + ] that ? = min 1, min x [? y, y ? sx (x) + j = y ? j, y ? j (x)] ? ? y, y ? sx (x) + j = y ? j, y ? j (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance on Clothing 1M Dataset</figDesc><table><row><cell>Method</cell><cell>Accuracy(%)</cell></row><row><cell cols="2">Standard Forward Backward AdaCorr 71.74 ? 0.12 68.94 69.84 69.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The classification accuracy of different methods. ? 0.5 51.4 ? 1.4 38.7 ? 0.8 17.4 ? 0.9 64.7 ? 0.2 57.4 ? 0.8 47.4 ? 1.7 Coteach 66.1 ? 0.5 60.0 ? 0.6 48.3 ? 0.1 16.1 ? 1.1 63.4 ? 0.9 57.6 ? 0.3 49.2 ? 0.3 Abstention 75.1? 5.4 60.0 ? 0.8 51.1? 0.8 10.3 ? 0.5 65.4 ? 0.5 56.8 ? 0.5 47.3 ? 0.3 AdaCorr 67.8 ? 0.1 60.2 ? 0.8 46.5 ? 1.2 24.6 ? 1.1 68.3 ? 0.2 61.1 ? 0.5 49.8 ? 0.7 Experiment for Validation of the Bound In Section 2.3 of the submitted manuscript, we used the output of deep neural networks f as an approximation of ? on the CIFAR10 dataset. We provided empirical estimates of the constants C and ? in the Tsybakov condition for ?, as well as estimates of the probability Pr[ y = h * (x), f y (x) &lt; ?].</figDesc><table><row><cell>Data Set</cell><cell>Method</cell><cell>0.2</cell><cell>Noise Level of Uniform Flipping 0.4 0.6 0.8</cell><cell>0.2</cell><cell>Noise Level of Pair Flipping 0.3 0.4</cell></row><row><cell cols="6">MNIST CIFAR10 CIFAR100 MentorNet 63.6 ModelNet40 Standard 99.0 ? 0.2 98.7 ? 0.4 98.1 ? 0.3 91.3 ? 0.9 99.3 ? 0.1 99.2 ? 0.1 98.8 ? 0.1 Forgetting 99.0 ? 0.1 98.8 ? 0.1 97.7 ? 0.2 62.6 ? 8.9 99.3 ? 0.1 96.5 ? 2.0 89.7 ? 1.9 Forward 99.1 ? 0.1 98.7 ? 0.2 98.0 ? 0.4 89.6 ? 4.8 99.4 ? 0.0 99.2 ? 0.2 96.5 ? 4.4 Decouple 99.3 ? 0.1 99.0 ? 0.1 98.5 ? 0.2 94.6 ? 0.2 99.4 ? 0.0 99.3 ? 0.1 99.1 ? 0.2 MentorNet 99.2 ? 0.2 98.7 ? 0.1 98.1 ? 0.1 87.5 ? 5.2 98.6 ? 0.4 99.1 ? 0.1 98.9 ? 0.1 Coteach 99.1 ? 0.2 98.7 ? 0.3 98.2 ? 0.3 95.7 ? 0.7 99.1 ? 0.1 99.0 ? 0.2 98.9 ? 0.2 Abstention 94.0 ? 0.3 76.8 ? 0.3 49.6 ? 0.1 21.2 ? 0.5 94.3 ? 0.3 88.5 ? 0.3 81.4 ? 0.2 AdaCorr 99.5 ? 0.0 99.4 ? 0.0 99.1 ? 0.0 97.7 ? 0.2 99.5 ? 0.0 99.6 ? 0.0 99.4 ? 0.0 Standard 87.5 ? 0.2 83.1 ? 0.4 76.4 ? 0.4 47.6 ? 2.0 88.8 ? 0.2 88.4 ? 0.3 84.5 ? 0.3 Forgetting 87.1 ?0.2 83.4 ? 0.2 76.5 ? 0.7 33.0 ? 1.6 89.6 ? 0.1 83.7 ? 0.1 86.4 ? 0.5 Forward 87.4 ? 0.8 83.1 ? 0.8 74.7 ? 1.7 38.3 ? 3.0 89.0 ? 0.5 87.4 ? 1.1 84.7 ? 0.5 Decouple 87.6 ? 0.4 84.2 ? 0.5 77.6 ? 0.1 48.5 ? 0.9 90.6 ? 0.3 89.1 ? 0.3 86.3 ? 0.5 MentorNet 90.3 ? 0.3 83.2 ? 0.5 75.5 ? 0.7 34.1 ? 2.5 90.4 ? 0.2 88.9 ? 0.1 83.3 ? 1.0 Coteach 90.1 ? 0.4 87.3 ? 0.5 80.9 ? 0.5 25.0 ? 3.6 91.8 ? 0.1 89.9 ? 0.2 80.1 ? 0.7 Abstention 85.3 ? 0.4 82.0 ? 0.7 68.8 ? 0.4 33.8 ? 7.7 88.5 ? 0.0 83.1 ? 0.5 77.4 ? 0.4 AdaCorr 91.0 ? 0.3 88.7 ? 0.5 81.2 ? 0.4 49.2 ? 2.4 92.2 ? 0.1 91.3 ? 0.3 89.2 ? 0.4 Standard 58.9 ? 0.8 52.1 ? 1.0 42.1 ? 0.7 20.8 ? 1.0 59.5 ? 0.4 52.9 ? 0.6 44.7 ? 1.3 Forgetting 59.3 ? 0.8 53.0 ? 0.2 40.9 ? 0.5 7.7 ? 1.1 61.4 ? 0.9 54.6 ? 0.6 37.7 ? 4.6 Forward 58.4 ? 0.5 52.2 ? 0.3 41.1 ? 0.5 20.6 ? 0.6 58.3 ? 0.7 53.2 ? 0.6 44.4 ? 2.8 Decouple 59.0 ? 0.7 52.2 ? 0.7 40.2 ? 0.4 18.5 ? 0.8 60.8 ? 0.7 56.1 ? 0.7 48.4 ? 1.0 Standard 79.1 ? 2.6 75.3 ? 3.3 70.0 ? 3.0 57.9 ? 2.3 84.4 ? 1.2 82.3 ? 1.3 78.9 ? 0.7 Forgetting 80.1 ? 1.8 73.9 ? 0.6 69.0 ? 0.7 26.2 ? 4.8 83.3 ? 1.1 62.0 ? 3.0 59.5 ? 2.9 Forward 52.3 ? 5.1 49.4 ? 6.8 43.5 ? 5.2 28.2 ? 5.5 48.1 ? 6.8 48.0 ? 3.7 49.1 ? 4.4 Decouple 82.5 ? 2.2 80.7 ? 0.7 72.9 ? 1.0 55.4 ? 2.7 85.7 ? 1.4 84.3 ? 1.0 80.5 ? 2.4 MentorNet 86.5 ? 0.5 75.4 ? 1.8 70.9 ? 1.9 52.7 ? 3.1 83.7 ? 1.8 81.0 ? 1.5 79.3 ? 2.1 Coteach 85.6 ? 0.9 84.2 ? 0.8 81.8 ? 1.1 68.9 ? 2.8 85.7 ? 0.8 79.1 ? 3.0 69.1 ? 2.4 Abstention 78.1 ? 0.6 65.6 ? 0.5 45.6 ? 1.5 23.5 ? 0.5 82.3 ? 0.5 80.4 ? 0.6 65.6 ? 0.5 AdaCorr 86.9 ? 0.3 85.1 ? 0.6 78.6 ? 1.4 72.1 ? 1.1 87.6 ? 0.4 84.6 ? 0.5 83.7 ? 0.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">arXiv:2011.10077v1 [cs.CV] 19 Nov 2020Error-Bounded Correction of Noisy Labels</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">in the second term of (9), by algebra we know</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Mayank Goswami is supported by National Science Foundation grants CRII-1755791 and CCF-1910873. The research of Songzhu Zheng and Chao Chen is partially supported by NSF IIS-1855759, CCF-1855760 and IIS-1909038. The research of Pengxiang Wu and Dimitris Metaxas is partially supported by NSF CCF-1733843. We thank anonymous referees for constructive comments and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. First look at cases where y is rejected.</p><p>For the first term in (2), we have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We substitute ? in (3) with</p><p>and continue the calculation:</p><p>In (4), the Tsybakov condition holds here because ? t 0 min i ? ii , which implies ? y, y ? t 0 .</p><p>For the second term in (2), we have:</p><p>for which our algorithm currently doesn't have a good way to deal with and we will leave it as future research problem.</p><p>Finally, summarize every piece and we finished the proof for cases where y is rejected:</p><p>For cases where y is accepted:</p><p>For the first term in (6), we have:</p><p>Firstly, observe that if ? &gt; 1, then Pr</p><p>Then notice that ? = max</p><p>. If we substitute ? in <ref type="formula">(7)</ref> with</p><p>?j,m x ?j (x) and continuing the calculation, we will have:</p><p>For the second term in <ref type="formula">(6)</ref>, our algorithm cannot deal with it properly. We will leave it as the future research problem.</p><p>Now we summarize all pieces and we get:</p><p>which compete the proof for cases that are accepted.</p><p>We give following several facts based on our theorem:</p><p>1. For binary case, if we set ? = 1?|?10??01| 1+|?10??01| and further assume ? t 0 (1 ? ? 10 ? ? 01 ) ? |?10??01| 2 , we have:</p><p>Proof. For binary case, we have:</p><p>Observe that ? = 1?|?10??01| 1+|?10??01| ? 1?? y,1? y +? 1? y, y 1+? y,1? y ?? 1? y, y . We also have ? 1+? = 1?|?10??01| 2 ? 1 2 . Now we substitute ? = 1?? y,1? y +? 1? y, y 1+? y,1? y ?? 1? y, y in the first term of (9) and substitute ? 1+? with 1 that :</p><p>2. For symmetric noise ? ij = ? ji = ?, ?i, j ? [N c ] and further assume (besides the assumption we made in Lemma 1) then :</p><p>(?m x ,mx ?? y,mx )?s x (x)+? y,mx then :</p><p>To show this:</p><p>. As a result, second term in (2) and second term in (6) will be 0. </p><p>Proof. The proof will be similar to the proof of Lemma 1, but we need to adjust the error introduced by picking?. Recall that ? and are both less than one.</p><p>If we pick? instead of ?, then for (3) in Lemma 1, we have:</p><p>The same upper bound holds for (5) with the same reason. Then:</p><p>We next analyze (7) in Lemma 1:  which compete the proof for cases that are accepted.</p><p>Other terms will not be affected by the choice of ?. By now we completes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abhinav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6575" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overfitting or perfect fitting? risk bounds for classification and regression rules that interpolate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2300" to="2311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rates of convergence for nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3437" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Consistency of multiclass empirical risk minimization methods based on convex loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2435" to="2447" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with bounded instance-and label-dependent label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning via gaussian herding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive regularization of weight vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="414" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kimin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mantas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devansh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Stanislaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Asja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">On the resistance of nearest neighbor to random noisy labels. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1607</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Certified defenses for data poisoning attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3520" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mentornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/?kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=726791" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random classification noise defeats all convex potential boosters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Servedio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="304" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ambuj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Error-Bounded Correction of Noisy Labels</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rates of convergence for large-scale nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10768" to="10779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1412</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning with bad training data via iterative trimmed loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5739" to="5748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inferring ground truth from subjective labelling of venus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6234" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimal aggregation of classifiers in statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="166" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing the robustness of nearest neighbors to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5133" to="5142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A light CNN for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="327" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
