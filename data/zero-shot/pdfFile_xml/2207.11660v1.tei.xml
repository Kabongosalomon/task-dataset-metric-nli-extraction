<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAR: Masked Autoencoders for Efficient Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20211">AUGUST 2021 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">MAR: Masked Autoencoders for Efficient Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20211">AUGUST 2021 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Efficient Action Recognition</term>
					<term>Masked Autoen- coders</term>
					<term>Vision Transformer</term>
					<term>Spatio-temporal Redundancy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Standard approaches for video action recognition usually operate on the full input videos, which is inefficient due to the widely present spatio-temporal redundancy in videos. Recent progress in masked video modelling, i.e., VideoMAE, has shown the ability of vanilla Vision Transformers (ViT) to complement spatio-temporal contexts given only limited visual contents. Inspired by this, we propose propose Masked Action Recognition (MAR), which reduces the redundant computation by discarding a proportion of patches and operating only on a part of the videos. MAR contains the following two indispensable components: cell running masking and bridging classifier. Specifically, to enable the ViT to perceive the details beyond the visible patches easily, cell running masking is presented to preserve the spatio-temporal correlations in videos, i.e., it ensures the patches at the same spatial location can be observed in turn for easy reconstructions. Additionally, we notice that, although the partially observed features can reconstruct semantically explicit invisible patches, they fail to achieve accurate classification. To address this, a bridging classifier is proposed to bridge the semantic gap between the ViT encoded features for reconstruction and the features specialized for classification. Our proposed MAR reduces the computational cost of ViT by 53% and extensive experiments show that MAR consistently outperforms existing ViT models with a notable margin. Especially, we found a ViT-Large trained by MAR outperforms the ViT-Huge trained by a standard training scheme by convincing margins on both Kinetics-400 and Something-Something v2 datasets, while our computation overhead of ViT-Large is only 14.5% of ViT-Huge. Codes and models will be made available here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, deep neural networks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref> have achieved impressive performance for the action recognition task on several large-scale video datasets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These methods <ref type="bibr" target="#b77">78</ref>  ViT-H ViT-L ViT-B <ref type="figure">Fig. 1</ref>: Accuracy vs. per-view GFLOPs on Kinetics-400 <ref type="bibr" target="#b6">[7]</ref> and Something-Something v2 <ref type="bibr" target="#b7">[8]</ref>. The model architecture is ViT <ref type="bibr" target="#b18">[19]</ref>, pre-trained by self-supervised VideoMAE <ref type="bibr" target="#b8">[9]</ref> and MAE <ref type="bibr" target="#b17">[18]</ref>. "Standard Training" means using all tokens with a linear classifier for training, which requires more computational resources. The proposed MAR costs only 47% of the computation to achieve better performance with the same pretrained encoder.</p><p>usually depend on full video frames to understand the visual content. Although this yields decent performances, the computation over full videos is highly redundant due to the excessive and widely present spatio-temporal redundancy of visual information <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref> in videos. In light of this, a branch of previous works has proposed to reduce the spatiotemporal redundancy by training an additional model to focus on relevant frames <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref> or spatio-temporal regions <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, which can significantly reduce the computation cost. However, they mostly require complicated operations, such as reinforcement learning and multi-stage training.</p><p>In self-supervised video representation learning, Masked Autoencoders (MAE) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> discard a high proportion of vision patches to yield a non-trivial and meaningful selfsupervised reconstruction task. The simple masking strategy with vanilla Vision Transformers (ViT) can achieve realistic reconstruction results <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which implies that the masked visual information in videos can be complemented from limited visible contents, and the ViT show this capability. This is possible because that the spatio-temporal redundancy in videos empowers the model to derive the visual semantics of the invisible parts from the visible contexts. After pre-training, models are fine-tuned on the downstream action recognition task following a standard scheme, which feeds all the details of videos into ViT.</p><p>In this work, we argue that given the strong ability of arXiv:2207.11660v1 [cs.CV] 24 Jul 2022</p><p>ViT to reconstruct visual semantics with only limited visual content and the high spatio-temporal redundancy in videos, the standard action recognition scheme that operates full video frames is highly inefficient. To this end, we propose a simple and computationally efficient end-to-end scheme for the action recognition task, termed as Masked Action Recognition (MAR). The core idea of MAR is to discard a subset of video patches to reduce the encoded tokens of ViT, such that the redundant computation can be avoided to some extent. We investigate this from two perspectives: (i) designing an appropriate input masking map with strong spatio-temporal correlations for ViTs; (ii) increasing the abstraction level for the features output by ViTs. For the first perspective, we aim to determine a reasonable mask form for action recognition. The existing VideoMAE methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> adopt a large proportion (i.e., 90%) of tube masking or random masking, which are mainly designed to avoid the "information leakage" caused by spatio-temporal redundancy and correlation in videos to improve the difficulty of the self-supervised reconstruction task. On the contrary, detailed contexts are crucial for action recognition. Therefore, training an accurate action recognizer with masks requires the model to easily complement the disappeared details. To this end, we present cell running masking, which encourages "spatio-temporal information leakage" to empower the detail perception of encoders for the lost patches. Specifically, we first design running masking strategy to drive the masks to move frame by frame so that the patches in the same spatial location can be observed in turn. To generate more flexible masking maps for training, we divide the running masking into multiple small running cells and place a specified proportion of masks in these cells. Executing the running strategy in the small cells can present multiple states to provide various spatio-temporally interleaved masks. Overall, the cell running masking preserves spatio-temporal correlations <ref type="bibr" target="#b8">[9]</ref>, which can be easily exploited by the encoder to perceive missing details for accurate action recognition. <ref type="figure" target="#fig_4">Figure 6</ref> shows that the semantically explicit reconstructed videos can be observed with this masking strategy.</p><p>Despite this, we also observe that the models with realistic reconstructed videos still fail to achieve on-par accuracy compared to using the full patches. MAE in the image domain <ref type="bibr" target="#b19">[20]</ref> identified a semantic gap between the lowlevel features required for the reconstruction task and the abstract features required for the recognition task. Meanwhile, the literature <ref type="bibr" target="#b20">[21]</ref> also found that the models pre-trained by MAE extract features at a lower abstract level. To solve this, we further propose bridging classifier with a similar structure to the reconstruction decoder in MAE. Nevertheless, compared to the reconstruction decoder, the bridging classifier is designed to further bridge the semantic gaps and make the encoded features more specialized for the classification task. In contrast, the reconstruction decoder is used to decode the low-level pixel-wise information.</p><p>In this way, compared to the standard action recognition scheme, MAR is shown to reduce the computation by up to 53% while achieving on par or better performances, as in <ref type="figure">Figure 1</ref>. In particular, when training a large model on Kinetics-400 <ref type="bibr" target="#b6">[7]</ref>, our ViT-Large costs only 14.5% of FLOPs and surpasses the performance of the ViT-Huge by 0.2%.</p><p>In a nutshell, our contributions can be summarized as:</p><p>? We explore the masked autoencoders for efficient action recognition, achieving better performance and a 2x wallclock speedup in training and testing. ? We present a novel MAR framework, consisting of cell running masking and bridging classifier to better exploit the spatio-temporal correlations and increase the abstract level of encoded features, respectively. ? Extensive experiments on different datasets show that MAR requires only 47% of the computation to consistently outperform standard action recognition with the same pre-trained models. Especially under comparable computational costs, models trained by MAR significantly outperform the existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Recognition</head><p>Action recognition is a fundamental task for video analysis and understanding. Recent video networks can be divided into two types, i.e., convolution-based and transformer-based.</p><p>The dominant convolution-based methods build upon 3D Convolution Neural Networks (CNNs) <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Inspired by the two-stream CNNs, separately modelling spatial appearance and temporal relations by 3D convolutions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> have also shown decent accuracy. However, they suffer from a large computation burden. To reduce complexity, some approaches attempt to disentangle 3D convolution into spatial and temporal convolution <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. Other works employ 2D convolutions for spatial modelling <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> and introduce additional temporal operations, such as calibrating the convolution weights by temporal context <ref type="bibr" target="#b31">[32]</ref>, generating adaptive temporal kernels <ref type="bibr" target="#b32">[33]</ref>, capturing temporal difference for motion modelling <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, and shifting part of the channels along the temporal dimension <ref type="bibr" target="#b35">[36]</ref>, etc. Limited by the small receptive field of the convolution operator, these convolution-based methods struggle to model long-range spatial-temporal dependency. In recent years, the great success of transformers in the image domain <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> has led to the exploration of transformer-based video networks. VTN <ref type="bibr" target="#b38">[39]</ref> adopts ViT <ref type="bibr" target="#b18">[19]</ref> to extract spatial features, followed by a Longformer <ref type="bibr" target="#b39">[40]</ref> to capture temporal relationships. Both TimeSformer <ref type="bibr" target="#b40">[41]</ref> and ViViT <ref type="bibr" target="#b5">[6]</ref> factorise different spatial-and temporal-attentions for transformer encoders. They suggest that factorised spatial and temporal attention can achieve better performance. MViT <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b41">[42]</ref> studies hierarchical transformers with several channel-resolution scale stages, and proposes pooling attention to reduce computation. In comparison, Video Swin Transformer <ref type="bibr" target="#b3">[4]</ref> introduces an inductive bias of locality for videos and achieves a better speed-accuracy trade-off. And Uniformer <ref type="bibr" target="#b42">[43]</ref> captures local spatio-temporal context and global token dependency by convolution in shallow layers and transformer in deep layers, respectively. All these studies are based on variants of transformer structure. In this work, the ViTs <ref type="bibr" target="#b18">[19]</ref> pre-trained by VideoMAE <ref type="bibr" target="#b8">[9]</ref> are adopted as our encoder. We investigate the input and output of transformer  <ref type="figure">figure)</ref>. Finally, the reconstruction decoder receives mask tokens and visible tokens to reconstruct the masked patches. In contrast, the bridging classifier in the classification branch receives only visible tokens from performing the action classification task. Note that the reconstruction branch is only performed in training, which is used to preserve the completion capability of the encoders for invisible patches.</p><p>encoders and empirically show that the proposed MAR owns advantages in both efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatio-temporal Redundancy</head><p>Reducing spatio-temporal redundancy for efficient video analysis has recently been a popular research topic. The mainstream approaches mostly train an additional lightweight network to achieve: (i) adaptive frame selection <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b43">[44]</ref>, i.e., dynamically determining the relevant frames for the recognition networks; (ii) adaptive frame resolution <ref type="bibr" target="#b11">[12]</ref>, i.e., learning an optimal resolution for each frame online; (iii) early stopping <ref type="bibr" target="#b44">[45]</ref>, i.e., terminating the inference process before observing all frames; (iv) adaptive spatio-temporal regions <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, i.e., localizing the most task-relevant spatiotemporal regions; (v) adaptive network architectures <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b45">[46]</ref>, i.e., adjusting the network architecture to save computation on less informative features. Another line is to manually define low redundant sampling rules, such as MGSampler <ref type="bibr" target="#b46">[47]</ref>, which selects frames containing rich motion information by the cumulative motion distribution. Nevertheless, in this work, the ViT trained by MAR adopts only a proportion of video patches for efficient action recognition, which takes advantage of the powerful completion capabilities of transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Masked Autoencoders</head><p>Masked autoencoder, as a form of denoising autoencoding <ref type="bibr" target="#b47">[48]</ref>, is a general methodology to learn effective representations by reconstructing the uncorrupted inputs from the corrupted inputs. In Natural Language Processing (NLP), the masked language modelling task proposed in BERT <ref type="bibr" target="#b48">[49]</ref> is one of the most successful explorations of masked autoencoding. Various variants <ref type="bibr" target="#b49">[50]</ref>- <ref type="bibr" target="#b52">[53]</ref> based on BERT also further improve the performance of language transformer pretraining. Recently, in the image domain, a series of masked autoencoding methods seek a framework for vision and language unification based on Transformer architectures <ref type="bibr" target="#b53">[54]</ref>, and continued progress has been made. iGPT <ref type="bibr" target="#b54">[55]</ref> first proposes to train a transformer to predict pixels from a sequence of low-resolution pixels for unsupervised representation learning. Then ViT <ref type="bibr" target="#b18">[19]</ref> takes image patches as tokens and performs masked patch prediction to mimick the masked language modelling in BERT <ref type="bibr" target="#b48">[49]</ref>. SimMIM <ref type="bibr" target="#b55">[56]</ref> suggests that a large masked patch size for pixel predictions makes a strong pre-text task. Image MAE <ref type="bibr" target="#b19">[20]</ref> investigates an asymmetric encoderdecoder structure, i.e., the heavy encoder only operates a small proportion (25%) of visible patches, while a lightweight decoder is used to reconstruct pixels. Apart from pixel prediction, another research line also proposes to reconstruct other targets, such as pre-trained dVAE <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> of BEiT <ref type="bibr" target="#b58">[59]</ref>, and HoG <ref type="bibr" target="#b59">[60]</ref> of MaskFeat <ref type="bibr" target="#b60">[61]</ref>. In the video domain, two MAEbased methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> find that an extremely high proportion of masks yields decent performance due to the large spatiotemporal redundancy in videos. Instead of predicting pixels, BEVT <ref type="bibr" target="#b61">[62]</ref> and VIMPAC <ref type="bibr" target="#b62">[63]</ref> also attempt to learn spatiotemporal representations by predicting features derived from a tokenizer. All the masked autoencoder based works discussed above focus on learning an effective self-supervised visual representation. We are inspired by the powerful completion ability of ViT in reconstruction tasks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> and propose to adopt this idea to empower supervised action recognition models at both the efficiency and performance levels.</p><p>III. APPROACH As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, instead of full video frames, the proposed MAR takes the masked videos as input. Specifically, the encoder of MAR only operates over the visible patches, which allows the encoder to process videos with only a fraction of computation and memory cost, e.g., 50% in <ref type="figure" target="#fig_0">Figure 2</ref>. The encoded visible tokens are then fed into two branches: <ref type="figure">Fig. 3</ref>: Different schemes for running masking: (a) randomly removing spatial patches in the first frame; (b) removing a large spatial block in the first frame; (c) uniformly distributing the masks in the first frame; (d) performing running masking in cells;</p><formula xml:id="formula_0">(d) Cell Running Masking (a) Random Running Masking (b) Block Running Masking (c) Uniform Running Masking (e) Running Cell t t s(? ?|? ?) A B C D flatten s(? ?|? ?) s(? ?|? ?) ? t s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) t s (? ?|? ?) s (? ?|? ?) s (? ?|? ?) t</formula><p>(e) the masks run circularly frame by frame in a running cell.  the reconstruction branch for pixel-level reconstruction, and the classification branch for efficient action recognition. Note that the auxiliary reconstruction branch is performed only in training and removed in inference. Besides the general framework, we also present two key designs in MAR: (i) To facilitate the perception of the MAR encoder over invisible patches, we propose a cell running masking strategy (in Sec. III-B) to generate a masking map that ensures strong spatio-temporal correlations; (ii) To bridge the semantic gaps between the encoded visible tokens and the features that can be better used for action classification task, we introduce a bridging classifier (in Sec. III-C) for the classification branch to increase the abstraction level of tokens. In this section, we first briefly revisit masked video modelling and then present the idea of cell running masking and bridging classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Masked Video Modelling</head><p>The existing masked video modelling approaches with encouraging performances <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref> are extended from image MAE. They first divide an input video into several nonoverlapped spatio-temporal patches, and then only a small proportion of patches (i.e., 10%) with positional embeddings are randomly selected as input for the ViT encoders <ref type="bibr" target="#b18">[19]</ref>. A lightweight decoder is then adopted to reconstruct the full video from the encoded latent representations of visible patches. VideoMAE <ref type="bibr" target="#b8">[9]</ref> identifies two crucial characteristics of video data, i.e., temporal redundancy and temporal correlation. The former means that the semantics vary slowly in the temporal dimension, and the spatio-temporal information is highly redundant, indicating that retaining all spatio-temporal patches for training and inference is inefficient and unnecessary. While the latter emphasizes the strong inherent correla-tion between adjacent frames, which could cause "information leakage" between frames, thus reducing the reconstruction difficulty of VideoMAE. Hence, a large proportion (i.e., 90%) of mask ratio and tube masking are proposed for the video reconstruction task. And the pre-trained models can still achieve satisfactory reconstruction videos with limited visible contents, which implies the powerful spatio-temporal association ability of ViTs. However, in the downstream action recognition task, all patches are still fed into the encoder, especially for a video clip with 1568 tokens, which still causes a heavy computational burden. In this work, we draw inspiration from the powerful capability of complementing invisible contexts shown by MAE pre-trained ViTs and propose MAR to achieve efficient action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cell Running Masking</head><p>Running masking. When performing the action recognition task with masks, a key issue is that the valuable details in masked patches are removed together, which is bound to damage accuracy. To this end, instead of avoiding "information leakage" between the adjacent frames like VideoMAE <ref type="bibr" target="#b8">[9]</ref>, we encourage "information leakage" caused by temporal correlation to lower the difficulty of the reconstruction task since easier reconstruction means that richer details can be redrawn to improve recognition performance.</p><p>Specifically, we first propose running masking strategy, where the masks run frame by frame, and the patches at different spatial locations are discarded in turn. It can be formulated as:</p><formula xml:id="formula_1">M t = s(M t?1 |M t?2 , . . . M 1 ),<label>(1)</label></formula><p>where M t is the masking map for frame t. Function s(?|?) indices that the masking map of frame t is transformed from frame t ? 1 circularly. <ref type="figure">Figure 3</ref>(e) also shows the masks in a small running cell between adjacent frames. With this idea, running masking ensures that the same spatial locations in consecutive frames can quickly observe the visible patches from adjacent frames, which exploits spatio-temporal visual redundancy to mitigate information loss. However, it is still a challenge to get an efficient and effective implementation of running masking. As shown in <ref type="figure">Figure 3</ref>(a), one possible option is to generate random masks on the first frame and then use the function s(?|?) to transform these masks frame by frame. This straightforward idea suffers from the randomness of masks, and patches at maskdense spatial locations may not be displayed, which still can not effectively exploit video visual correlations. Similar defects appear in the block-wise running masking illustrated in <ref type="figure">Figure 3</ref>(b), where the dense masks destroy chunks of spatio-temporal context. <ref type="figure">Figure 3</ref>(c) shows a different uniform running masking. It arranges uniform grid masks on the first frame to form spatial dislocations that enable the model to infer the spatial semantics for the masked patches with the image spatial redundancy. Then these grid masks are shifted in subsequent frames to form temporal intersections, which allows the model to recall the detailed information of the masked parts by temporal correlations in the video. Hence, the uniform running masking can already efficiently exploit for the spatio-temporal redundancy. However, it is still worth noting that adopting the rigid "uniform masking" can easily lead to overfitting in training. Since the diversity of uniform masking maps is limited. Running cell. For this reason, we propose to decompose the running masking into multiple small repeated units, and one unit is termed a running cell. The combination of these simple units can also implement complex masks, called cell running masking, as shown in <ref type="figure">Figure 3</ref>(d, e) and <ref type="figure" target="#fig_2">Figure 4</ref>(a, b). Suppose we define the spatial size of a running cell as r ? q (e.g., r = q = 2 in <ref type="figure">Figure 3</ref>(e)). When the spatial size is small, there are only a few patches in the running cell, and the arrangement of masks is clear. For example, in <ref type="figure">Figure 3</ref>(e), only two masks need to be placed, when the mask ratio is set to 50%, We simply put the two masks in the first two patches and define this as state A. Driven by the function s(?|?), there are four different states, i.e., A ? B ? C ? D, in this cell. And the cell starts with state A and performs state switching with a period of 4 in temporal dimension to realize a spatiotemporal uniform masking map. This design ensures that each patch has an equal chance of being visible in four temporal patches, thus providing enough temporal correspondences for other masked patches with the same spatial locations.</p><p>We also observe that a running cell with a spatial size of 2 ? 2 can only provide three different mask ratios, i.e., 25%, 50%, and 75%, which already meets most of the needs. Using running cells with a large spatial size can produce more mask ratios, but the diversity of mask combinations is weakened since there are fewer cells. For example, when the size of the running cell is equal to the spatial size of the ViT encoded tokens, i.e., 14?14, it degenerates to uniform running masking. Augmentations of cell running masking. <ref type="figure">Figure 3</ref>(e) shows that there are four different states in turn, i.e., A ? B ? C ? D, in the illustrated running cell. Clearly, the fixed sequence and positions may cause overfitting in training, similar to the uniform running masking in <ref type="figure">Figure 3</ref>(c). However, with the design of the running cell, we can put multiple different cells in the space, and these cells are free to choose the starting states. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>(a), each running cell selects the starting state randomly, and their combination is close to the random masking, but the regulation of running masking is followed within each small cell. <ref type="figure">Figure 3(d)</ref> and <ref type="figure" target="#fig_2">Figure 4</ref>(b) repeat running cells spatially, but their starting states are selected randomly as A and B, respectively, resulting in visually different masking maps. Both the spatially random and spatially repeated masking in <ref type="figure" target="#fig_2">Figure 4</ref> can be used as a data augmentation in training. In inference, we simply use the spatially repeated running masking for evaluation. The experiments in Table III (b) demonstrate that the model is robust to the different starting states of running cells in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bridging Classifier</head><p>The design of cell running masking preserves the spatiotemporal correlations between visible patches and invisible patches, effectively reducing the difficulty of reconstruction and further improving the reconstruction quality. For example, when the mask ratio is set to 50%, the reconstruction branch can already achieve sufficiently satisfactory reconstructed videos, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>. However, when using a fully connected layer as a classifier in <ref type="table" target="#tab_6">Table V</ref>, the recognition accuracy with 50% of the masked patches still struggles to reach the performance without masking. Our bridging classifier sets out to close this performance gap.</p><p>MAE <ref type="bibr" target="#b19">[20]</ref> mentioned that the pixel-level reconstruction and the recognition tasks require latent representations at different abstract levels. Specifically, representations with higher-level semantics can lead to better recognition accuracy but are not specialized for reconstruction. In this work, the satisfactory reconstructions prove that the low-level semantic information in features is sufficient. Hence, we speculate that the weak classification accuracy is actually because the linear classifier cannot fully exploit low-level semantic information.</p><p>To this end, we propose bridging classifier, consisting of a series of transformer blocks like the reconstruction decoder, to bridge the semantic gaps between the encoded features and the classification features:</p><formula xml:id="formula_2">p = ?(h(F v )),<label>(2)</label></formula><p>where F v ? R Nv?D , is the encoded visible tokens output by the encoder. ?(?) and h(?) indicate average pooling operation and proposed bridging classifier. N v is the number of visible tokens, and D is the channel. p ? R C is the model prediction for C classes. The reconstruction decoder needs to infer the encoded F v to pixels, while the bridging classifier only extracts the more concise classification features in F v . Therefore, the bridging classifier is designed to be more lightweight.</p><p>More importantly, the bridging classifier only processes visible tokens, which only costs 8% FLOPs vs. linear classifier with 50% of visible tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>MAR contains the reconstruction and classification branches, and their loss functions are denoted as L r and L c , respectively. The reconstruction loss L r is pixel-wise meansquared loss, following <ref type="bibr" target="#b8">[9]</ref>. And L c is a widely used cross entropy loss in classification tasks. The training objective function can be written as follows: </p><formula xml:id="formula_3">L r = 1 ?(x M ) ||x M ? y M || 2<label>(3)</label></formula><formula xml:id="formula_4">L c = ? C i=1 z i log(p i )<label>(4)</label></formula><formula xml:id="formula_5">L = ?L r + L c ,<label>(5)</label></formula><p>where x, y are the input RGB pixel values and the predicted pixel values, respectively; M denotes the masked pixels; ?(?) calculates the number of masked pixels; ||?|| 2 refers to 2 loss; z is a one-hot label vector for classification; ? is the balance parameter for the reconstruction branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>Datasets. We evaluate MAR on four widely-used action recognition datasets, i.e., Kinetics-400 <ref type="bibr" target="#b6">[7]</ref>, Something-Something v2 <ref type="bibr" target="#b7">[8]</ref>, HMDB51 <ref type="bibr" target="#b73">[74]</ref>, and UCF101 <ref type="bibr" target="#b74">[75]</ref>. Kinetics-400 is a large-scale benchmark with around 240k training videos and 20k validation videos from 400 different action categories. Something-Something v2 is a temporal-related video dataset with 174 action classes, which contains 169k videos for training and 20k videos for validation. HMDB51 and UCF101 are two small datasets for action recognition, which only have 3.5k/1.5k train/val videos and 9.5k/3.5k train/val videos, respectively. Architecture. Following VideoMAE <ref type="bibr" target="#b8">[9]</ref> and MAE <ref type="bibr" target="#b17">[18]</ref>, we use ViT <ref type="bibr" target="#b18">[19]</ref> with the joint spatial-temporal attention. The attention mechanism with quadratic complexity can lead to computational bottlenecks, while MAR can effectively alleviate this problem. We set the same spatio-temporal patch size (i.e., 2 ? 16 ? 16) as VideoMAE for both ViT-Base and ViT-Large to utilize its pre-trained models conveniently. The spatio-temporal resolution of input videos is 16?224?224 for both training and inference, and the embedding tokens output by encoders are 8 ? 14 ? 14 = 1568. When the mask ratio is set to 50%, the encoders only operate 784 tokens. Data pre-processing and training settings. Our MAR training configurations follow the training settings in Video-MAE <ref type="bibr" target="#b8">[9]</ref>. We use the same training augmentations as Video-MAE, as shown in <ref type="table" target="#tab_1">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>In this section, we present ablation studies for the in-depth analysis of our proposed complements (i.e., cell running masking and bridging classifier) in MAR. Something-Something v2 <ref type="bibr" target="#b7">[8]</ref> is employed as our evaluation benchmark. The default encoder is ViT-Base <ref type="bibr" target="#b18">[19]</ref> with 16 frames, and other default settings are marked in gray in tables. The notation ? in tables means the mask ratio. If not specific, the mask ratio is set to 50% by default. Different mask sampling strategies. We replace the cell running masking in MAR with other mask sampling methodologies under different mask ratios ? <ref type="table" target="#tab_1">Table II</ref>. Several observations can be summarized from the table: (i) Compared with "random standard masking", our "cell running masking" can provide stable accuracy improvements. Especially, more gains can be yielded by cell running masking with larger the mask ratios, e.g., the gains are 0.18%, 0.62% and 0.67% for mask ratios of 25%, 50% and 75%, respectively. This can be explained by the less destroyed contexts with the small mask ratio, and cell running masking can better show its advantages with a large proportion of masks; (ii) "block standard masking" and "frame standard masking" remove chunks of spatio-temporal information and destroy the natural spatio-temporal correlations in videos, significantly impairing performance. Nevertheless, improvements are still observed from "block standard masking" to "block running masking", which demonstrates the effectiveness of our running masking strategy; (iii) Downsampling is also a straightforward way to reduce the computational costs. We downsample the width and height of the input videos to half of the standard training settings, i.e., 16 ? 112 2 , and the computation is comparable to that when we set the mask ratio to 75%. It can be observed that downsampling notably degenerates the performance by    Reconstruction Loss Sth-SthV2 Top1 Accuracy (%) <ref type="figure">Fig. 5</ref>: Accuracy vs. reconstruction loss on Something-Something v2. The mask ratio is 50%, except that the standard training scheme uses all patches as input (i.e., no mask). There is a clear negative correlation between reconstruction loss and accuracy.</p><p>2.23% from cell running masking, which indicates that the low resolution drops more detailed information than masks; (iv) Compared with "random standard masking", "uniform running masking" without running cells has limited improvement, while our 'cell running masking' performs much stronger. The results validate that our cell running masking can benefit from the diversity brought by various cell states in training. The spatial size of running cell. Next, we show that our defined running cell plays a critical role in running masking. As in <ref type="table" target="#tab_1">Table IV</ref>, the running masking with a large cell size, e.g., 14 ? 14 or 7 ? 7, shows weaker performance than a small cell size, e.g., 2 ? 2. This is caused by the unitary form of masks. Specifically, to prevent the masks in the large cell from degenerating into a block-wise mask, the first frame should be uniformly initialized. For example, the cell size of uniform running masking in <ref type="figure">Figure 3(c)</ref> is 14?14. Hence, as discussed in Sec. III-B, large cell sizes with uniform masks suffer from limited diversities and lead to overfitting in training. Our small cell size (2 ? 2) is more flexible and achieves the highest performance. Reconstruction loss and accuracy. Here, we show the correlation between reconstruction loss and accuracy in <ref type="figure">Figure 5</ref>. We can draw two intriguing findings. First, the accuracy is inversely related to reconstruction loss. For example, cell running masking can achieve the highest accuracy with the lowest reconstruction loss, while other mask sampling strategies with larger reconstruction loss maybe yield weaker performances. This can be interpreted as a small reconstruction loss that can recover rich details for invisible patches and thus boost the classification accuracy. Second, the block masking and the random masking with our running strategy can reduce the reconstruction error and improve the accuracy by ? 0.3%, demonstrating that the encoders can exploit more spatiotemporal correlations from proposed running strategy.</p><p>Training augmentations of cell running masking. One of the advantages of the running cell with a small spatial size is its flexibility, i.e., a simple combination of multiple cells can achieve a complex mask. We further ablate different spatial and temporal augmentations in training for cell running masking in <ref type="table" target="#tab_1">Table III</ref>(a). "Spatially Random" means that each cell chooses starting state randomly, as shown in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. We observe that the random combinations of the cells converge to random masking, and therefore show poor performance. "Spatially Repeated" indicates that the cells share the same randomly selected starting state, as shown in <ref type="figure">Figure 3</ref>(c) and <ref type="figure" target="#fig_2">Figure 4(b)</ref>, demonstrating a better performance. The temporal shuffling slightly increases the randomness and thus the best performance. This implies that the training masks with reasonable randomness can introduce about 0.4% improvements, and the simple control of running cells in the spatial and temporal space achieves this regulation. Different starting states in testing. There are four different running states in our proposed running cell with a spatial size of 2 ? 2. Different starting states will show different masking maps when evaluating the trained models. It is also worth evaluating the impact of different starting states on performance. As shown in <ref type="table" target="#tab_1">Table III(b)</ref>, the different starting states present comparable performance, and their differences are negligible. This proves that the models trained by MAR are robust to different states of cell running masking. Cross validations of different mask ratios. <ref type="table" target="#tab_1">Table III(c)</ref> presents the accuracies with different mask ratios for training and testing. We can draw the following observations: (i) When testing the models with a large mask ratio, e.g., 75%, the models also should be trained with the akin mask ratios. Otherwise, the model cannot learn to complement the lost contexts; (ii) appropriately increasing the mask ratio in training can also improve accuracy. For example, training with 25% masks and testing with no mask has an advantage over the training with no mask by 0.34%, i.e., 71.36% vs. 71.02%; (iii) large mask ratios can remarkably save computational costs. Our default setting with only 50% masks can save more than 50% of computation while preserving comparable performance to that when training and testing with no mask. Thus our proposed MAR saves both training and testing overhead.</p><p>The effect of bridging classifier. The comparisons of the proposed bridging classifier and linear classifier under different mask ratios are ablated in <ref type="table" target="#tab_6">Table V</ref>. It can be observed that larger mask ratios can lead to more performance degradation. While simply replacing the linear classifier with a bridging classifier brings notable improvements. Especially for large mask ratios, the bridging classifier has an advantage over the linear baseline of around 1.0%. This suggests that the mask exacerbates the nonlinearity of the features and confirms our above statement that the abstraction level of encoded features still needs to be further bridged for classification. The width and depth design of bridging classifier. The lightweight bridging classifier consists of multiple vanilla transformer <ref type="bibr" target="#b53">[54]</ref> blocks. <ref type="table" target="#tab_1">Table VI</ref> shows the impact of different decoder designs on the performance. First, Multi-Layer Perceptron (MLP) is widely known to be a simple design with nonlinear modelling capability. However, its performance  is slightly weaker than that of linear classifiers. This indicates that the simple structure of MLP can not squeeze out abstract information from the encoded features. In contrast, a notable improvement of 0.56% is observed with only an extremely lightweight decoder, i.e., width=256 and depth=1, which shows the superior nonlinear expressiveness of the transformer block. Enlarging the width and depth can further boost the performance, with width=512 and depth=2 having the strongest performance. Wider or deeper decoders not only lead to overfitting but also tend to introduce a more unnecessary computational burden. Finally, since the decoder only operates visible tokens, the setting with the highest performance adds only 6.51 GFLOPs (around 8%) compared to the linear classifier. The input of bridging classifier. MAE-based approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> skip the masked tokens in the encoder and apply them with positional embeddings in the lightweight reconstruction decoder. We also evaluate their designs in our proposed bridging classifier in <ref type="table" target="#tab_1">Table VII</ref>. It can be summarized that both the additional positional embeddings and the masked tokens damage the accuracy. This is probably because both factors are low-level priors and thus more specialized for the reconstruction task. In contrast, the high-level semantics required by the classification task is not strongly dependent on these two factors. Reconstruction target. MAE in the image domain <ref type="bibr" target="#b19">[20]</ref> and video domain <ref type="bibr" target="#b17">[18]</ref> both demonstrate that reconstructing perpatch normalized pixels works well for self-supervised pretraining. As shown in <ref type="table" target="#tab_1">Table VIII</ref>, we are interested in whether this finding still holds in MAR. Compared with reconstructing the original video pixels, using normalized pixels <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> always performs better, which is in line with the pre-training settings. The initialization of reconstruction decoder. We utilize VideoMAE <ref type="bibr" target="#b8">[9]</ref> pre-trained parameters to initialize the ViT encoder and the reconstruction decoder by default. The effect of the initialization state of the reconstruction decoder is explored in <ref type="table" target="#tab_1">Table VIII</ref>. It can be observed that the randomly initialized decoder performs decently, while the pre-trained decoder still improves by 0.15%. We speculate that the pretrained decoder has already converged, which can regularize the encoder directly.   The effect of reconstruction loss weight, i.e., ?. The reconstruction branch is only involved in the computation in training, and its main purpose is to further enhance the encoder's ability to perceive the missing context by reconstructing the invisible patches. In <ref type="table" target="#tab_1">Table IX</ref>(a), we analyze the parameter sensitivity of ?. When ? is set to 0.0, no reconstruction branch shows the weakest performance. We observe that a slight increase in ?, i.e., 0.1, brings improvement. Larger ?, i.e., 1.0 with the stronger reconstruction constraint, may lead the encoder to preserve more low-level cues in features, also resulting in a 0.19% performance degradation.</p><p>Pre-training Dataset. In Table IX(b), we compare randomly initialized encoder and self-supervised MAE pre-trained models on three different datasets, i.e., ImageneNet-1K <ref type="bibr" target="#b75">[76]</ref>, Kinetics-400 <ref type="bibr" target="#b6">[7]</ref> and Something-Something v2 <ref type="bibr" target="#b7">[8]</ref>. When using the model pre-trained on ImageneNet-1K, the 2D patch  embedding layer is inflated to the 3D embedding layer following <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>. We see that the encoders pre-trained on the video datasets outperform the training from scratch as well as the image-based pre-trained ones. Further, the model pretrained on Something-Something v2 shows better accuracy than the Kinetics-400 pre-trained one, suggesting that the domain gap between target and pre-training datasets found by VideoMAE <ref type="bibr" target="#b8">[9]</ref> still exists in our MAR. Data augmentations. Although the self-supervised MAE pretrainings <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> require only multi-scale cropping for training, data augmentation is still one of the crucial factors for transformer models. If not specified, our used data augmentations follow the setting in VideoMAE <ref type="bibr" target="#b8">[9]</ref>. In <ref type="table">Table 7</ref>, we disable four different data augmentations, i.e., RandAugment <ref type="bibr" target="#b65">[66]</ref>, Random Erasing <ref type="bibr" target="#b76">[77]</ref>, MixUp <ref type="bibr" target="#b66">[67]</ref> and CutMix <ref type="bibr" target="#b67">[68]</ref> to observe the sensitivity of MAR to data augmentation. We observe that each data augmentation can bring a performance gain of 0.2-0.3%. In fact, using masks to delete a proportion of the spatio-temporal patches can also be considered one of the data augmentations. However, since the masked patches can be easily reconstructed, other data augmentation strategies are still needed.</p><p>Wall-clock training time. <ref type="figure" target="#fig_6">Figure 7</ref> compares the training time  Visualizations. We qualitatively visualize the reconstructed videos by reconstruction branch in <ref type="figure" target="#fig_4">Figure 6</ref>. We observe that even if the cell running masking discards 50% of the spatio-temporal patches, the reconstructed videos can still fully express the high-level semantics of the videos. This suggests that it is not necessary for the encoder to operate all spatiotemporal patches. <ref type="table" target="#tab_1">Table XI and Table XII</ref> compare our training scheme with other state-of-the-art methods on Kinetics-400 <ref type="bibr" target="#b6">[7]</ref> and Something-Something v2 <ref type="bibr" target="#b7">[8]</ref>. The relevant settings are listed in detail for comparison, including network architectures and calculation costs. We can draw the following observations Moreover, it is also worth noting that our trained ViT-Large models even exceed ViT-Huge by 0.2% on Kinetics-400 and Something-Something v2. Especially, our ViT-Large only costs 14% GFLOPs compared to ViT-Huge with standard training. Third, the models trained by MAR achieve superior performance on both datasets compared to previous approaches under the similar GFLOPs, even though they use the supervised pre-training on larger datasets. In addition, comparisons on two small video datasets, i.e., UCF101 <ref type="bibr" target="#b74">[75]</ref> and HMDB51 <ref type="bibr" target="#b73">[74]</ref> are also reported in <ref type="table" target="#tab_1">Table XIII</ref>. It can be observed that our MAR still leads to on par or better performance under multiple settings on these two small datasets. We speculate that the small datasets with limited videos can be easily reconstructed by models, thus leading to overfitting, especially for UCF101 datasets with simple backgrounds. Therefore, our MAR is superior on large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with the Previous Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we propose Masked Action Recognition (MAR), a simple and computationally friendly training scheme for vanilla Vision Transformers (ViTs) in videos. MAR is investigated from two perspectives of ViTs, i.e., reducing the number of input patches and bridging the semantic gaps of output features. For the former, the cell running masking strategy is designed to generate spatio-temporal interleaved masks, which preserves the spatio-temporal correlations in videos. For the latter, the lightweight bridging classifier is proposed to bridge the semantic gaps between encoded features and specialized classification features. Empirical results show that MAR costs only 47% of the computation and exceeds the performance of the standard training scheme. In addition, strong generalizations of MAR have also been demonstrated on several video datasets with different scales. Overall, this work exploits the powerful context modelling capability of ViTs, and significantly improves the training and testing efficiency with better performance. In future works, to further save computational costs with a less performance penalty, semantic-based mask ratios and masking maps are worthwhile prospects to be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of Masked Action Recognition (MAR). A specified proportion (e.g., 50% here) of patches is first discarded by cell running masking (C.R.M.), which retains sufficient spatio-temporal correlations. Next, the remaining visible patches are fed into the encoder to extract their spatio-temporal features (i.e., visible tokens in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Different combinations of running cells in space: (a) each running cell randomly selects a starting state; (b) all running cells share the same random starting state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>III: (a) Masking augmentations. Spatial and temporal augmentations of cell running masking in training. "Random" and "Repeated" are two cell combinations shown in Figure 4. "Fixed" means the fixed running state order. "Shuffled" indicates the shuffled order. (b) Starting states in testing. Running cells with varying starting states for model inference. (c) Cross validations of different mask ratios. Here only Top-1 accuracy is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Reconstructed videos of our reconstruction branch on Something-Something v2 [8] validation videos. The mask ratio is 50%. For each video, we show the original video (top), masked video (middle) and reconstructed video (bottom). The model reconstructs the original pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>The training time (32 V100 GPUs) vs. validation error rate on Something-Something v2. "C.R.M.(50%)" means the cell running masking with a mask ratio of 50%. "Linear" and "Bridging" refer to the linear classifier and bridging classifier, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>MAR training settings.</figDesc><table><row><cell>Config</cell><cell cols="2">Sth-Sth v2 Kinetics-400</cell></row><row><cell>Optimizer</cell><cell></cell><cell>AdamW [64]</cell></row><row><cell>Momentum</cell><cell cols="2">?1 = 0.9, ?2 = 0.999</cell></row><row><cell>Weight Decay</cell><cell></cell><cell>0.5</cell></row><row><cell>Base LR  ?</cell><cell></cell><cell>1e-3</cell></row><row><cell>Batch Size</cell><cell cols="2">512(B),128(L)</cell></row><row><cell>LR Schedule</cell><cell cols="2">cosine decay [65]</cell></row><row><cell>Layer-wise Decay</cell><cell></cell><cell>0.75</cell></row><row><cell>Filp Augmentation</cell><cell></cell><cell>yes</cell></row><row><cell>RandAugment [66]</cell><cell></cell><cell>(9, 0.5)</cell></row><row><cell>Mixup [67]</cell><cell></cell><cell>0.8</cell></row><row><cell>Cutmix [68]</cell><cell></cell><cell>1.0</cell></row><row><cell>Label Smoothing [69]</cell><cell></cell><cell>0.1</cell></row><row><cell>Droppath [70]</cell><cell cols="2">0.1(B), 0.2(L)</cell></row><row><cell>Dropout [71]</cell><cell></cell><cell>0.1</cell></row><row><cell>Warmup Epochs [72]</cell><cell></cell><cell>5</cell></row><row><cell>Training Epochs</cell><cell>40</cell><cell>100(B),60(L)</cell></row><row><cell>Repeated Sampling [73]</cell><cell>1</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>? is linear learning rate scaling rule [72]: ActualLR = BaseLR ? BatchSize/256.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Mask sampling. "?" is the mask ratio. "D.S. 2x" means that the spatial resolution is downsampled from 224 2 to 112 2 .</figDesc><table><row><cell>?</cell><cell cols="4">Masking Strategy Top-1 Top-5 GFLOPs</cell></row><row><cell>25%</cell><cell cols="4">Random Standard 71.12 93.26 138.04 Cell Running 71.30 93.15 138.04</cell></row><row><cell></cell><cell cols="3">Block Standard 64.73 89.37</cell><cell>86.35</cell></row><row><cell></cell><cell cols="3">Block Running 65.00 89.74</cell><cell>86.35</cell></row><row><cell></cell><cell cols="3">Random Standard 70.35 92.63</cell><cell>86.35</cell></row><row><cell>50%</cell><cell cols="3">Random Running 70.46 92.74</cell><cell>86.35</cell></row><row><cell></cell><cell cols="3">Frame Standard 66.25 89.85</cell><cell>86.35</cell></row><row><cell></cell><cell>Tube</cell><cell cols="2">Standard 70.25 92.73</cell><cell>86.35</cell></row><row><cell></cell><cell cols="3">Uniform Running 70.55 92.84</cell><cell>86.35</cell></row><row><cell></cell><cell>Cell</cell><cell cols="2">Running 70.97 92.75</cell><cell>86.35</cell></row><row><cell cols="2">0% D.S. 2x</cell><cell>-</cell><cell>67.23 90.85</cell><cell>39.56</cell></row><row><cell>75%</cell><cell cols="3">Random Standard 68.79 91.55 Cell Running 69.46 91.88</cell><cell>40.95 40.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>The spatial size of running cell. "r ? q" is the spatial size described in Sec. III-B.</figDesc><table><row><cell cols="7">r ? q 14 ? 14 7 ? 7 7 ? 2 2 ? 7 2 ? 2</cell></row><row><cell>Top-1</cell><cell></cell><cell>70.55</cell><cell cols="3">70.54 70.65 70.84</cell><cell>70.97</cell></row><row><cell>72.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Standard Training (Accuracy=70.44%)</cell></row><row><cell>70.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cell</cell><cell cols="3">Running Masking (Accuracy=70.97%)</cell><cell></cell><cell></cell></row><row><cell>68.0</cell><cell cols="4">Uniform Running Masking (Accuracy=70.55%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Random Running Masking (Accuracy=70.46%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Block Running Masking (Accuracy=65.00%)</cell><cell></cell><cell></cell></row><row><cell>66.0</cell><cell cols="4">Random Standard Masking (Accuracy=70.35%) Tube Standard Masking (Accuracy=70.25%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Frame Standard Masking (Accuracy=66.25%)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Block Standard Masking (Accuracy=64.73%)</cell><cell></cell><cell></cell></row><row><cell>64.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Bridging classifier with different mask ratios. "?" is the mask ratio and ? is an absolute improvement of Top1 accuracy. "GPU Mem." indicates the usage of GPU memory in training when the batch size is set to 1.</figDesc><table><row><cell>?</cell><cell cols="2">Classifier Top-1 Top-5</cell><cell>?</cell><cell>GPU Mem.</cell></row><row><cell>0%</cell><cell>Linear</cell><cell>70.44 92.67</cell><cell>-</cell><cell>14.15 G</cell></row><row><cell>0%</cell><cell>Bridging</cell><cell>71.02 93.18</cell><cell>+0.58</cell><cell>15.08 G</cell></row><row><cell>25%</cell><cell>Linear</cell><cell>70.25 92.50</cell><cell>-</cell><cell>10.54 G</cell></row><row><cell cols="2">25% Bridging</cell><cell>71.30 93.15</cell><cell>+1.05</cell><cell>11.21 G</cell></row><row><cell>50%</cell><cell>Linear</cell><cell>69.94 92.48</cell><cell>-</cell><cell>7.31 G</cell></row><row><cell cols="2">50% Bridging</cell><cell>70.97 92.75</cell><cell>+1.03</cell><cell>7.63 G</cell></row><row><cell>75%</cell><cell>Linear</cell><cell>68.43 91.47</cell><cell>-</cell><cell>5.01 G</cell></row><row><cell cols="2">75% Bridging</cell><cell>69.46 91.88</cell><cell>+1.03</cell><cell>5.21 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>The design of bridging classifier. "MLP" means Multi-Layer Perceptron.</figDesc><table><row><cell cols="5">Width Classifier Depth Top-1 Top-5 GFLOPs</cell></row><row><cell>-</cell><cell>Linear</cell><cell>-</cell><cell>69.94 92.48</cell><cell>79.84</cell></row><row><cell>512</cell><cell>MLP</cell><cell>2</cell><cell>69.90 92.30</cell><cell>80.15</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>70.50 92.62</cell><cell>80.93</cell></row><row><cell>256</cell><cell>Bridging</cell><cell>2 4</cell><cell>70.61 92.76 70.74 92.75</cell><cell>81.86 83.73</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell>70.78 92.86</cell><cell>87.46</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>70.67 92.73</cell><cell>81.93</cell></row><row><cell>384</cell><cell>Bridging</cell><cell>2</cell><cell>70.89 92.89</cell><cell>83.80</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>70.76 92.73</cell><cell>87.52</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>70.67 92.78</cell><cell>83.25</cell></row><row><cell>512</cell><cell>Bridging</cell><cell>2</cell><cell>70.97 92.75</cell><cell>86.35</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>70.78 92.91</cell><cell>92.55</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>70.72 92.72</cell><cell>84.87</cell></row><row><cell>640</cell><cell>Bridging</cell><cell>2</cell><cell>70.25 92.65</cell><cell>89.52</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>69.05 91.94</cell><cell>98.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>The input of bridging classifier. "Pos.Emb." is positional embeddings.</figDesc><table><row><cell cols="2">Pos.Emb. Masked Tokens Top-1 Top-5 GFLOPs</cell></row><row><cell>70.66 92.78</cell><cell>86.35</cell></row><row><cell>70.44 92.79</cell><cell>95.06</cell></row><row><cell>70.97 92.75</cell><cell>86.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>Reconstruction target and the initialization of reconstruction decoder (Re.Decoder).</figDesc><table><row><cell>Target</cell><cell cols="2">Initialization of Re.Decoder Top-1 Top-5</cell></row><row><cell>Pixels w/o norm</cell><cell>Random</cell><cell>70.69 92.61</cell></row><row><cell cols="3">Pixels w/o norm VideoMAE Pre-trained [9] 70.71 93.04</cell></row><row><cell>Pixels w/ norm</cell><cell>Random</cell><cell>70.82 92.83</cell></row><row><cell cols="3">Pixels w/ norm VideoMAE Pre-trained [9] 70.97 92.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>(a) Reconstruction loss weight. "?" is the balance parameter in Equation 5. Note that the reconstruction branch does not work when ? = 0. (b) Pre-training Datasets.</figDesc><table><row><cell>? Top-1 Top-5</cell><cell>Dataset</cell><cell>Top-1 Top-5</cell></row><row><cell>0.0 70.72 92.79</cell><cell>None</cell><cell>44.37 73.18</cell></row><row><cell>0.1 70.97 92.75</cell><cell cols="2">IN-1K [76] 64.29 88.34</cell></row><row><cell>0.2 70.82 92.80</cell><cell cols="2">K400 [7] 70.49 92.66</cell></row><row><cell>0.4 70.82 92.93</cell><cell cols="2">SSv2 [8] 70.97 92.75</cell></row><row><cell>1.0 70.78 92.72</cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X :</head><label>X</label><figDesc>Data augmentations. "RandAug." and "RandEra." are RandAugment and random erasing, respectively.</figDesc><table><row><cell>RandAug. [66]</cell><cell>RandEra. [77]</cell><cell>MixUp [67]</cell><cell>CutMix [68]</cell><cell>Top-1 Top-5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.61 92.60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.78 92.92</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.63 92.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.70 92.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.97 92.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI :</head><label>XI</label><figDesc>System-level comparisons on Kinetics-400 action classification. "?" is the mask ratio. "FLOPs?Cr.?Cl." refers to "FLOPs?Crops?Clips". To avoid confusion, we mark the MAE in the video domain as "MAE-v".</figDesc><table><row><cell>Method</cell><cell>Pre-training Dataset</cell><cell>Supervised Pre-training</cell><cell>Architecture</cell><cell>Input Size</cell><cell>FLOPs?Cr.?Cl. (G)</cell><cell>Param (M)</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell cols="2">NonLocal I3D [78] TAdaConvNeXt-T [32] ImageNet-1K ImageNet-1K Motionformer [79] ImageNet-21K Video Swin [4] ImageNet-1K TimeSformer [41] ImageNet-21K ViViT FE [6] ImageNet-21K Video Swin [4] ImageNet-21K ip-CSN [24] -SlowFast [1] -</cell><cell></cell><cell cols="4">ResNet101 ConvNeXt-T ViT-L Swin-B ViT-L ViT-L Swin-L ResNet152 R101+NL (16 + 64) ? 224 2 359 ? 3 ? 10 128 ? 224 2 234 ? 3 ? 10 32 ? 224 2 94 ? 3 ? 4 32 ? 224 2 1185 ? 3 ? 10 382 62 38 32 ? 224 2 282 ? 3 ? 4 88 96 ? 224 2 8353 ? 3 ? 1 430 128 ? 224 2 3980 ? 3 ? 1 N/A 32 ? 224 2 604 ? 3 ? 4 197 32 ? 224 2 109 ? 3 ? 10 33 60</cell><cell>77.3 79.1 80.2 80.6 80.7 81.7 83.1 77.8 79.8</cell><cell>93.3 93.7 94.8 94.6 94.7 93.8 95.9 92.8 93.9</cell></row><row><cell>BEVT [62] MViT [5] MViT [5] MaskFeat [61] MaskFeat [61]</cell><cell>IN-1K+DALLE --Kinetics-400 Kinetics-600</cell><cell></cell><cell>Swin-B MViT-B MViT-B MViT-L MViT-L</cell><cell>32 ? 224 2 32 ? 224 2 64 ? 224 2 16 ? 224 2 16 ? 224 2</cell><cell>282 ? 3 ? 5 170 ? 1 ? 5 455 ? 1 ? 5 377 ? 1 ? 10 377 ? 1 ? 10</cell><cell>88 37 37 218 218</cell><cell>80.6 80.2 81.2 84.3 85.1</cell><cell>N/A 94.4 95.1 96.3 96.6</cell></row><row><cell>VideoMAE [9] MAE-v [18]</cell><cell>Kinetics-400 Kinetics-400</cell><cell></cell><cell>ViT-B ViT-B</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>180 ? 3 ? 5 180 ? 3 ? 7</cell><cell>87 87</cell><cell>80.7 81.3</cell><cell>94.7 94.9</cell></row><row><cell>MAR ?=75% MAR ?=50%</cell><cell>Kinetics-400 Kinetics-400</cell><cell></cell><cell>ViT-B ViT-B</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>41 ? 3 ? 5 86 ? 3 ? 5</cell><cell>94 94</cell><cell>79.4 81.0</cell><cell>93.7 94.4</cell></row><row><cell>VideoMAE [9] MAE-v [18]</cell><cell>Kinetics-400 Kinetics-400</cell><cell></cell><cell>ViT-L ViT-L</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>597 ? 3 ? 5 598 ? 3 ? 7</cell><cell>305 304</cell><cell>83.9 84.8</cell><cell>96.3 96.2</cell></row><row><cell>MAR ?=75% MAR ?=50%</cell><cell>Kinetics-400 Kinetics-400</cell><cell></cell><cell>ViT-L ViT-L</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>131 ? 3 ? 5 276 ? 3 ? 5</cell><cell>311 311</cell><cell>83.9 85.3</cell><cell>96.0 96.3</cell></row><row><cell>MAE-v [18]</cell><cell>Kinetics-400</cell><cell></cell><cell>ViT-H</cell><cell>16 ? 224 2</cell><cell>1193 ? 3 ? 7</cell><cell>632</cell><cell>85.1</cell><cell>96.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XII :</head><label>XII</label><figDesc>System-level comparisons on Something-Something v2 (SSv2). "?" is the mask ratio. "FLOPs?Cr.?Cl." refers to "FLOPs?Crops?Clips". To avoid confusion, we mark the MAE in the video domain as "MAE-v". It can be observed that MAR can reduce half of the training cost. Specifically, the standard training takes 11.8 hours, while our MAR only needs 5.9 hours and achieves better accuracy than the standard scheme. Although the introduction of the bridging classifier increases the training time slightly, it is still acceptable compared to the standard training scheme.</figDesc><table><row><cell>Method</cell><cell>Pre-training Dataset</cell><cell>Supervised Pre-training</cell><cell>Architecture</cell><cell>Input Size</cell><cell>FLOPs?Cr.?Cl. (G)</cell><cell>Param (M)</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>TimeSformer [41] SlowFast [1]</cell><cell>ImageNet-21K Kinetics-400</cell><cell></cell><cell cols="3">ViT-L ResNet101 (8 + 32) ? 224 2 106 ? 3 ? 1 64 ? 224 2 5549 ? 3 ? 1</cell><cell>430 53</cell><cell>62.4 63.1</cell><cell>N/A 87.6</cell></row><row><cell>TAdaConvNeXt-T [32] Motionformer [79] MViT [5] Video Swin [4] BEVT [62] MaskFeat [61]</cell><cell>ImageNet-1K IN-21K+K400 Kinetics-600 IN-21K+K400 IN-1K+K400+DALLE Kinetics-400</cell><cell></cell><cell>ConvNeXt-T ViT-L MViT-B-24 Swin-B Swin-B MViT-L</cell><cell>32 ? 224 2 32 ? 224 2 32 ? 224 2 32 ? 224 2 32 ? 224 2 40 ? 312 2</cell><cell>94 ? 3 ? 2 1185 ? 3 ? 1 236 ? 3 ? 1 321 ? 3 ? 1 321 ? 3 ? 1 2828 ? 3 ? 1</cell><cell>38 382 53 88 88 218</cell><cell>67.1 68.1 68.7 69.6 70.6 74.4</cell><cell>90.4 91.2 91.5 92.7 N/A 94.6</cell></row><row><cell>VideoMAE [9]</cell><cell>SSv2</cell><cell></cell><cell>ViT-B</cell><cell>16 ? 224 2</cell><cell>180 ? 3 ? 2</cell><cell>87</cell><cell>70.3</cell><cell>92.7</cell></row><row><cell>MAR ?=75% MAR ?=50%</cell><cell>SSv2 SSv2</cell><cell></cell><cell>ViT-B ViT-B</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>41 ? 3 ? 2 86 ? 3 ? 2</cell><cell>94 94</cell><cell>69.5 71.0</cell><cell>91.9 92.8</cell></row><row><cell>VIMPAC [63] MAE-v [18] VideoMAE [9]</cell><cell>HowTo100M+DALLE Kinetics-400 SSv2</cell><cell></cell><cell>ViT-L ViT-L ViT-L</cell><cell>10 ? 224 2 16 ? 224 2 16 ? 224 2</cell><cell>N/A ? 3 ? 10 598 ? 3 ? 1 597 ? 3 ? 2</cell><cell>307 304 305</cell><cell>68.1 72.1 74.2</cell><cell>N/A 93.9 94.7</cell></row><row><cell>MAR ?=75% MAR ?=50%</cell><cell>Kinetics-400 Kinetics-400</cell><cell></cell><cell>ViT-L ViT-L</cell><cell>16 ? 224 2 16 ? 224 2</cell><cell>131 ? 3 ? 2 276 ? 3 ? 2</cell><cell>311 311</cell><cell>73.8 74.7</cell><cell>94.4 94.9</cell></row><row><cell>MAE-v [18]</cell><cell>Kinetics-400</cell><cell></cell><cell>ViT-H</cell><cell>16 ? 224 2</cell><cell>1193 ? 3 ? 1</cell><cell>632</cell><cell>74.1</cell><cell>94.5</cell></row><row><cell cols="4">consumed by the different training methods on Something-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Something v2 dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIII :</head><label>XIII</label><figDesc>Comparisons to other state-of-the-art methods on UCF101 and HMDB51. from the table. First, with the same encoders, i.e., pre-trained by VideoMAE, MAR can consistently outperform the standard training scheme by at least 0.3% with only around 47% of GFLOPs. Second, MAR can easily scale up to large models and achieve more improvements. The ViT-Large trained by MAR on Kinetics-400 dataset surpasses standard training by 1.4%. With a large mask ratio (i.e., 75%), our ViT-Large has an advantage over the standard trained ViT-Base of 2.6% (81.3% vs. 83.9%) on Kinetics-400, while our ViT-Large saves 27% of computation overhead (180 GFLOPs vs. 131 GFLOPs).</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone Extra Data UCF101 HMDB51</cell></row><row><cell>CoCLR [80]</cell><cell>S3D-G</cell><cell>UCF101</cell><cell>81.4</cell><cell>52.1</cell></row><row><cell>Vi 2 CLR [81]</cell><cell>S3D</cell><cell>UCF101</cell><cell>82.8</cell><cell>52.9</cell></row><row><cell>MoSI [82]</cell><cell>R(2+1)D</cell><cell>-</cell><cell>82.8</cell><cell>51.8</cell></row><row><cell>VideoMAE [9]</cell><cell>ViT-B</cell><cell>-</cell><cell>90.8</cell><cell>61.1</cell></row><row><cell>MAR ?=50%</cell><cell>ViT-B</cell><cell>-</cell><cell>91.0</cell><cell>61.4</cell></row><row><cell>MemDPC [83]</cell><cell>R-2D3D</cell><cell>K400</cell><cell>86.1</cell><cell>54.5</cell></row><row><cell>CoCLR [80]</cell><cell>S3D-G</cell><cell>K400</cell><cell>87.9</cell><cell>54.6</cell></row><row><cell>Vi 2 CLR [81]</cell><cell>S3D</cell><cell>K400</cell><cell>89.1</cell><cell>55.7</cell></row><row><cell>ParamCrop [84]</cell><cell>S3D-G</cell><cell>K400</cell><cell>91.3</cell><cell>63.4</cell></row><row><cell>RSPNet [85]</cell><cell>S3D-G</cell><cell>K400</cell><cell>93.7</cell><cell>64.7</cell></row><row><cell>HiCo [86]</cell><cell>S3D-G</cell><cell>UK400</cell><cell>91.0</cell><cell>66.5</cell></row><row><cell>CVRL [87]</cell><cell>Slow-R152</cell><cell>K600</cell><cell>94.4</cell><cell>70.6</cell></row><row><cell>?BYOL [88]</cell><cell>Slow-R50</cell><cell>K400</cell><cell>94.2</cell><cell>72.1</cell></row><row><cell>VideoMAE [9]</cell><cell>ViT-B</cell><cell>K400</cell><cell>96.1</cell><cell>73.3</cell></row><row><cell>MAR ?=50%</cell><cell>ViT-B</cell><cell>K400</cell><cell>95.9</cell><cell>74.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12602</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive focus for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adafocus v2: End-to-end training of spatial dynamic networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.14238</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning based frame sampling for effective untrimmed video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Liteeval: A coarse-tofine framework for resource efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2d or not 2d? adaptive 3d convolution selection for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6155" to="6164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Masked autoencoders as spatiotemporal learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09113</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A closer look at self-supervised lightweight vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14443</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="284" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06178</idno>
		<title level="m">Tada! temporally-adaptive convolutions for video understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tam: Temporal adaptive module for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3163" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01526</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Uniformer: Unified transformer for efficient spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04676</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic sampling networks for efficient action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7970" to="7983" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Watching a small portion could be as good as watching all: Towards efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic inference: A new approach toward efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mgsampler: An explainable sampling strategy for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1513" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9653" to="9663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="668" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bevt: Bert pretraining of video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="733" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Vimpac: Video pre-training via masked token prediction and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11250</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--</forename><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="493" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Vi2clr: Video and image for visual contrastive learning of representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Safdari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1502" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Selfsupervised motion learning from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1276" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Memory-augmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01065</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10501</idno>
		<title level="m">Paramcrop: Parametric cubic cropping for video contrastive learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Rspnet: Relative speed perception for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07949</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning from untrimmed videos: Self-supervised video representation learning with hierarchical consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="13" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A largescale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
