<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide and Contrast: Self-supervised Learning from Uncurated Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J H?naff</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oord</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Divide and Contrast: Self-supervised Learning from Uncurated Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pretraining data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes-which is more diverse and heavytailed-resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current stateof-the-art on curated datasets. arXiv:2105.08054v1 [cs.CV] 17 May 2021 ual "expert" models on each subset and distilling them into a single model. As a result, DnC can be used in combination with any self-supervised learning technique, and requires the same amount of computation, as each expert is trained for significantly less time. Finally, this computation is trivially parallelized, allowing it to be scaled to massive datasets.</p><p>The remainder of this paper is structured as follows. We first review related work in self-supervised learning. We then present a new stronger baseline (MoCLR) which improves over current contrastive methods, matching the performance of the current state-of-the-art (BYOL [35]). Next we present the main method, Divide and Contrast, and how this model can be used together with any SSL method. In the experiments we evaluate the different hypotheses that support DnC, and compare its ability to learn from uncurated dataset with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent self-supervised representation learning generally includes three types of methods: generative models that directly model the data distribution, pretext tasks that are manually designed according to the data, and contrastive learning that contrasts positive pairs with negative pairs. Generative models. While the primary goal of generative models such as GAN [30,<ref type="bibr" target="#b13">14]</ref> or VAE [51] is to model the data distribution (e.g., sample new data or estimate likelihood), the encoder network can also extract good representations [80]. Recent state of the art generative models for representation learning include BiGAN [24] and BigBi-GAN [25], which learn a bidirectional mapping between the latent codes and the images, and iGPT [11] which trains an autoregressive model on raw pixels. Pretext tasks. Good representations may also be learned by solving various pretext tasks. Examples include denoising [97], relative patch prediction [22], image inpainting [77], noise prediction [5], colorization [112, 113, 98], Jigsaw [72], exemplar modeling [26], motion segmentation [76], image transformation prediction [29, 111], tracking [100], or even the combination of multiple tasks [23]. Another line of methods generates pseudo labels by clustering features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b110">110,</ref><ref type="bibr" target="#b0">1]</ref>. Most recently, SeLa [106] jointly clusters images and balances the clusters. SwAV [10] learns representatons by having different views of the same image assigned to the same cluster. Another work <ref type="bibr" target="#b46">[46]</ref> directly optimizes the transferability of representation by integrating clustering with meta-learning. Contrastive learning. Contrastive learning is a widelyused generic method. The loss function for contrastive learning has evolved from early margin-based binary classification <ref type="bibr" target="#b37">[37]</ref>, to triplet loss <ref type="bibr" target="#b84">[84]</ref>, and to recent k-pair loss <ref type="bibr" target="#b86">[86,</ref><ref type="bibr" target="#b73">73]</ref>. The core idea lying at the heart of the recent series of self-supervised contrastive learning meth-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent developments in self-supervised learning have shown that it is possible to learn high-level representations of object categories from unlabeled images <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b101">101]</ref>, phonetic information from speech <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b83">83]</ref> and language understanding from raw text <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b105">105]</ref>. The most studied benchmark in self-supervised learning is Ima-geNet <ref type="bibr" target="#b19">[20]</ref>, where representations learned from unlabeled images can surpass supervised representations, both in terms of their data-efficiency and transfer-learning performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>One of the caveats with self-supervised learning on Im-ageNet is that it is not completely "self-supervised". The training set of ImageNet, on which the representations are learned, is heavily curated and required extensive human effort to create <ref type="bibr" target="#b19">[20]</ref>. In particular, ImageNet contains many fine-grained classes (such as subtly different dog breeds), each one containing roughly the same number of images. While this consistency may facilitate the learning of highlevel visual representations, limiting self-supervised learn-*Work done while interning at DeepMind. ing to such curated datasets risks biasing their development towards methods which require this consistency, limiting their applicability to more diverse downstream tasks and larger datasets for pre-training.</p><p>In this paper we assess how well recent self-supervised learning methods perform on downstream tasks (including ImageNet) when they are pre-trained on significantly less curated datasets, such as YFCC100M <ref type="bibr" target="#b91">[91]</ref>. We observe a notable drop in performance of over 9% Top-1 accuracy (from 74.3% to 65.3%) for a ResNet50 model trained with the current state-of-the-art in self-supervised learning.</p><p>We hypothesize that this curation gap is due to the heavy-tailed nature of images collected in the wild, which present much more diverse content, breaking the global consistency exploited in previous datasets. We test this hypothesis with a new method, Divide and Contast (DnC), which attempts to recover local consistency in subsets of the larger, uncurated dataset, such that self-supervised learning methods can learn high-level features that are specific to each subset. We find that such semantically coherent subsets can be straightforwardly obtained by clustering the representations of standard self-supervised models.</p><p>Divide and Contrast (DnC) proceeds by training individ-ods <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b118">118,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b94">94,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b6">7]</ref> is to maximize the agreement between two "views" of the same image while repulsing "views" from different images. Such views can be created by color decomposition <ref type="bibr" target="#b92">[92]</ref>, patch cropping <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b2">3]</ref>, data augmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b87">87]</ref>, or image segmentation <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b96">96,</ref><ref type="bibr" target="#b114">114]</ref>. Indeed, contrastive learning is very general such that it can be easily adapted to different data types. Examples include different frames of video <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b117">117,</ref><ref type="bibr" target="#b85">85,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">39]</ref>, point clouds <ref type="bibr" target="#b104">[104]</ref>, multiple sensory data <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b78">78]</ref>, text and its context <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b105">105,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b53">53]</ref>, or video and language <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b59">59]</ref>. A set of other work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b94">94,</ref><ref type="bibr" target="#b115">115,</ref><ref type="bibr" target="#b103">103,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b99">99]</ref> focuses on providing empirical and theoretical understanding of contrastive learning. Recently a non-contrastive method BYOL <ref type="bibr" target="#b34">[35]</ref> applies a momentum-encoder to one view and predicts its output from the other, inspired by bootstrapping RL <ref type="bibr" target="#b36">[36]</ref>. Finally, contrastive learning has also been applied to supervised image classification <ref type="bibr" target="#b49">[49]</ref>, image translation <ref type="bibr" target="#b74">[74]</ref>, knowledge distillation <ref type="bibr" target="#b93">[93,</ref><ref type="bibr" target="#b81">81]</ref>, and adversarial learning <ref type="bibr" target="#b50">[50]</ref>. This paper is also related to knowledge distillation <ref type="bibr" target="#b45">[45]</ref>. In <ref type="bibr" target="#b45">[45]</ref>, several expert models were also trained in parallel on a large scale dataset, and then distilled into a single model. While labels are assumed available in <ref type="bibr" target="#b45">[45]</ref> to partition the dataset and distill into a single model, we are dealing with self-supervised learning without supervision. Our distillation procedure is also inspired by FitNet <ref type="bibr" target="#b82">[82]</ref>.</p><p>Lastly, while self-supervised representation learning on uncurated datasets is largely unexplored, there are a few prior attempts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, clustering is applied to generate training targets, and in order to capture the long-tailed distribution of images in the uncurated YFCC100m <ref type="bibr" target="#b91">[91]</ref>, a hierachical formulation is proposed. The work of <ref type="bibr" target="#b33">[34]</ref> benchmarked pretext-based self-supervised methods in a large scale setting, e.g., jigSaw, colorization and rotation prediction, and found that these pretext tasks are not 'hard' enough to take full advantage of large scale data. Concurrent work SEER <ref type="bibr" target="#b31">[32]</ref> directly scales up SwAV with larger models and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Divide and Contrast</head><p>Though Divide and Contrast can be used in combination with any self-supervised learning technique, in this paper we will combine it with recent state of the art techniques (BYOL, SimCLR, MoCo), such that the model can be compared to a strong baseline and make the experiments relevant with respect to recent developments in the literature. We will start by introducing our baseline, MoCLR, which is a simple hybrid based on BYOL <ref type="bibr" target="#b34">[35]</ref>, SimCLR <ref type="bibr" target="#b11">[12]</ref> and MoCo <ref type="bibr" target="#b40">[40]</ref>, and as a contrastive method outperforms Sim-CLR v2 <ref type="bibr" target="#b12">[13]</ref>, achieving similar performance to BYOL (by using a momentum encoder similar to BYOL and MoCo). Even though DnC can be coupled with BYOL, empirically we have found it to work better with methods that use a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Train expert models on subsets</head><p>Clustered images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Distillation</head><p>Predict base model Predict experts <ref type="figure">Figure 2</ref>. Overview of Divide and Contrast (DnC). DnC can be used in conjunction with any self-supervised learning method (we use MoCLR, an improvement to SimCLR). In the first step a self-supervised learning method is trained on the whole dataset, which we call the base model. The image representations of the base model are then clustered with k-means into 5, 10 or more groups. In the second step, the clustered dataset is then used to train an expert model on each of the image clusters. In the third step the experts and base model are distilled into a single model by predicting their representations. By splitting the dataset into semantically-similar subsets, contrastive methods need to pay more attention to the differences between the images in those clusters and learn more specific representations. contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">An Improved Contrastive Baseline: MoCLR</head><p>MoCLR roughly uses a similar setup to SimCLR and BYOL, we will briefly describe the main components of this setup and highlight the differences. Augmenting two views. Given an image x and two distributions of image augmentations T and T 1 , two views are created v " ? tpxq and v 1 " ? t 1 pxq by respectively applying random image augmentations t " T and t 1 " T 1 from these distributions. The augmentations T and T 1 here are exactly the same as in BYOL <ref type="bibr" target="#b34">[35]</ref>. Architecture. The first augmented view v is fed into an online encoder f p?q, followed up with an MLP projection head gp?q to produce a projection z. Similarly, an exponential moving average of the online encoder and projection head, also known as momentum encoder <ref type="bibr" target="#b40">[40]</ref> or mean teacher <ref type="bibr" target="#b90">[90]</ref>, is applied on the second view v 1 to generate z 1 . The MLP head consists of two layers with a hidden dimension of 4096 and output size of 256, similar to BYOL <ref type="bibr" target="#b34">[35]</ref>. Loss function. Given a batch B, we follow the InfoNCE loss <ref type="bibr" target="#b73">[73]</ref> with a cosine similarity function sp?,?q and a scalar temperature value ? :</p><formula xml:id="formula_0">L NCE "?1 |B| ? iPB log e spzi,z 1 i q{? e spzi,z 1 i q{?`? jPB{i e spzi,z 1 j q{? (1)</formula><p>We symmetrize the loss L NCE by separately feeding v 1 to the online network and v to the momentum encoder, resulting in r L NCE . The final loss is L " L NCE`r L NCE . The difference between MoCLR and other standard methods are as follows. Compared with SimCLR <ref type="bibr" target="#b11">[12]</ref>, we use a momentum encoder, and double the size of the projection head (from 2048 to 4096 for the hidden layer, and from 128 to 256 for the output layer). In comparison with BYOL <ref type="bibr" target="#b34">[35]</ref>, we remove the predictor head and use the contrastive loss instead of the mean squared prediction loss. While concurrent work MoCo v3 <ref type="bibr" target="#b15">[16]</ref> inherits from BYOL the asymmetric "projector &amp; predictor" design (the online encoder has an additional predictor compared to the momentum network), our MoCLR removes the predictor for simplicity.</p><p>In our experiments we set the batch size to 4096 and do not use a memory buffer <ref type="bibr" target="#b101">[101,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b40">40]</ref>. As we will show in our experiments, with these simple changes our MoCLR baseline trained for 1,000 epochs outperforms Sim-CLR v1/v2 <ref type="bibr" target="#b12">[13]</ref> and is on par with BYOL <ref type="bibr" target="#b34">[35]</ref>, see <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Divide and Contrast</head><p>The motivation behind Divide and Contrast is that, when training on diverse, large-scale datasets, the density of informative negatives will be sparse if we sample randomly from the whole dataset. Instead, if we contrast locally be-  tween semantically-similar classes, the sampled negatives will be more informative and the learned model will capture a more discriminative representation.</p><p>As visualized in <ref type="figure">Figure 2</ref>, the training of our DnC model consists of three stages:</p><p>(1) We first train a MoCLR model on the given dataset for N 1 epochs (though other self-supervised learning methods can be used as well). We will call it the base model. We use the base model to extract representations for a set of samples in the training set, and cluster them in to K clusters. With these clusters we partition the dataset into K subsets.</p><p>(2) For each subset, we train a separate MoCLR model from scratch, which we call expert models. In this stage, we distribute a total computational budget of N 2 epochs (measured on the whole dataset) to these expert models, proportionally to their corresponding cluster sizes.</p><p>(3) Finally, given a base model that captures the general knowledge of the dataset and expert models focusing on locally similar categories, we distill knowledge from these models into a distillation model. In this stage, we train for N 3 epochs.</p><p>The encoder-architecture for the base model, expert models, and the distillation model are all identical. Therefore, the computational footprint can roughly be measured by summing up the training epochs across all three stages, resulting in a total training of N 1`N2`N3 epochs (except for the clustering overhead and extra forward pass during distillation, which we discuss later).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distillation</head><p>To leverage the information learned by each of the different experts and more general information from the base model, we distill their representation into one model in the last stage of training. During the distillation we use a single augmented image (instead of the 2-view setup) in combination with a simple regression loss to predict the representations in these models (no contrastive loss).</p><p>The distillation model's architecture is mostly identical to the other models and is visualized in <ref type="figure" target="#fig_1">Figure 3</ref>. All have a backbone encoder f p?q and an MLP projection head gp?q. On top of the projection head in the distillation model there are K`1 regression networks: r k p?q, k " 1, ..., K, one to predict each of the K expert models and another regression network r b p?q to predict the base model. The architecture of these regressors is the same as the projecton head, except that we remove the final global BatchNorm after the last output layer.</p><p>For distillation we use the same augmentation as during self-supervised learning. Given an augmented input image x with clustering id k, we feed it into the distillation model to produce the projection-head output z. Similarly we get z b and z k from the base model and the k-th expert model respectively. We also 2 -normalize z b and z k to be unitnorm. The distillation objective is then the average of the two mean squared errors:</p><formula xml:id="formula_1">Lpxq " 1 2 }r b pzq?z b } 2 2`1 2 }r k pzq?z k } 2 2</formula><p>(2)</p><p>Note that the outputs of r b and r k are not 2 normalized.</p><p>To make it possible to compare to our baseline methods in terms of the number of epochs trained, we use two augmented views from the same input image and average their losses. This is otherwise not necessary and alternatively one could also increase the batch-size.</p><p>The computational cost in this stage is slightly higher than the self-supervised learning stage (e.g., BYOL and MoCLR), as there are now two forward passes (for the expert and base model) for each view (not backward pass and gradient computation). In contrast, BYOL and MoCLR only need one forward pass from the momentum encoder. However, we found that always feeding a center crop to expert and base models only leads to very marginal drop in performance (instead of an augmented view). This strategy offers the possibility of first doing a single forward pass over the dataset and storing the activations offline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare DnC to BYOL and MoCLR by pre-training on two large-scale uncurated datasets and evaluating transfer performance on different downstream tasks. Datasets. We consider two large-scale uncurated datasets. The first is a private dataset of roughly 300 million images (JFT-300M <ref type="bibr" target="#b89">[89]</ref>). For the second dataset we use YFCC100M <ref type="bibr" target="#b91">[91]</ref>, a public dataset of 95M Flickr images available under the Creative Commons license. Figures 4 and 5 show a visual comparison between images from Ima-geNet and YFCC100M. ImageNet images often contain the object or animal of interest in the center of the image. Im-ageNet also does not have a long-tailed distribution (e.g., power law) over object-classes but only considers a specific set of 1000 different classes, which are (roughly) equally represented in the dataset. As a result, specific objects or animals (e.g., common tench, Bedlington terrier, . . . ) are overrepresented compared to more typically occurring scenes such as human faces and landscapes (which are better represented in YFCC100M). Settings. ResNet-50 <ref type="bibr" target="#b42">[42]</ref> is used in all experiments, unless noted otherwise. For ease of comparison, we report the computational footprint of all experiments in ImageNetepoch equivalents (e.g. 1 "epoch" " 1281167{batch size iterations). More implementation and optimization details are included in Appendix. DnC Schedules. <ref type="table" target="#tab_3">Table 2</ref> shows three training schedules with different number of epochs. For example, in the schedule of 3,000 epochs, we first train the base model for 1,000 epochs, after which we cluster the samples into 5 groups. The 5 experts are trained in parallel on these subsets. We  <ref type="table" target="#tab_4">Table 3</ref> shows the results of models pre-trained on YFCC100M and JFT-300M and tested on ImageNet and Places-365 <ref type="bibr" target="#b116">[116]</ref> with linear evaluation, i.e., features are frozen and a linear classifier is trained. For JFT-300M the results are also visualized in <ref type="figure" target="#fig_0">Figure 1</ref>. On the ImageNet linear benchmark we see a large drop in performance compared to pre-training on ImageNet <ref type="table" target="#tab_1">(Table 1)</ref>: -9.0%, -7.3% for BYOL-1k and -9.2%, -7.7% for MoCLR-1k, showing the difficulty of learning representations from uncurated (and more diverse) data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Linear Evaluation on ImageNet and Places-365</head><p>In these experiments DnC always uses MoCLR-1k for the clustering, and uses the remaining pre-training epochs for the expert training and distillation. Therefore <ref type="table">Table 4</ref>. Transfer learning experiments. We evaluate models pre-trained on ImageNet, YFCC100M and JFT-300M with a linear classifier on 12 downstream classification tasks: Food-101 <ref type="bibr" target="#b5">[6]</ref>, CIFAR-10/100 <ref type="bibr" target="#b56">[56]</ref>, Birdsnap <ref type="bibr" target="#b3">[4]</ref>, SUN397 <ref type="bibr" target="#b102">[102]</ref>, Stanford Cars <ref type="bibr" target="#b55">[55]</ref>, FGVC Aircraft <ref type="bibr" target="#b65">[65]</ref>, PASCAL VOC 2007 <ref type="bibr" target="#b26">[27]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b18">[19]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b75">[75]</ref>, Caltech-101 <ref type="bibr" target="#b27">[28]</ref> and Oxford 102 Flowers <ref type="bibr" target="#b71">[71]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer Learning</head><p>In this section, we consider both using frozen representations for fine-grained linear classification and fine-tuning for different downstream tasks. Fine-grained linear classification.</p><p>Following Sim-CLR <ref type="bibr" target="#b11">[12]</ref> and BYOL <ref type="bibr" target="#b34">[35]</ref>, we further perform linear classification evaluation on 12 classification datasets (introduced by <ref type="bibr" target="#b54">[54]</ref>), to assess whether the learned representation is generic across different image domains (see more details in Section F.1). As shown in <ref type="table">Table 4</ref>, when pre-training on YFCC100M or JFT-300M, DnC significantly and consistently outperforms BYOL and MoCLR. Detection, segmentation, and depth estimation. In Table 5, we evaluate the representation on three different finetuning tasks: (1) for object detection and instance segmentation on COCO <ref type="bibr" target="#b61">[61]</ref>, we train a standard Mask-RCNN <ref type="bibr" target="#b41">[41]</ref> using FPN <ref type="bibr" target="#b60">[60]</ref> with a 1x schedule, i.e., 12 epochs; (2) for semantic segmentation on VOC2012, we used FCN <ref type="bibr" target="#b63">[63]</ref> as in <ref type="bibr" target="#b40">[40]</ref>; (3) for depth estimation on NYU-v2 dataset <ref type="bibr" target="#b70">[70]</ref>, the setup is the same as <ref type="bibr" target="#b34">[35]</ref>. In all three tasks, DnC significantly outperforms ImageNet supervised pre-training, e.g., +2.2 in AP bb and +1.8 in AP mk for detection, +2.5 in mIoU for segmentation, and +5.0 in ?1.25 metric for depth prediction. DnC also significantly outperforms both self-supervised baselines when transferring to PASCAL and COCO tasks, and performs on-par with MoCLR while outperforming BYOL for depth estimation.</p><p>For more implementation details, please refer to Section F in the Appendix; Complete results for transfer learning are included in Section I.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Hypothesis and Analysis</head><p>The Divide and Contrast (DnC) method hinges on two main hypotheses. The first hypothesis is that clustering activations of powerful self-supervised learning models should <ref type="table">Table 6</ref>. Evaluating clustered representations from self-supervised learning methods using Top-1 accuracy and Mutual Information (MI) with the class labels. The representations of each method are clustered using k-means with 1000 centroids. To compute Top-1 accuracy, every cluster is mapped to the most frequent class label for the images in that cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Layer provide us with locally consistent clusters of images (e.g. having similar class labels). The second is that contrasting against similar (but different) object categories allows selfsupervised methods to learn more fine-grained, discriminative representations. We empirically assess these hypotheses in isolation. Next we compare DnC with current state of the art methods on ImageNet to see how well it performs on standard (curated) datasets, and analyze the design choices of DnC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Clustered Representations are Object Categories</head><p>Our first hypothesis is that clusters of self-supervised representations are semantically meaningful. To this effect we cluster the representations of various self-supervised learning methods trained on ImageNet with k-means. Specifically, we consider representations from three different layers: the pool layer right after the mean pooling, the hidden layer of the projection head, and the final projection.</p><p>We start with 1000-way clustering, and assign every cluster to a single ImageNet class with a simple majority vote to measure the Top-1 Accuracy. We also measure the mutual information between the clustering assignments and the class labels. <ref type="table">Table 6</ref> gives an overview of these results. In particular, these methods can group images from the same category surprisingly well, with some representations achieving over 50% Top-1 clustering accuracy. We also notice that the hidden layer performs the best for all methods, and thus use this layer for clustering in the DnC method.</p><p>To give an orthogonal view with a smaller number of clusters, <ref type="figure" target="#fig_4">Figure 6</ref> plots the 5 clusters used in the DnC model for ImageNet (based on the clustering of a MoCLR ResNet-50). Qualitatively, it appears that groups of classes are <ref type="table">Table 7</ref>. Linear classification evaluated on the Canine subset of ImageNet (130 classes). The feature extraction models were pretrained either on ImageNet (Full) or the subset of Canine images without using any labels. All computation is reported in terms of "full imagenet" epochs. We report the difference in performance relative to training on the full dataset for 1,000 epochs. Even though the canine-only models were trained with 5?fewer gradient updates, they largely outperform the self-supervised learning models that were trained on the full dataset. We also observe that contrastive methods (SimCLR, MoCLR) benefit the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Pre jointly assigned to the same cluster. Indeed, the fraction of images in each class that belong to the same cluster is 87.4%, lending further evidence that individual clusters are semantically coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training on Semantically Similar Data Subsets</head><p>DnC is based on the second hypothesis that training selfsupervised learning methods on a subset of images from similar object categories should improve performance on those object classes. Contrastive methods in particular stand to benefit from this procedure, as distinguishing positive samples from negatives from nearby classes might require learning more fine-grained features (similarly to hardnegative mining <ref type="bibr" target="#b84">[84,</ref><ref type="bibr" target="#b48">48]</ref>). On the other hand, it might hinder performance by drawing negative samples that are too similar, including more false-negatives <ref type="bibr" target="#b16">[17]</ref>.</p><p>To test this hypothesis in isolation and gain a better intuitive understanding of our method, we train various self-supervised learning models on the subset of ImageNet classes that belong to the canine family (including dogs, wolves and foxes, 130 classes in total) and compare them to models trained on the full dataset. For all models, we train a linear classifier on the canine-only subset, and evaluate on validation images from the canine subset.</p><p>From <ref type="table">Table 7</ref> it can be seen that models pre-trained on the canine-only subset perform significantly better than those trained on the entire ImageNet dataset, even though they have significantly less images to learn from and were trained with 5?less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ImageNet Results</head><p>Even though our main goal is to improve self-supervised learning on uncurated datasets, we asked whether it remains competitive on heavily studied datasets such as ImageNet. From <ref type="table" target="#tab_8">Table 8</ref> we see that training the baseline MoCLR for 2,000 more epochs does not improve the results by much (+0.2). DnC on the other hand convincingly outperforms the baseline (+1.3). Interestingly, DnC even slightly outperforms MoCLR or BYOL when giving a computational budget of 1,000 epochs. Though DnC aims at uncurated datasets, the previous results from Section 5.2 on the canine subset have shown that even on ImageNet it might be beneficial to draw negatives from more similar categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Semi-supervised learning</head><p>We evaluate the performance of DnC when fine-tuning on a subset of ImageNet's train set. Following the semisupervised protocol <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b109">109,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35]</ref>, we use the same splits of 1% and 10% ImageNet data as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. As shown in <ref type="table">Table 9</ref>, DnC consistently outperforms BYOL, SwAV, and MoCLR and Barlow Twins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ablations</head><p>We provide further experiments for isolating the factors that make DnC work, shown in <ref type="table" target="#tab_1">Table 10</ref>. If we train the expert models on the full dataset instead of subsets (similar to an <ref type="table">Table 9</ref>. Semi-supervised results with a fraction of ImageNet labels following the protocol of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. The encoder is ResNet-50. In <ref type="table" target="#tab_1">Table 11</ref> we study the distillation process, showing it is important to regress to both the base model and experts. And as discussed in Section 3.3, using center-crops instead of augmented views does not hurt the performance by much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we have studied how state of the art selfsupervised learning methods perform when they are pretrained on uncurated data -datasets that did not require human annotations or labels to create -as a step towards fully self-supervised learning. We have observed that current methods suffer from a large drop in performance of up to -9% when pre-trained on these uncurated datasets. To alleviate this issue, we have proposed Divide and Contrast (DnC) that requires a few simple changes to existing self-supervised learning methods, and which largely outperforms state of the art SSL methods on uncurated datasets, as well as achieving similar or better performance on Im-ageNet. We hope this work draws more attention to uncurated datasets as a benchmark for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Augmentations</head><p>For a fair comparison, we used exactly the same image augmentations as BYOL <ref type="bibr" target="#b34">[35]</ref> (which are a subset of the ones presented in SimCLR <ref type="bibr" target="#b11">[12]</ref>):</p><p>? random resized cropping: a random patch is cropped, whose area is uniformly sampled between 0.08?and 1?that of the raw image, and aspect ratio is logarithmically sampled between 3{4 and 4{3. We resize the patch to 224?224 pixels using bicubic interpolation;</p><p>? random horizontal flip;</p><p>? color jittering: the brightness, contrast, saturation and hue of the image are shifted by a uniformly distributed offset applied on all the pixels of the same image;</p><p>? color dropping: randomly convert images to grayscale, computed as 0.2989R`0.5870G`0.1140B;</p><p>? Gaussian blurring: a Gaussian kernel of size 23?23 is used, whose standard deviation is uniformly sampled from r0.1, 2.0s;</p><p>? solarization: an optional color transformation x ? ? x1 tx?0.5u`p 1?xq?1 tx?0.5u for pixels with values in r0, 1s.</p><p>Augmentations from the sets T and T 1 are compositions of the above image augmentations, each applied with a predetermined probability. The parameters for T and T 1 are listed in <ref type="table" target="#tab_1">Table 12</ref>.</p><p>In the evaluation or representation clustering stage, we follow the standard center-crop strategy: resize images to 256 pixels along the shorter side, and crop out the central 224?224 window.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-trainning Datasets</head><p>ImageNet. We split out 10009 images from the train set as our local validation set, and use the remaining 1271158 images for both unsupervised pre-training and linear classifier training. After selecting hyper-parameters based on the performance of the local validation set, we report accuracy on the official validation set consisting of 50000 images. JFT-300M. The JFT-300M dataset contains 301.7 millions of images in total. YFCC100M. YFCC-100M is a widely used uncurated dataset that includes "95 millions of images, which are all used in our pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Clustering Representations</head><p>We apply the vanilla k-means algorithm on the representations extracted from the hidden layer of the projection network, with cosine similarity as a distance metric. When pre-training on ImageNet, we use all training images for clustering; when pre-training on JFT-300M and YFCC, we randomly sample 1.5 million images for clustering and extracting the centroids, and then use these centroids to assign clustering labels to all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Run Time Analysis of DnC</head><p>While DnC has three stages of training, its computational complexity or running time is similar as other state-of-theart approaches trained for the same number of epochs. As a illustration, we compare the training FLOPs of DnC with other methods such as BYOL, MoCLR, and SwAV. As discussed in the main paper, the base or expert training stage of DnC has exactly the same FLOPs as the chosen base approach, i.e., MoCLR here. The only different lies in the third stage, where DnC requires one additional forward pass. Therefore, for DnC we compute a weighted average of FLOPs across three stages (we use the normalized number of training epochs as weights). <ref type="table" target="#tab_1">Table 13</ref> summarizes the comparison with other approaches: DnC is comparable with BYOL and MoCLR, while SwAV costs more flops because it uses eight views per image per step.</p><p>Besides, we also run BYOL, MoCLR and DnC on Ima-geNet for 3000 epochs to compare the running time. <ref type="table" target="#tab_1">Table 14</ref> reports the comparison when using 512 TPU v3 cores. DnC only introduces ?5% extra training time, compared with BYOL and MoCLR. Besides, the time cost for clustering the representations is small, e.g., it takes about 20-30 minutes to extract representations on the training set and cluster them into groups, even only with 8 V100 GPUs on a single node. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optimization</head><p>Unsupervised pre-training. All the hyper-parameters for optimization directly follow BYOL, except for base learning rate (which we discuss in the next paragraph). Specifically, we use LARS optimizer <ref type="bibr" target="#b107">[107]</ref> with a cosine decay learning rate schedule <ref type="bibr" target="#b64">[64]</ref> and a warm-up period of 10 epochs for all unsupervised pre-training. In addition, we use a global weight decay parameter of 1.5?10?6 while excluding the biases and batch normalization parameters from both LARS adaptation and weight decay. For the momentum encoder, its parameters ? EMA are updated by ? EMA " ? ? EMA`p 1?? q?, where ? are the parameters of the online encoder. The EMA parameter ? starts from ? base " 0.996 and is increased to one during training. Following BYOL, we set</p><formula xml:id="formula_2">? " ? 1?p1?? base q?pcos p?k{Kq`1q {2<label>(3)</label></formula><p>with k the current training step and K the maximum number of training steps. Specifically for the base learning rate, we used 0.2 for BYOL as in the original paper (we sweep over t0.2, 0.3, 0.4u for 1000 epochs pre-training on ImageNet to confirm 0.2 is the best). For MoCLR, we found 0.3 is slightly better than 0.2, and therefore we kept using 0.3 for MoCLR and all stages of DnC (The only exception is that for DnC with 1000 epoch schedule, we increase the base learning rate to 0.5 to compensate for short training of models at each stage). The final learning rate is scaled linearly <ref type="bibr" target="#b32">[33]</ref> with the batch size (LearningRate " BaseLearningRate?BatchSize{256). Linear evaluation on ImageNet/Places-365. On top of the global pooling layer of the frozen pre-trained encoder, we train a supervised 1000-or 365-way linear classifier, as in <ref type="bibr" target="#b112">[112,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b11">12]</ref>. We optimize the cross-entropy loss using SGD with Nesterov momentum over 80 epochs, using a batch size of 1024 and a cosine learning rate decay schedule. We sweep the base learning rate (of batch size 256) over t0.4, 0.3, 0.2, 0.1, 0.05u for models pre-trained on Im-ageNet, and t1.0, 0.6, 0.4, 0.2, 0.1u for models pre-trained on JFT-300M and YFCC. We chose the best learning rate on a local validation set split out from the ImageNet train set, and report the accuracy on the official ImageNet validation set.</p><p>F. Transfer to Other Datasets F.1. Implementation: fine-grained linear classificaton F.2. Implementation: Pascal VOC segmentation</p><p>Following BYOL, we use the same fully-convolutional network (FCN)-based <ref type="bibr" target="#b63">[63]</ref> architecture as <ref type="bibr" target="#b40">[40]</ref>. The backbone consists of the convolutional layers in ResNet-50. The 3?3 convolutions in the conv5 blocks use dilation 2 and stride 1. This is followed by two extra 3?3 convolutions with 256 channels, each followed by batch normalization and ReLU activations, and a 1?1 convolution for per-pixel classification. The dilation is set to 6 in the two extra 3?3 convolutions. The total stride is 16 (FCN-16s <ref type="bibr" target="#b63">[63]</ref>).</p><p>Similar as BYOL, we train on the train2012 set and report results on val2012. Hyperparameters are selected on a 2119 images held-out validation set. Training is done with random scaling (by a ratio in r0.5, 2.0s), cropping, and horizontal flipping. The crop size is 513. Inference is performed on the r513, 513s central crop. For training we use a batch size of 16 and weight decay of 0.0001. We select the base learning rate by sweeping across 5 logarithmically spaced values between 10?3 and 10?1. The learning rate is multiplied by 0.1 at the 70-th and 90-th percentile of training. We train for 30000 iterations, and average the results on 5 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Implementation: COCO detection</head><p>We use the standard Mask R-CNN <ref type="bibr" target="#b41">[41]</ref> with the FPN [60] backbone, with cross-replica BN tuned, similar as that in MoCo <ref type="bibr" target="#b40">[40]</ref>. We fine-tune all layers end-to-end. We finetune on the train2017 set ("118k images) and evaluate on val2017. We use the standard "1x schedule".</p><p>We directly use the public Cloud TPU implementation without modification. Specifically, we use a batch size of 64 images split over 16 workers. We linearly warmup the learning rate to 0.3 for the first 500 iterations, and drop it twice by a factor of 2, after 2 3 and 8 9 of the total training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4. Implementation: NYU v2 depth estimation</head><p>Similar as BYOL, we follow the same protocol as in <ref type="bibr" target="#b57">[57]</ref>. With a standard ResNet-50 backbone, we feed the conv5 features into 4 fast up-projection blocks with respective filter sizes 512, 256, 128, and 64. We use a reverse Huber loss function for training.</p><p>The original NYU Depth v2 frames of size r640, 480s are down-sampled by a factor 0.5 and center-cropped to r304, 228s pixels. Input images are randomly horizontally flipped and the same set of color transformations as in <ref type="bibr" target="#b34">[35]</ref> are applied. We train for 7500 steps with batch size 256, weight decay 0.0005, and learning rate 0.16 (scaled linearly from the setup of <ref type="bibr" target="#b57">[57]</ref> to account for the larger batch size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. DnC with other Self-supervised Methods</head><p>While the main paper demonstrate the effectiveness of DnC with MoCLR, we found DnC can potentially improves other state-of-the-art self-supervised approaches as well. In <ref type="table" target="#tab_1">Table 15</ref>, we demonstrate that DnC can improve SimCLR significantly and also benefit BYOL, when both pre-training and evaluating on ImageNet. Besides, we notice that DnC with BYOL gets a larger improvement when pre-training on the uncurated dataset YFCC. As shwon in <ref type="table" target="#tab_1">Table 16</ref>, naively extending BYOL from 1000 to 5000 epochs only increases the performance by 1.7%, while DnC-4500 leverages the computation more efficiently and improves the accuracy by 3.4%. https : / / github . com / tensorflow / tpu / tree / master/models/official/detection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Additional Ablations and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Length of the distillation stage</head><p>While the distillation stage introduces additional FLOPs compared to the base or expert training stage, this stage can be short. In this section, we conduct ablation on the number of epochs for distillation stage. We train DnC on ImageNet following the the DnC-3k schedule (i.e., 1000 epochs for base training and 1500 epochs for experts). We vary the number of epochs used for distillation and report the linear evaluation accuracy in <ref type="table" target="#tab_1">Table 18</ref>. Short distillation schedule such as 60 epochs can yield 74.0%, as long as a larger learning rate is utilized to compensate for the smaller number of gradient steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. 2 -normalization on regressor output</head><p>We study whether it's better to normalize the output of the regressor by 2 -normalization in the distillation stage. We conducted this ablation on ImageNet, and found normalizing the output of the regressor actually hurts the performance a bit, as shown in <ref type="table" target="#tab_1">Table 19</ref>. <ref type="table" target="#tab_1">Table 19</ref>. Abalation on whether 2-normalize the output of regressors r b and r k pk " 1, 2, ...Kq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-normalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3. Semi-supervised learning with projection layer</head><p>As found in SimCLR v2 <ref type="bibr" target="#b12">[13]</ref>, fine-tuning from the hidden layer of projection head gives better semi-supervised accuracy. In this seciton, we also report the semi-supervised accuracy fine-tuned from the hidden layer of the projection head in <ref type="table" target="#tab_3">Table 20</ref>. Models are all pre-trained using ImageNet data. <ref type="table" target="#tab_3">Table 20</ref>. Semi-supervised results by fine-tuning from the first layer of the projection head, following <ref type="bibr" target="#b11">[12]</ref>. The encoder is ResNet-50. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4. Complete results of transfer learning</head><p>In <ref type="table" target="#tab_1">Table 21</ref>, we summarize the transfer learning results on fine-grained linear classification tasks, with different computational budgets in pre-training stage for each method.</p><p>In <ref type="table" target="#tab_3">Table 22</ref>, we provide the complete results of transfer learning on COCO detection, Pascal VOC semantic segmentation, and NYU depth estimation. <ref type="table" target="#tab_1">Table 21</ref>. Transfer learning experiments. We evaluate models pre-trained on ImageNet, YFCC100M and JFT-300M with a linear classifier on 12 downstream classification tasks: Food-101 <ref type="bibr" target="#b5">[6]</ref>, CIFAR-10/100 <ref type="bibr" target="#b56">[56]</ref>, Birdsnap <ref type="bibr" target="#b3">[4]</ref>, SUN397 <ref type="bibr" target="#b102">[102]</ref>, Stanford Cars <ref type="bibr" target="#b55">[55]</ref>, FGVC Aircraft <ref type="bibr" target="#b65">[65]</ref>, PASCAL VOC 2007 <ref type="bibr" target="#b26">[27]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b18">[19]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b75">[75]</ref>, Caltech-101 <ref type="bibr" target="#b27">[28]</ref> and Oxford 102 Flowers <ref type="bibr" target="#b71">[71]</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Linear evaluation on ImageNet of representations learned from a large-scale uncurated dataset using ResNet-50. Divide and Contrast (DnC) is better able to handle the diverse and long-tailed distribution of images and improves more with longer training. X-axis represents total computation, in ImageNetequivalent epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Detailed architectural overview of the distillation of the expert's features. The distillation model and experts are applied to the same augmented image view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Example ImageNet images Example YFCC100M images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the 5-way ImageNet clustering used in DnC. For each ImageNet class we compute the fraction of images that belong to each cluster. For better visualization the x-axis was sorted per cluster. From the figure it is clear that most images in each class belong to a single cluster. The fraction of images in each class that belong to the same cluster is 87.4%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1. Train base model &amp; cluster representations</head><label></label><figDesc></figDesc><table><row><cell>K-means</cell></row><row><cell>Encoder</cell></row><row><cell>Training</cell></row><row><cell>images</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">Evaluating the MoCLR baseline which is used in the pro-</cell></row><row><cell cols="4">posed Divide And Contrast (DnC) model. This comparison is</cell></row><row><cell cols="4">on the ImageNet linear classification benchmark. MoCLR is a</cell></row><row><cell cols="4">contrastive method which achieves similar performance as BYOL,</cell></row><row><cell cols="4">while requiring only a small change to SimCLR (see Section 3.1).</cell></row><row><cell>Method</cell><cell cols="3">Epochs Top-1 Top-5</cell></row><row><cell>SimCLR [12]</cell><cell>1000</cell><cell>69.3</cell><cell>89.0</cell></row><row><cell>SimCLR v2 [13]</cell><cell>1000</cell><cell>71.7</cell><cell>90.4</cell></row><row><cell>MoCo v3 [16]</cell><cell>800</cell><cell>73.8</cell><cell>-</cell></row><row><cell>BYOL [35]</cell><cell>1000</cell><cell>74.3</cell><cell>91.6</cell></row><row><cell>MoCLR (ours)</cell><cell>1000</cell><cell>74.3</cell><cell>92.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>We consider three different training schedules for DnC. The number of total training epochs in each stage, as well as the number of clusters, is specified.</figDesc><table><row><cell>Schedules</cell><cell>Base epochs</cell><cell>Experts total epochs</cell><cell>Distillation epochs</cell></row><row><cell>1,000 epochs</cell><cell>200</cell><cell>600 (5 clusters)</cell><cell>200</cell></row><row><cell>3,000 epochs</cell><cell>1,000</cell><cell>1,500 (5 clusters)</cell><cell>500</cell></row><row><cell>4,500 epochs</cell><cell>1,000</cell><cell>3,000 (10 clusters)</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of self-supervised learning methods pretrained on uncurated datasets YFCC100M and JFT-300M. For evaluation a linear classifier is trained on ImageNet and Places-365. Computation is measured as ImageNet-equivalent epochs. use 1,500 epochs in total, spread out over the experts according to the number of images in each cluster (300 on average per expert). The distillation model is then trained for 500 epochs. See Section D for analysis of run time.</figDesc><table><row><cell>Method</cell><cell>Arch</cell><cell cols="3">pre-training ImageNet Places 365</cell></row><row><cell></cell><cell></cell><cell cols="3"># epochs Top-1 Acc Top-1 Acc</cell></row><row><cell cols="4">Concurrent work trained on IG 1B images:</cell><cell></cell></row><row><cell>SEER [32]</cell><cell>R-50</cell><cell>?1,000</cell><cell>61.6</cell><cell>-</cell></row><row><cell></cell><cell>R-101</cell><cell>?1,000</cell><cell>65.8</cell><cell>-</cell></row><row><cell cols="3">Pre-training on YFCC100M:</cell><cell></cell><cell></cell></row><row><cell>MoCLR BYOL</cell><cell>R-50</cell><cell>1,000 1,000</cell><cell>65.1 65.3</cell><cell>53.2 52.9</cell></row><row><cell>MoCLR</cell><cell></cell><cell>3,000</cell><cell>65.7</cell><cell>53.2</cell></row><row><cell>BYOL</cell><cell>R-50</cell><cell>3,000</cell><cell>66.6</cell><cell>52.9</cell></row><row><cell>DnC</cell><cell></cell><cell>3,000</cell><cell>67.8</cell><cell>54.1</cell></row><row><cell>MoCLR</cell><cell></cell><cell>5,000</cell><cell>66.1</cell><cell>53.5</cell></row><row><cell>BYOL</cell><cell>R-50</cell><cell>5,000</cell><cell>67.0</cell><cell>53.2</cell></row><row><cell>DnC</cell><cell></cell><cell>4,500</cell><cell>68.5</cell><cell>54.4</cell></row><row><cell cols="3">Pre-training on JFT-300M:</cell><cell></cell><cell></cell></row><row><cell>MoCLR</cell><cell></cell><cell>1,000</cell><cell>66.6</cell><cell>52.1</cell></row><row><cell>BYOL</cell><cell>R-50</cell><cell>1,000</cell><cell>67.0</cell><cell>51.9</cell></row><row><cell>DnC</cell><cell></cell><cell>1,000</cell><cell>67.9</cell><cell>52.5</cell></row><row><cell>MoCLR</cell><cell></cell><cell>3,000</cell><cell>67.4</cell><cell>52.5</cell></row><row><cell>BYOL</cell><cell>R-50</cell><cell>3,000</cell><cell>67.6</cell><cell>52.4</cell></row><row><cell>DnC</cell><cell></cell><cell>3,000</cell><cell>69.8</cell><cell>53.3</cell></row><row><cell>MoCLR</cell><cell></cell><cell>5,000</cell><cell>67.6</cell><cell>52.4</cell></row><row><cell>BYOL</cell><cell>R-50</cell><cell>5,000</cell><cell>67.9</cell><cell>52.4</cell></row><row><cell>DnC</cell><cell></cell><cell>4,500</cell><cell>70.7</cell><cell>53.5</cell></row><row><cell cols="2">With larger ResNet:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MoCLR DnC</cell><cell>R-200x2</cell><cell>3,000 3,000</cell><cell>74.2 77.3</cell><cell>54.6 56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Fine-tuning pre-trained model for transfer learning experiments, including object detection on COCO dataset, semantic segmentation on Pascal VOC 2012, and depth estimation on NYU v2 dataset. For the evaluation metrics of rms and rel, lower is better.</figDesc><table><row><cell>YFCC</cell><cell>BYOL-5k MoCLR-5k DnC-4.5k</cell><cell>69.1 68.4 72.1</cell><cell>85.8 87.6 88.0</cell><cell>66.8 69.7 71.1</cell><cell>35.5 30.5 35.5</cell><cell>64.1 63.9 67.2</cell><cell>50.1 41.0 52.6</cell><cell>51.9 46.7 49.2</cell><cell>82.5 82.4 83.7</cell><cell>74.5 76.2 76.5</cell><cell>74.0 68.5 75.9</cell><cell>87.6 86.0 87.0</cell><cell>95.8 93.0 97.8</cell><cell>69.8 67.8 71.4</cell></row><row><cell>JFT-300M</cell><cell>BYOL-5k MoCLR-5k DnC-4.5k</cell><cell>73.3 72.8 78.7</cell><cell>89.8 90.7 91.7</cell><cell>72.4 72.5 74.9</cell><cell>38.2 33.8 42.1</cell><cell>61.8 62.2 65.0</cell><cell>64.4 60.6 75.3</cell><cell>54.4 50.9 54.1</cell><cell>81.3 81.9 83.1</cell><cell>75.5 75.3 76.6</cell><cell>77.0 75.8 86.1</cell><cell>90.1 89.5 90.2</cell><cell>94.3 93.8 98.2</cell><cell>72.7 71.7 76.3</cell></row><row><cell></cell><cell></cell><cell cols="3">COCO detection</cell><cell cols="3">COCO instance seg.</cell><cell cols="2">PASCAL seg.</cell><cell></cell><cell cols="3">NYU v2 depth estimation</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP mk AP mk 50</cell><cell>AP mk 75</cell><cell>mIoU</cell><cell cols="5">?1.25 ?1.25 2 ?1.25 3 rms?</cell><cell>rel?</cell></row><row><cell cols="3">ImageNet Super. 39.5</cell><cell>60.1</cell><cell>43.3</cell><cell>35.4</cell><cell>56.9</cell><cell>38.1</cell><cell>74.4</cell><cell></cell><cell>81.1</cell><cell>95.3</cell><cell>98.8</cell><cell cols="2">0.573 0.127</cell></row><row><cell>YFCC</cell><cell>BYOL-5k MoCLR-5k DnC-4.5k</cell><cell>41.1 40.8 41.5</cell><cell>62.0 61.7 62.5</cell><cell>45.1 44.8 45.6</cell><cell>36.6 36.6 37.0</cell><cell>58.6 58.5 59.3</cell><cell>38.9 39.0 39.6</cell><cell>75.5 75.1 76.6</cell><cell></cell><cell>83.5 86.7 86.2</cell><cell>96.4 97.4 97.2</cell><cell>99.0 99.3 99.3</cell><cell cols="2">0.558 0.130 0.503 0.117 0.512 0.121</cell></row><row><cell>JFT-300M</cell><cell>BYOL-5k MoCLR-5k DnC-4.5k</cell><cell>40.6 41.1 41.7</cell><cell>61.2 62.0 62.5</cell><cell>44.3 45.4 45.9</cell><cell>36.2 36.9 37.2</cell><cell>58.1 58.9 59.3</cell><cell>38.8 39.5 39.8</cell><cell>75.8 76.1 76.9</cell><cell></cell><cell>84.4 86.3 86.1</cell><cell>96.5 97.2 97.2</cell><cell>99.0 99.3 99.4</cell><cell cols="2">0.544 0.129 0.513 0.120 0.509 0.119</cell></row><row><cell cols="8">a good comparison is with MoCLR trained for longer (from</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">scratch). For 3,000 epochs of training, MoCLR-3k im-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>proves over MoCLR-1k by +0.6 and +0.8 on YFCC100M and JFT-300M respectively, while DnC-3k improves by +2.7 and +3.2. On Places-365 we see similar relative im- provements. We also include BYOL-3k for completeness and again see small improvements with longer training for BYOL. For further longer schedules (4,500-5,000 epochs), we notice similar behavior on both YFCC100M and JFT- 300M. Besides, we see DnC significantly outperforms con- current efforts SEER [32] when using ResNet-50. We fur- ther test DnC with a larger model (i.e., ResNet-200 with a width multiplier of 2) and observe that DnC outperforms MoCLR by +3.1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparison with BYOL and MoCLR on ImageNet linear evaluation benchmark with training budgets of 1000 and 3000 epochs. Top-1 accuracy is reported using ResNet-50.</figDesc><table><row><cell>Method</cell><cell cols="2">1000 epochs 3000 epochs</cell><cell>?</cell></row><row><cell>BYOL</cell><cell>74.3</cell><cell>73.9</cell><cell>-0.4</cell></row><row><cell>MoCLR</cell><cell>74.3</cell><cell>74.5</cell><cell>0.2</cell></row><row><cell>DnC (ours)</cell><cell>74.5</cell><cell>75.8</cell><cell>1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .Table 11 .</head><label>1011</label><figDesc>Ablating the experts used in DnC: We notice a big drop in performance if the experts models are trained on the whole dataset (ensemble), or on random subsets. Evaluation of what models to predict during distillation.</figDesc><table><row><cell></cell><cell>Top-1</cell><cell></cell><cell cols="2">Top-5</cell></row><row><cell>method</cell><cell cols="2">Label fraction</cell><cell cols="2">Label fraction</cell></row><row><cell></cell><cell cols="4">1% 10% 100% 1% 10% 100%</cell></row><row><cell>SimCLR [12]</cell><cell cols="4">48.3 65.6 76.0 75.5 87.8 93.1</cell></row><row><cell>BYOL [35]</cell><cell cols="4">53.2 68.8 77.7 78.4 89.0 93.9</cell></row><row><cell>SwAV [10]</cell><cell>53.9 70.2</cell><cell>-</cell><cell>78.5 89.9</cell><cell>-</cell></row><row><cell>MoCLR</cell><cell cols="4">53.0 68.8 77.4 79.1 89.6 94.0</cell></row><row><cell cols="2">Barlow Tw. [108] 55.0 69.7</cell><cell>-</cell><cell>79.2 89.3</cell><cell>-</cell></row><row><cell>DnC</cell><cell cols="4">59.9 71.1 78.2 83.0 90.4 94.2</cell></row><row><cell></cell><cell cols="4">Partitioning Experts trained on Top-1 Acc</cell></row><row><cell>DnC</cell><cell>Clustering</cell><cell cols="2">local partition</cell><cell>75.8</cell></row><row><cell>-local experts</cell><cell>-</cell><cell cols="2">full dataset</cell><cell>74.3</cell></row><row><cell>-clustering</cell><cell>Randomly</cell><cell cols="2">local partition</cell><cell>73.1</cell></row><row><cell cols="5">base model local experts use center-crop Top-1 Acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">74.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">75.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">75.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">75.6</cell></row><row><cell cols="5">ensemble), but with the same computational budget, the re-</cell></row><row><cell cols="5">sulting model achieves the same performance as the base</cell></row><row><cell cols="5">model (no improvement). Alternatively, splitting the dataset</cell></row><row><cell cols="5">into random subsets hurts the final performance, showing</cell></row><row><cell cols="3">the importance of the clustering used.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Parameters used to generate image augmentations during training, which are exactly the same as those in<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .</head><label>13</label><figDesc>Training FLOPs of different methods per image when using ResNet-50.</figDesc><table><row><cell cols="2">SwAV BYOL MoCLR DnC</cell></row><row><cell>training FLOPS 38.4B 24.7B</cell><cell>24.7B 25.4B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>Training time for 3000 epochs on ImageNet when using 512 TPU v3 cores.</figDesc><table><row><cell cols="3">BYOL MoCLR DnC</cell></row><row><cell>training time (hours) "24</cell><cell>"24</cell><cell>"25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>Applying DnC with other self-supervised methods, when pre-training and evaluating on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="2">w/ DnC Epochs Accuracy (%)</cell><cell>?</cell></row><row><cell>SimCLR</cell><cell>1000</cell><cell>69.4</cell></row><row><cell></cell><cell>5000</cell><cell>70.2</cell><cell>+0.8</cell></row><row><cell></cell><cell>3000</cell><cell>73.0</cell><cell>+3.6</cell></row><row><cell>BYOL</cell><cell>1000</cell><cell>74.3</cell></row><row><cell></cell><cell>3000</cell><cell>73.9</cell><cell>-0.5</cell></row><row><cell></cell><cell>3000</cell><cell>75.1</cell><cell>+0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 .</head><label>16</label><figDesc>Applying DnC with BYOL on YFCC pre-training. Evaluation is conducted on ImageNet linear evaluation. and evaluating on ImageNet) is not the main focus of this paper, we still provides a comparison between DnC and recent SoTA methods, as shown inTable 17.</figDesc><table><row><cell cols="3">Method w/ DnC Epochs Accuracy (%)</cell><cell>?</cell></row><row><cell>BYOL</cell><cell>1000</cell><cell>65.3</cell><cell></cell></row><row><cell></cell><cell>3000</cell><cell>66.6</cell><cell>+1.3</cell></row><row><cell></cell><cell>5000</cell><cell>67.0</cell><cell>+1.7</cell></row><row><cell></cell><cell>3000</cell><cell>67.9</cell><cell>+2.6</cell></row><row><cell></cell><cell>4500</cell><cell>68.7</cell><cell>+3.4</cell></row><row><cell cols="4">Table 17. Linear evaluation benchmark on ImageNet (self-</cell></row><row><cell cols="4">supervised pre-training is also conducted on ImageNet). * indi-</cell></row><row><cell cols="2">cates using eight views for pre-training.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Epochs Top-1 Acc Top-5 Acc</cell></row><row><cell cols="2">Clustering methods with ResNet-50:</cell><cell></cell><cell></cell></row><row><cell>SeLa [106]</cell><cell>400</cell><cell>61.5</cell><cell>84.0</cell></row><row><cell>DeepClusterV2* [10]</cell><cell>800</cell><cell>75.2</cell><cell>-</cell></row><row><cell>SwAV* [10]</cell><cell>800</cell><cell>75.3</cell><cell>-</cell></row><row><cell cols="3">Contrastive learning with designed architecture:</cell><cell></cell></row><row><cell>AMDIM [3]</cell><cell>150</cell><cell>68.1</cell><cell>-</cell></row><row><cell>CMC [92]</cell><cell>240</cell><cell>70.6</cell><cell>89.7</cell></row><row><cell cols="2">Contrastive learning with ResNet-50:</cell><cell></cell><cell></cell></row><row><cell>NPID [101]</cell><cell>200</cell><cell>56.5</cell><cell>-</cell></row><row><cell>Local Agg. [118]</cell><cell>200</cell><cell>58.8</cell><cell>-</cell></row><row><cell>CPC v2 [44]</cell><cell>-</cell><cell>63.8</cell><cell>85.3</cell></row><row><cell>MoCo [40]</cell><cell>200</cell><cell>60.6</cell><cell>-</cell></row><row><cell>PIRL [68]</cell><cell>800</cell><cell>67.4</cell><cell>-</cell></row><row><cell>PCL [58]</cell><cell>200</cell><cell>67.6</cell><cell>-</cell></row><row><cell>SimCLR [12]</cell><cell>1,000</cell><cell>69.3</cell><cell>89.0</cell></row><row><cell>PIC [7]</cell><cell>1,600</cell><cell>70.8</cell><cell>90.0</cell></row><row><cell>MoCo v2 [15]</cell><cell>800</cell><cell>71.1</cell><cell>-</cell></row><row><cell>SimCLR v2 [13]</cell><cell>1,000</cell><cell>71.7</cell><cell>90.4</cell></row><row><cell>InfoMin Aug. [94]</cell><cell>800</cell><cell>73.0</cell><cell>91.1</cell></row><row><cell>BYOL [35]</cell><cell>1,000</cell><cell>74.3</cell><cell>91.6</cell></row><row><cell>BYOL [35]</cell><cell>3,000</cell><cell>73.9</cell><cell>92.2</cell></row><row><cell>MoCLR (ours)</cell><cell>1,000</cell><cell>74.3</cell><cell>92.2</cell></row><row><cell>MoCLR (ours)</cell><cell>3,000</cell><cell>74.5</cell><cell>92.3</cell></row><row><cell>DnC (ours)</cell><cell>1,000</cell><cell>74.5</cell><cell>92.2</cell></row><row><cell>DnC (ours)</cell><cell>3,000</cell><cell>75.8</cell><cell>92.8</cell></row><row><cell cols="3">H. Comparing with SoTA on ImageNet</cell><cell></cell></row><row><cell cols="4">Though ImageNet linear evaluation benchmark (both</cell></row><row><cell>pre-training</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 .</head><label>18</label><figDesc>Abalation on length of the distillation stage.</figDesc><table><row><cell>Epoch</cell><cell>60</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>500</cell></row><row><cell cols="5">Learning rate 0.45 0.45 0.35 0.35</cell><cell>0.3</cell></row><row><cell cols="6">Accuracy (%) 74.0 74.9 75.1 75.4 75.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 22 .</head><label>22</label><figDesc>Fine-tuning pre-trained model for transfer learning experiments, including object detection on COCO dataset, semantic segmentation on Pascal VOC 2012, and depth estimation on NYU v2 dataset. For the evaluation metrics of rms and rel, lower is better.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">COCO object detection, 1x schedule</cell><cell></cell><cell>Seg.</cell><cell></cell><cell cols="3">NYU v2 depth estimation</cell></row><row><cell></cell><cell></cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell>AP mk</cell><cell>AP mk 50</cell><cell>AP mk 75</cell><cell cols="5">mIoU ?1.25 ?1.25 2 ?1.25 3 rms?</cell><cell>rel?</cell></row><row><cell cols="2">ImageNet Super.</cell><cell>39.5</cell><cell>60.1</cell><cell>43.3</cell><cell>35.4</cell><cell>56.9</cell><cell>38.1</cell><cell>74.4</cell><cell>81.1</cell><cell>95.3</cell><cell>98.8</cell><cell>0.573 0.127</cell></row><row><cell>ImageNet</cell><cell cols="2">BYOL-3k MoCLR-3k 41.5 (`2.0) 40.9 (`1.4) DnC-3k 41.7 (`2.2)</cell><cell>61.9 62.3 62.6</cell><cell cols="2">45.0 36.7 (`1.3) 45.4 37.0 (`1.6) 45.6 37.3 (`1.9)</cell><cell>58.5 59.0 59.2</cell><cell>39.2 39.7 40.1</cell><cell>76.3 76.2 76.9</cell><cell>84.7 84.6 85.1</cell><cell>97.0 97.0 97.0</cell><cell>99.1 99.3 99.2</cell><cell>0.525 0.126 0.527 0.126 0.525 0.124</cell></row><row><cell></cell><cell>BYOL-1k</cell><cell>40.8 (`1.3)</cell><cell>61.9</cell><cell cols="2">45.0 36.4 (`1.0)</cell><cell>58.4</cell><cell>38.8</cell><cell>75.5</cell><cell>85.8</cell><cell>97.2</cell><cell>99.2</cell><cell>0.511 0.122</cell></row><row><cell></cell><cell>BYOL-3k</cell><cell>41.0 (`1.5)</cell><cell>61.6</cell><cell cols="2">45.0 36.6 (`1.2)</cell><cell>58.5</cell><cell>39.2</cell><cell>75.5</cell><cell>85.2</cell><cell>96.9</cell><cell>99.0</cell><cell>0.537 0.124</cell></row><row><cell>YFCC</cell><cell cols="2">BYOL-5k MoCLR-1k 40.2 (`0.7) 41.1 (`1.6) MoCLR-3k 40.7 (`1.2)</cell><cell>62.0 61.1 61.6</cell><cell cols="2">45.1 36.6 (`1.2) 44.2 36.0 (`0.6) 44.4 36.3 (`0.9)</cell><cell>58.6 57.8 58.3</cell><cell>38.9 38.2 38.8</cell><cell>75.1 75.0 75.3</cell><cell>83.5 85.7 86.6</cell><cell>96.4 97.1 97.2</cell><cell>99.0 99.3 99.3</cell><cell>0.558 0.130 0.515 0.122 0.502 0.120</cell></row><row><cell></cell><cell cols="2">MoCLR-5k 40.8 (`1.3)</cell><cell>61.7</cell><cell cols="2">44.8 36.6 (`1.2)</cell><cell>58.5</cell><cell>39.0</cell><cell>75.5</cell><cell>86.7</cell><cell>97.4</cell><cell>99.3</cell><cell>0.503 0.117</cell></row><row><cell></cell><cell>DnC-3k</cell><cell>41.0 (`1.5)</cell><cell>61.6</cell><cell cols="2">44.7 36.6 (`1.2)</cell><cell>58.5</cell><cell>39.5</cell><cell>76.1</cell><cell>86.7</cell><cell>97.3</cell><cell>99.3</cell><cell>0.506 0.117</cell></row><row><cell></cell><cell>DnC-4.5k</cell><cell>41.5 (`2.0)</cell><cell>62.5</cell><cell cols="2">45.6 37.0 (`1.6)</cell><cell>59.3</cell><cell>39.6</cell><cell>76.6</cell><cell>86.2</cell><cell>97.2</cell><cell>99.3</cell><cell>0.512 0.121</cell></row><row><cell></cell><cell>BYOL-1k</cell><cell>40.5 (`1.0)</cell><cell>61.3</cell><cell cols="2">44.4 36.4 (`1.0)</cell><cell>58.2</cell><cell>38.8</cell><cell>75.5</cell><cell>85.8</cell><cell>97.1</cell><cell>99.2</cell><cell>0.519 0.121</cell></row><row><cell></cell><cell>BYOL-3k</cell><cell>40.5 (`1.0)</cell><cell>61.1</cell><cell cols="2">44.7 36.4 (`1.0)</cell><cell>57.9</cell><cell>39.2</cell><cell>75.7</cell><cell>85.6</cell><cell>97.0</cell><cell>99.2</cell><cell>0.525 0.122</cell></row><row><cell>JFT-300M</cell><cell cols="2">BYOL-5k MoCLR-1k 40.3 (`0.8) 40.6 (`1.1) MoCLR-3k 40.5 (`1.0) MoCLR-5k 41.1 (`1.6)</cell><cell>61.2 61.0 61.2 62.0</cell><cell cols="2">44.3 36.2 (`0.8) 44.2 36.3 (`0.9) 44.4 36.4 (`1.0) 45.4 36.9 (`1.5)</cell><cell>58.1 58.0 58.1 58.9</cell><cell>38.8 38.8 39.0 39.5</cell><cell>75.8 75.7 75.8 76.1</cell><cell>84.4 84.9 85.9 86.3</cell><cell>96.5 96.8 97.2 97.2</cell><cell>99.0 99.2 99.3 99.3</cell><cell>0.544 0.129 0.526 0.126 0.514 0.121 0.513 0.120</cell></row><row><cell></cell><cell>DnC-3k</cell><cell>41.6 (`2.1)</cell><cell>62.3</cell><cell cols="2">45.5 37.2 (`1.8)</cell><cell>59.1</cell><cell>39.8</cell><cell>76.8</cell><cell>86.0</cell><cell>97.3</cell><cell>99.3</cell><cell>0.517 0.119</cell></row><row><cell></cell><cell>DnC-4.5k</cell><cell>41.7 (`2.2)</cell><cell>62.5</cell><cell cols="2">45.9 37.2 (`1.8)</cell><cell>59.3</cell><cell>39.8</cell><cell>76.9</cell><cell>86.1</cell><cell>97.2</cell><cell>99.4</cell><cell>0.509 0.119</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">-regularization, where we select the regularization parameters from a range of 45 logarithmically-spaced values between 10?6 and 10 5 .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We are grateful to Florent Altch?, Bilal Piot, Jean-Bastien Grill, Elena Buchatskaya, and Florian Strub for significant help with reproducing BYOL results; Jeffrey De Fauw for providing the initial code base for Sim-CLR; Carl Doersch, Lucas Beyer, Phillip Isola, and Oriol Vinyals for valuable feedback on the manuscript.</p><p>We perform transfer via linear classification and finetuning on the same set of datasets as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, namely Food-101 <ref type="bibr" target="#b5">[6]</ref>, CIFAR-10/100 <ref type="bibr" target="#b56">[56]</ref>, Birdsnap <ref type="bibr" target="#b3">[4]</ref>, SUN397 <ref type="bibr" target="#b102">[102]</ref>, Stanford Cars <ref type="bibr" target="#b55">[55]</ref>, FGVC Aircraft <ref type="bibr" target="#b65">[65]</ref>, PASCAL VOC 2007 classification task <ref type="bibr" target="#b26">[27]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b18">[19]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b75">[75]</ref>, Caltech-101 <ref type="bibr" target="#b27">[28]</ref> and Oxford 102 Flowers <ref type="bibr" target="#b71">[71]</ref>. As in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, we used the validation sets specified by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC 2007, DTD, and Oxford 102 Flowers. On other datasets, we use the validation examples as test set, and hold out a subset of the training examples as validation set while performing hyperparameter tuning.</p><p>We follow the linear evaluation protocol of <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35]</ref>. We train a regularized multinomial logistic regression classifier on top of the frozen representation without data augmentation. Images are resized to 224 pixels along the shorter side and cropped by the center 224?224 pixels. We minimize the cross-entropy objective using L-BFGS with</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09229</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parametric instance classification for unsupervised visual feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Debiased contrastive learning. NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perfect match: Improved cross-modal embeddings for audio-visual synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Goo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Watching the world go by: Representation learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07990</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Selfsupervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<publisher>Mohammad</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Bernardo Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeanbastien</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Gheshlaghi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14646</idno>
		<title level="m">Bootstrap latent-predictive representations for multitask reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01065</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient visual pretraining with contrastive detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10957</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulent</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noe</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial self-supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features via video and text pair discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05691</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06430</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Selfsupervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01991</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">dian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Multimodal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13916</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">A unified framework for shot type classification based on subject centric lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuekun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04136</idno>
		<title level="m">Curl: Contrastive unsupervised representations for reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Contrastive learning, multi-view redundancy, and linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05659</idno>
		<title level="m">What should not be contrastive in contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Pointcontrast: Unsupervised pre-training for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10985</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Scaling SGD batch size to 32k for imagenet training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Splitbrain autoencoders: Unsupervised learning by crosschannel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Self-supervised visual representation learning from hierarchical grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">What makes instance discrimination good for transfer learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06606</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Unsupervised learning from video with deep neural embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11954</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12355</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byol-1k</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
