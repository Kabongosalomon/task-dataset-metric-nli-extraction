<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Chiley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vithursan</forename><surname>Thangarasa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Samar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hestness</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
						</author>
						<title level="a" type="main">RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces the RevSilo, the first reversible module for bidirectional multi-scale feature fusion. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. Existing reversible methods, however, do not apply to multi-scale feature fusion and are therefore not applicable to a large class of networks. Bidirectional multiscale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks e.g. HRNet  and Efficient-Det (Tan et al., 2020). When paired with highresolution inputs, these networks achieve stateof-the-art results across various computer vision tasks, but training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements cap network size and limit progress. Using reversible recomputation, the RevSilo alleviates memory issues while still operating across resolution scales. Stacking RevSilos, we create RevBiFPN, a fully reversible bidirectional feature pyramid network. For classification, RevBiFPN is competitive with networks such as EfficientNet while using up to 19.8x lesser training memory. When fine-tuned on COCO, RevBiFPN provides up to a 2.5% boost in AP over HRNet using fewer MACs and a 2.4x reduction in training-time memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art (SOTA) computer vision (CV) networks have large memory requirements that complicate training and limit scalability. <ref type="bibr" target="#b60">Tan &amp; Le (2019)</ref> and <ref type="bibr">Doll?r et al. (2021)</ref> show how compound scaling, i.e. scaling input resolution, network width, and depth, results in efficient networks across a wide range of parameter and MAC counts.  Even when resources are optimally allocated, scaling networks produces large feature maps thus training requires large amount of accelerator memory <ref type="figure">(Figure 1)</ref>. While lowresolution intermediate representations work well for classification tasks <ref type="bibr" target="#b24">(LeCun et al., 1998;</ref><ref type="bibr" target="#b22">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b55">Simonyan &amp; Zisserman, 2015;</ref><ref type="bibr" target="#b60">Tan &amp; Le, 2019)</ref>, dense prediction tasks, such as detection and segmentation, require the construction of spatially informative, high-resolution feature maps which further exacerbates memory issues.</p><p>U-Net <ref type="bibr">(Ronneberger et al., 2015)</ref>, used for segmentation, was one of the first multi-scale feature fusion networks. Initially, detection networks would perform multi-scale inference by processing every scale of an image pyramid independently, but they soon adopted multi-scale feature fusion to directly produce a feature pyramid, i.e. multi-scale features <ref type="bibr" target="#b27">(Lin et al., 2017a;</ref><ref type="bibr" target="#b46">Redmon &amp; Farhadi, 2018)</ref>. Bidirectional multi-scale feature fusion networks iteratively merge information between high and low resolution feature maps, producing robust <ref type="bibr">(Hendrycks &amp; Dietterich, 2019)</ref> scale invariant models. These models promote local and global coherence by iteratively aligning the semantic representations of fine-grained and high-level features. As a result, these networks are often the backbone of SOTA computer vision systems <ref type="bibr" target="#b29">(Liu et al., 2018;</ref><ref type="bibr" target="#b6">Cai &amp; Vasconcelos, 2018;</ref><ref type="bibr" target="#b57">Sun et al., 2019a;</ref><ref type="bibr">Tan et al., 2020)</ref>, but the memory requirements of backpropagating through multi-scale feature fusion complicate training and limit scalability. Training arXiv:2206.14098v1 <ref type="bibr">[cs.</ref>LG] 28 Jun 2022</p><p>RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network CV networks pushes the memory bounds of modern hardware with hardware memory setting a hard limit on how far researchers scale these networks, enforcing an upper bound on network performance.</p><p>Hidden activations are needed to compute the gradient of the loss with respect to a neural network's parameters. Traditionally, the activations computed during the forward pass are cached for use in the backward pass. While this method of neural network training has worked well in the past, the growth of neural networks has outpaced increases in accelerator memory. Motivated by flow structures <ref type="bibr">(Dinh et al., 2017;</ref><ref type="bibr" target="#b18">Kingma &amp; Dhariwal, 2018)</ref>, <ref type="bibr">Gomez et al. (2017)</ref> recognized that if a network is designed using a series of invertible operations the activations can be recomputed during the backward pass. Using this paradigm, reversible networks perform "backpropagation without storing activations" <ref type="bibr">(Gomez et al., 2017)</ref>, reducing their activation memory complexity with respect to depth from linear to constant. Although reversible structures have been successfully used in image classification <ref type="bibr">(Gomez et al., 2017;</ref><ref type="bibr">Jacobsen et al., 2018)</ref> and language modeling <ref type="bibr" target="#b20">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b33">MacKay et al., 2018)</ref>, they have yet to be used where they are needed most: in multi-scale feature fusion networks to produce high-resolution feature maps. Appendix A provides further context for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>To address the memory challenges of training models for spatially sensitive tasks, this work introduces the RevSilo and the network built with it, RevBiFPN. The main contributions of this work are:</p><p>1. The RevSilo <ref type="figure" target="#fig_1">(Figure 2</ref>), the first bidirectional multi-scale feature fusion module that is invertible. <ref type="figure">(Figure 3</ref>) is the first fully reversible bidirectional multi-scale feature fusion pyramid network. It is built using the RevSilo and uses a fraction of the memory when compared to the same network without reversible recomputation (Figures 1, 4 to 6 and 12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RevBiFPN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">With a classification head, RevBiFPN is pretrained on</head><p>ImageNet <ref type="bibr">(Deng et al., 2009</ref>) to accuracies competitive with networks designed specifically for classification ( <ref type="figure">Figure 1</ref> and Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>To our knowledge, this work is the first to fine-tune a reversible backbone on downstream CV tasks. With the appropriate heads, RevBiFPN is competitive with similar networks on detection and segmentation tasks while using a fraction of the accelerator memory for training (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Reversible Residual Silo</head><p>The Reversible Residual Silo, or RevSilo, generalizes both affine coupling <ref type="bibr">(Dinh et al., 2014)</ref> and the reversible residual block <ref type="bibr">(Gomez et al., 2017)</ref> to create an invertible module for bidirectional multi-scale feature fusion. <ref type="figure" target="#fig_1">Figure 2</ref> shows two halves of the RevSilo with N = 4 spatial resolutions. The left half communicates information down the feature pyramid and the right sends information up the feature pyramid. g i can be any, potentially parameterized, invertible transformation. In this work, g i is element-wise addition, and therefore its inverse, g ?1 i , is element-wise subtraction for all i. If h i and h j are on the same row, i.e. i % N == j % N , the RevSilo's residual structure requires that the shape of h i equals the shape of h j . Otherwise, F i should transform the shape of its inputs to match the shape of h i . Besides this shape constraint, F i can be any transformation. The RevSilo construct remain invertible even if some inputs are set to 0. Setting h 3 to 0 can, for example, be used to expand an N = 3 feature pyramid into an N = 4 feature pyramid. Appendix B shows the equations for the N = 4 RevSilo depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RevBiFPN</head><p>RevBiFPN uses the RevSilo to create a fully reversible backbone that utilizes bidirectional multi-scale feature fusion and produces a feature pyramid output. Directly using a reversible multi-scale feature fusion module, RevBiFPN circumvents the issues seen in the RevNet and i-RevNet design (Appendix A). The high-level network structure of RevBiFPN is shown in <ref type="figure">Figure 3</ref>. The output feature pyramid can then be used as an input to different task-specific heads (Appendix C.5).</p><p>The network uses the invertible SpaceToDepth stem <ref type="bibr" target="#b49">(Ridnik et al., 2021;</ref><ref type="bibr" target="#b54">Shi et al., 2016;</ref><ref type="bibr">Dinh et al., 2017;</ref><ref type="bibr">Jacobsen et al., 2018)</ref> to initially downsample the input by a factor of 4 and produce c = 4 2 ? 3 = 48 channels. The baseline model (RevBiFPN-S0) uses c 0 = 48, c 1 = 64, c 2 = 80, and c 3 = 160 channels in its N = 4 spatial resolutions. As the network size is increased, the input image channels are duplicated to ensure the network is fully reversible regardless of network width. The rest of the network has a structure similar to HRNet  where transfor-  <ref type="figure">Figure 3</ref>. A RevBiFPN that creates an N = 4 feature pyramid. Given the output feature pyramid, all activations can be recomputed going backwards through the network. The I components are reversible residual blocks. The network builds an N = 4 multi-scale hidden representation using 3 RevSilos and has an extra depth of d = 2 RevSilos for further feature fusion. mations in the same spatial resolution use reversible residual blocks <ref type="bibr">(Gomez et al., 2017)</ref>.</p><p>For simplicity, the network uses the RevSilo variant shown in <ref type="figure">Figure 11</ref> (in Appendix B.1). Here, the F operations independently transform each input and sum them together. The network isn't designed with a specific hardware target in mind and therefore uses the MBConv Block <ref type="bibr">(Howard et al., 2017)</ref>, a building block that efficiently utilizes parameters and multiply-accumulates (MAC). <ref type="bibr">2</ref> The MBConv Block is used for both transformations in the reversible residual blocks and also the F transformations of the RevSilo. Using the MBConv Block in network design produces networks that have fast on inference devices <ref type="bibr">(Howard et al., 2017;</ref><ref type="bibr" target="#b53">Sandler et al., 2018;</ref><ref type="bibr">Howard et al., 2019;</ref><ref type="bibr" target="#b60">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b34">Mehta &amp; Rastegari, 2021)</ref>.</p><p>Within its RevSilos, RevBiFPN upsamples and downsamples features by factors of 2. To upsample a feature by a factor of 2 k , the depthwise convolution of the MBConv Block uses a stride of 1 and a kernel size of 3 or 5; this is then followed by bilinear upsampling. To downsample a feature by a factor of 2 k , the depthwise convolution of the MBConv Block uses a stride of 2 k and a kernel size of 2 k+1 ? 1. As a result, the network uses a diverse set of kernel sizes as suggested by <ref type="bibr" target="#b60">Tan et al. (2019)</ref>. Network parameters are initialized using Kaiming initialization <ref type="bibr">(He et al., 2015)</ref>. Batch Normalization biases are initialized to zero and weights are initialized to one, except the weights of the normalization layer before a residual add which are initialized to zero to promote stability <ref type="bibr" target="#b18">(Kingma &amp; Dhariwal, 2018)</ref>.</p><p>The network uses the MBConv variant with squeeze-excite layers <ref type="bibr" target="#b60">(Tan &amp; Le, 2019)</ref> and the hard-swish non-linearity <ref type="bibr">(Howard et al., 2019)</ref>. The network has larger expansion ratios on the lower resolution streams and uses larger squeeze-excite ratios on the large resolution streams <ref type="bibr" target="#b49">(Ridnik et al., 2021)</ref>. With a classification head, the result-  . Single GPU memory usage with batch size 64 for training RevBiFPN-S0 on ImageNet with and without reversible recomputation (RevRecomp) as depth is scaled. Reversible recomputation decreases the activation memory complexity from linear to constant. These memory savings can be reallocated to scaling network width and input resolution to produce RevBiFPN S1-S6.</p><p>ing network has a parameter and MAC profile similar to common classification networks. RevBiFPN-S0 is then scaled (Appendix C.6) and compared to other network families on the commonly used ImageNet <ref type="bibr">(Deng et al., 2009)</ref> benchmark. The heads used and how the network is scaled are described in Appendices C.5 and C.6 respectively. The reference implementation can be found at https: //github.com/CerebrasResearch/RevBiFPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Memory Savings</head><p>The activation memory complexity of training a CV network is O(nchwd) where n is the batch size, c is the number of channels representing the network's width, h and w specify the input resolution, and d denotes the depth of the network. By decoupling depth from activation memory requirements, reversible networks have an activation memory complexity of O(nchw). <ref type="figure" target="#fig_3">Figure 4</ref> shows the measured memory usage of the RevBiFPN-S0 network as the network depth is scaled with and without reversible recomputation. This demonstrates that measured memory usage is approximately constant when reversible recomputation is used but increases linearly otherwise. Appendix C.4 shows how memory scales as the input resolution is scaled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The RevBiFPN family of networks is pretrained on Ima-geNet and fine-tuned with task-specific heads for detection and segmentation on MS COCO <ref type="bibr" target="#b26">(Lin et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>The experimental setup for ImageNet classification is in Appendix D.1. Although RevBiFPN-S0 and RevBiFPN-S1 can be trained without reversible recomputation, all of the results are shown for networks trained with reversible recomputation. <ref type="figure" target="#fig_3">Figure 14</ref> (in Appendix D.3) shows that reversible recomputation does not hurt training.  <ref type="table" target="#tab_2">Tables 9 and 10</ref>.</p><p>Detection <ref type="table" target="#tab_12">Table 9</ref> shows that RevBiFPN-S5 achieves an absolute gain of 3.3% in AP over HRNetV2p-W18 trained using the 2x schedule while uses 0.75GB less memory. RevBiFPN-S3 achieves an absolute gain of 2.5% in AP over HRNetV2p-W18 using fewer MACs and a ?2.4x reduction in training-time memory usage and still outperforms HRNetV2p-W18 by 0.7% AP even if it is trained using a 2x  schedule. When the scaled HRNetV2p-W48 is trained using a 2x schedule it uses ?1.6x the memory and still does not outperform RevBiFPN-S6 trained using the 1x schedule.</p><p>Segmentation The overall performance of RevBiFPN-S2 is comparable to HRNetV2p-W18 but uses ?1.2x fewer MACs and ?2.5x less GPU memory during training. RevBiFPN-S6 outperforms HRNetV2p-W32 by 2% Mask AP and 2.4% Bbox AP while using 1.6GB less memory and still outperforms HRNetV2p-W32 when it is trained using the 2x schedule. Generally, RevBiFPN enables larger inputs for training detection and segmentation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Bidirectional multi-scale feature fusion has driven progress in computer vision, but accelerator memory often limits network scale. Reversible methods decrease activation memory complexity but were previously not applicable to bidirectional multi-scale feature fusion. This work introduces the RevSilo, a bidirectional multi-scale feature fusion module that is invertible. The RevSilo is used to design RevBiFPN which is competitive on classification, segmentation, and detection tasks while using a fraction of the memory for training. This makes RevBiFPN applicable to memoryconstrained settings such as high-resolution segmentation and detection and enables SOTA research without needing hardware with the latest memory capacity. Future work is in Appendix E. Multi-Scale Feature Generation eg: VGG, ResNet, EfficientNet, ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Feature Fusion Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background</head><p>Systems using low-resolution features were often applied to image pyramids for detection <ref type="bibr">(Girshick, 2015;</ref><ref type="bibr">Ren et al., 2015;</ref><ref type="bibr" target="#b47">Redmon et al., 2016;</ref><ref type="bibr" target="#b45">Redmon &amp; Farhadi, 2017)</ref>. <ref type="bibr" target="#b27">Lin et al. (2017a)</ref> proposed augmenting a pretrained classification network with a low to high resolution decoder to perform multi-scale feature fusion similar to the U-Net design. Rather than a single high-resolution feature map, the network outputs features from multiple spatial resolutions to create a feature pyramid. The success of the Feature Pyramid Network (FPN) motivated similar methodologies to be used throughout the computer vision community <ref type="bibr" target="#b46">(Redmon &amp; Farhadi, 2018;</ref><ref type="bibr" target="#b3">Bochkovskiy et al., 2020;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr" target="#b28">Lin et al., 2017b;</ref><ref type="bibr">Goyal et al., 2021)</ref>. Bidirectional Feature Pyramid Networks (BiFPNs) further improve performance by iteratively applying multi-scale feature fusion modules <ref type="bibr">(Tan et al., 2020;</ref><ref type="bibr">Ghiasi et al., 2019;</ref><ref type="bibr" target="#b29">Liu et al., 2018;</ref><ref type="bibr" target="#b6">Cai &amp; Vasconcelos, 2018;</ref><ref type="bibr" target="#b10">Chen et al., 2018b</ref>). This allows local information from high-resolution feature maps to be repeatedly fused with global input contexts from low-resolution feature maps <ref type="figure" target="#fig_5">(Figure 7)</ref>.   <ref type="bibr" target="#b21">Kosson et al., 2021)</ref>. Another approach to alleviating accelerator memory usage is to offload activations to host <ref type="bibr" target="#b43">(Rajbhandari et al., 2021)</ref>. However, for bandwidthconstrained systems, this results in poor FLOP utilization. When performing operations with low arithmetic intensity, such as non-linearities or depthwise convolutions <ref type="bibr" target="#b32">(Lu et al., 2021;</ref><ref type="bibr" target="#b41">Qin et al., 2018)</ref>, on GPUs, limited bandwidth from device memory already results in poor FLOP utilization. Offloading activations to host memory uses bandwidth that is even further constrained, exacerbating the issue.</p><p>Volin &amp; Ostrovskii <ref type="formula" target="#formula_1">(1985)</ref> ing activations instead of storing them. This decreases the activation memory complexity from linear to constant. 3 Reversible recomputation enables SOTA research without needing hardware with the latest memory capacity which prolongs the useful life of existing hardware. As a result, less e-waste is produced, but it comes at the cost of recomputing activations which contributes to the carbon footprint of training reversible models. It should be noted that reversible networks relying only reversible residual block (RevBlock) (Gomez et al., 2017) are not fully reversible. RevBlock cannot operate across different dimensionalities, therefore <ref type="bibr">RevNet (Gomez et al., 2017)</ref>, and other networks built using RevBlocks, must cache activations in computational blocks that change tensor shape. Fully reversible models have the added benefit of also being used for generation with Normalizing Flow <ref type="bibr">(Dinh et al., 2014;</ref><ref type="bibr">Germain et al., 2015;</ref><ref type="bibr">Dinh et al., 2017;</ref><ref type="bibr" target="#b19">Kingma et al., 2016;</ref><ref type="bibr" target="#b38">Papamakarios et al., 2017;</ref><ref type="bibr" target="#b18">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr">Huang et al., 2018a;</ref><ref type="bibr">Jacobsen et al., 2018;</ref><ref type="bibr">Keller et al., 2021)</ref> but are often not as efficient. For instance, the injective version of i-RevNet (Jacobsen et al., 2018) is a fully reversible variant of RevNet, but requires a 7x increase in size to match RevNet's performance. Other approaches to reversible recomputation impose architectural limits <ref type="bibr" target="#b1">(Bai et al., 2019)</ref>, limit optimization <ref type="bibr" target="#b2">(Behrmann et al., 2019;</ref><ref type="bibr">Thangarasa et al., 2019)</ref>, or are computationally expensive <ref type="bibr" target="#b2">(Behrmann et al., 2019)</ref>. While any reversible model or method could be used for saving activation memory, none were previously applicable to bidirectional multi-scale feature fusion.</p><p>As existing reversible structures keep tensor dimensionality constant, they cannot be directly applied to multi-scale networks such as EfficientDet or HRNet. One approach to produce high-resolution feature maps would be to apply the reversible residual block <ref type="bibr">(Gomez et al., 2017)</ref> to an entire subnetwork, such as each hourglass of the Stacked Hourglass Network <ref type="bibr" target="#b37">(Newell et al., 2016)</ref>. While feasible, the entire subnetwork of activations would still need to be stored, limiting memory savings (Appendix A.1.1). The specific case of the hourglass design also produces high MAC count networks (Appendix A.1.2) and does not provide bidirectional multi-scale feature fusion with a feature pyramid output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Reversible Stacked Hourglass Networks</head><p>The reversible residual block <ref type="bibr">(Gomez et al., 2017)</ref> is only applicable to networks that have constant hidden dimensionality. Stacked Hourglass <ref type="bibr" target="#b37">(Newell et al., 2016)</ref> networks are built using a stack of hourglass structures that maintain constant dimensionality. Placing each hourglass structure inside a reversible residual block allows the network to produce high-resolution feature maps without the need to store hidden activations. To enable comparisons with RevBiFPN variants, we implement a Fully Reversible Stacked Hourglass Network, RevSHNet. The baseline RevSHNet uses the MBConv Block, a SpaceToDepth stem, channel counts similar to RevBiFPN-S0 channel counts, and a comparable classification head.</p><p>A.1.1. MEMORY Even with reversible recomputation enabled, RevSHNet needs to store an entire hourglass of activations. With an input size of 224, this results in a memory usage increase of about 40% when compared to the RevBiFPN network <ref type="figure" target="#fig_7">(Figure 8)</ref>. When the input size is increased to 288, RevSHNet uses almost twice the memory used by RevBiFPN <ref type="figure" target="#fig_8">(Figure 9</ref>). The increased memory usage limits the available memory savings and ultimately limits how much the network can be scaled.</p><p>A.1.2. COMPUTE COMPLEXITY When RevSHNet is scaled, the produced network has a high compute complexity <ref type="figure" target="#fig_9">(Figure 10</ref>). This is potentially wasteful when scaling to large networks. It should be noted that the above analysis does not take into consideration network performance. Given comparable networks, we expect RevBiFPN to outperform RevSHNet since RevBiFPN has full bidirectional multi-scale feature fusion with a feature pyramid output, whereas RevSHNet does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RevSilo Equations</head><p>The equations for the N = 4 RevSilo in <ref type="figure" target="#fig_1">Figure 2</ref> are:</p><formula xml:id="formula_0">h 4 = h 0 (1) h 5 = g 5 (h 1 , F 5 (h 0 )) (2) h 6 = g 6 (h 2 , F 6 (h 1 , h 0 )) (3) h 7 = g 7 (h 3 , F 7 (h 2 , h 1 , h 0 ))<label>(4)</label></formula><p>followed by:</p><p>h 8 = g 8 (h 4 , F 8 (h 7 , h 6 , h 5 )) (5) h 9 = g 9 (h 5 , F 9 (h 7 , h 6 )) (6) h 10 = g 10 (h 6 , F 10 (h 7 )) (7) h 11 = h 7 (8)</p><p>The corresponding inverse equations are:</p><formula xml:id="formula_1">h 7 = h 11<label>(9)</label></formula><p>h 6 = g ?1 10 (h 10 , F 10 (h 7 )) (10)</p><p>h 5 = g ?1 9 (h 9 , F 9 (h 7 , h 6 )) (11)</p><formula xml:id="formula_2">h 4 = g ?1 8 (h 8 , F 8 (h 7 , h 6 , h 5 ))<label>(12)</label></formula><p>and h 0 = h 4 (13)</p><formula xml:id="formula_3">h 1 = g ?1 5 (h 5 , F 5 (h 0 ))<label>(14)</label></formula><p>h 2 = g ?1 6 (h 6 , F 6 (h 1 , h 0 )) (15)</p><formula xml:id="formula_4">h 3 = g ?1 7 (h 7 , F 7 (h 2 , h 1 , h 0 ))<label>(16)</label></formula><p>For the N = 4 RevSilo, Equations (1) to (8) are used to compute the forward pass. Instead of saving activations, they can be recomputed during the backward pass using <ref type="figure">Figure 11</ref>. A RevSilo using additive coupling. Equations <ref type="formula" target="#formula_1">(9)</ref> to <ref type="formula" target="#formula_4">(16)</ref>. While the inverse equations must be computed in order, the forward equations allow the N hidden tensors of the RevSilo to be computed simultaneously. This enables more parallelism in the resulting inference network. It should also be noted that if h i is a scalar, g i is an addition, and F i is the dot product operation for all i, then the forward equations can be rewritten as matrix-vector products with unitriangular matrices. The underlying structure that makes unitriangular matrices invertible (Thoma, 2013), makes all coupling structures <ref type="bibr" target="#b19">(Kingma et al., 2016;</ref><ref type="bibr">Germain et al., 2015;</ref><ref type="bibr" target="#b38">Papamakarios et al., 2017;</ref><ref type="bibr">Huang et al., 2018a;</ref><ref type="bibr">Dinh et al., 2014;</ref><ref type="bibr">Gomez et al., 2017)</ref> invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. RevSilo with Additive Coupling</head><p>While g i can be any invertible coupling function, this work uses additive coupling as shown in <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Details</head><p>As noted in Section 3, RevBiFPN is structurally similar to HRNet. In this section, RevBiFPN is trained on ImageNet for 150 epoch at an input resolution of 96?96 to ablate architectural design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Down and Up Sampling Operation</head><p>HRNet uses k stride 2 convolution blocks to downsample by 2 k (ld). An alternative downsampling schema would use a single block with stride 2 k and increased kernel size such that the entire input is used to produce the output (sd).</p><p>To upsample feature maps, HRNet uses a 1?1 convolution paired with an upsample operation in 'nearest' mode (su). The 1?1 convolution does not operate in the spatial domain limiting expressivity. To rectify this, the ablations changes the upsampling block to use a 3?3 convolution paired with bilinear upsampling (lu). While replacing ld with sd curbs accuracy on ImageNet, augmenting this change by replacing su with lu results in a total MAC decease of about 8% while having no affect on ImageNet accuracy <ref type="table" target="#tab_7">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Backbone Stem</head><p>Common practice dictates the use of a convolutional stem for neural network design. <ref type="bibr" target="#b49">Ridnik et al. (2021)</ref> propose replacing this with the SpaceToDepth stem. Their work shows this does not effect network accuracy, while increasing the networks GPU throughput performance. <ref type="table" target="#tab_8">Table 4</ref> reaffirms their results and highlights the resulting MAC decrease. <ref type="bibr" target="#b49">Ridnik et al. (2021)</ref> note that Squeeze-Excite, when applied to "low-resolution maps, does not get a large accuracy benefit from the global average pooling operation that SE provides." They advocate for the use of Squeeze-Excite on large as opposed to small spatial resolutions as this provides a good accuracy vs throughput tradeoff. <ref type="table">Table 5</ref> affirms their result by showing how Squeeze-Excite when applied to the low-resolution path leaves the network accuracy relatively unaffected, but when applied to the high resolution path, improves network performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Squeeze-Excite</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Memory as Resolution is Scaled</head><p>When scaling width, batch size or input resolution, networks with and without reversible recomputation have the same complexity, but using reversibility creates a memory offset that enables larger variants to be trained. As an example, <ref type="figure" target="#fig_1">Figure 12</ref> shows the measured memory usage of RevBiFPN-S0 as the resolution is varied. The reversible variant has an advantageous offset and can run resolutions about 4x larger than is possible with a network without reversible recomputation. On a 16GB system, the largest image a network without reversible recomputation can process is just over 2K?2K. With reversibility, the same network can process images with resolutions up to 8K?8K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Network Heads</head><p>While the RevBiFPN backbone is fully reversible, it can be used with non-reversible heads. Note that before each head is applied, a set of MBConv Blocks is used as a neck, with reverse checkpointing, to transform the output channels of RevBiFPN-S0 to 48, 64, 128, and 320. The dimensionality of the neck and heads are scaled using the width multipliers shown in <ref type="table" target="#tab_9">Table 6</ref>. For the detection and segmentation networks, the input resolution is also modified.</p><p>ImageNet classification is used to pretrain the RevBiFPN backbone before it is fine-tuned for object detection and segmentation. The backbone outputs a feature pyramid which is transformed using the neck and non-reversible classification head shown in <ref type="figure">Figure 13</ref>. In the head, the highest resolution feature map is downsampled by a factor of 2 using an MBConv Block with stride 2 and is added to the next largest feature map. This is repeated multiple times until all information is aggregated into the lowest resolution feature map. At this point, a 1 ? 1 convolution is applied, followed by global average pooling and a dense layer. This design is inspired by <ref type="bibr" target="#b57">Sun et al. (2019a)</ref> but uses the MBConv block.</p><p>Object detection and instance segmentation are done with the Faster R-CNN and Mask R-CNN heads provided in MMDetection .  <ref type="formula" target="#formula_0">(4)</ref> and <ref type="formula">(5)</ref>  This work uses these scaling strategies but sets ? = 2/3. While (Doll?r et al., 2021) recommends ? = 4/5, they also show ? = 2/3 is nearly as fast but enables more depth and resolution scaling. Scaling depth gives added memory benefits in the reversible setting (Section 3.1). Given the outputs of the scaling strategy, m w is chosen such that channel counts are multiples of 16, the depth is rounded to the nearest integer, and the resolution is set to a multiple of 2 5 <ref type="table" target="#tab_9">(Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments Setup and Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Classification Setup</head><p>The ImageNet dataset is used to pretrain the networks at different scales before they are fine-tuned on downstream tasks. All RevBiFPN variants are pretrained for 350 epochs using 8 GPUs with a per GPU batch size of 64. SGD is used with a learning rate of 0.1 and momentum of 0.9, and an exponential moving average (EMA) of the network parameters is used with a decay of 0.9999. A 5 epoch learning rate warm-up is used with a starting learning rate of 10 ?3 followed by cosine decay <ref type="bibr" target="#b31">(Loshchilov &amp; Hutter, 2017)</ref>. The 4 Without reversible recomputation, the memory used for activations, n ? c ? h ? w ? d, dominates accelerator memory usage. |RevBiFPN-S6| / |RevBiFPN-S1| = (n ? 6.67c ? 352 ? 352 ? 5)/(n ? 1.33c ? 256 ? 256 ? 2) = 23.7.  <ref type="bibr" target="#b14">(Cubuk et al., 2020)</ref>. Details can be found in Appendix D.2. Tuning the hyperparameters above could result in further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. ImageNet Training and Regularization</head><p>Training is regularized using label smoothing <ref type="bibr" target="#b59">(Szegedy et al., 2016)</ref>, weight decay, dropout <ref type="bibr" target="#b56">(Srivastava et al., 2014)</ref>, stochastic depth <ref type="bibr">(Huang et al., 2016</ref><ref type="bibr">), CutMix (Yun et al., 2019</ref><ref type="bibr">), mixup (Zhang et al., 2018</ref>, and the timm library (Wightman, 2019) variant of RandAugment <ref type="bibr" target="#b14">(Cubuk et al., 2020)</ref>. To prevent larger scales of the network from overfitting, regularization increases with network scale. Without knowing how much augmentation was needed for each network, training began with the regularization shown in <ref type="table">Table 8</ref>.</p><p>When the validation accuracy of the EMA model began to plateau, the regularization of the models was increased. The final regularization used for each network is shown in <ref type="table" target="#tab_10">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Training With and Without Reversibility</head><p>Under infinite precision, training with and without reversible recomputation would produce identical results. <ref type="figure" target="#fig_3">Figure 14</ref> shows that while there are differences using finite precision, these are inconsequential. Training RevBiFPN-S0 with reversible recomputation requires only 2GB of accelerator memory and produces results that are nearly indistinguishable from regular training which consumes 12GB of accelerator memory.  <ref type="bibr">-dev)</ref>. The average precision (AP) metric is adopted, which is the standard COCO evaluation procedure. The multi-level feature representations from RevBiFPN, as shown in <ref type="figure">Figure 3</ref>, are applied for the task of object detection and instance segmentation. There is no additional data augmentation besides the standard horizontal flipping. For training and testing, the input images are resized such that the shorter edge is 800 pixels <ref type="bibr" target="#b27">(Lin et al., 2017a)</ref>. Evaluation is performed using a single image scale.</p><p>RevBiFPN is compared with HRNet and ResNet. The object detection performance is evaluated on COCO minival under the two-stage anchor-based framework, Faster R-CNN <ref type="bibr">(Ren et al., 2015)</ref>. Faster R-CNN models are trained using RevBiFPN, HRNet, and ResNet pretrained backbones on the MMDetection open-source object detection toolbox . <ref type="table" target="#tab_12">Table 9</ref> summarizes parameters, the evaluation MACs per sample, 5 the GPU memory usage during training, and scores. Similar to , the GPU memory usage is measured during training on a 4 GPU system, with an input size of 800?1333 and batch size of 8.  <ref type="formula" target="#formula_1">(2019)</ref> shows how longer fine-tuning schedules can entirely eliminate the benefits of pretraining. This can make a network trained for 100 epochs and fine-tuned with a 2x schedule comparable to a network trained for 350 epochs and fine-tuned with a 1x schedule. Tables 9 and 10 include results from 1x and 2x schedules to enable such comparisons. <ref type="bibr">Tan et al. (2020)</ref> fine-tunes networks for up to 600 epochs. As a result, EfficientDet <ref type="bibr">(Tan et al., 2020)</ref> serves as a strong baseline for work pursuing SOTA results. Being subject to resource constraints, we do not make such comparisons and instead focus on the memory saving, but note that <ref type="bibr">Tan et al. (2020)</ref> shows how longer training schedules even further differentiate networks using bidirectional multi-scale feature fusion.</p><p>Tables 9 and 10 extend the results in <ref type="figure" target="#fig_4">Figures 5 and 6</ref> E. Future Work  <ref type="bibr" target="#b37">(Newell et al., 2016;</ref><ref type="bibr">Zhou et al., 2018)</ref>.</p><p>Tuning a model for inference is an important aspect of a models development and it would be prudent to make device specific optimizations before deploying RevBiFPN to a specific inference platform since GPU speeds does not correlate with speeds on inference devices <ref type="bibr" target="#b34">(Mehta &amp; Rastegari, 2021)</ref>. Using the MBConv building block produces networks which are generally fast on mobile inference platforms <ref type="bibr">(Howard et al., 2017;</ref><ref type="bibr" target="#b53">Sandler et al., 2018)</ref>, but exploring the use of ResNet, ResNeXt, or Transformer blocks could produce networks better optimized for other metrics. The building block could also use 3D convolutions for application to highly memory-intensive 3D workloads. Future work could also explore other architectural modifications such as the use of dilated convolutions <ref type="bibr">(Yu &amp; Koltun, 2016)</ref> or weight sharing for repeated blocks in RevBiFPN to produce more parameter efficient networks. With regards to scaling, future work could investigate alternate strategies and the effects of modifying N , the number of spatial resolutions in the feature pyramid. In general, the architectural design could be improved by Neural Architecture Search <ref type="bibr" target="#b60">(Tan et al., 2019;</ref><ref type="bibr">Ghiasi et al., 2019;</ref><ref type="bibr" target="#b60">Tan &amp; Le, 2019)</ref>.</p><p>Lastly, the RevSilo and RevBiFPN could be applied to other domains. The RevSilo can be used as a reversible multimodal fusion module or for fusing information from multiple input sensors. For instance, self-driving cars use multiple imaging systems. Training these systems can be highly memory-intensive, but using RevSilo to fuse different inputs could provide a memory-efficient solution. Lastly, given RevBiFPN is fully reversible, it can be used for flow-based generation. <ref type="bibr" target="#b0">Awiszus et al. (2020)</ref> argue that multi-scale processing is needed in GAN generation as it provides local and global coherence. Prior to this work, bidirectional multi-scale feature fusion wasn't possible in flow models, but now the RevSilo and RevBiFPN can provide local and global coherence for flow-based generation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ImageNet Model Comparisons</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Presented at the ICML 2022 Workshop on Hardware Aware Efficient Training, Baltimore, Maryland, USA. Copyright 2022 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An N = 4 RevSilo. The left half generalizes the affine coupling block of Dinh et al. (2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Single GPU memory usage with batch size 64 for training RevBiFPN-S0 on ImageNet with and without reversible recomputation (RevRecomp) as depth is scaled. Reversible recomputation decreases the activation memory complexity from linear to constant. These memory savings can be reallocated to scaling network width and input resolution to produce RevBiFPN S1-S6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Instance segmentation results on COCO minival in the Mask R-CNN framework using a 1x learning schedule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Connectivity of multi-scale networks: Features are depicted as boxes and the lines depict the possible connectivity of networks processing features at multiple scales. Networks like VGG<ref type="bibr" target="#b55">(Simonyan &amp; Zisserman, 2015)</ref>,ResNet (He et al., 2016), and EfficientNet<ref type="bibr" target="#b60">(Tan &amp; Le, 2019)</ref> can be used to generate multi-scale features. These features are often fused by networks such as U-Net(Ronneberger et al.,  2015), Mask R-CNN (He et al., 2017), or YOLO<ref type="bibr" target="#b46">(Redmon &amp; Farhadi, 2018)</ref> for completing spatially sensitive tasks. For example, the multi-scale connectivity of U-Net, with high-resolution outputs, is identified using red arrows. Low-resolution features communicate global information, while high-resolution features are able to capture detailed local information such as texture and object boundaries. By iteratively mixing high and low-resolution features, bidirectional multi-scale feature fusion networks such as HRNet, EfficientDet(Tan et al., 2020), and UNet++ (Zhou et al., 2018)  promote local and global coherence, boosting performance in CV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FPNs</head><label></label><figDesc>are often created by using feature fusion modules to augment existing classification networks not originally designed for feature fusion. However, Zhou et al. (2015), Jacobsen et al. (2017), Ke et al. (2017), Huang et al. (2018b), Sun et al. (2019a), Sun et al. (2019b), Wang et al. (2020), Cheng et al. (2020), Fan et al. (2021), and Li et al. (2021) advocate for treating bidirectional multi-scale feature fusion as a first class design principle in computer vision networks and show the effectiveness of this approach for classification, detection, and segmentation. Bidirectional multi-scale feature fusion networks reduce the semantic gap between consecutive feature maps (Zhou et al., 2018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>,Griewank &amp; Walther (2000),Dauvergne &amp; Hasco?t (2006), and<ref type="bibr" target="#b9">Chen et al. (2016)</ref> propose gradient (or reverse) checkpointing where a subset of activations are recomputed instead of being stored. For backpropagation, network activations are needed to compute parameter gradients resulting in an activation memory complexity that is linear with respect to network depth. Checkpointing can reduce this complexity from O(D) to O( ? D).Reversible models(Gomez et al., 2017;<ref type="bibr" target="#b33">MacKay et al., 2018;</ref><ref type="bibr" target="#b5">Br?gger et al., 2019;</ref><ref type="bibr" target="#b39">Pendse et al., 2020;</ref> Yamazaki et al.,  2021;<ref type="bibr" target="#b52">Sander et al., 2021;</ref><ref type="bibr" target="#b13">Chun et al., 2020;</ref><ref type="bibr" target="#b20">Kitaev et al., 2020;</ref> Nestler &amp; Gill, 2021)  save memory by recomput-Memory used by RevBiFPN-S0 and a baseline RevSH-Net as depth is scaled with and without reversible recomputation (RevRecomp).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Recreates Figure 8with an input resolution of 288. This demonstrates how RevBiFPN becomes more favorable as resolution is scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>MACs vs Parameter count of RevBiFPN-S0 and a baseline RevSHNet as depth is scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>The measured activation memory of training a network using a batch size of 16 on a single GPU with and without reversible recomputation (RevRecomp) as the input resolution is scaled. Neck and classification head with feature pyramid input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>of Doll?r et al. (2021) produce a "family of scaling strategies parameterized by ?."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 .</head><label>14</label><figDesc>ImageNet validation accuracy when training RevBiFPN-S0 with and without reversible recomputation (RevRecomp). last 10 epochs use a constant learning rate of 10 ?4 . The network uses batch normalization with a momentum of 0.9 and epsilon of 10 ?3 . The running means and standard deviations are averaged across all accelerators at the end of each epoch. Training is regularized using label smoothing (Szegedy et al., 2016), weight decay, dropout (Srivastava et al., 2014), stochastic depth (Huang et al., 2016), CutMix (Yun et al., 2019), mixup (Zhang et al., 2018), and RandAugment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MACs vs Measured Memory Usage for ImageNet Training on 1 GPU: RevBiFPNs significantly outperform Effi-cientNets at all scales. In particular, RevBiFPN-S6 achieves comparable accuracy (84.2%) to EfficientNet-B7 on ImageNet while using comparable MACs (38.1B) and 19.8x lesser training memory per sample. Details in Table 2 and Table 11 (in Appendix F).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EfficientNet-B7</cell></row><row><cell>RevBiFPN-S4</cell><cell>83.0%</cell><cell>11B</cell><cell>0.11</cell></row><row><cell cols="2">EFFICIENTNET-B5 83.6%</cell><cell>10B</cell><cell>1.44</cell></row><row><cell>RevBiFPN-S5</cell><cell>83.7%</cell><cell>22B</cell><cell>0.23</cell></row><row><cell cols="2">EFFICIENTNET-B6 84.0%</cell><cell>19B</cell><cell>2.61</cell></row><row><cell>RevBiFPN-S6</cell><cell>84.2%</cell><cell>38B</cell><cell>0.25</cell></row><row><cell cols="2">EFFICIENTNET-B7 84.3%</cell><cell>37B</cell><cell>5.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell>B6</cell></row><row><cell></cell><cell>B5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>S4</cell><cell></cell><cell>S5</cell><cell>RevBiFPN-S6</cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>ImageNet accuracy. Extended inTable 11.</figDesc><table><row><cell>MODEL</cell><cell cols="3">PARAMS RES MACS</cell><cell>TOP1</cell></row><row><cell>RevBiFPN-S0</cell><cell cols="2">3.42M 224</cell><cell>0.31B 72.8%</cell></row><row><cell>RevBiFPN-S1</cell><cell cols="2">5.11M 256</cell><cell>0.62B 75.9%</cell></row><row><cell>RevBiFPN-S2</cell><cell cols="2">10.6M 256</cell><cell>1.37B 79.0%</cell></row><row><cell>RevBiFPN-S3</cell><cell cols="2">19.6M 288</cell><cell>3.33B 81.1%</cell></row><row><cell>RevBiFPN-S4</cell><cell cols="2">48.7M 320</cell><cell>10.6B 83.0%</cell></row><row><cell>RevBiFPN-S5</cell><cell cols="2">82.0M 352</cell><cell>21.8B 83.7%</cell></row><row><cell>RevBiFPN-S6</cell><cell cols="2">142.3M 352</cell><cell>38.1B 84.2%</cell></row><row><cell cols="4">Table 2. Training Memory (GB) used per sample.</cell></row><row><cell>MODEL</cell><cell></cell><cell cols="2">INPUT RESOLUTION</cell></row><row><cell></cell><cell></cell><cell>TRAIN RES</cell><cell>224</cell><cell>384</cell></row><row><cell>RevBiFPN-S6</cell><cell></cell><cell cols="2">0.254 0.086 0.291</cell></row><row><cell cols="2">EFFICIENTNET-B7</cell><cell cols="2">5.047 0.673 1.786</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Computer Vision and PatternRecognition, pp.  924-932, 2021.   Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,  D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,  M., Heigold, G., Gelly, S., Uszkoreit, J., and  Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2021. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. Multiscale Vision Transformers. In Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016. He, K., Gkioxari, G., Doll?r, P., and Girshick, R. Mask R-CNN. In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969, 2017. He, K., Girshick, R., and Doll?r, P. Rethinking ImageNet Pre-Training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4918-4927, 2019. Krueger, D., Lacoste, A., and Courville, A. Neural Autoregressive Flows. In International Conference on Machine Learning, pp. 2078-2087. PMLR, 2018a. Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep Networks with Stochastic Depth. In European Conference on Computer Vision, pp. 646-661. Springer, 2016. Huang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., and Weinberger, K. Multi-Scale Dense Networks for Resource Efficient Image Classification. In International Conference on Learning Representations, 2018b. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism. Advances in Neural Information Processing Systems, 32:103-112, 2019. Ioffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning, pp. 448-456. PMLR, 2015. Jacobsen, J.-H., Oyallon, E., Mallat, S., and Smeulders, A. W. Multiscale Hierarchical Convolutional Networks. In International Conference on Machine Learning. PMLR, 2017. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820-2828, 2019. Tan, M., Pang, R., and Le, Q. V. EfficientDet: Scalable and Efficient Object Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10781-10790, 2020. Cord, M., Sablayrolles, A., Synnaeve, G., and Jegou, H. Going Deeper With Image Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 32-42, October 2021. Wu, Y. and He, K. Group Normalization. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 3-19, 2018. Yamazaki, K., Rathour, V. S., and Le, T. Invertible Residual Network with Regularization for Effective Medical Image Segmentation. arXiv preprint arXiv:2103.09042, 2021. Yu, F. and Koltun, V. Multi-Scale Context Aggregation by Dilated Convolutions. In International Conference on Learning Representations, 2016. Yuan, L., Hou, Q., Jiang, Z., Feng, J., and Yan, S. VOLO: Vision Outlooker for Visual Recognition. arXiv preprint arXiv:2106.13112, 2021. Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019.Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.</figDesc><table><row><cell>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6824-6835, October 2021. Germain, M., Gregor, K., Murray, I., and Larochelle, H. 7036-7045, 2019. Girshick, R. Fast R-CNN. In Proceedings of the IEEE international conference on computer vision, pp. 1440-1448, 2015. Gomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The Reversible Residual Network: Backpropagation Without Storing Activations. In Proceedings of the 31st Inter-national Conference on Neural Information Processing Systems, pp. 2211-2221, 2017. Goyal, A., Bochkovskiy, A., Deng, J., and Koltun, V. Non-deep Networks. arXiv preprint arXiv:2110.07641, 2021. Griewank, A. and Walther, A. Algorithm 799: Revolve: An Implementation of Checkpointing for the Reverse or Adjoint Mode of Computational Differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1): 19-45, 2000. He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034, 2015. Huang, C.-W., of Tao, A., Sapra, K., and Catanzaro, B. Hierarchical Multi-Network Robustness to Common Corruptions and Pertur-bations. In International Conference on Learning Repre-sentations, 2019. Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., Le, Q. V., and Adam, H. Searching for MobileNetV3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314-1324, 2019. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. MobileNets: Efficient Convolutional Neural Net-works for Mobile Vision Applications. arXiv preprint arXiv:1704.04861, 2017. Scale Attention for Semantic Segmentation. arXiv preprint arXiv:2005.10821, 2020. Thangarasa, V., Tsai, C.-Y., Taylor, G. W., and K?ster, U. Reversible Fixup Networks for Memory-Efficient Train-ing. In NeurIPS Systems for ML (SysML) Workshop, 2019. Thoma, M. Solving Equations of Lower Unitriangu-lar Matrices. https://martin-thoma.com/ solving-equations-of-unipotent-lower-triangular-matrices/, 2013. Touvron, H., Wightman, R. PyTorch Image Models. https: //github.com/rwightman/pytorch-image-He, K., Hendrycks, D. and Dietterich, T. Benchmarking Neural models, 2019.</cell></row></table><note>Dauvergne, B. and Hasco?t, L. The Data-Flow Equations of Checkpointing in Reverse Automatic Differentiation. In International Conference on Computational Science, pp. 566-573. Springer, 2006. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei- Fei, L. ImageNet: A Large-Scale Hierarchical Image Database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255. IEEE, 2009. Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-Linear In- dependent Components Estimation. In International Con- ference on Learning Representations Workshop, 2014. Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density Esti- mation using Real NVP. In International Conference on Learning Representations, 2017. Doll?r, P., Singh, M., and Girshick, R. Fast and Accurate Model Scaling. In Proceedings of the IEEE/CVF Con- ference onMade: Masked Autoencoder for Distribution Estimation. In International Conference on Machine Learning, pp. 881-889. PMLR, 2015. Ghiasi, G., Lin, T.-Y., and Le, Q. V. NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object De- tection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.Jacobsen, J.-H., Smeulders, A., and Oyallon, E. i-RevNet: Deep Invertible Networks. In International Conference on Learning Representations, 2018.Venigalla, A., Kosson, A., Chiley, V., and K?ster, U. Adap- tive Braking for Mitigating Gradient Delay. In Interna- tional Conference on Machine Learning Deep Learning Workshop, 2020. Volin, Y. M. and Ostrovskii, G. M. Automatic Computation of Derivatives With the Use of The Multilevel Differen- tiating Technique-1. Algorithmic Basis. Computers &amp; Mathematics with Applications, 11(11):1099-1114, 1985. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., Liu, W., and Xiao, B. Deep High-Resolution Representation Learning for Visual Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.Yang, B., Zhang, J., Li, J., R?, C., Aberger, C., and De Sa, C. PipeMare: Asynchronous Pipeline Parallel DNN Train- ing. Proceedings of Machine Learning and Systems, 3, 2021.mixup: Beyond Empirical Risk Minimization. In Inter- national Conference on Learning Representations, 2018. Zhang, W., Gupta, S., Lian, X., and Liu, J. Staleness-Aware Async-SGD for Distributed Deep Learning. In Proceed- ings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI'16, pp. 2350-2356. AAAI Press, 2016. ISBN 9781577357704. Zhou, Y., Hu, X., and Zhang, B. Interlinked Convolutional Neural Networks for face parsing. In International sym- posium on neural networks, pp. 222-231. Springer, 2015. Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., and Liang, J. UNet++: A Nested U-Net Architecture for Medical Image Segmentation. In Deep Learning in Medical Image analysis and Multimodal Learning for Clinical Decision Support, pp. 3-11. Springer, 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Down &amp; Up Sampling Operation's influence on Accuracy.</figDesc><table><row><cell cols="2">DOWN / UP SAMPLING PARAMS MACS</cell><cell>TOP1</cell></row><row><cell>LD / SU</cell><cell cols="2">3.49M 75.7M 61.5%</cell></row><row><cell>SD / SU</cell><cell cols="2">3.28M 67.2M 60.8%</cell></row><row><cell>SD / LU</cell><cell cols="2">3.47M 69.5M 61.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Stem's influence on Accuracy.</figDesc><table><row><cell>STEM</cell><cell>PARAMS MACS</cell><cell>TOP1</cell></row><row><cell>CONVOLUTIONAL</cell><cell cols="2">3.49M 75.7M 61.5%</cell></row><row><cell>SPACETODEPTH</cell><cell cols="2">3.49M 73.7M 61.5%</cell></row><row><cell cols="3">Table 5. Squeeze-Excite's influence on Accuracy.</cell></row><row><cell cols="2">SQUEEZE-EXCITE PARAMS MACS</cell><cell>TOP1</cell></row><row><cell>NONE</cell><cell cols="2">3.40M 75.5M 61.3%</cell></row><row><cell>LOW-RES PATH</cell><cell cols="2">3.49M 75.7M 61.4%</cell></row><row><cell>HIGH-RES PATH</cell><cell cols="2">3.46M 76.1M 61.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Network width multiplier (mw), depth (d), and input height and width (h and w) of RevBiFPN variants trained on Ima-geNet at different scales. Without reversibility, the training setup would need to be modified to accommodate scales past RevBiFPN-S1. Reversible recomputation allows us to train RevBiFPN-S6 with an activation set that is 24? larger than that of RevBiFPN-S1 4 .</figDesc><table><row><cell>MODEL</cell><cell>mw</cell><cell cols="2">d h AND w</cell></row><row><cell>RevBiFPN-S0</cell><cell>1</cell><cell>2</cell><cell>224</cell></row><row><cell cols="3">RevBiFPN-S1 1.33 2</cell><cell>256</cell></row><row><cell>RevBiFPN-S2</cell><cell>2</cell><cell>2</cell><cell>256</cell></row><row><cell cols="3">RevBiFPN-S3 2.67 3</cell><cell>288</cell></row><row><cell>RevBiFPN-S4</cell><cell>4</cell><cell>4</cell><cell>320</cell></row><row><cell cols="3">RevBiFPN-S5 5.33 4</cell><cell>352</cell></row><row><cell cols="3">RevBiFPN-S6 6.67 5</cell><cell>352</cell></row><row><cell>C.6. Network Scaling</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Once the baseline network is designed, scaling the input</cell></row><row><cell cols="4">resolution, network width, and network depth generally</cell></row><row><cell cols="4">results in better performance. Classically, networks such as</cell></row><row><cell cols="4">VGG (Simonyan &amp; Zisserman, 2015) and ResNet (He et al.,</cell></row><row><cell cols="4">2016) focus on scaling network depth. Tan &amp; Le (2019)</cell></row><row><cell cols="4">shows how compound scaling, i.e. scaling all dimensions,</cell></row><row><cell cols="4">results in efficient networks across a range of parameter and</cell></row><row><cell cols="4">MAC counts. Doll?r et al. (2021) shows how to scale such</cell></row><row><cell cols="4">that the network run-time is minimized for large networks.</cell></row><row><cell>Equations</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Weight decay (WD), dropout, number of RandAugment ops applied (N), mixup, CutMix, and stochastic depth used at the end of training. Label smoothing uses a coefficient of 0.1 and RandAugment uses a magnitude of 9 and mstd of 0.5.</figDesc><table><row><cell>MODEL</cell><cell>WD</cell><cell cols="5">DROPOUT N MIXUP CUTMIX STOCHASTIC DEPTH</cell></row><row><cell cols="2">RevBiFPN-S0 4 ? 10 ?5</cell><cell>0.25</cell><cell>2</cell><cell>0.00</cell><cell>0.0</cell><cell>0.00</cell></row><row><cell cols="2">RevBiFPN-S1 4 ? 10 ?5</cell><cell>0.25</cell><cell>2</cell><cell>0.00</cell><cell>0.0</cell><cell>0.00</cell></row><row><cell cols="2">RevBiFPN-S2 4 ? 10 ?5</cell><cell>0.3</cell><cell>3</cell><cell>0.00</cell><cell>0.0</cell><cell>0.00</cell></row><row><cell cols="2">RevBiFPN-S3 4 ? 10 ?5</cell><cell>0.3</cell><cell>3</cell><cell>0.10</cell><cell>1.0</cell><cell>0.05</cell></row><row><cell cols="2">RevBiFPN-S4 2 ? 10 ?5</cell><cell>0.4</cell><cell>4</cell><cell>0.10</cell><cell>1.0</cell><cell>0.10</cell></row><row><cell cols="2">RevBiFPN-S5 2 ? 10 ?5</cell><cell>0.4</cell><cell>4</cell><cell>0.20</cell><cell>1.0</cell><cell>0.10</cell></row><row><cell cols="2">RevBiFPN-S6 2 ? 10 ?5</cell><cell>0.6</cell><cell>5</cell><cell>0.20</cell><cell>1.0</cell><cell>0.30</cell></row><row><cell cols="3">Table 8. Initial dropout, number of RandAugment ops applied (N).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Training initially uses a weight decay of 4 ? 10 ?5 , label smooth-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ing coefficient of 0.1, the RandAugment magnitude is set to 9,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">and mstd is set to 0.5, and the network is trained without mixup,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CutMix, or stochastic depth.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODEL</cell><cell cols="2">DROPOUT N</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S0</cell><cell>0.25</cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S1</cell><cell>0.25</cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S2</cell><cell>0.25</cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S3</cell><cell>0.25</cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S4</cell><cell>0.4</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S5</cell><cell>0.4</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RevBiFPN-S6</cell><cell>0.5</cell><cell>5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">D.4. MS COCO Experiments Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Experimental results are presented on the MS COCO 2017</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">detection dataset, which contains about 118k images for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">training, 5k for validation (minival) and ?20k testing</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">without provided annotations (test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Object detection results on COCO minival in the Faster R-CNN framework. LS = learning schedule. 1x = 12 epochs, 2x = 24 epochs. Mem = per sample GPU memory used during training.</figDesc><table><row><cell>BACKBONE</cell><cell>PARAMS</cell><cell>MACS</cell><cell>MEM LS</cell><cell>AP AP50 AP75 APS APM APL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 extends</head><label>11</label><figDesc>Table 1. This enable comparisons of RevB-iFPN variants to other state of the art networks trained on ImageNet1k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 .</head><label>11</label><figDesc>Models trained using only ImageNet1k. While most networks are trained using 300 to 400 epochs, HRNet and RegNetY use a 100 epoch training schedule.</figDesc><table><row><cell>EFFICIENTNET-B0</cell><cell></cell><cell>5.3M</cell><cell>224 224</cell><cell>0.39B</cell><cell>77.1%</cell></row><row><cell>EFFICIENTNET-B1</cell><cell></cell><cell>7.8M</cell><cell>240 240</cell><cell>0.70B</cell><cell>79.1%</cell></row><row><cell>EFFICIENTNET-B2</cell><cell></cell><cell>9.2M</cell><cell>260 260</cell><cell>1.0B</cell><cell>80.1%</cell></row><row><cell>EFFICIENTNET-B3</cell><cell>(TAN &amp; LE, 2019)</cell><cell>12M</cell><cell>300 300</cell><cell>1.8B</cell><cell>81.6%</cell></row><row><cell>EFFICIENTNET-B4</cell><cell></cell><cell>19M</cell><cell>380 380</cell><cell>4.2B</cell><cell>82.9%</cell></row><row><cell>EFFICIENTNET-B5</cell><cell></cell><cell>30M</cell><cell>456 456</cell><cell>9.9B</cell><cell>83.6%</cell></row><row><cell>EFFICIENTNET-B6</cell><cell></cell><cell>43M</cell><cell>528 528</cell><cell>19B</cell><cell>84.0%</cell></row><row><cell>EFFICIENTNET-B7</cell><cell></cell><cell>66M</cell><cell>600 600</cell><cell>37B</cell><cell>84.3%</cell></row><row><cell>EFFICIENTNET-B5</cell><cell>(CUBUK ET AL., 2020)</cell><cell>30M</cell><cell>456 456</cell><cell>9.9B</cell><cell>83.9%</cell></row><row><cell>EFFICIENTNET-B7</cell><cell></cell><cell>66M</cell><cell>600 600</cell><cell>37B</cell><cell>85.0%</cell></row><row><cell>EFFICIENTNETV2-S</cell><cell></cell><cell>24M</cell><cell>128 -300 300</cell><cell>8.8B</cell><cell>83.9%</cell></row><row><cell>EFFICIENTNETV2-M</cell><cell>(TAN &amp; LE, 2021)</cell><cell>55M</cell><cell>128 -380 380</cell><cell>24B</cell><cell>85.1%</cell></row><row><cell>EFFICIENTNETV2-L</cell><cell></cell><cell>121M</cell><cell>128 -380 380</cell><cell>53B</cell><cell>85.7%</cell></row><row><cell>NFNET-F0</cell><cell></cell><cell>72.0M</cell><cell>192 256</cell><cell>12B</cell><cell>83.6%</cell></row><row><cell>NFNET-F1</cell><cell></cell><cell>133M</cell><cell>224 320</cell><cell>36B</cell><cell>84.7%</cell></row><row><cell>NFNET-F2</cell><cell>(BROCK ET AL., 2021)</cell><cell>194M</cell><cell>256 352</cell><cell>63B</cell><cell>85.1%</cell></row><row><cell>NFNET-F3</cell><cell></cell><cell>255M</cell><cell>320 416</cell><cell>115B</cell><cell>85.7%</cell></row><row><cell>NFNET-F4</cell><cell></cell><cell>316M</cell><cell>384 512</cell><cell>215B</cell><cell>85.9%</cell></row><row><cell>NFNET-F5</cell><cell></cell><cell>377M</cell><cell>416 544</cell><cell>290B</cell><cell>86.0%</cell></row><row><cell>VOLO-D1</cell><cell></cell><cell>27M</cell><cell>224 384</cell><cell>22.8B</cell><cell>85.2%</cell></row><row><cell>VOLO-D2</cell><cell></cell><cell>59M</cell><cell>224 384</cell><cell>46.1B</cell><cell>86.0%</cell></row><row><cell>VOLO-D3</cell><cell>(YUAN ET AL., 2021)</cell><cell>86M</cell><cell>224 448</cell><cell>67.9B</cell><cell>86.3%</cell></row><row><cell>VOLO-D4</cell><cell></cell><cell>193M</cell><cell>224 448</cell><cell>197B</cell><cell>86.8%</cell></row><row><cell>VOLO-D5</cell><cell></cell><cell>269M</cell><cell>224 448</cell><cell>304B</cell><cell>87.0%</cell></row><row><cell>VOLO-D5</cell><cell></cell><cell>269M</cell><cell>224 512</cell><cell>412B</cell><cell>87.1%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Cerebras Systems, Sunnyvale, CA, USA. Correspondence to: Vitaliy Chiley &lt;vitaliy@cerebras.net&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ML researchers often use FLOP when they mean MAC. This work uses MAC to mean multiply-accumulate since FLOP means a single floating point operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When training in layer pipeline mode<ref type="bibr" target="#b40">(P?trowski et al., 1993;</ref><ref type="bibr" target="#b21">Kosson et al., 2021)</ref>, activation complexity is quadratic with respect to depth. While gradient checkpointing decreases activation memory complexity from O(D 2 ) to O(D 1.5 ) (Yang et al., 2021), reversible recomputation decreases it to O(D).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Reversible networks recompute activations during the backward pass instead of storing the activations computed in the forward pass. The reconstruction error of recomputation, the impact of numerical precision on this error, and the effect this has on optimization are all potential research directions. Along with RevBiFPN's sensitivity to numerical precision, research could also look at RevBiFPN's sensitivity to normalizers, network sparsification methods, adversarial perturbations, and delayed gradient optimization along with its mitigation methods<ref type="bibr" target="#b9">(Zhang et al., 2016;</ref> Venigalla et al.,  2020). Additionally, RevBiFPN generates entire feature pyramids at multiple semantic levels making it amenable to5  The tool used to analyze the evaluation MACs per sample for the various models can be found here: https://github.com/open-mmlab/mmcv/blob/ master/mmcv/cnn/utils/flops counter.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Shreyas Saxena, Nolan Dey, and Valentina Popescu for their help and comments that improved the manuscript. We also thank Ben Wang, Ross Wightman, Lucas Nestler, Jan XMaster, Alexander Mattick, Atli Kosson, Abhinav Venigalla, Xin Wang, Vinay Rao, and Kenyon (Chuan-Yung) Tsai for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coherent Style Level Generation From a Single Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awiszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toad-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Equilibrium Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolov4</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<title level="m">Optimal Speed and Accuracy of Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-Performance Large-Scale Image Recognition Without Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Br?gger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving into High Quality Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cascade R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02839</idno>
		<title level="m">Efficient and Robust Parallel DNN Training Through Model Parallelism on Multi-GPU Platform</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmdetection</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open MMLab Detection Toolbox and Benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Training Deep Nets with Sublinear Memory Cost. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online Normalization for Training Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sharapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samaniego De La Fuente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
		<title level="m">Momentum-Net: Fast and Convergent Iterative Neural Network for Inverse Problems. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RandAugment: Practical Automated Data Augmentation with a Reduced Search Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CoAtNet: Marrying Convolution and Attention for All Data Sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04803</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multigrid Neural Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6665" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>Generative Flow with Invertible 1x1 Convolutions</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Variational Inference with Inverse Autoregressive FLOW. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pipelined Backpropagation at Scale: Training Large Models without Batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>Smola, A., Dimakis, A., and Stoica, I.</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="479" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Labatie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luschi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03743</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved Multiscale Vision Transformers for Classification and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01526</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context. In European conference on computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Path Aggregation Network for Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic Gradient Descent with Warm Restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimizing Depthwise Separable Convolution Operations on GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reversible Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mobilevit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<title level="m">General-purpose, and Mobile-friendly Vision Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Light-weight</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP 2019)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Homebrewnlp</surname></persName>
		</author>
		<ptr target="https://github.com/HomebrewNLP/HomebrewNLP" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Masked Autoregressive Flow for Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">;</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Memory Efficient 3D U-Net with Reversible Mobile Inverted Bottlenecks for Brain Tumor Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pendse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thangarasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holmdahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BrainLes@MICCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance Analysis of a Pipelined Backpropagation Parallel Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>P?trowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Girault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="970" to="81" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diagonalwise Refactorization: An Efficient Training Method for Depthwise Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing Network Design Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zero-Infinity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07857</idno>
		<title level="m">Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Is Batch Norm Unique? An Empirical Investigation and Prescription to Emulate the Best Properties of Common Normalizers without Batch Dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolov3</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">An Incremental Improvement. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faster R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High Performance GPU-Dedicated Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Momentum Residual Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ablin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9276" to="9287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-Resolution Representations for Labeling Pixels and Regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnetv2</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m">Smaller Models and Faster Training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno>55M 135.12B 0.84GB 1X 31.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings RevBiFPN-S0 19</title>
		<meeting>RevBiFPN-S0 19</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">RevBiFPN-S3 30</title>
	</analytic>
	<monogr>
		<title level="m">40M 180.99B 1.31GB 1X</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<title level="m">48M 196.18B 3.13GB 1X</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<title level="m">Table 10. Instance segmentation and object detection results on COCO minival in the Mask R-CNN framework. BACKBONE PARAMS MACS MEM LS MASK BBOX AP APS APM APL AP APS APM APL RevBiFPN</title>
		<imprint/>
	</monogr>
	<note>S0 22.19M 188.20B 0.87GB 1X 29</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B ; Dosovitskiy Et</forename><surname>Vit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Swin-S (liu Et</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Coatnet-2 ; Dai Et</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Touvron</surname></persName>
		</author>
		<idno>CAIT-S-24</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrnet-W40-C (</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Radosavovic</surname></persName>
		</author>
		<idno>REGNETY-4.0GF</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
