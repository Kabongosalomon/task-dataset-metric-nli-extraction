<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 TOWARDS FASTER AND STABILIZED GAN TRAINING FOR HIGH-FIDELITY FEW-SHOT IMAGE SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
							<email>bingchen.liu@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Playform -Artrendex Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
							<email>yizhe.zhu@rutgers.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
							<email>kunpeng.song@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Playform -Artrendex Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
							<email>elgammal@artrendex.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Playform -Artrendex Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 TOWARDS FASTER AND STABILIZED GAN TRAINING FOR HIGH-FIDELITY FEW-SHOT IMAGE SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024 ? 1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains 1 , we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited. 1  The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch 1 arXiv:2101.04775v1 [cs.CV] 12 Jan 2021</p><p>Published as a conference paper at ICLR 2021 Moreover, It was shown that in most cases artists want to train their models with datasets of less than 100 images <ref type="bibr" target="#b9">(Elgammal et al., 2020)</ref>. Dynamic data-augmentation (Karras et al., 2020a; smooths the gap and stabilizes GAN training with fewer images. However, the computing cost from the SOTA models such as StyleGAN2 (Karras et al., 2020b)  and BigGAN (Brock et al.,  2018)  remain to be high, especially when trained with the image resolution on 1024 ? 1024.</p><p>In this paper, our goal is to learn an unconditional GAN on high-resolution images, with low computational cost and few training samples. As summarized in <ref type="figure">Fig. 2, these</ref> training conditions expose the model to a high risk of overfitting and mode-collapse <ref type="bibr" target="#b31">Zhang &amp; Khoreva, 2018)</ref>. To train a GAN given the demanding training conditions, we need a generator G that can learn fast, and a discriminator D that can continuously provide useful signals to train G. To address these challenges, we summarize our contribution as:</p><p>? We design the Skip-Layer channel-wise Excitation (SLE) module, which leverages lowscale activations to revise the channel responses on high-scale feature-maps. SLE allows a more robust gradient flow throughout the model weights for faster training. It also leads to an automated learning of a style/content disentanglement like StyleGAN2.</p><p>? We propose a self-supervised discriminator D trained as a feature-encoder with an extra decoder. We force D to learn a more descriptive feature-map covering more regions from an input image, thus yielding more comprehensive signals to train G. We test multiple selfsupervision strategies for D, among which we show that auto-encoding works the best.</p><p>? We build a computational-efficient GAN model based on the two proposed techniques, and show the model's robustness on multiple high-fidelity datasets, as demonstrated in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Few training samples Low VRAM High image resolution Short training time Training settings Requirements Small batch-size Small model size Overfitting and mode-collapse Risks</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The fascinating ability to synthesize images using the state-of-the-art (SOTA) Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref> display a great potential of GANs for many intriguing real-life applications, such as image translation, photo editing, and artistic creation. However, expensive computing cost and the vast amount of required training data limit these SOTAs in real applications with only small image sets and low computing budgets.</p><p>In real-life scenarios, the available samples to train a GAN can be minimal, such as the medical images of a rare disease, a particular celebrity's portrait set, and a specific artist's artworks. Transferlearning with a pre-trained model <ref type="bibr" target="#b16">(Mo et al., 2020;</ref><ref type="bibr" target="#b29">Wang et al., 2020)</ref> is one solution for the lack of training images. Nevertheless, there is no guarantee to find a compatible pre-training dataset. Furthermore, if not, fine-tuning probably leads to even worse performance . In a recent study, it was highlighted that in art creation applications, most artists prefers to train their models from scratch based on their own images to avoid biases from fine-tuned pre-trained model. Speed up the GAN training: Speeding up the training of GAN has been approached from various perspectives. <ref type="bibr">Ngxande et al.</ref> propose to reduce the computing time with depth-wise convolutions. <ref type="bibr">Zhong et al. adjust</ref> the GAN objective into a min-max-min problem for a shorter optimization path. <ref type="bibr">Sinha et al. suggest</ref> to prepare each batch of training samples via a coreset selection, leverage the better data preparation for a faster convergence. However, these methods only bring a limited improvement in training speed. Moreover, the synthesis quality is not advanced within the shortened training time.</p><p>Train GAN on high resolution: High-resolution training for GAN can be problematic. Firstly, the increased model parameters lead to a more rigid gradient flow to optimize G. Secondly, the target distribution formed by the images on 1024 ? 1024 resolution is super sparse, making GAN much harder to converge. <ref type="bibr" target="#b7">Denton et al. (2015)</ref>; <ref type="bibr" target="#b32">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b32">Huang et al. (2017)</ref>; ; <ref type="bibr">Karras et al. (2019)</ref>; <ref type="bibr">Karnewar &amp; Wang (2019)</ref>; <ref type="bibr">Karras et al. (2020b)</ref>; <ref type="bibr" target="#b11">Liu et al. (2020a)</ref> develop the multi-scale GAN structures to alleviate the gradient flow issue, where G outputs images and receives feedback from several resolutions simultaneously. However, all these approaches further increase the computational cost, consuming even more GPU memory and training time.</p><p>Stabilize the GAN training: Mode-collapse on G is one of the big challenges when training GANs. And it becomes even more challenging given fewer training samples and a lower computational budget (a smaller batch-size). As D is more likely to be overfitting on the datasets, thus unable to provide meaningful gradients to train <ref type="bibr">G (Gulrajani et al., 2017)</ref>.</p><p>Prior works tackle the overfitting issue by seeking a good regularization for D, including different objectives <ref type="bibr">Lim &amp; Ye, 2017;</ref><ref type="bibr" target="#b25">Tran et al., 2017)</ref>; regularizing the gradients <ref type="bibr">(Gulrajani et al., 2017;</ref><ref type="bibr" target="#b14">Mescheder et al., 2018)</ref>; normalizing the model weights <ref type="bibr" target="#b15">(Miyato et al., 2018)</ref>; and augmenting the training data <ref type="bibr">(Karras et al., 2020a;</ref>. However, the effects of these methods degrade fast when the training batch-size is limited, since appropriate batch statistics can hardly be calculated for the regularization (normalization) over the training iterations.</p><p>Meanwhile, self-supervision on D has been shown to be an effective method to stabilize the GAN training as studied in <ref type="bibr" target="#b26">Tran et al. (2019)</ref>; <ref type="bibr" target="#b5">Chen et al. (2019)</ref>. However, the auxiliary self-supervision tasks in prior works have limited using scenario and image domain. Moreover, prior works only studied on low resolution images (32 2 to 128 2 ), and without a computing resource limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We adopt a minimalistic design for our model. In particular, we use a single conv-layer on each resolution in G, and apply only three (input and output) channels for the conv-layers on the high resolutions (? 512 ? 512) in both G and D. <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4</ref> illustrate the model structure for our G and D, with descriptions of the component layers and forward flow. These structure designs make our GAN much smaller than SOTA models and substantially faster to train. Meanwhile, our model remains robust on small datasets due to its compact size with the two proposed techniques. The forward flow of our Generator Image <ref type="figure">Figure 3</ref>: The structure of the skip-layer excitation module and the Generator. Yellow boxes represent feature-maps (we show the spatial size and omit the channel number), blue box and blue arrows represent the same up-sampling structure, red box contains the SLE module as illustrated on the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SKIP-LAYER CHANNEL-WISE EXCITATION</head><p>For synthesizing higher resolution images, the generator G inevitably needs to become deeper, with more conv-layers, in concert with the up-sampling needs. A deeper model with more convolution layers leads to a longer training time of GAN, due to the increased number of model parameters and a weaker gradient flow through G <ref type="bibr" target="#b32">(Zhang et al., 2017;</ref><ref type="bibr">Karras et al., 2017;</ref><ref type="bibr">Karnewar &amp; Wang, 2019)</ref>. To better train a deep model, He et al. design the Residual structure (ResBlock), which uses a skip-layer connection to strengthen the gradient signals between layers. However, while ResBlock has been widely used in GAN literature <ref type="bibr">Karras et al., 2020b)</ref>, it also increases the computation cost.</p><p>We reformulate the skip-connection idea with two unique designs into the Skip-Layer Excitation module (SLE). First, ResBlock implements skip-connection as an element-wise addition between the activations from different conv-layers. It requires the spatial dimensions of the activations to be the same. Instead of addition, we apply channel-wise multiplications between the activations, eliminating the heavy computation of convolution (since one side of the activations now has a spatial dimension of 1 2 ). Second, in prior GAN works, skip-connections are only used within the same resolution. In contrast, we perform skip-connection between resolutions with a much longer range (e.g., 8 2 and 128 2 , 16 2 and 256 2 ), since an equal spatial-dimension is no longer required. The two designs make SLE inherits the advantages of ResBlock with a shortcut gradient flow, meanwhile without an extra computation burden.</p><p>Formally, we define the Skip-Layer Excitation module as:</p><formula xml:id="formula_0">y = F(x low , {W i }) ? x high<label>(1)</label></formula><p>Here x and y are the input and output feature-maps of the SLE module, the function F contains the operations on x low , and W i indicates the module weights to be learned. The left panel in <ref type="figure">Fig. 3</ref> shows an SLE module in practice, where x low and x high are the feature-maps at 8 ? 8 and 128 ? 128 resolution respectively. An adaptive average-pooling layer in F first down-samples x low into 4 ? 4 along the spatial-dimensions, then a conv-layer further down-samples it into 1 ? 1. A LeakyReLU is used to model the non-linearity, and another conv-layer projects x low to have the same channel size as x high . Finally, after a gating operation via a Sigmoid function, the output from F multiplies x high along the channel dimension, yielding y with the same shape as x high .</p><p>SLE partially resembles the Squeeze-and-Excitation module (SE) proposed by Hu et al.. However, SE operates within one feature-map as a self-gating module. In comparison, SLE performs between feature-maps that are far away from each other. While SLE brings the benefit of channel-wise feature re-calibration just like SE, it also strengthens the whole model's gradient flow like ResBlock. The channel-wise multiplication in SLE also coincides with Instance Normalization <ref type="bibr" target="#b27">(Ulyanov et al., 2016;</ref><ref type="bibr">Huang &amp; Belongie, 2017)</ref>, which is widely used in style-transfer. Similarly, we show that SLE enables G to automatically disentangle the content and style attributes, just like <ref type="bibr">StyleGAN (Karras et al., 2019)</ref>. As SLE performs on high-resolution feature-maps, altering these feature-maps is shown to be more likely to change the style attributes of the generated image <ref type="bibr">(Karras et al., 2019;</ref><ref type="bibr" target="#b11">Liu et al., 2020a)</ref>. By replacing x low in SLE from another synthesized sample, our G can generate an image with the content unchanged, but in the same style of the new replacing image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SELF-SUPERVISED DISCRIMINATOR</head><p>Our approach to provide a strong regularization for D is surprisingly simple. We treat D as an encoder and train it with small decoders. Such auto-encoding training forces D to extract image features that the decoders can give good reconstructions. The decoders are optimized together with D on a simple reconstruction loss, which is only trained on real samples:</p><formula xml:id="formula_1">L recons = E f ?D encode (x), x?I real [||G(f ) ? T (x)||],<label>(2)</label></formula><p>where f is the intermediate feature-maps from D, the function G contains the processing on f and the decoder, and the function T represents the processing on sample x from real images I real .  <ref type="figure">Figure 4</ref>: The structure and the forward flow of the Discriminator. Blue box and arrows represent the same residual down-sampling structure, green boxes mean the same decoder structure.</p><p>Our self-supervised D is illustrated in <ref type="figure">Fig. 4</ref>, where we employ two decoders for the feature-maps on two scales: f 1 on 16 2 and f 2 on 8 2 . The decoders only have four conv-layers to produce images at 128?128 resolution, causing little extra computations (much less than other regularization methods).</p><p>We randomly crop f 1 with 1 8 of its height and width, then crop the real image on the same portion to get I part . We resize the real image to get I. The decoders produce I part from the cropped f 1 , and I from f 2 . Finally, D and the decoders are trained together to minimize the loss in eq. 2, by matching I part to I part and I to I.</p><p>Such reconstructive training makes sure that D extracts a more comprehensive representation from the inputs, covering both the overall compositions (from f 2 ) and detailed textures (from f 1 ). Note that the processing in G and T are not limited to cropping; more operations remain to be explored for better performance. The auto-encoding approach we employ is a typical method for self-supervised learning, which has been well recognized to improve the model robustness and generalization ability <ref type="bibr">(He et al., 2020;</ref><ref type="bibr">Hendrycks et al., 2019;</ref><ref type="bibr">Jing &amp; Tian, 2020;</ref><ref type="bibr" target="#b24">Goyal et al., 2019)</ref>. In the context of GAN, we find that a regularized D via self-supervision training strategies significantly improves the synthesis quality on G, among which auto-encoding brings the most performance boost.</p><p>Although our self-supervision strategy for D comes in the form of an auto-encoder (AE), this approach is fundamentally different from works trying to combine <ref type="bibr">GAN and AE (Larsen et al., 2016;</ref><ref type="bibr">Guo et al., 2019;</ref><ref type="bibr" target="#b34">Zhao et al., 2016;</ref><ref type="bibr" target="#b3">Berthelot et al., 2017)</ref>. The latter works mostly train G as a decoder on a learned latent space from D, or treat the adversarial training with D as an supplementary loss besides AE's training. In contrast, our model is a pure GAN with a much simpler training schema. The auto-encoding training is only for regularizing D, where G is not involved.</p><p>In sum, we employ the hinge version of the adversarial loss (Lim &amp; Ye (2017); <ref type="bibr" target="#b25">Tran et al. (2017)</ref>) to iteratively train our D and G. We find the different GAN losses make little performance difference, while hinge loss computes the fastest:</p><formula xml:id="formula_2">L D = ? E x?I real [min(0, ?1 + D(x))] ? Ex ?G(z) [min(0, ?1 ? D(x)] + L recons (3) L G = ? E z?N [D(G(z))],<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>Datasets:</p><p>We conduct experiments on multiple datasets with a wide range of content categories. On 256 ? 256 resolution, we test on Animal-Face Dog and Cat <ref type="bibr" target="#b23">(Si &amp; Zhu, 2011)</ref>, 100-Shot-Obama, Panda, and Grumpy-cat . On 1024 ? 1024 resolution, we test on Flickr-Face-HQ (FFHQ) (Karras et al., 2019), Oxford-flowers <ref type="bibr" target="#b18">(Nilsback &amp; Zisserman, 2006)</ref>, art paintings from WikiArt (wikiart.org), photographs on natural landscape from Unsplash (unsplash.com), Pokemon (pokemon.com), anime face, skull, and shell. These datasets are designed to cover images with different characteristics: photo realistic, graphic-illustration, and art-like images.</p><p>Metrics: We use two metrics to measure the models' synthesis performance: 1) Fr?chet Inception Distance (FID) (Heusel et al., 2017) measures the overall semantic realism of the synthesized images. For datasets with less than 1000 images (most only have 100 images), we let G generate 5000 images and compute FID between the synthesized images and the whole training set. 2) Learned perceptual similarity (LPIPS)  provides a perceptual distance between two images. We use LPIPS to report the reconstruction quality when we perform latent space back-tracking on G given real images, and measure the auto-encoding performance. We find it unnecessary to involve other metrics, as FID is unlikely to be inconsistent with the others, given the notable performance gap between our model and the compared ones. For all the testings, we train the models 5 times with random seeds, and report the highest scores. The relative error is less than five percent on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Models:</head><p>We compare our model with: 1) the state-of-the-art (SOTA) unconditional model, StyleGAN2, 2) a baseline model ablated from our proposed one. Note that we adopt Style-GAN2 with recent studies from <ref type="bibr">(Karras et al., 2020a;</ref>, including the model configuration and differentiable data-augmentation, for the best training on few-sample datasets. Since StyleGAN2 requires much more computing-cost (cc) to train, we derive an extra baseline model. In sum, we compare our model with StyleGAN2 on the absolute image synthesis quality regardless of cc, and use the baseline model for the reference within a comparable cc range.</p><p>The baseline model is the strongest performer that we integrated from various GAN techniques based on DCGAN <ref type="bibr" target="#b21">(Radford et al., 2015)</ref>: 1) spectral-normalization <ref type="bibr" target="#b15">(Miyato et al., 2018)</ref>, 2) exponentialmoving-average <ref type="bibr" target="#b30">(Yaz?c? et al., 2018</ref>) optimization on G, 3) differentiable-augmentation, 4) GLU (Dauphin et al., 2017) instead of ReLU in G. We build our model upon the baseline with the two proposed techniques: the skip-layer excitation module and the self-supervised discriminator.  <ref type="bibr" target="#b20">(Paszke et al., 2017)</ref>. Importantly, the slimed StyleGAN2 with 1 4 parameters cannot converge on the tested datasets at 1024 2 resolution. We compare to the StyleGAN2 with 1 2 parameters (if not specifically mentioned) in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE SYNTHESIS PERFORMANCE</head><p>Few-shot generation: Collecting large-scale image datasets are expensive, or even impossible, for a certain character, a genre, or a topic. On those few-shot datasets, a data-efficient model becomes especially valuable for the image generation task. In <ref type="table" target="#tab_3">Table. 2 and Table.</ref> 3, we show that our model not only achieves superior performance on the few-shot datasets, but also much more computationalefficient than the compared methods. We save the checkpoints every 10k iterations during training and report the best FID from the checkpoints (happens at least after 15 hours of training for Style-GAN2 on all datasets). Among the 12 datasets, our model performs the best on 10 of them.</p><p>Please note that, due to the VRAM requirement for StyleGAN2 when trained on 1024 2 resolution, we have to train the models in <ref type="table">Table.</ref> 3 on a RTX TITAN GPU. In practice, 2080-TI and TITAN share a similar performance, and our model runs the same time on both GPUs. Training from scratch vs. fine-tuning: Fine-tuning from a pre-trained GAN <ref type="bibr" target="#b16">(Mo et al., 2020;</ref><ref type="bibr" target="#b19">Noguchi &amp; Harada, 2019;</ref><ref type="bibr" target="#b29">Wang et al., 2020)</ref> has been the go-to method for the image generation task on datasets with few samples. However, its performance highly depends on the semantic consistency between the new dataset and the available pre-trained model. According to Zhao et al., fine-tuning performs worse than training from scratch in most cases, when the content from the new dataset strays away from the original one. We confirm the limitation of current fine-tuning methods from <ref type="table" target="#tab_3">Table. 2 and Table.</ref> 3, where we fine-tune StyleGAN2 trained on FFHQ use the Freeze-D method from Mo et al.. Among all the tested datasets, only Obama and Skull favor the fine-tuning method, making sense since the two sets share the most similar contents to FFHQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Module ablation study:</head><p>We experiment with the two proposed modules in <ref type="table">Table.</ref> 2, where both SLE (skip) and decoding-on-D (decode) can separately boost the model performance. It shows that the two modules are orthogonal to each other in improving the model performance, and the self-supervised D makes the biggest contribution. Importantly, the baseline model and StyleGAN2 diverge fast after the listed training time. In contrast, our model is less likely to mode collapse among the tested datasets. Unlike the baseline model which usually model-collapse after trained for 10 hours, our model maintains a good synthesis quality and won't collapse even after trained for 20 hours. We argue that it is the decoding regularization on D that prevents the model from divergence.     with a batch-size of 8 on a single 2080-Ti GPU. Specifically, for FFHQ with all 70000 images, we train our model with a larger batch-size of 32, to reflect an optimal performance of our model.</p><p>In this test, we follow the common practice of computing FID by generating 50k images and use the whole training set as the reference distribution. Note that StyleGAN2 has more than double the parameters compared to our model, and trained with a much larger batch-size on FFHQ. These factors contribute to its better performances when given enough training samples and computing power. Meanwhile, our model keeps up well with StyleGAN2 across all testings with a considerably lower computing budget, showing a compelling performance even on larger-scale datasets, and a consistent performance boost over the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative results:</head><p>The advantage of our model becomes more clear from the qualitative comparisons in <ref type="figure" target="#fig_3">Fig. 6</ref>. Given the same batch-size and training time, StyleGAN2 either converges slower or suffers from mode collapse. In contrast, our model consistently generates satisfactory images. Note that the best results from our model on Flower, Shell, and Pokemon only take three hours' training, and for the rest three datasets, the best performance is achieved at training for eight hours. For StyleGAN2 on "shell", "anime face", and "Pokemon", the images shown in <ref type="figure" target="#fig_3">Fig. 6</ref> are already from the best epoch, which they match the scores in <ref type="table" target="#tab_3">Table. 2 and Table.</ref> 3. For the rest of the datasets, the quality increase from StyleGAN2 is also limited given more training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MORE ANALYSIS AND APPLICATIONS</head><p>Testing mode collapse with back-tracking: From a well trained GAN, one can take a real image and invert it back to a vector in the latent space of G, thus editing the image's content by altering the back-tracked vector. Despite the various back-tracking methods <ref type="bibr" target="#b38">(Zhu et al., 2016;</ref><ref type="bibr">Lipton &amp; Tripathi, 2017;</ref><ref type="bibr" target="#b0">Abdal et al., 2019)</ref>, a well generalized G is arguably as important for the good inversions. To this end, we show that our model, although trained on limited image samples, still gets a desirable performance on real image back-tracking.</p><p>In <ref type="table" target="#tab_6">Table 5</ref>, we split the images from each dataset with a training/testing ratio of 9:1, and train G on the training set. We compute a reconstruction error between all the images from the testing set and   <ref type="bibr" target="#b21">Radford et al., 2015;</ref><ref type="bibr" target="#b22">Robb et al., 2020)</ref>.</p><p>In addition, we show qualitative comparisons in appendix D, where our model maintains a good generation while StyleGAN2 and baseline are model-collapsed.</p><p>The self-supervision methods and generalization ability on D: Apart from the auto-encoding training for D, we show that D with other common self-supervising strategies also boost GAN's performance in our training settings. We test five self-supervision settings, as shown in <ref type="table" target="#tab_7">Table 6</ref>, which all brings a substantial performance boost compared to the baseline model. Specifically, setting-a refers to contrastive learning which we treat each real image as a unique class and let D classify them. For setting-b, we train D to predict the real image's original aspect-ratio since they are reshaped to square when fed to D. Setting-c is the method we employ in our model, which trains D as an encoder with a decoder to reconstruct real images. To better validate the benefit of self-supervision on D, all the testings are conducted on full training sets with 10000 images, with a batch-size of 8 to be consistent with <ref type="table" target="#tab_5">Table 4</ref>. We also tried training with a larger batch-size of 16, which the results are consistent to the batch-size of 8.</p><p>Interestingly, according to <ref type="table" target="#tab_7">Table 6</ref>, while setting-c performs the best, combining it with the rest two settings lead to a clear performance downgrade. The similar behavior can be found on some other self-supervision settings, e.g. when follow <ref type="bibr" target="#b5">Chen et al. (2019)</ref> with a "rotation-predicting" task on art-paintings and FFHQ datasets, we observe a performance downgrade even compared to the baseline model. We hypothesis the reason being that the auto-encoding forces D to pay attention to more areas of the input image, thus extracts a more comprehensive feature-map to describe the input image (for a good reconstruction). In contrast, a classification task does not guarantee D to cover the whole image. Instead, the task drives D to only focus on small regions because the model can find class cues from small regions of the images. Focusing on limited regions (i.e., react to limited image patterns) is a typical overfitting behavior, which is also widely happening for D in vanilla GANs. More discussion can be found in appendix B.</p><p>Style mixing like StyleGAN. With the channel-wise excitation module, our model gets the same functionality as StyleGAN: it learns to disentangle the images' high-level semantic attributes (style and content) in an unsupervised way, from G's conv-layers at different scales. The style-mixing results are displayed in <ref type="figure" target="#fig_4">Fig. 7</ref>, where the top three datasets are 256 ? 256 resolution, and the bottom three are 1024 ? 1024 resolution. While StyleGAN2 suffers from converging on the bottom high-resolution datasets, our model successfully learns the style representations along the channel dimension on the "excited" layers (i.e., for feature-maps on 256?256, 512?512 resolution). Please refer to appendix A and C for more information on SLE and style-mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduce two techniques that stabilize the GAN training with an improved synthesis quality, given sub-hundred high-fidelity images and a limited computing resource. On thirteen datasets with a diverse content variation, we show that a skip-layer channel-wise excitation mechanism (SLE) and a self-supervised regularization on the discriminator significantly boost the synthesis performance of GAN. Both proposed techniques require minor changes to a vanilla GAN, enhancing GAN's practicality with a desirable plug-and-play property. We hope this work can benefit downstream tasks of GAN <ref type="bibr" target="#b13">(Liu et al., 2020c;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b8">Elgammal et al., 2017)</ref> and provide new study perspectives for future research.  attention to all the regions of an input image, and encode the image with a minimum information lost, then the decoder is easier to reconstruct the images encoded by D. In contrast, if D is overfitting and only focus on limited local patterns of the images, the it outputs feature-maps with lost information, thus a decoder is unable to reconstruct the images from D's output feature-map.</p><p>We extract the second-last layer's activation for the decoder, which is the one for D to determine the real/fake of an image. <ref type="table" target="#tab_9">Table 7</ref> shows the result, where we train all the decoders for the same 100000 iterations (all the decoders are converged). Note that such feature-extracting performance on D does not necessarily imply a better synthesis performance for G. Moreover, the D from StyleGAN2 is not comparable to the D from baseline, since they have totally different model structure and complexity.</p><p>Instead, according to <ref type="table" target="#tab_9">Table 7</ref>, we can get some interesting information. Firstly, the GAN training is actually making D performs worse as a feature-encoder. According to row. 3 (StyleGAN2) and row. 4 (baseline), we find that the D after a GAN training extracts less meaningful features compared to a randomly initialized D (col. 6 and col. 8). It means that while the GAN training leads D to find the discriminative features between the real and fake samples, it also effective let D to ignore quite amount of information from the input images.</p><p>Secondly, we compare the baseline model to the ones with self-supervised learning guidance <ref type="bibr">(row. 4,5,6,7)</ref>. It shows that the self-supervisions on D indeed lead to a more descriptive feature-extraction compared to the randomly initialization on D. Moreover, contrastive learning may also result in overfitting, since only a partial image (some local patterns) may be enough for the classification task. In comparison, the reconstruction task is more likely to let D cover more information from the input images. To our surprise, combining auto-encoding training and the contrastive learning result in a worse performance on D. It shows that the classification objective affects the auto-encoding objective and changes the behavior of D, in a negative way.</p><p>Last but not least, we do find that a better feature-extracting performance on D result in a better synthesis performance of GAN . And it seems true for both StyleGAN2 and the baseline model. For StyleGAN2 trained on FFHQ, D trained with more data indeed preserves more information from the input images than D trained on only 1000 images. For our baseline model, the feature-extracting performance on D aligns well with the respective FID scores. Besides, the self-supervision methods all effectively letting D extracts more information from the images, compared to the randomly initialization and the vanilla GAN training.   <ref type="figure" target="#fig_0">Fig. 11</ref> shows the results from the 1000 samples training on Art paintings and FFHQ at 1024 ? 1024 resolution, and the 100 samples Obama at 256 ? 256 resolution. In each row, we swap the x low in the SLE layer from the image in col. 1 to the one from each image on row. 1. The best style-mixing results is achieved when the feature-map swapping is done on all resolutions. And the most effective layer that causes the most style changes is the layer on 128 resolution. On 256 ? 256 resolution, the model behaviors the same, where the SLE on lower resolution makes the most style difference.</p><p>On the Art-paintings data, the model performs well on style-mixing, where not only the coloring but also the texture can be controlled. The models transfers the style of flat or pointy brush stroke among the style-mixed synthetic images. However, the model does not perform as well on the FFHQ data. There are some cases where even the hair color can not be properly transferred. We speculate that the worse performance on FFHQ is due to the limited training sample and the dramatically varied background. The is no clear relationship between the front-end face and the background contents given the limited training samples, which confuses the model to disentangle more detailed style attributes. In contrast, Art-paintings have consistent style cues within each image and obvious connections between each object inside a scene, making it arguably easier than the FFHQ data. On the other hand, the model performs great on Obama given a even less 100 training images. It successfully transfers the style for both the face attributes and the background. Learning on 256 2 resolution is a simpler task, and the model capacity is more sufficient on only 100 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MORE QUALITATIVE COMPARISON</head><p>(a) (b) (c) <ref type="figure" target="#fig_0">Figure 10</ref>: Comparison between our model and the baseline For each dataset, the images are generated by the same set of randomly sampled noises. Images from our model is shown on the left, and the baseline results are on the right. All the model are trained for 50000 iterations with batch size of 8, which is more than enough for both models to converge. On (a) Grumpy-cat and (b) Panda, baseline model shows a clear mode collapse, while our model is generating diverse images; on (c) Animalface-dog, although not mode collapse, the baseline model shows a clear quality disadvantage compared to our model.    In <ref type="table" target="#tab_11">Table 8</ref>, we report the average LPIPS score between the generated samples from our model to their closest real samples ranked by LPIPS score. In comparison, we show the baseline as the LPIPS between real images and their randomly augmented variants (randomly horizontal flipping and random cropping with 0.8 spatial portion). We run each experiment 3 times with 100 randomly synthesized samples or real images, and report the lowest one. The std among the trials are usually lower than 0.005. This experiment shows that, instead of memorizing the real images in the training set, our model is able to perceive the features from the real images, and generate images that are different and novel, in terms of compositions, shapes, and color patterns. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Synthetic results on 1024 2 resolution of our model, trained from scratch on single RTX 2080-Ti GPU, with only 1000 images. Left: 20 hours on Nature photos; Right: 10 hours on FFHQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The causes and challenges for training GAN in our studied conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Latent space back-tracking and interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison between our model and StyleGAN2 on 1024 2 resolution datasets. The left-most panel shows the training images, and the right two panels show the uncurated samples from StyleGAN2 and our model. Both models are trained from scratch for 10 hours with a batch-size of 8. The samples are generated from the checkpoint with the lowest FID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Style-mixing results from our model trained for only 5 hours on single GPU. their inversions from G, after the same update of 1000 iterations on the latent vectors (to prevent the vectors from being far off the normal distribution). The baseline model's performance is getting worse with more training iterations, which reflects mode-collapse on G. In contrast, our model gives better reconstructions with consistent performance over more training iterations.Fig. 5presents the back-tracked examples (left-most and right-most samples in the middle panel) given the real images. The smooth interpolations from the back-tracked latent vectors also suggest little mode-collapse of our G (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Style-mixing results by swapping the features for SLE on different resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>StyleGAN2 results during training We show the results of the slimed StyleGAN2 at half channel numbers. StyleGAN2 converges much slower than our model on dataset (a) Pokemon and (c) Shell, and mode collapsed on (b) Anime-Face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Nearest real images to the synthesized ones trained on 1000 images For each pair of images, the left is the synthesized image from our model, and the right image is the closest image found from the real training data ranked by LPIPS score. The samples are uncurated, and our model is able to create new contents that well fitted to the training domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Nearest real images to the synthesized ones trained on 100 images For each image pair, the left is the synthesized image from our model, and the right is the closest image found from the real training data ranked by LPIPS score. Even with only 100 training samples, these uncurated samples show our model is still able to combine the features learned from the real samples and synthesize new compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Reconstruction results from the decoder for training the auto-encoding discriminator. For each dataset, the first panel shows the augmented real images during training, the second panel shows the reconstruction on full image, and the last panel shows the reconstruction on random cropped portions of the full image. Add the reconstructions are done on 128 ? 128 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Computational cost comparison of the models.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">StyleGAN2@0.25 StyleGAN2@0.5 StyleGAN2 Baseline</cell><cell>Ours</cell></row><row><cell>Resolution: 256 2 Batch-size: 8</cell><cell>Training time (hour / 10k iter) Training vram (GB) Model parameters (million)</cell><cell>1 7 27.557</cell><cell>1.8 16 45.029</cell><cell>3.8 18 108.843</cell><cell cols="2">0.7 5 44.359 47.363 1 6.5</cell></row><row><cell>Resolution: 1024 2 Batch-size: 8</cell><cell>Training time (hour / 10k iter) Training vram (GB) Model parameters (million)</cell><cell>3.6 12 27.591</cell><cell>5 23 45.15</cell><cell>7 36 109.229</cell><cell cols="2">1.3 9 44.377 47.413 1.7 10</cell></row></table><note>Table. 1 presents the normalized cc figures of the models on Nvidia's RTX 2080-Ti GPU, imple- mented using PyTorch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>FID comparison at 256 2 resolution on few-sample datasets.</figDesc><table><row><cell></cell><cell cols="2">Image number</cell><cell>389</cell><cell>160</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell></cell><cell>20 hour</cell><cell>StyleGAN2 StyleGAN2 finetune</cell><cell>58.85 61.03</cell><cell>42.44 46.07</cell><cell cols="2">46.87 12.06 35.75 14.5</cell><cell>27.08 29.34</cell></row><row><cell>Training time on one RTX 2080-Ti</cell><cell>5 hour</cell><cell>Baseline Baseline+Skip Baseline+decode</cell><cell>108.19 94.21 56.25</cell><cell>150.3 72.97 36.74</cell><cell cols="2">62.74 52.50 14.39 15.4 44.34 10.12</cell><cell>42.13 38.17 29.38</cell></row><row><cell></cell><cell></cell><cell>Ours (B+Skip+decode)</cell><cell>50.66</cell><cell>35.11</cell><cell cols="2">41.05 10.03</cell><cell>26.65</cell></row></table><note>Animal Face -Dog Animal Face -Cat Obama Panda Grumpy-cat</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>FID comparison at 1024 2 resolution on few-sample datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Art Paintings FFHQ Flower Pokemon Anime Face</cell><cell>Skull</cell><cell>Shell</cell></row><row><cell></cell><cell cols="2">Image number</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>800</cell><cell>120</cell><cell>100</cell><cell>60</cell></row><row><cell>Training time on one</cell><cell>24 hour</cell><cell>StyleGAN2 StyleGAN2 finetune</cell><cell>74.56 N/A</cell><cell>25.66 N/A</cell><cell>45.23 36.72</cell><cell>190.23 60.12</cell><cell cols="2">152.73 127.98 241.37 61.23 107.68 220.45</cell></row><row><cell>RTX TITAN</cell><cell>8 hour</cell><cell>Baseline Ours</cell><cell>62.27 45.08</cell><cell>38.35 24.45</cell><cell>42.25 25.66</cell><cell>67.86 57.19</cell><cell cols="2">101.23 186.45 202.32 59.38 130.05 155.47</cell></row></table><note>Training with more images: For more thorough evaluation, we also test our model on datasets with more sufficient training samples, as shown in Table. 4. We train the full StyleGAN2 for around five days on the Art and Photograph dataset with a batch-size of 16 on two TITAN RTX GPUs, and use the latest official figures on FFHQ from Zhao et al.. Instead, we train our model for only 24 hours,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>FID comparison at 1024 2 resolution on datasets with more images. 48.36 41.23 18.38 10.45 7.86 4.4 67.12 41.47 39.05 Baseline 60.02 51.23 49.38 36.45 27.86 25.12 17.62 71.47 66.05 62.28 Ours 44.57 43.27 42.53 19.01 17.93 16.45 12.38 52.47 45.07 43.65</figDesc><table><row><cell>Model</cell><cell>Dataset</cell><cell cols="2">Art Paintings</cell><cell></cell><cell></cell><cell>FFHQ</cell><cell></cell><cell cols="3">Nature Photograph</cell></row><row><cell></cell><cell>Image number 2k</cell><cell>5k</cell><cell>10k</cell><cell>2k</cell><cell>5k</cell><cell>10k</cell><cell>70k</cell><cell>2k</cell><cell>5k</cell><cell>10k</cell></row><row><cell cols="3">StyleGAN2 70.02 Panda</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Obama</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FFHQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shell</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Art</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real image</cell><cell cols="3">Interpolation between back-tracked images</cell><cell>Real image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>LPIPS of back-tracking with G</figDesc><table><row><cell></cell><cell>Cat</cell><cell>Dog</cell><cell>FFHQ Art</cell></row><row><cell>Resolution</cell><cell></cell><cell>256</cell><cell>1024</cell></row><row><cell cols="4">Baseline @ 20k iter 2.113 2.073 2.589</cell><cell>2.916</cell></row><row><cell cols="4">Baseline @ 40k iter 2.513 2.171 2.583</cell><cell>2.812</cell></row><row><cell>Ours @ 40k iter</cell><cell cols="3">1.821 1.918 2.425</cell><cell>2.624</cell></row><row><cell>Ours @ 80k iter</cell><cell cols="3">1.897 1.986 2.342</cell><cell>2.601</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>FID of self-supervisions for D</figDesc><table><row><cell></cell><cell cols="2">Art paintings Nature photos</cell></row><row><cell>a. contrastive loss</cell><cell>47.14</cell><cell>57.04</cell></row><row><cell>b. predict aspect ratio</cell><cell>49.21</cell><cell>59.22</cell></row><row><cell>c. auto-encoding</cell><cell>42.53</cell><cell>43.65</cell></row><row><cell>d. a+b</cell><cell>46.02</cell><cell>54.23</cell></row><row><cell>e. a+b+c</cell><cell>44.21</cell><cell>47.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in neural information processing systems, pp. 5767-5777, 2017. Yong Guo, Qi Chen, Jian Chen, Qingyao Wu, Qinfeng Shi, and Mingkui Tan. Auto-embedding generative adversarial networks for high resolution image synthesis. Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7132-7141, 2018. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. Animesh Karnewar and Oliver Wang. Msg-gan: multi-scale gradient gan for stable image synthesis. arXiv preprint arXiv:1903.06048, 2019. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4401-4410, 2019. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020a.</figDesc><table><row><cell>IEEE Transactions on Mul-</cell></row><row><cell>timedia, 21(11):2726-2737, 2019.</cell></row><row><cell>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-</cell></row><row><cell>nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.</cell></row><row><cell>770-778, 2016.</cell></row><row><cell>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for</cell></row><row><cell>unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.</cell></row><row><cell>Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning</cell></row><row><cell>can improve model robustness and uncertainty. In Advances in Neural Information Processing</cell></row><row><cell>Systems, pp. 15663-15674, 2019.</cell></row><row><cell>Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.</cell></row><row><cell>Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances</cell></row><row><cell>in neural information processing systems, pp. 6626-6637, 2017.</cell></row><row><cell>Jie Hu, 1501-</cell></row><row><cell>1510, 2017.</cell></row><row><cell>Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative</cell></row><row><cell>adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern</cell></row><row><cell>recognition, pp. 5077-5086, 2017.</cell></row><row><cell>Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks:</cell></row><row><cell>A survey. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-</cell></row><row><cell>ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b.</cell></row><row><cell>Anders Boesen Lindbo Larsen, S?ren Kaae S?nderby, Hugo Larochelle, and Ole Winther. Autoen-</cell></row><row><cell>coding beyond pixels using a learned similarity metric. In International conference on machine</cell></row><row><cell>learning, pp. 1558-1566. PMLR, 2016.</cell></row></table><note>Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance nor- malization. In Proceedings of the IEEE International Conference on Computer Vision, pp.Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017. Zachary C Lipton and Subarna Tripathi. Precise recovery of latent vectors from generative adver- sarial networks. arXiv preprint arXiv:1702.04782, 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>LPIPS on D's feature-extracting performance</figDesc><table><row><cell></cell><cell cols="2">Grumpy Cat Obama</cell><cell></cell><cell>FFHQ</cell><cell></cell><cell>Art</cell><cell></cell></row><row><cell>Image number</cell><cell>100</cell><cell>100</cell><cell>1k</cell><cell>70k</cell><cell>0</cell><cell>1k</cell><cell>0</cell></row><row><cell>StyleGAN2</cell><cell>0.914</cell><cell>0.652</cell><cell cols="5">3.177 2.43 2.289 3.051 2.761</cell></row><row><cell>Baseline</cell><cell>1.632</cell><cell>0.733</cell><cell cols="5">2.421 N/A 1.943 2.677 2.421</cell></row><row><cell>Baseline + Contrastive</cell><cell>1.251</cell><cell>0.647</cell><cell cols="5">1.821 N/A 1.943 2.124 2.421</cell></row><row><cell>Baseline + AE</cell><cell>0.725</cell><cell>0.405</cell><cell cols="5">1.075 N/A 1.943 1.806 2.421</cell></row><row><cell cols="2">Baseline + AE + Contrastive 1.156</cell><cell>0.578</cell><cell cols="5">1.345 N/A 1.943 1.927 2.421</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Apart from the observations, we would like to emphasize that the experiments are mostly conducted on few-shot datasets. The results does not give a full picture of the relationship between the featureextraction performance on D and the synthesis performance of GAN, further study on larger-scale datasets are required. However, the experiments do validate the effectiveness of the self-supervision strategies on D for an enhanced performance of GAN, on few-shot datasets.C STYLE-MIXING ON DIFFERENT RESOLUTIONSHere we present more qualitative results on the style-mix performance of our model. For the model trained on 1024 ? 1024 resolution, there are three SLE layers that we can swap the feature-maps between generated samples, and there are two SLE layers for model on 256 ? 256 resolution.</figDesc><table><row><cell>Mix</cell></row><row><cell>resolution</cell></row><row><cell>512</cell></row><row><cell>512, 256</cell></row><row><cell>512, 256,</cell></row><row><cell>128</cell></row><row><cell>128</cell></row><row><cell>Mix</cell></row><row><cell>resolution</cell></row><row><cell>512</cell></row><row><cell>512, 256</cell></row><row><cell>512, 256,</cell></row><row><cell>128</cell></row><row><cell>128</cell></row><row><cell>Mix</cell></row><row><cell>resolution</cell></row><row><cell>256</cell></row><row><cell>256, 128</cell></row><row><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>LPIPS between synthetic images and their closest real images.</figDesc><table><row><cell></cell><cell cols="3">Art paintings 1k FFHQ 1k Skull</cell><cell>Cat</cell><cell>Dog</cell><cell>Shell</cell></row><row><cell cols="2">augmented 0.5499</cell><cell>0.5279</cell><cell>0.389</cell><cell>0.3898 0.3847 0.3853</cell></row><row><cell>Our G</cell><cell>0.637</cell><cell>0.5859</cell><cell cols="2">0.3168 0.5486 0.5647 0.4275</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking selfsupervised visual representation learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6391-6400, 2019.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A PERFORMANCE BOOST FROM SKIP-LAYER EXCITATION <ref type="figure">Figure 8</ref>: Ablation study for SLE module on 1024 ? 1024 resolution datasets. Each unit on the x-axis represents 1000 training iterations, and y-axis represents the FID score.</p><p>Here we present a more detailed ablation study for the skip-layer excitation (SLE) module. We compare between the baseline model and the baseline equipped with SLE. On four 1024 ? 1024 resolutions datasets: Flower, FFHQ, Shell and Art-paintings, we record the FID performance every 10000 iterations for every model. As shown in <ref type="figure">Fig. 8</ref>, SLE brings a constant performance boost on the baseline model over all iterations.</p><p>Our key observation is, SLE speeds up the convergence of GAN, where the most noticeable effect happens at the beginning of the training. In the first 20000 iterations, the generator G is able to converge faster and reach to a good point where the baseline model needs much more training iterations to reach. On the other hand, although SLE provides a faster convergence on G, the overall model behavior with SLE seems follow the baseline model quite well, with a slightly better overall performance.</p><p>In other words, the lines for the two models are parallel in each sub-plot in <ref type="figure">Fig. 8</ref>. Specifically, on Shell, the model with SLE also collapsed after 60000 iterations training, just like the baseline model. And on the rest three datasets, the FID improves much slower and almost stop changing in the later half training iterations. We think such model behavior makes sense, because the SLE module neither increases the model capacity (have very few parameter increase) nor exert any explicit regularization or guidance on the training of GAN. Therefore, SLE is unlikely to make a big difference after the model reaches a good converged state.</p><p>On the other hand, SLE does a good job speeding up the convergence for G, and improves the performance of G. More importantly, it is SLE that enables the unsupervised style-content disentanglement for our model, in a simpler and more cost-efficient way than StyleGAN and StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FEATURE-EXTRACTION PERFORMANCE OF DISCRIMINATOR</head><p>Here we continue the discussion on the effectiveness of the self-supervised auto-encoding training for the discriminator D. Specifically, we explore the relationship between the feature-extracting behavior on the discriminator D and the synthesis performance of GAN . By feature-extracting performance, we mean how comprehensive the feature-maps extracted by D cover the information from the input images. This feature-extracting performance can be easily checked via an autoencoding training. In detail, we take D trained in GAN and fix it, then train a decoder for D which tries to reconstruct the images from the feature-maps encoded by D. The intuition is, if D pays</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<title level="m">Towards principled methods for training generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12154" to="12163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Can: Creative adversarial networks, generating&quot; art&quot; by learning about styles and deviating from style norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Mazzone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07068</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Artists, artificial intelligence and machine-based creativity in playform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Mazzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artnodes</title>
		<imprint>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Time: Text and image mutual-translation adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13192</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sketch-to-art: Synthesizing stylized art images from sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Oogan: Disentangling gan with one-hot sampling and orthogonal regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04406</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Freeze discriminator: A simple baseline for fine-tuning gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10964</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depthwisegans: Fast training generative adversarial networks for realistic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mkhuseli</forename><surname>Ngxande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jules-Raymond</forename><surname>Tapamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image generation from small datasets via batch statistics adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2750" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Few-shot adaptation of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Robb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11943</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning hybrid image templates (hit) by information projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangzhang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1354" to="1367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Small-gan: Speeding up gan training using core-sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13540</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-supervised gan: Analysis and improvement with multi-class minimax game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Ngoc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13253" to="13264" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minegan: effective knowledge transfer from gans to target domains with few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9332" to="9341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yaz?c?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04498</idno>
		<title level="m">The unusual effectiveness of averaging in gan training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pa-gan: Improving gan training by progressive augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving the speed and quality of gan by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03364</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">In-domain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00049</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
