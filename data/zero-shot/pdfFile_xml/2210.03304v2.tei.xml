<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yang</surname></persName>
							<email>zhichaoyang@umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufan</forename><surname>Wang</surname></persName>
							<email>shufanwang@umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhanu</forename><surname>Pratap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singh</forename><surname>Rawat</surname></persName>
							<email>brawat@umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Mitra</surname></persName>
							<email>avijitmitra@umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
							<email>hong_yu@uml.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a promptbased fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P&lt;0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label learning has many real-word applications in natural language processing (NLP), including but not limited to academic paper labeling , news framing <ref type="bibr" target="#b0">(Aky?rek et al., 2020)</ref>, waste crises response , amazon product labeling <ref type="bibr" target="#b31">(McAuley et al., 2015;</ref><ref type="bibr" target="#b9">Dahiya et al., 2021)</ref>, and medical coding <ref type="bibr" target="#b2">(Atutxa et al., 2019)</ref>. In contrast to multi-class classification, an instance in multi-label learning is frequently linked with more than one class labels, making the task more challenging due to the combination of potential class labels.</p><p>In real-world tasks, there are often insufficient training data for rare class labels. Taking automatic international classification of diseases (ICD) coding as example, given discharge summaries notes as input, the task is to assign multiple ICD disease and procedure label codes associated with each note. The assigned codes need to be accurate and complete for the billing purposes. As an example, the MIMIC-III dataset <ref type="bibr" target="#b20">(Johnson et al., 2016)</ref> contains 8,692 unique ICD-9 codes, among which 4,115 (47.3%) codes occur less than 6 times and 203 (2.3%) occur zero times. Clinical practice requires a high accuracy, hence, it is not acceptable for a multi-label classifier to fail a disease diagnosis (or code assignment) because it is rare, since such a diagnosis may be of the most clinical importance for the patient. Therefore, the classifier is required to perform with high precision even for infrequent codes. This translates to data sparsity due to availability of few training examples.</p><p>To mitigate the data sparsity problem, additional structured knowledge could be applied. ICD codes are organized with an ontological/hierarchical structure where a text description is associated to each code. For instance, ICD 250 (Diabetes mellitus), shown in <ref type="figure">Figure 1</ref>, is the parent of several child codes including 250.0 (Diabetes mellitus without mention of complication), 250.1 (Diabetes with ketoacidosis), and 250.2 (Diabetes with hyperosmolarity). Such child ICD codes are more semantically different from each other than their parent code 250.</p><p>Synonyms including acronyms and abbreviations are common in medical notes. For instance, the description of code 250.00 is disease "type II diabetes mellitus". However, this code can be described in different text forms such as "insulinresistant diabetes", "non-insulin dependent diabetes", "DM2", and "T2DM". Therefore, one naive way to assign ICD codes is to identify matching between candidate code descriptions and their syn- <ref type="figure">Figure 1</ref>: An illustration of self-alignment pretraining from medical knowledge UMLS, including the usage of (a) Hierarchy, (b) Synonym, (c) Abbreviation. Pink region is the dynamic margin ranges from ?/2 to ? where we wish to pull negatives apart with a dynamic distance. onyms in medical notes. In this work, we separate synonyms from both acronyms and abbreviations due to its importance in medical domain <ref type="bibr" target="#b64">(Yu et al., 2002)</ref>. While synonymous relations could be implicitly learned from pretrained language model (LM) <ref type="bibr" target="#b32">(Michalopoulos et al., 2022;</ref>, previous researches show that language models are only limited biomedical <ref type="bibr" target="#b48">(Sung et al., 2021)</ref> or clinical knowledge bases <ref type="bibr" target="#b62">(Yao et al., 2022)</ref> due to the data sparsity challenge in the medical domain. An explicit way of adding such medical knowledge into language model should be explored.</p><p>In this paper, we present a simple but effective Knowledge Enhanced PrompT (KEPT) framework. We implement and evaluate KEPT using a LM based on Longformer because clinical notes are typically more than 500 tokens. Specifically, we first pretrain mimic a Longformer LM on MIMIC-III dataset. Then, we further pretrain umls on structured medical knowledge UMLS (Unified Medical Language System) using self-alignment learning with contrastive loss to inject medical knowledge into pretrained LM. For the downstream ICD-code assignment fine-tuning, we add a sequence of ICD code descriptions (label semantics) as prompts in addition to each clinical note as KEPT LM input. This allows early fusion of code descriptions and the input note. Experiments on full disease coding (MIMIC-III-full) and common disease coding (MIMIC-III-50) show that our KEPTLongformer outperforms previous SOTA MSMN . In order to test its few-shot ability, we create a new few-shot rare diseases coding dataset named MIMIC-III-rare50, and results show significant improvements compared between MSMN and our method. To facilitate future research, we publicly release the code and trained models 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prompt-based Fine-tuning</head><p>Prompt-based fine-tuning has been shown to be effective in few-shot tasks <ref type="bibr" target="#b24">(Le Scao and Rush, 2021;</ref>, even when the language model is relatively small <ref type="bibr" target="#b41">(Schick and Sch?tze, 2021)</ref> because they introduce no new parameter during few shot fine-tuning. Additional tuning techniques such as to tune bias-term or language model head have shown to be efficient on memory and training time <ref type="bibr" target="#b4">(Ben Zaken et al., 2022;</ref><ref type="bibr" target="#b28">Logan IV et al., 2022)</ref>. However, most previous works focus injecting knowledge into prompt on single-label multi-class classification task <ref type="bibr" target="#b17">(Hu et al., 2022;</ref><ref type="bibr" target="#b54">Wang et al., 2022a;</ref><ref type="bibr" target="#b63">Ye et al., 2022)</ref>. To the best of our knowledge, this is the first work that applies prompting to multi-label classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Representation Pretraining</head><p>Many recent researches use synonyms to conduct biomedical entity representation learning <ref type="bibr" target="#b47">(Sung et al., 2020;</ref><ref type="bibr" target="#b23">Lai et al., 2021;</ref><ref type="bibr" target="#b1">Angell et al., 2021;</ref><ref type="bibr" target="#b22">Kong et al., 2021;</ref><ref type="bibr">Seneviratne et al., 2022)</ref>. Our work is most similar to , which uses additional pretraining scheme that self-aligns the representation space of biomedical entities from pretrained medical LM. They collect self-supervised synonym examples from the biomedical ontology UMLS, and use multi-similarity contrastive loss to keep the representation of similar entities closer to each other, before fine-tuning them to the downstream specific task. However, their work differs from ours in (1) their testing being limited to only medical entity linking tasks and (2) not using hierarchical information, which has been shown to be useful in KRISSBERT . In contrast to KRISSBERT, our contrastive learning selects negative samples from siblings (1-hop nodes) instead of random nodes in the graph. Our method follows InfoMin proposition that selected samples should contain as much task-relevant information while discarding as much irrelevant information in the input as possible <ref type="bibr" target="#b50">(Tian et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ICD Coding</head><p>ICD coding uses NLP models to predict expert labeled ICD codes given discharge summaries as input. Currently, the most straightforward method is to take the best language model for encoding notes, and later use the label attention mechanism to attend labeled ICD codes to input notes for prediction <ref type="bibr" target="#b35">(Mullenbach et al., 2018)</ref>. In comparison, we apply attention between codes and notes way before within the encoder with the help of prompt. The label representations in attention played an important role in many previous works. <ref type="bibr" target="#b25">Li and Yu (2020)</ref> and <ref type="bibr" target="#b52">Vu et al. (2020)</ref> first randomly initialize the label representations. <ref type="bibr" target="#b8">Chen and Ren (2019)</ref>; <ref type="bibr" target="#b13">Dong et al. (2021)</ref>; <ref type="bibr" target="#b69">Zhou et al. (2021)</ref> initialize the label representation with code description from shallow representation using Word2Vec <ref type="bibr" target="#b33">(Mikolov et al., 2013)</ref>.  further add description synonyms semantic information. In comparison, we use deep contextual representation from Longformer pretrained on both MIMIC and UMLS with contrastive loss. Similar pretrained language models have shown to be effective in previous works <ref type="bibr" target="#b58">(Wu et al., 2020;</ref><ref type="bibr">DeYoung et al., 2022;</ref><ref type="bibr" target="#b32">Michalopoulos et al., 2022)</ref>.</p><p>As stated previously, the high dimensions of available label codes, such as 14,000 diagnosis codes and 3,900 procedure codes in ICD-9 and 80,000 in industry coding <ref type="bibr" target="#b70">(Ziletti et al., 2022)</ref>, makes ICD coding challenging. Another challenge is the long-tail distribution, in which few codes are frequently used but most codes may only be used a few times due to the rareness of diseases <ref type="bibr" target="#b43">(Shi et al., 2017;</ref><ref type="bibr" target="#b59">Xie et al., 2019)</ref>. <ref type="bibr" target="#b34">Mottaghi et al. (2020)</ref> use active learning with extra human labeling to solve this issue. Other recent works focus on using additional medical domain-specific knowledge to better understand the few training instances <ref type="bibr" target="#b6">(Cao et al., 2020;</ref><ref type="bibr" target="#b44">Song et al., 2020;</ref><ref type="bibr" target="#b29">Lu et al., 2020;</ref><ref type="bibr" target="#b14">Falis et al., 2022;</ref><ref type="bibr" target="#b56">Wang et al., 2022b)</ref>. <ref type="bibr" target="#b57">Wu et al. (2017)</ref> perform entity linking to identify medical phrase in document note. <ref type="bibr" target="#b59">Xie et al. (2019)</ref> map label codes as entities in medical hierarchy graph. Compared to a baseline which uses a shallow convolutional neural network to learn n-gram features from notes, they add complex hierarchy structure between codes by allowing the loss to propagate through graph convolutional neural network. In contrast with the previous systems which adopt complex pipelines and different tools, our method applies a much simpler training procedure by incorporating knowledge into language model without requiring any knowledge pre or post-processing (i.e. MedSpacy, Gensim, NLTK) during the finetuning. Additionally, previous methods use knowledge graph as an input source, however, we train our language model to include knowledge graph as a target with contrastive loss.</p><p>3 Methods ICD coding: ICD coding is a multi-label multiclass classification task. Specifically, considering thousands of words from an input medical note t, the task is to assign a binary label y i ? {0, 1} for each ICD code in the label space Y , where 1 means that note is positive for an ICD disease or procedure and i ? range [1, N c ]. In this study, we define and evaluate the number of candidate codes N c as 50, although N c could be higher or lower depending on specific applications. Each candidate code has a short code description phrase c i in free text. For instance, code 250.1 has description diabetes with ketoacidosis. Code descriptions c is the set of all N c number of c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Text with Longformer</head><p>To solve this task, we first need to encode free text into hidden representation with a pretrained clinical longformer. Specifically, we convert free text a to a sequence of tokens x a , the vocab embedding then maps x a to a sequence of hidden vectors. Next, the 1st layer of LM encoder attends one hidden vector to another hidden vector in the sequence with selfattention mechanism. This encoding process is repeated l times to produce a sequence of final contextual hidden vectors h h h a ? R Lt?H d for each free text a where H d is the hidden layer dimension and L t is the number of token in t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning with Prompt</head><p>Prompt based fine-tuning is different from standard fine-tuning. During standard fine-tuning, we usually make input x a = [CLS] a, where a ? {t, c 1 , c 2 , ..., c Nc }. To assist LM in finding a mention of a label code in note text, we fusion final contextual hidden representation of note text t and code description c with attention. Specifically, we first build code description representation</p><formula xml:id="formula_0">h h h c ? R Nc?H d by concatenating encoded hidden vector h h h c i {[CLS]} ? R H d of token [CLS]</formula><p>for each code description c i . We then build note aware code representation h h h f ? R Nc?H d for each code using cross attention between sequence of vectors h h h c as query and sequence of vectors h h h t as key, with attention weight ? ij between ith item in query and jth item in key as follow:</p><formula xml:id="formula_1">? ij = sof tmax((W W W q h h h c i {[CLS]})(W W W k h h h t j ))</formula><p>where W W W q and W W W k are query weight and key weight to be trained. To learn the probability of a code to assign, we train a binary label head, sof tmax(W W W b h h h f ), by maximizing log-probability of correct label for each code. An illustration of such a standard fine-tuning pipeline is provided in <ref type="figure" target="#fig_0">Figure 2</ref> (a). This standard fine-tuning approach introduces many new parameter weights (589,824 with cross attention and 1,536 with binary label head for longformer), making it hard to learn in few shot setting where the number of training data is limited for each code . Similar training approaches were carried out in previous researches <ref type="bibr" target="#b35">(Mullenbach et al., 2018;</ref><ref type="bibr" target="#b25">Li and Yu, 2020;</ref><ref type="bibr" target="#b21">Kim and Ganapathi, 2021;</ref><ref type="bibr" target="#b30">Luo et al., 2021;</ref><ref type="bibr" target="#b69">Zhou et al., 2021)</ref> (specific label attention calculation may differ) , instead of a pretrained language model, they used unpretrained LSTM or CNN to encode free text, which added more untrained parameters during ICD training.</p><p>An alternative approach to multi-label classification is prompt based fine-tuning, where masks in prompt are filled-in by LM in cloze style . We reformulate multi-label classification tasks with free text prompt template as input:</p><formula xml:id="formula_2">xp =c1 : [MASK] , c2 : [MASK] , ... , cN c : [MASK] . t .</formula><p>and use LM to decide if note is positive (or negative) for a code by filling [MASK] with vocab token yes (or no). This step is repeated N c times for each <ref type="bibr">[MASK]</ref> and associated code c i . Specifically, we encode free text prompt as mentioned before, and obtain final hidden vectors h h h p for input x p . Notice that this encoding step would fusion code descriptions and note text with self-attention in every layer of LM encoder. We define a mapping function M from y i in label space to vocab tokens as:</p><formula xml:id="formula_3">M (y i ) = "yes" if y i = 1; "no" if y i = 0; (1) where i ? range [1, N c ].</formula><p>In this way, we transfer downstream multi-label classification task into a mask language model task like pretraining. For ith code, the label probability would be calculated as: 3.3 Hierarchical Self-Alignment Pretrain umls (HSAP) using Knowledge Graph UMLS</p><formula xml:id="formula_4">P (yi|xp) = P ([MASK] c i = M (yi)|xp) = exp(W W W M (y i ) ? h h hp{[MASK] c i }) ??Y exp(W W W M (?) ? h h hp{[MASK] c i }) (2) where h p {[MASK] c i } ? R H d is</formula><p>Since no new parameters are added to LM in prompt based fine-tuning, the performance on medical downstream task heavily relies on the quality of clinical pretrained LM. However, encoded hidden representations of similar medical terms are not guaranteed to be close to each other. Thus we apply self-alignment pretraining  to align similar terms closer to each other with additional knowledge. This additional pretrain umls is after masked language pretrain mimic and before auto ICD finetuning. We first build self-supervised data from synonyms, abbreviations, hierarchy in the medical knowledge graph of the UMLS and ICD ontology ( ?3.3.1), and inject such structural knowledge into a LM by pretraining it on self-supervised data with hierarchical contrastive loss ( ?3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Generating Self-Supervised Data</head><p>To generate pretraining examples, we first build a mapping between medical terms and codes as entities in the medical knowledge graph UMLS. Specifically, synonyms of an entity are collected from multiple English free text descriptions of entity via UMLS "MRCONSO" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Contrastive Learning</head><p>Given self-supervised data, we further train clinical longformer using contrastive learning, with the intention of pushing target medical terms and positive medical terms closer, while pulling negative medical terms further away. We formulate this problem into hierarchical triplet loss on a sampled mini batch encoded by LM. Encoding Medical Terms: Each medical term is usually a short phrase of multiple tokens. Similar to Phrase-BERT <ref type="bibr" target="#b55">(Wang et al., 2021)</ref>, a medical term is encoded into a sequence of hidden vectors as descried in ?3.1. We use clinical longformer  as encoder for this process. We define a medical term's hidden representation p p p as the first item in hidden vector sequence, which has shown to be effective in <ref type="bibr" target="#b51">Toshniwal et al. (2020)</ref>.</p><p>Hierachical Neighbor Sampling: We randomly select i number of target anchor entities from the ICD hierarchy level l. Each medical terms represents a disease class. Collecting entities from each level could preserve the diversity of samples in the mini batch. Then j ? 1 parents and siblings are randomly chosen for each of i entities. The purpose of choosing intra-class parents and siblings is to encourage model to discriminate anchor entities from close neighbor entities. Finally, k medical terms for each entity are randomly collected, resulting in n = ijk medical terms in a mini batch B of hierarchy level l. We collect mini batch from other hierarchy levels in the same way.</p><p>Minibatch Triplet Loss with Dynamic Margin: Similar to <ref type="bibr" target="#b16">Ge et al. (2018)</ref>, hierarchical triplet loss of a mini batch B can be formulated as:</p><formula xml:id="formula_5">LB = 1 2NB Tx?T B max(0, mx ? |p p p a x ? p p p ? x | + |p p p a x ? p p p + x |)<label>(3)</label></formula><p>where T B is all the triplets in the minibatch B. N B is the number of triplets in minibatch B, and each triplet T x consists of an anchor sample p p p a x , a positive sample p p p + x from positive class, a negative sample p p p ? x from intra-class or inter-class negative class. m x is a dynamic margin. It is computed according to entity's clinical term similarity between the anchor class entity and the negative class entity <ref type="bibr" target="#b66">(Zakharov et al., 2017)</ref>. Specifically, for a triplet T x , the dynamic margin m x is computed as:</p><formula xml:id="formula_6">mx = ? ? ? ? ? ?/2</formula><p>if parent ; ?/2 + arccos(|p p p a x ? p p p ? x |) if siblings ; else ( = ?).</p><p>(4)</p><p>where condition clause parent and siblings means that negative sample p p p ? x comes from intra-class parent and siblings of anchor sample. In practice, we set = ?. Thus, inter-class negative sample would be at least ? distance away from anchor sample, while intra-class negative sample would be at least d ? [?/2, ?] range distance away from anchor sample. Such dynamic margin is different from constant margin in previous contrastive loss work in medical domain , and has shown to be effective in visual retrieval task in computer vision <ref type="bibr" target="#b16">(Ge et al., 2018)</ref>.</p><p>By minimizing loss defined in Equation 3, we pretrain a medical knowledge injected clinical longformer. We then use such longformer to encode prompt and context ( ?3.2), and thus gain knowledge injected prompt for downstream coding task.</p><p>When applied to MIMIC-III-full data, it is infeasible to encode all 8,692 candidate ICD codes in the prompt due to high memory cost (to be specified in ?6). Instead, we used a two-stages approach. Specifically, we used model MSMN as 1st stage coder to select top 300 candidate codes, and then use our KEPTLongformer as 2nd stage coder to further narrow down the candidates to final prediction. Our 2nd stage coder functions similar to reranker in passage ranking <ref type="bibr" target="#b36">(Nogueira and Cho, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>MIMIC-III dataset <ref type="bibr" target="#b20">(Johnson et al., 2016)</ref> contains data instances of de-identified discharge summaries with expert labeled ICD-9 codes. The discharge summaries are from real patients. We applied the following text pre-processing step before tokenizer:</p><p>(1) removing all de-identification tokens; (2) replacing characters other than punctuation marks and alphanumerical into white space (e.g. /n); (3) stripping extra white spaces. Previous work <ref type="bibr" target="#b35">(Mullenbach et al., 2018)</ref> truncated discharge summaries at 4,000 words. Since longformer used tokens instead of words, we truncated discharge summaries at 8,192 tokens unless otherwise specified. This roughly aligns with our observation that word token ratio is about 1:2. Since procedure codes are related to subjective section of the note <ref type="bibr" target="#b61">(Yang and Yu, 2020)</ref>, we include relevant sections of discharge summaries for those length exceeds 8,192, and remove irrelevant sections such as discharge followup. The header names of the relevant sections are provided in <ref type="table">Table A</ref>.1. We named this dataset MIMIC-III-full.</p><p>For the top-50 frequent codes prediction task, we filtered each instance that has at least one of the top 50 most frequent codes, and used the same splits as the previous work <ref type="bibr" target="#b52">(Vu et al., 2020;</ref>. We named this dataset MIMIC-III-50. Detailed statistics are included in <ref type="table">Table A</ref>.2.</p><p>To benchmark auto ICD coding task on few-shot learning, we also created a rare-50 codes prediction using original MIMIC-III dataset. Among 8,692 different types of ICD-9 codes, we first selected codes with less than 10 times occurrences to fit into the few-shot setting. This constitutes more than 90% of original codes. We then ranked the filtered codes by test/train ratio and select top 50, so that testing samples are available for evaluation. We also removed some potential common diseases by hand in the process. This would include true rare diseases (e.g. Kaposi's sarcoma) listed in expert labeled rare diseases dictionary <ref type="bibr" target="#b38">(Pavan et al., 2017;</ref><ref type="bibr" target="#b53">Wakap et al., 2019)</ref>. We named this dataset MIMIC-III-rare50. The average number of examples per label code (shot) is about 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For medical domain knowledge graph, we used UMLS 2021AA, containing 4.4 million entities. When mapping entity to its description, we preferred ICD description. If it is not found, then we used UMLS description. When recreating previous baselines, we used the same hyperparameter setting as mentioned in their published work. We removed R-Drop in  and used plain cross-entropy loss only for a fair comparison among all baselines. Code descriptions in prompt use longformer global attention unless otherwise specified. Our full hyperparameter and config setting using wandb is provided in github. Selfalignment Pretraining took about 48 hours with 1 NVIDIA V100 GPU. Fine-tuning took about 10 hours with 2 NVIDIA A100 40GB memory GPUs on MIMIC-III-50, and 0.5 hours on MIMIC-III-rare50. During testing, we used dev set to select best threshold for F1 score. Similar to BERT, no hyper-parameters were further searched on the dev set with our longformer. We evaluated with 5 dif- ferent random seeds for each model and report the median test results across these seeds unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>MultiResCNN <ref type="bibr" target="#b25">(Li and Yu, 2020)</ref> encode free text with Multi-Filter Residual CNN, and applied label code attention mechanism to enable each ICD code to attend different parts of the document. MSATT-KG <ref type="bibr" target="#b59">(Xie et al., 2019)</ref> apply multi-scale attention and graph neural network to capture potential relations between codes, without any changes in the training objectives.</p><p>JointLAAT <ref type="bibr" target="#b52">(Vu et al., 2020)</ref> propose a hierarchical joint learning with training objectives to predict both ICD code and its parent ICD code in the hierarchy graph. MSMN  use synonyms with adapted multi-head attention, which achieved SOTA performance on MIMIC-III-50 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Results show that our longformer with knowledge pretrained prompt (KEPTLongformer) outperforms the previous state-of-art model MSMN <ref type="table" target="#tab_3">(top of  Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref>). For the common disease code assignment (MIMIC-III-50) task, our KEPT-Longformer achieves macro AUC of 92.63 (+0.13), micro AUC of 94.76 (+0.36), macro F1 of 68.91 (+1.27), and micro F1 of 72.85 (+1.07). Number in parentheses shows the improvements compared to MSMN. For the rare disease code assignment (MIMIC-III-rare50) task, our KEPTLongformer achieves macro AUC of 82.70 (+7.39), micro AUC of 83.28 (+7.11), macro F1 of 30.44 (+13.39), micro F1 of 32.63 (+15.44). We notice that the im-provements on rare disease codes are much higher than improvements on common disease code, indicating the strong advantage of our KEPTLongformer for few-shot settings. In contrast to previous work that leads to improvements on rare disease codes but worse results on frequent ones <ref type="bibr" target="#b40">(Rios and Kavuluru, 2018)</ref>, our approach shows improvements on both tasks. We finally applied our KEPT-Longformer to MIMIC-III-full. <ref type="table" target="#tab_6">Table 3</ref> shows that reranker with KEPTLongformer outperforms previous SOTA MSMN in F1 marco from 10.3 to 11.8 by +1.5 (95%CI +0.93 to +1.99, P&lt;0.001) and F1 micro from 58.2 to 59.9 by +1.6 (95%CI +0.95 to +2.33, P&lt;0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>Our final KEPTLongformer model could be interpreted as a hybrid of 3 closely interrelated components: longformer, prompt based fine-tuning, and knowledge injected pretraining. Here we provide an ablation study on each part. Longformer vs. BERT. Increasing max token limit is important under clinical note analysis task, because most clinical notes are long documents with an average of 3000 tokens in MIMIC-III discharge summaries. Due to the high number of tokens in a medical note, it is essential to encode as many tokens as possible before downstream analysis. However, BERT based LM, which could only encode a few sentences, is known to be ineffective for long documents <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref>. To test the effect of max token limit in auto ICD coding task, we compare the performance between Clinical Longformer with max limit of 8,192 tokens and ClinicalBert with max limit of 512 tokens. As shown in  (KEPTLongformer without HSAP &amp; Prompt) substantially outperforms ClinicalBERT in AUC from 7.5 to 8.6 and F1 from 14.9 to 15.6. Other previous methods (e.g. MultiResCNN) use non-pretrained LSTM or CNN with max limit of 8192 tokens. We also observe that these previous methods outperform ClinicalBERT, indicating the importance of max token limit over LM in auto ICD coding task. This finding correlates to previous LM researches <ref type="bibr" target="#b37">Pascual et al., 2021;</ref><ref type="bibr" target="#b5">Biswas et al., 2021)</ref> which only uses longformer/BigBird <ref type="bibr" target="#b32">(Michalopoulos et al., 2022)</ref> or hierarchical BERT <ref type="bibr" target="#b10">Dai et al., 2022)</ref> of 4096 max token limit, and our method with max limit of 8192 tokens could alleviate the issues mentioned by them.</p><p>Prompt based fine-tuning as early fusion. In order to test the effect of prompt based fine-tuning as its own, we further compare Longformer trained with prompt based fine-tuning with longformer trained with original fine-tuning on MIMIC-III-50. As shown in <ref type="table" target="#tab_3">Table 1</ref>, prompt based fine-tuning (KEPTLongformer w/o HSAP) improves AUC and F1, and converges faster to achieve best F1 score from epoch 17 to epoch 5. Our prompt based finetuned longformer also slightly outperforms Mul-tiResCNN, and other baselines such as MSATT-KG and JointLAAT that uses structured knowledge as addition resources. Under few-shot setting, our prompt based fine-tuning significantly increase AUC and F1 score compared to traditional finetuning as shown in <ref type="table" target="#tab_4">Table 2</ref>. This finding supports previous research <ref type="bibr" target="#b49">(Taylor et al., 2022</ref>) that shows prompt based fine-tuning outperforms traditional fine-tuning in many few-shot clinical tasks such as length of stay and mortality prediction. Compared to recent models on auto ICD coding, our prompt based model could be seen as an early fusion of label code description and input note text. Instead of fusing label description representations and note text representations after encoder with label attention <ref type="bibr" target="#b69">(Zhou et al., 2021;</ref><ref type="bibr" target="#b13">Dong et al., 2021;</ref>, we fuse the two starting from first layer within the encoder with cross attention. Such similar early fusion method has shown to be effective in combining information from knowledge graph and information from text in question answering over knowledge base facts <ref type="bibr" target="#b11">(Das et al., 2017)</ref> and open domain question answering . Hierarchical self alignment pretraining (HSAP) improves multi-label classification with label domain knowledge. In order to test the effect of HSAP as its own, we further compare Longformer with HSAP (KEPTLongformer) and without HSAP (w/o HSAP). HSAP improves 0.45 on micro AUC and 1.09 on micro F1 in dataset MIMIC-III-50, and 1.1 on micro AUC and 2.7 on micro F1 in dataset MIMIC-III-rare50. Thus we showed that our contrastive learning in label space is more effective in the tasks with limited labeled data, which supports similar finding in text classification <ref type="bibr">(Qian et al., 2022)</ref>. We also observe that HSAP could reduce  false negative predictions which mistakenly predict their siblings. Out of 78 false negative predictions on code 285.1, 2 predict sibling code 285.9 with HSAP. In contrast, out of 89 false negative predictions on code 285.1, 15 predict sibling code 285.9 without HSAP. HSAP reduces false negative predictions on 285.1 caused by sibling 285.9 from 15 to 2. HSAP works as a good polish to further improve the coding accuracy by injecting domain knowledge into language model. Parameter efficiency on few-shot learning. One could argue that accuracy improvements come from more number of parameters during training. Our KEPTLongformer is finetuned with 7 times more trainable parameters compared to baseline MSMN. To counter such argument, we also finetune our KEPTLongformer with limited parameters while keeping most parameters fixed. Specifically, we considered the following 4 settings: a) tuning LM head and first encoder layer, b) tuning LM head and last encoder layer, c) tuning LM head only, d) tuning no parameter as zero-shot. Compared to MSMN, settings a, b, c, d improve micro AUC by +5.4, +2.2, +0.7, +0.3 and micro F1 by +12.4, +6.1, -0.3, -0.5 respectively, as shown in <ref type="table" target="#tab_4">Table 2</ref>. Setting a and b with 9.4 million trainable parameters significantly outperforms MSMN with 16.4 million trainable parameters. Setting c and d with almost no trainable parameters shows competitive results compared to MSMN. We also observe that training first layer outperforms training last layer, this could also be an evidence to support the advantage of early fusion for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we investigate pretrained clinical language model on auto ICD coding task for both common and rare disease, the latter of which has received limited attention in the past. Built on recent advances in contrastive learning, entity representation training and prompt based fine-tuning, our KEPTLongformer easily achieves a competitive performance over state of the art system in common code assignment, and significantly outperforms baseline model in rare code assignment task. Finally, our novel Hierarchical Self-Alignment Pretrain could be easily applied to other multi-label classification problems such as tumor detection using other ontology such as OncoTree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Our work is limited to auto ICD coding task with 50 label codes including MIMIC-III-50 or MIMIC-III-rare50, and could not be directly applied to ICD coding task MIMIC-III-full with 8,692 labels in practice due to memory constraint. Using our KEPTLongformer would create at least 26,076 tokens and 8,692 [MASK] in a single prompt, which easily explodes the max token limit of a longformer and GPU memory. A more memory efficient method for auto ICD coding could be explored for future work. Our clinical knowledge pretrained KEPTLongformer is only tested on auto ICD coding task, but such pretrained language model could be easily applied to other clinical NLP applications such as clinical entity linking or clinical question answering tasks. We also only use part of UMLS knowledge graph, including hierarchy, synonym, and abbreviation. Other knowledge including disease co-occurrence, disease-symptom, disease-lab relations and others could also potentially useful for auto ICD coding task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of (a) standard training method and (b) our proposed prompt-based fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the hidden vector of the [MASK] associated with each code c i in input x p , and W M is the original parameter pretrained in LM head. Prompt based fine-tuning reuses all parameters during pretraining, and does not introduce new parameters, making the whole model easy to fine-tune in a few-shot setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>table. Abbreviations of an entity are collected from multiple English free text descriptions of entity in UMLS SPECIALIST Lexicon and Lexical Tools "lrabr" table. Medical terms of an entity is defined as the union of synonyms set and abbreviations set of an entity. Diabetes with ketoacidosis) is the sibling of ICD 250.0 (Diabetes mellitus without mention of complication).</figDesc><table /><note>To sample negative examples for contrastive loss, we then build a hierarchy tree of entities using ICD-9 code ontology. For example, ICD 250 (Diabetes mellitus) is the parent of ICD 250.0 (Diabetes mel- litus without mention of complication), and ICD 250.1 (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, Clinical Longformer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results on the MIMIC-III-rare50 test set compared between MSMN (previous SOTA on MIMIC-III-</cell></row><row><cell>50) and our final model KEPTLongformer, where Pretrained: model is trained from previous pretraining check-</cell></row><row><cell>point, Finetuned: model is trained from best checkpoint after finetuned from MIMIC-III-50, HSAP: Hierarchical</cell></row><row><cell>Self-Alignment Pretraining. We also explore training partial model including: parameters of LM head, Last self-</cell></row><row><cell>attention layer, First self-attention layer as ablation study. Zero shot: No training on rare, directly inference using</cell></row><row><cell>finetuned model from MIMIC-III-50.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on the MIMIC-III-full compared between previous SOTA MSMN and our final model KEPTLongformer reranker. mean(st.dev.) are reported with 5 different random seeds.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the UMass BioNLP and MLFL group for many helpful discussions and related talks which inspired this work. We would also like to thank the anonymous reviewers for their insightful feedback. Research reported in this study was supported by the National Science Foundation under award 2124126. The work was also in part supported by the National Institutions of Health R01DA045816 and R01MH125027. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Science Foundation and National Institutes of Health.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Section header chief complaint: procedure: history of present illness: past medical history: brief hospital course: discharge diagnosis: discharge condition: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label and multilingual news framing analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Afra Feyza Aky?rek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randa</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Elanwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry Tanti</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wijaya</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.763</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8614" to="8624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clusteringbased inference for biomedical entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Angell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Monath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2598" to="2608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interpretable deep learning to map diagnostic texts to icd-10 codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantza</forename><surname>D?az De Ilarraza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Oronoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatz</forename><surname>Perez De Vi?aspre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of medical informatics</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transicd: Transformer based code-wise attention model for explainable icd coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biplob</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Hoang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2104.10652</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HyperCore: Hyperbolic and co-graph representation for automatic ICD coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3105" to="3114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperbolic capsule networks for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Jing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3115" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic icd code assignment utilizing textual descriptions and hierarchical structure of icd code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Siamesexml: Siamese networks meet extreme classifiers with 100m labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting transformer-based models for long document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elliott</surname></persName>
		</author>
		<idno>abs/2204.06683</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Question answering on knowledge bases and text using universal schema and memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2057</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Chin</forename><surname>Shing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Kong</surname></persName>
		</author>
		<idno>abs/2208.07444</idno>
	</analytic>
	<monogr>
		<title level="m">Christopher Winestock, and Chaitanya P. Shivade. 2022. Entity anchored icd coding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V&amp;apos;ictor</forename><surname>Su&amp;apos;arez-Paniagua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Whiteley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="page">103728</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Horses to zebras: Ontology-guided data augmentation and synthesis for ICD-9 coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat??</forename><surname>Falis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Alex</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.bionlp-1.39</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<meeting>the 21st Workshop on Biomedical Language Processing<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="389" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledgeable prompttuning: Incorporating knowledge into prompt verbalizer for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.158</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2225" to="2240" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PLM-ICD: Automatic ICD coding with pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Clinical Natural Language Processing Workshop</title>
		<meeting>the 4th Clinical Natural Language Processing Workshop<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Does the magic of bert apply to medical code assignment? a quantitative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Holtta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">104998</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Scientific Data</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Read, attend, and code: Pushing the limits of medical codes prediction from clinical notes by machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Hak</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLHC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot medical entity retrieval without annotation: Learning from rich knowledge graph semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winestock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.212</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2401" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BERT might be overkill: A tiny but effective biomedical entity linker based on residual convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.140</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1631" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icd coding from clinical text using multi-filter residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8180" to="8187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faraz</forename><forename type="middle">S</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno>abs/2201.11838</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4228" to="4238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.222</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2824" to="2835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-label few/zero-shot learning with knowledge aggregated from multiple label graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jueqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Dipnall</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.235</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2935" to="2943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fusion: Towards automated ICD coding via feature compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.184</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2096" to="2101" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inferring networks of substitutable and complementary products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ICDBig-Bird: A contextual embedding model for ICD code classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Michalopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Malyska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Sahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.bionlp-1.32</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<meeting>the 21st Workshop on Biomedical Language Processing<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="330" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Medical symptom recognition from patient text: An active learning approach for long-tailed multilabel distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prathusha Kameswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kannan</surname></persName>
		</author>
		<idno>abs/2011.06874</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Explainable prediction of medical codes from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mullenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1901.04085</idno>
		<title level="m">Passage re-ranking with bert. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards BERT-based automatic ICD coding: Limitations and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clinical practice guidelines for rare diseases: The orphanet database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathrin</forename><surname>Rommel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><forename type="middle">Elena</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Marquina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>H?hn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Val?rie</forename><surname>Lanneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ping Fan, and WenHua Dai. 2022. Contrastive learning from label distribution: A case study on text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guonian</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<publisher>Neurocomputing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fewshot and zero-shot multi-label learning for structured label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1352</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3132" to="3142" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">2022. m-networks: Adapting the triplet networks for acronym disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandaru</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Daskalaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Lenskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Clinical Natural Language Processing Workshop</title>
		<meeting>the 4th Clinical Natural Language Processing Workshop<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards automated icd coding using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1711.04075</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized zero-shot text classification for icd coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4018" to="4024" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1455</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-task balanced and recalibrated network for medical code prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marttinen</surname></persName>
		</author>
		<idno>abs/2109.02418</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Biomedical entity representations with synonym marginalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwisang</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3641" to="3650" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can language models be biomedical knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minji</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.388</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4723" to="4734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Clinical prompt learning with frozen language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niall</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">W</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejo</forename><forename type="middle">J</forename><surname>Nevado-Holgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kormilitzin</surname></persName>
		</author>
		<idno>abs/2205.05535</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>abs/2005.10243</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A cross-task analysis of text span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="166" to="176" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A label attention model for icd coding from clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3335" to="3341" />
		</imprint>
	</monogr>
	<note>Main track</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Estimating cumulative point prevalence of rare diseases: analysis of the orphanet database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">M</forename><surname>St?phanie Nguengang Wakap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Olry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Rodwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Val?rie</forename><surname>Gueydan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lanneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cam</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Human Genetics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="165" to="173" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic multi-label prompting: Simple and interpretable few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.401</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="5483" to="5492" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Phrase-BERT: Improved phrase embeddings from BERT with an application to corpus exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.846</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10837" to="10851" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A novel framework based on medical concept driven attention for explainable medical code prediction via external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.110</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1407" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Semehr: A generalpurpose semantic search system to surface semantic data from clinical notes for tailored care, trial recruitment, and clinical research*</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Toti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">I</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zina</forename><forename type="middle">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">A</forename><surname>Folarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Emre</forename><surname>Kartoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asha</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clive</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Broadbent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J B</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association : JAMIA</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="530" to="537" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2012.15466</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ehr coding with multi-scale feature attention and structured knowledge graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiancheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Risk response for municipal solid waste crisis using ontology-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Environmental Research and Public Health</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generating Accurate Electronic Health Assessment from Medical Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.336</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Extracting biomedical factual knowledge using pretrained language model and electronic health record context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghai</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijeta</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium proceedings. AMIA Symposium</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ontology-enhanced prompt-tuning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference</title>
		<meeting>the ACM Web Conference</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mapping abbreviations to full forms in biomedical articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Hripcsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="272" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Code Synonyms Do Matter: Multiple synonyms matching network for automatic ICD coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="808" to="814" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Andreas Hutter, and Slobodan Ilic. 2017. 3d object instance recognition and pose estimation using triplet loss with dynamic margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Planche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="552" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Knowledge-rich self-supervision for biomedical entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno>abs/2112.07887</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">BERT-XML: Large scale automated ICD coding using BERT pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachariah</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Automatic ICD coding via interactive shared representation networks with self-distillation mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.463</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5948" to="5957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Medical coding with biomedical transformer ensembles and zero/few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Ziletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Berns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Herold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Legler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><surname>Viell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-industry.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track<address><addrLine>Hybrid: Seattle, Washington + Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
