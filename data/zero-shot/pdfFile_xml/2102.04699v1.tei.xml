<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Unsupervised Cross-domain Image-to-Image Translation Using a Shared Discriminator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSE Department</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
							<email>rdabral@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">CSE Department</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sivakumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSE Department</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<settlement>Mumbai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Unsupervised Cross-domain Image-to-Image Translation Using a Shared Discriminator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image-to-image Translation</term>
					<term>Unsupervised Learning</term>
					<term>Cross-domain Image Translation</term>
					<term>Shared Discriminator</term>
					<term>Generative Adversarial Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised image-to-image translation is used to transform images from a source domain to generate images in a target domain without using source-target image pairs. Promising results have been obtained for this problem in an adversarial setting using two independent GANs and attention mechanisms. We propose a new method that uses a single shared discriminator between the two GANs, which improves the overall efficacy. We assess the qualitative and quantitative results on image transfiguration, a cross-domain translation task, in a setting where the target domain shares similar semantics to the source domain. Our results indicate that even without adding attention mechanisms, our method performs at par with attention-based methods and generates images of comparable quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative Adversarial Networks(GANs) <ref type="bibr" target="#b4">(Goodfellow et al., 2014)</ref> belong to the class of generative models <ref type="bibr" target="#b13">(Kingma and Welling, 2013)</ref> widely used in various image generation and translation tasks like computer vision and image processing <ref type="bibr" target="#b10">(Johnson et al., 2016)</ref>, , <ref type="bibr" target="#b40">(Wu et al., 2017)</ref>. While the state-of-the-art methods <ref type="bibr" target="#b7">(Huang et al., 2018)</ref>, , <ref type="bibr" target="#b23">(Park et al., 2019)</ref> in image-to-image translation tasks are significantly good <ref type="bibr" target="#b37">(Wang et al., 2018b)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>,  across multi-domain and crossdomain tasks, there is still room for improvement in image transfiguration tasks. Most of the imageto-image translation tasks assume the availability of source-target image pairs , <ref type="bibr" target="#b45">(Zhu et al., 2017b)</ref> or expect the source-target pairs to have rough alignment between them , <ref type="bibr" target="#b37">(Wang et al., 2018b)</ref>. However, there are scenarios where source-target image pairs are not available or when arbitrarily selected source-target image pairs have poor alignment between them.</p><p>While most image-to-image translation tasks involve translation over the complete image, there are cases where only an object of interest needs to be a https://orcid.org/0000-0003-4174-8587 b https://orcid.org/0000-0003-2890-6421 translated in the source and target domain. Let's consider the case of translating images of apples to oranges or horses to zebras. In both cases, only the object of interest needs to be translated, without affecting the rest of the image or it's background. This calls for the need of attention mechanisms <ref type="bibr" target="#b11">(Kastaniotis et al., 2018)</ref>, , <ref type="bibr" target="#b43">(Zhang et al., 2018)</ref>, <ref type="bibr" target="#b29">(Talreja et al., 2019)</ref> to attend to the objects of interest. Contrast-GAN <ref type="bibr" target="#b16">(Liang et al., 2017</ref>) is a work that has used object-mask annotations to guide the translation at high-level semantic levels at the cost of extra data. However, recent works have used attention mechanisms <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>,  without using any extra data or pretrained models. Moreover, very few works focus on image transfiguration, a cross-domain imageto-image translation task in an unsupervised setting without using additional networks, extra data or attention mechanisms.</p><p>In this paper, we focus on the above problem by proposing a framework that unifies the capabilities of multiple discriminators into a shared one, which not only improves the efficacy but also works without using extra data(object masks) or attention mechanisms. Adversarial training of the network involves combining the labels of the domains from different tasks conditioned on the input image and optimizing the objectives of the networks. We believe that there has not been any previous work where a dual generator shared discriminator setup has been used for crossdomain image-to-image translation and we are the first to propose a novel method. We summarize the paper contribution as follows:</p><p>1. We improve the efficacy of the GANs used for unsupervised cross-domain image-to-image translation tasks by introducing a novel shared discriminator setup. We empirically demonstrate the effectiveness of our method on image transfiguration tasks and report the qualitative and quantitative results on two datasets.</p><p>2. We conduct an ablation study to study the efficacy of the networks, training objectives and architectures keeping the dataset and other parameters constant and report the quantitative results of the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Generative Adversarial Networks: GANs are generative networks that use a trainable loss function to adapt to the differences between the data distributions of generated images and the real images. Since their inception <ref type="bibr" target="#b4">(Goodfellow et al., 2014)</ref>  <ref type="bibr" target="#b25">(Radford et al., 2015)</ref>, GANs have been used in various applications from computer vision <ref type="bibr" target="#b20">(Ma et al., 2017)</ref>, <ref type="bibr" target="#b33">(Vondrick et al., 2016)</ref>, image-to-image translation <ref type="bibr" target="#b28">(Taigman et al., 2016)</ref>, <ref type="bibr" target="#b32">(Tung et al., 2017)</ref>, video-tovideo translation , <ref type="bibr" target="#b36">(Wang et al., 2018a)</ref>, image super-resolution <ref type="bibr" target="#b15">(Ledig et al., 2016)</ref>, etc. among others. We refer interested readers to read more about GANs from <ref type="bibr" target="#b3">(Creswell et al., 2018)</ref>, <ref type="bibr" target="#b9">(Jabbar et al., 2020)</ref>, <ref type="bibr" target="#b14">(Kurach et al., 2018)</ref> and <ref type="bibr" target="#b39">(Wang et al., 2020)</ref>. Image-to-image translation: Recent image-toimage translation works like pix2pix , pix2pixHD <ref type="bibr" target="#b37">(Wang et al., 2018b)</ref> use conditional GANs to learn a mapping from source domain images to target domain images. While some rely on paired source-target images, works like CycleGAN, DualGAN, DiscoGAN <ref type="bibr" target="#b12">(Kim et al., 2017)</ref> and <ref type="bibr" target="#b32">(Tung et al., 2017)</ref>, <ref type="bibr" target="#b28">(Taigman et al., 2016)</ref>, <ref type="bibr" target="#b19">(Liu and Tuzel, 2016)</ref>, , <ref type="bibr" target="#b1">(Bousmalis et al., 2016)</ref> learn the mapping between the source domain and target domain without using any paired images. CoGAN <ref type="bibr" target="#b19">(Liu and Tuzel, 2016)</ref> also learns the joint distribution of multi-domain images by sharing weights of generators and discriminators. UNIT  uses a shared latent space framework built on CoGANs to learn a joint distribution of different domain images and achieves very high quality image translation results.</p><p>In an adversarial setting, image-to-image translation involves generators that learn mappings to translate images from a source domain to a target domain and vice-versa. Furthermore, adversarial methods that involve GAN either share network weights <ref type="bibr" target="#b19">(Liu and Tuzel, 2016)</ref>, <ref type="bibr" target="#b29">(Talreja et al., 2019)</ref> or use mechanisms <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, <ref type="bibr" target="#b44">(Zhu et al., 2017a</ref>) that involve a primal GAN and a dual GAN. A Dual-GAN (or DualGAN) <ref type="bibr" target="#b42">(Yi et al., 2017)</ref> setup employs two discriminators: a primal GAN and a dual GAN, performing inverse tasks of each other. Each discriminator is trained to discriminate target domain images as positive samples and translated source domain images as negative samples. Similarly, in CycleGAN <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, the primal-dual relation is regularized by a forward consistency loss and backward cycle consistency loss, which constitutes the cycleconsistency loss. This reduces the space of possible mappings by enforcing a strong relation across domains.</p><p>Conventionally, separate task-specific generators and discriminators are needed for image-to-image translation, since each network deals with a different set of real and fake images. However, StarGAN <ref type="bibr" target="#b2">(Choi et al., 2018)</ref> achieves multi-domain image translation using a single generator by considering each domain as a set of images with a common attribute (for e.g. hair color, gender, age, etc.) and by exploiting the commonalities in the datasets. Similarly, a Dual Generator GAN(G 2 GAN)  consists of two task-specific generators and single discriminator focusing on multi-domain image-to-image translation. However, their optimization objective is complex, consisting of five components including color consistency loss, MS-SSIM loss and conditional identity preserving loss for preventing mode collapse. While Dual Generator GAN uses a single discriminator, the underlying task is multi-domain image translation. However, in this paper we focus on the task of cross-domain image translation using a single shared discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We briefly explain the problem formulation in subsection 3.1, proposed framework in subsection 3.2, image pools in subsection 3.2.1, training stages in subsection 3.3 and loss functions in subsection 3.4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>For the image-to-image translation problem, our goal is to learn two mapping functions, G AB : A ? B and G BA : B ? A, between domains A and B modelled by generators G AB and G BA respectively. We consider task-specific generators since the input distribution is different for each task in cross-domain image-toimage translation. A domain is referred to as either source or target domain, based on its role in the translation task. The goal of the generator G AB is to translate an input image a from source domain A to the target domain, such that the generated image b * follows the distribution of the target domain B, p data (B). Likewise, the task of generator G BA is to translate an image b ? B to an image a * such that it follows the distribution of the target domain A, p data (A). We propose to provide adversarial supervision using a novel shared discriminator, D shared common to both the generators without using extra networks, masks or additional data. In this paper, we focus our method on transfiguration tasks, which requires translation of objects of interest while keeping other objects and the background same. Some transfiguration tasks include apples ? oranges, horses ? zebras, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Framework</head><p>Each translation task (A ? B and B ? A) is mapped to a separate generator. For guided image generation, we use conditional GANs <ref type="bibr" target="#b22">(Mirza and Osindero, 2014)</ref> that condition using the input images. During training, each generator learns to translate its input from source domain to the corresponding target domain. However, our approach differs from the conventional setting, which treats the target domain samples as real and translated images as fake. Instead, we exploit the fact that the data distributions of the source and the target domains of one translation task are the same as that of the target and the source domains of its inverse translation task.</p><p>In our novel formulation, the proposed shared discriminator D shared is trained to classify the generated images into either belonging to domain A or domain B. The translated images and random images from the two domains are conditioned on the input images to form the base for adversarial training using the shared discriminator. We hypothesize that this unification allows for domain-aware discrimination which is crucial for tasks like transfiguration, where a specific part of the image with distinct feature sets are to be transformed. GANs are infamous for unstable training and prone to model oscillation. To stabilize the model training, we leverage the power of image pools with modifications tailored for our approach. Once the training is complete, the generator outputs are treated as final prediction and the discriminators are not needed in inference stage. <ref type="figure">Figure 1</ref>: The above image corresponds to training stage 1, with two generators and two image pools. Here the generated images are pushed to the same image pool as that of the translation task. Shared discriminator has been avoided for brevity. <ref type="figure">Figure 2</ref>: The above image corresponds to training stage 2, with two generators and two image pools. Here the generated images are pushed to the image pool of the corresponding inverse translation task. Shared discriminator has been avoided for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Image Pools</head><p>Generally, the generator outputs are reused in imageto-image translation techniques that involves a reconstruction loss between the source image and the reconstructed image(the resulting image after undergoing two translations, from source domain to the target domain and back to the source domain). An image pool <ref type="bibr" target="#b27">(Shrivastava et al., 2016)</ref> is generally used to store a history of generated images to feed the discriminator in order to reduce the model oscillation during adversarial training. In our method, we associate an image pool to the generator of each translation task, such that the translated images can be reused as inputs to either of the generators by pushing to one image pool or the other, i.e. image pool I AB is associated with G AB and image pool I BA is associated with G BA (see <ref type="figure">Fig. 1 and Fig. 2</ref>). We use this simple tweak to improve the robustness of the generators to deal with variety of input images. In some cases, we also observe performance improvements, which we discuss later in the ablation study. Since the generated images are pushed to the image pools, each image pool gets a static input set of source domain images and an evolving input set of generated images from one generator or the other, depending upon the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training stages:</head><p>We consider the image-to-image translation task A ? B by learning the translation mapping G AB : A ? B and it's inverse translation mapping G BA : B ? A. Throughout the training process, the inputs to the generators G AB and G BA are from the image pools</p><formula xml:id="formula_0">I AB = {a 1 , a 2 , . . . , a |I AB | } and I BA = {b 1 , b 2 , . . . , b |I BA | } respectively.</formula><p>The image pools are initialized by the images of their source domains, A and B. The details of each training stage are given below. Training stage 1: If we consider the initial stages of training, the translated images appear closer in appearance to the source domain with very few target domain features. Therefore, we interleave the translated images from the generator with the source domain images using the same image pool, i.e. I AB would pool-in images a from A and b * from G AB (a), and I BA would pool-in images b from B and a * from G BA (b) as depicted in <ref type="figure">Fig. 1</ref>. Training stage 2: As training proceeds, the generators improve upon their translation capabilities and the generated images possess more target domain features and very few source domain features. Therefore, each generator can take the outputs of the other generator as adversarial images, in addition to their respective source domain images, i.e. I AB would pool-in images a from A and a * from G BA (b), and I BA would pool-in images b from B and b * from G AB (a) as depicted in <ref type="figure">Fig. 2</ref>. The generated images are pushed to the image pool of the inverse translation task, to mimic cyclic translations as done in some related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Functions</head><p>Conventionally, a discriminator is used to distinguish between real images from the dataset and fake images generated by the generator. However, we avoid the usage of the terms real images and fake images, and use abstract binary labels True and False instead. We assign the same labels for a domain irrespective of the translation task or their role in the translation task, i.e. the label assigned for the source domain images in the forward translation is the same as that of the target domain images in the inverse translation task. We assign true labels for domain B images and false labels for domain A images. Discriminator loss: All translated images are conditioned on the their input images when subjected to the discriminator, while optimizing the objectives of D shared , i.e. b * is conditioned on a and a * is conditioned on b . The generated images, a * from G AB (a ) and b * from G BA (b ) are labelled the same labels as their source domain images a and b respectively, while subjecting to the discriminator. The shared discriminator D shared is trained with a binary cross entropy loss L D shared . The goal of D shared is to classify the generated images into either domain A or domain B depending upon the source domain of the translation task. In addition, we subject the shared discriminator to random domain B images labelled as true and random domain A images labelled as false. These random images are conditioned on input images a or b depending on the translation task or whether they are input to generator G AB or G BA respectively. Formally, the complete training objective of D shared or the discriminator loss function is given by,</p><formula xml:id="formula_1">L D shared (G AB , G BA , D shared , A, B, I AB , I BA ) = E b?p data (b) [log(D shared (b|a ))]+ E a?p data (a) [log(1 ? D shared (a|a ))]+ E a ?p data (a ) [log(1 ? D shared (G AB (a )|a ))]+ E a?p data (a) [log(1 ? D shared (a|b ))]+ E b?p data (b) [log(D shared (b|b ))]+ E b ?p data (b ) [log(D shared (G BA (b )|b ))]. (1)</formula><p>The first three parts of Eq. 1 are conditioned on input images a from image pool I AB and represent the translation A ? B, while the latter parts are conditioned on the input images b from image pool I BA and represent the translation B ? A. Generator loss: We enforce a reconstruction loss between the generator's input and it's output involving only one image translation, in contrast to conventional pixel reconstruction objectives that involves translations over both directions. We choose a loss function that can preserve the median values, so that the objects of interest are translated without translating other objects in the image or the background. This motivates the use of L 1 pixel reconstruction loss between the input and output of each generator with additional help from adversarial training. The adversarial goal of each generator is to fool the shared discriminator into identifying generated images as belonging to the target domain images, i.e. G AB tries to map b * as belonging to B while G BA tries to map a * as belonging to A. The adversarial losses overrule the reconstruction loss over the membership score of the generated image, which results in the source images to take target domain features. We can express the full objective of G AB as the sum of Eq. 2, which corresponds to the adversarial loss and Eq. 3, which corresponds to the L 1 reconstruction loss. Similarly, we can express the full objective of G BA as the sum of Eq. 4, which corresponds to the adversarial loss and Eq. 5, which corresponds to the L 1 reconstruction loss.</p><formula xml:id="formula_2">L G AB (G AB , D shared , I AB ) = E a ?p data (a ) [log(D shared (G AB (a )))].</formula><p>(2)</p><formula xml:id="formula_3">L G AB pixel (G AB , I AB ) = E a ?p data (a ) [ G AB (a ) ? a 1 ].</formula><p>(3)</p><formula xml:id="formula_4">L G BA (G BA , D shared , I BA ) = E b ?p data (b ) [log(1 ? D shared (G BA (b )))].<label>(4)</label></formula><formula xml:id="formula_5">L G BA pixel (G BA , I BA ) = E b ?p data (b ) [ G BA (b ) ? b 1 ].<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We trained the tasks on 128x128 size images as well as on 256x256 size images. For training, the training images were resized to 1.125 times and were randomly cropped to the required size. The batch size for all our experiments was 4. Smaller batch sizes enable training with larger image sizes. Also, the image pools could be stored in the main memory or cuda device memory. We experimented with the Adam optimizer as well as RMSProp, and found that Adam gives better performance for most of our experiments. We used a learning rate of 0.0001 with the Adam optimizer with betas of 0.5 and 0.999. We used the adversarial loss for membership score with the vanilla GAN or binary cross entropy with logit loss. We used a lambda of 10.0 for the adversarial losses and a lambda in [100.0, 200.0] for the reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture:</head><p>We use identical network architecture for both the generators throughout an experiment. We conduct experiments with the Resnet <ref type="bibr" target="#b5">(He et al., 2015)</ref> architecture as well as Unet <ref type="bibr" target="#b26">(Ronneberger et al., 2015)</ref> architecture. While using the Unet architecture, the generator has the same number of downsampling layers and upsampling layers with a bottleneck in between and skip connections connecting the downsampling and upsampling layers. Our proposed method doesn't use noise vectors as in the pix2pix implementation . Also, using dropout doesn't affect the performance of our method when implemented with the Unet architecture. In the Resnet architecture, the skip connections exist between Resnet blocks. The discriminator's architecture used in our experiments is PatchGAN <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>.  <ref type="figure">Figure 3</ref>: Comparison of test images generated by different methods <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>, , <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>,  (left to right) on zebra to horse task. Leftmost column shows the input, rightmost column shows results from our method. <ref type="figure">Figure 4</ref>: Comparison of test images generated by different methods <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>, , <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>,  (left to right) on horse to zebra task. Leftmost column shows the input, rightmost column shows results from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets:</head><p>We used apples to oranges dataset and horse to zebra dataset which were originally used in CycleGAN <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>. These images are available from Imagenet with a training set size of each class having 939 (horse), 1177 (zebra), 996 (apple), and 1020 (orange) images. <ref type="table">Table 2</ref>: FID scores between generated samples and target samples for horse to zebra translation task on methods , <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b41">(Yang et al., 2018)</ref>,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation metric:</head><p>We use the Frechet Inception Distance(FID) <ref type="bibr" target="#b6">(Heusel et al., 2017)</ref> and Kernel Inception Distance(KID) <ref type="bibr" target="#b0">(Bi?kowski et al., 2018)</ref> preferably over metrics like Inception score. For both metrics, lower scores imply similarities in features between the compared sets of images. However, both metrics are adversely affected by the presence of adversarial noise and hallucinated features in the generated images that these metrics do not correlate to the judgement by human perception. This suggests that either metrics aren't better than each other, and better scores doesn't always imply better translation results. Hence, we consider those FID and KID scores from our experiments which are positively correlated.  <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>, <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, , <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>We compute the KID score over 100 iterations and return its mean, while the FID scores are computed over 10 iterations and the mean value is returned. We compute the KID scores and FID scores on the test data using the generator models from the same checkpoint. We trained the tasks on 128x128 size images as well as on 256x256 size images and tested both category of models on 256x256 test images. We refer  to compile the experimental results in the qualitative comparisons and metric scores in <ref type="table" target="#tab_2">Table 3</ref> and 2. We report the performance comparison of different architectures on translation tasks horse ? zebra and apples ? oranges, measured in FID and KID scores in <ref type="table" target="#tab_0">Table 1</ref>. We report the FID scores on horse ? zebra translation task in <ref type="table">Table 2</ref>. KID scores are compared over the horses ? zebras task and apples ? oranges task in <ref type="table" target="#tab_2">Table 3</ref>. The results of qualitative comparisons includes the comparison of translated images from zebras ? horses task in <ref type="figure">Fig. 3</ref>, horses ? zebras task in <ref type="figure">Fig. 4</ref>, oranges ? apples task in <ref type="figure">Fig. 5</ref>, and apples ? oranges task in <ref type="figure">Fig. 6</ref>. <ref type="figure">Figure 5</ref>: Comparison of test images generated by different methods <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>, , <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>, ) (left to right) on oranges to apples task. Leftmost column shows the input, rightmost column shows results from our method. <ref type="figure">Figure 6</ref>: Comparison of test images generated by different methods <ref type="bibr" target="#b44">(Zhu et al., 2017a)</ref>, <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>, , <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>, ) (left to right) on apples to oranges task. Leftmost column shows the input, rightmost column shows results from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>The results from <ref type="table" target="#tab_0">Table 1</ref> suggest that there is a slight gain in the performance by using dropout with the Resnet network architecture for the horse ? zebra task, while the same is more or less not true for apples ? oranges task. We hypothesize that this observation could be due to the simplicity of the apples ? oranges task while the former task is more complex.</p><p>The results from <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 2</ref> suggest that our method is at par to existing image translation methods where some related methods have an upper hand due to the underlying attention mechanisms. On comparing the qualitative results for zebras ? horses in <ref type="figure">Fig.3</ref>, in the second row we can notice that the text color and the background are preserved only in the translated images from UAIT <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>, AttentionGAN  and our method. Also, our method has comparable results to <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref> and , which uses attention mechanisms. While CycleGAN does a great job in translating all zebra images to horse images, the background color and tint are affected in some of the images and is severe than ours.</p><p>On comparing the qualitative results for horses ? zebras in <ref type="figure">Fig. 4</ref>, we notice that residual attention based method <ref type="bibr" target="#b34">(Wang et al., 2017)</ref> generates convincing translation results, while there is a green tint in all the translated images which makes it unfavourable. Similarly, UNIT  also has artifacts in the background that makes the appearance just acceptable. Our method falls behind UAIT <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref> and AttentionGAN results  but appears better than CycleGAN results, which has undesirable background tint in many images. Note that the translation quality drastically dropped for DualGAN <ref type="bibr" target="#b42">(Yi et al., 2017)</ref>, with slightly better results from DiscoGAN <ref type="bibr" target="#b12">(Kim et al., 2017)</ref>.</p><p>On comparing the qualitative results for oranges ? apples in <ref type="figure">Fig. 5</ref>, we notice that our translation results are at par with UAIT <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref> and AttentionGAN , which uses attention mechanisms. CycleGAN also follows our results except that it fails in some of the images with unwanted background translations. DualGAN, DiscoGAN, UNIT and residual attention <ref type="bibr" target="#b34">(Wang et al., 2017)</ref> fails on a task much easier than the horses ? zebras task.</p><p>On comparing the qualitative results for apples ? oranges in <ref type="figure">Fig. 6</ref>, we notice that our method consistently keeps the quality upto the mark of attention guided methods <ref type="bibr" target="#b21">(Mejjati et al., 2018)</ref>, . While CycleGAN is able to translate convincingly, it is affected by a strong tint in some of the images. The translation results from UNIT and residual attention method appear similar to that of DualGAN and DiscoGAN, despite the use of attention mechanisms. Ablation study: We perform an ablation study to isolate the effects and understand the effectiveness of various components of our method using FID and KID metric over horse ? zebra task comparing the baseline to different variants. We consider the original shared discriminator setup, D shared to be the baseline for comparing the variants. The quantitative results of the ablation study are available in <ref type="table" target="#tab_4">Table 4</ref> and 5 for images trained on 128x128 size images and tested on 256x256 size images and in <ref type="table" target="#tab_6">Table 6</ref> and 7 for images both trained and tested on 256x256 size images.</p><p>First, we modify the objective of the shared discriminator in a variant D shared 1 to see if all six components are really necessary. Out of the six components of the shared discriminator objective, four of them involves random source or target domain images conditioned on either a or b . It may seem logical to remove two random image components of one translation task or the other to make the shared discriminator objective compact, since they differ only in the conditioned part, i.e. a or b . To verify that, we deal with each domain only once and as target domain in the variant D shared 1 . The source domain images conditioned on the input images are not subjected to the shared discriminator and avoided in the objective assuming that the same domain images as target domain and labels will suffice. In other words, for the image translation task A ? B, we consider only random target domain images b from B with true labels conditioned on the input images a to the shared discriminator. Analogously, for the image translation task B ? A, we consider only random target domain images a from A, conditioned on the input images b with false labels to the shared discriminator. The generator's goal and objectives are unaltered in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dshared Dshared1</head><p>No Image pool No stage-1 No Stage-2 Horse 0.065 ? 0.003 0.084 ? 0.002 0.088 ? 0.002 0.085 ? 0.002 0.107 ? 0.002 Zebra 0.036 ? 0.002 0.047 ? 0.003 0.063 ? 0.003 0.067 ? 0.003 0.050 ? 0.002  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dshared Dshared1</head><p>No Image pool No stage-1 No Stage-2 Horse 0.069 ? 0.002 0.090 ? 0.002 0.070 ? 0.002 0.072 ? 0.002 0.077 ? 0.002 Zebra 0.030 ? 0.002 0.076? 0.004 0.036 ? 0.003 0.047 ? 0.003 0.045 ? 0.003 this variant. The results in <ref type="table" target="#tab_4">Table 4</ref>, 5, 6 and 7 from the ablation study for D shared 1 (or variant 1 ) indicate that irrespective of the image sizes used for training, the performance of the shared discriminator setup drops on removing the components of D shared 's objectives. Both FID and KID values have gone higher for D shared 1 , which is not desirable for good image translation results.</p><p>The second variant that we consider is a shared discriminator setup without the image pool, i.e. the translated images are not reused as inputs to any of the generators. While the results in <ref type="table" target="#tab_6">Table 6</ref> and 7 suggest that using image pool doesn't improve the performance of our method, the results in <ref type="table" target="#tab_4">Table 4</ref> and 5 suggest that there is considerable drop in performance when the image pool is not used while training on smaller images and testing the model on larger images. We hypothesize that the generators become more robust when trained with additional translated images with the help of image pools.</p><p>The third variant that we consider is a shared discriminator setup without the training stage-1, i.e. the translated images are pushed to the image pool of the inverse translation task, throughout the training process. Similarly, the fourth variant that we consider is a shared discriminator setup without the training stage-2, i.e. the translated images are pushed back to the image pool of the same translation task throughout the training process. The results in <ref type="table" target="#tab_6">Table 6</ref> suggest that FID values are not really affected for variant 3 and variant 4 , while <ref type="table" target="#tab_7">Table 7</ref> suggests that the KID values increase (or performance drops) for variant 3 and variant 4 when either training stage-1 or stage-2 is used throughout the training process. Similarly, the results from <ref type="table" target="#tab_4">Table 4</ref> and 5 for variant 3 and variant 4 indicates that the performance drops on using only one of the training stages. We hypothesize that simply reusing translated images with an image pool doesn't improve the performance and can result in a drop in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>In this paper, we propose a framework for image transfiguration, a cross-domain image-to-image translation task improving the efficacy using a shared discriminator in an unsupervised setting. We also introduce a novel application of image pools to keep the generators more robust in the process. The qualitative and quantitative results, using metrics like FID and KID, suggest that our method, even without using masks or attention mechanisms, is at par with attention-based methods. For particular tasks, where the source domain shares similar semantics with the target domain, our method performs better than previous methods. Also, we observe that metrics like KID and FID are insufficient to evaluate the quality of translated images. They are also vulnerable to adversarial noise and hallucinated features, hampering a fair comparison of image translation methods. Future work could use attention mechanisms to further improve the results and better comparison metrics that correlate to human perception.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of network architectures<ref type="bibr" target="#b26">(Ronneberger et al., 2015)</ref> and<ref type="bibr" target="#b5">(He et al., 2015)</ref> on translation tasks horse ? zebra and apples ? oranges. The results are compared using FID and KID scores.</figDesc><table><row><cell>FID</cell><cell>Horse</cell><cell>Zebra</cell><cell>Apples</cell><cell>Oranges</cell></row><row><cell>Unet</cell><cell cols="4">211.76 ? 3.65 119.99 ? 14.01 164.87 ? 4.20 172.30 ? 2.33</cell></row><row><cell cols="2">Resnet 210.37 ? 5.10</cell><cell>97.47 ? 7.85</cell><cell cols="2">168.86 ? 3.20 172.30 ? 2.33</cell></row><row><cell>KID</cell><cell>Horse</cell><cell>Zebra</cell><cell>Apples</cell><cell>Oranges</cell></row><row><cell>Unet</cell><cell>0.063? 0.002</cell><cell>0.046?0.003</cell><cell cols="2">0.051 ? 0.003 0.044? 0.002</cell></row><row><cell cols="2">Resnet 0.058 ? 0.002</cell><cell>0.030 ? 0.002</cell><cell cols="2">0.052 ? 0.002 0.044 ? 0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(from top to bottom). For this metric, lower is better.</figDesc><table><row><cell>Method</cell><cell>Horse ? Zebra</cell></row><row><cell>UNIT</cell><cell>241.13</cell></row><row><cell>CycleGAN</cell><cell>109.36</cell></row><row><cell>SAT (Before Attention)</cell><cell>98.90</cell></row><row><cell>SAT (After Attention)</cell><cell>128.32</cell></row><row><cell>AttentionGAN</cell><cell>68.55</cell></row><row><cell>Ours</cell><cell>92.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>KID ? 100 ? std. ? 100 compared for different methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>DiscoGAN 13.68 ? 0.28 16.60 ? 0.50 18.34 ? 0.75 21.56 ? 0.80 RA 10.16 ? 0.12 10.97 ? 0.26 12.75 ? 0.49 13.84 ? 0.78 DualGAN 10.38 ? 0.31 12.86 ? 0.50 13.04 ? 0.72 12.42 ? 0.88 UNIT 11.22 ? 0.24 13.63 ? 0.34 11.68 ? 0.43 11.76 ? 0.51</figDesc><table><row><cell></cell><cell></cell><cell cols="3">(from left to right). Abbreviations:</cell></row><row><cell cols="3">(H)orse, (Z)ebra (A)pple, (O)range.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>H ? Z</cell><cell>Z ? H</cell><cell>A ? O</cell><cell>O ? A</cell></row><row><cell>CycleGAN</cell><cell cols="3">10.25 ? 0.25 11.44 ? 0.38 8.48 ? 0.53</cell><cell>9.82 ? 0.51</cell></row><row><cell>UAIT</cell><cell>6.93 ? 0.27</cell><cell>8.87 ? 0.26</cell><cell>6.44 ? 0.69</cell><cell>5.32 ? 0.48</cell></row><row><cell cols="2">AttentionGAN 2.03 ? 0.64</cell><cell cols="3">6.48 ? 0.51 10.03 ? 0.66 4.38 ? 0.42</cell></row><row><cell>Ours</cell><cell>3.00 ? 0.20</cell><cell>5.80 ?0.20</cell><cell>4.40 ? 0.20</cell><cell>5.10 ? 0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on baseline, variant 1 , variant 2 , variant 3 and variant 4 (from left to right) evaluated using FID scores. The study included training on 128x128 size images and testing on 256x256 size images. ? 7.495 136.63 ? 10.444 139.77 ? 10.643 119.39 ? 7.025</figDesc><table><row><cell>FID</cell><cell>Dshared</cell><cell>Dshared1</cell><cell>No Image pool</cell><cell>No stage-1</cell><cell>No Stage-2</cell></row><row><cell cols="2">Horse 207.93 ? 6.26</cell><cell>218.74 ? 4.69</cell><cell>216.10 ? 7.659</cell><cell>221.04 ? 5.005</cell><cell>224.02 ? 6.056</cell></row><row><cell>Zebra</cell><cell>92.91 ? 6.58</cell><cell>100.90</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study results on baseline, variant 1 , variant 2 , variant 3 and variant 4 (from left to right) evaluated using KID scores. The study included training on 128x128 size images and testing on 256x256 size images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study results on baseline, variant 1 , variant 2 , variant 3 and variant 4 (from left to right) trained and tested on 256x256 sizes and evaluated using FID scores.Horse 212.81 ? 4.835 221.66 ? 6.185 213.64 ? 4.357 216.28 ? 4.884   217.67 ? 6.864</figDesc><table><row><cell>FID</cell><cell>Dshared</cell><cell>Dshared1</cell><cell>No Image pool</cell><cell>No stage-1</cell><cell>No Stage-2</cell></row><row><cell>Zebra</cell><cell>92.72 ? 9.915</cell><cell>148.95 ? 4.470</cell><cell>96.16 ? 5.251</cell><cell cols="2">118.63 ? 9.380 113.30 ? 11.212</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Ablation study results on baseline, variant 1 ,</cell></row><row><cell>variant 2 , variant 3 and variant 4 (from left to right) trained</cell></row><row><cell>and tested on 256x256 sizes and evaluated using KID</cell></row><row><cell>scores.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<title level="m">Demystifying MMD GANs. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno>abs/1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">; Z</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>Ghahramani,</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1804.04732</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<title level="m">A survey on generative adversarial networks: Variants, applications, and training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention-aware generative adversarial networks (ata-gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kastaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ntinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsourounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Economou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fotopoulos</surname></persName>
		</author>
		<idno>abs/1802.09070</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The GAN landscape: Losses, architectures, regularization, and normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>abs/1807.04720</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generative semantic manipulation with contrasting GAN. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1708.00315</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1703.00848</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>abs/1905.01723</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno>abs/1606.07536</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1705.09368</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised attention-guided image to image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Mejjati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1806.02311</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<idno>abs/1612.07828</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attribute-guided coupled gan for crossresolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Talreja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Valenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11897</idno>
		<title level="m">AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual Generator Generative Adversarial Networks for Multi-domain Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11361</biblScope>
			<biblScope unit="page" from="3" to="21" />
			<date type="published" when="2019" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno>abs/1705.11166</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1609.02612</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1704.06904</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12713</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Few-shot Video-to-Video Synthesis</note>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<title level="m">Deep Learning for Image Super-resolution: A Survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generative adversarial networks in computer vision: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">GP-GAN: Towards Realistic High-Resolution Image Blending</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Show, attend and translate: Unsupervised image translation with self-regularization and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno>abs/1806.06195</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
