<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Alignment for Approximated Reversibility in Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>De</surname></persName>
							<email>tiago939@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de F?sica</orgName>
								<orgName type="department" key="dep2">Centro de Ci?ncias Naturais e Exatas</orgName>
								<orgName type="institution">Universidade Federal de Santa Maria Avenida Roraima 1000</orgName>
								<address>
									<settlement>Santa Maria</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souza</forename><surname>Farias</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de F?sica</orgName>
								<orgName type="department" key="dep2">Centro de Ci?ncias Naturais e Exatas</orgName>
								<orgName type="institution">Universidade Federal de Santa Maria Avenida Roraima 1000</orgName>
								<address>
									<settlement>Santa Maria</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Maziero</surname></persName>
							<email>jonas.maziero@ufsm.br</email>
							<affiliation key="aff1">
								<orgName type="institution">Rio Grande do Sul</orgName>
								<address>
									<postCode>97105-900</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Alignment for Approximated Reversibility in Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural networks</term>
					<term>features</term>
					<term>reversibility</term>
					<term>local training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce feature alignment 1 , a technique for obtaining approximate reversibility in artificial neural networks. By means of feature extraction, we can train a neural network to learn an estimated map for its reverse process from outputs to inputs. Combined with variational autoencoders, we can generate new samples from the same statistics as the training data. Improvements of the results are obtained by using concepts from generative adversarial networks. Finally, we show that the technique can be modified for training neural networks locally, saving computational memory resources. Applying these techniques, we report results for three vision generative tasks: MNIST, CIFAR-10, and celebA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature extraction <ref type="bibr" target="#b50">(Olah et al., 2017)</ref> is a set of techniques aiming to find inputs, known as features, that maximize the activation of one or more neurons. Usually, feature extraction is used as a method for model interpretability, where one seeks to understand a neural network by analyzing how much each neuron contributes for modelling the training data. The process of obtaining these features is, in a sense, an attempt towards reversing a neural network. Since a neural network is composed by functions which map inputs to outputs, a feature is the input we would have given a target activation for a group of posterior selected neurons.</p><p>Reversibility of neural networks relates to how well one can reverse the map from the activation of target neurons to the input neurons (Aidan N. <ref type="bibr" target="#b1">Gomez and Grosse, 2017)</ref>. Usually, neural networks are not reversible, due to a few reasons: non-reversible activation functions, for example ReLU (Agarap, 2019), non-orthogonal weights, and lack of one-to-one relationships. One advantage of reversible neural networks, besides the reversing mapping, is its memory efficiency: while non-invertible networks need to store all the activations for the backward pass during training, reversible networks allows the storage of fewer activations to update the trainable parameters.</p><p>Reversibility also constrains the number of possible models, as there are many possible parameters configurations which model the data. For example, if one considers an analytical function that one wishes to approximate by a sufficiently parameterized neural network, with the pair of data {x, f (x)}, there are a number of local minima which estimates the function x ? f (x), each one obtained by a different random initialization of the neural network parameters (assuming optimal convergence). By imposing the restriction of reversibility f (x) ? x, we can reduce the number of optimal points toward which a neural network can converge.</p><p>Memory is often a bottleneck for neural networks. The backpropagation algorithm <ref type="bibr" target="#b42">(Linnainmaa, 1976;</ref><ref type="bibr" target="#b54">Rumelhart et al., 1986)</ref>, widely used in modern deep learning for training, requires the storage of the activation of all neurons of a network in order to update its parameters. Local training rules <ref type="bibr" target="#b5">(Baldi and Sadowski, 2016</ref>) allow a more efficient memory optimization of neural networks. By constraining the trainable parameters, such as the weights, to be updated only by local variables (the information contained in the neurons that share the same parameter), we can reduce the memory requirements to load a model in hardware such as CPUs and GPUs. This constraint can save memory resources and has many potential applications, from low-memory devices <ref type="bibr" target="#b68">(Velichko, 2020;</ref><ref type="bibr" target="#b63">Sohoni et al., 2019)</ref> to training large batch sizes <ref type="bibr" target="#b15">(Gao and Zhong, 2020;</ref><ref type="bibr" target="#b70">You et al., 2017)</ref>, and, even further, to train very large neural networks <ref type="bibr" target="#b27">(Jing and Xu, 2019)</ref>.</p><p>This paper makes use of feature extraction for inverse mapping, a technique for approximated reversibility of neural networks that can be implemented in several architectures. We show that feature alignment makes neural networks approximately reversible for a given training data. We also adapt the technique for local training, showing results for the reconstruction of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Several works have been done on the area of feature extraction, specially applied for model interpretability and explainability <ref type="bibr" target="#b61">(Shahroudnejad, 2021;</ref><ref type="bibr" target="#b13">Fan et al., 2021;</ref><ref type="bibr" target="#b16">Gilpin et al., 2019)</ref>. These techniques, used for extracting features, usually consist in activation maximization <ref type="bibr" target="#b45">(Mahendran and Vedaldi, 2016)</ref>, where a group of neurons, which can involve from a single neuron up to an entire layer (or channel for convolutions), is selected to extract the feature by maximising its activation.</p><p>Many techniques of feature extraction consist in studying features in already pretrained classifiers <ref type="bibr" target="#b49">(Nguyen et al., 2016b)</ref>. Other techniques consist in searching for features in the latent space <ref type="bibr" target="#b71">(Yujun Shen and Zhou, 2020)</ref>. Feature extraction can also be used for understanding which parts of an input contribute the most for the target activations <ref type="bibr" target="#b60">(Selvaraju et al., 2020;</ref><ref type="bibr" target="#b65">Springenberg et al., 2015;</ref><ref type="bibr" target="#b72">Zintgraf et al., 2017)</ref>.</p><p>In a generative process, we want to produce new examples with the same statistical distribution as the training data. There are different techniques to model the data for generation. Among these techniques, autoencoders, generative adversarial networks, and normalizing flows are very popular. Autoencoders (AE) have two networks: an encoder that projects the inputs into a vector, usually with smaller dimension, and a decoder that reconstructs the input from this vector. The compressed vector has a high level representation of the model, in which each neuron contributes to properties beyond the data level at the input layer <ref type="bibr" target="#b38">(Lee et al., 2011)</ref>. Autoencoders are commonly trained in a unsupervised fashion, nevertheless there are variants that include labeled information to further increase training for a specific objective. Variational autoencoders (VAE) <ref type="bibr">Welling, 2014, 2019;</ref><ref type="bibr" target="#b10">Doersch, 2021)</ref> improve the sampling ability by projecting the data into a prob-abilistic latent vector. Generative adversarial networks (GANs) <ref type="bibr" target="#b17">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b55">Salehi et al., 2020;</ref><ref type="bibr" target="#b19">Gui et al., 2020)</ref> are another example of a generative method. By having two competing networks, a generative which takes a random low-dimensional input and outputs an image, and a discriminator network that compares the images from the training data set and the sampled ones from the generator. The competition arises by training the generator to fool the discriminator by generating images as closest to the training data set as possible. Normalizing flows <ref type="bibr" target="#b52">(Papamakarios, 2019;</ref><ref type="bibr" target="#b33">Kobyzev et al., 2020;</ref><ref type="bibr" target="#b30">Kingma and Dhariwal, 2018)</ref> generate images by transforming a simple distribution to a more complex one by a series of reversible transformations.</p><p>There have been works combining autoencoders with <ref type="bibr">GANs (Larsen et al., 2016)</ref>. The work done in Refs. <ref type="bibr" target="#b48">Nguyen et al. (2016a)</ref>; <ref type="bibr" target="#b12">Dosovitskiy and Brox (2016)</ref> is related to ours. They synthesized new images with the same statistics as the training data by imputing features to a generator network. The main difference is that, in these previous articles, the features are obtained with a pretrained network.</p><p>Most works on reversibility consist in architectural changes of neural networks <ref type="bibr" target="#b4">(Atapattu and Rekabdar, 2019;</ref><ref type="bibr" target="#b58">Schirrmeister et al., 2018;</ref><ref type="bibr" target="#b18">Grathwohl et al., 2018;</ref><ref type="bibr" target="#b6">Behrmann et al., 2019;</ref><ref type="bibr" target="#b39">Leemon Baird and Ingkiriwang, 2005)</ref>. These changes guarantee a one-to-one relationship between inputs and outputs. BiGAN <ref type="bibr" target="#b11">(Donahue et al., 2017)</ref> constructs a generative network and a reverse network that inputs images back to noise, which can be used to obtain a latent representation of a data set directly.</p><p>Local learning rules have been explored since Donald Hebb proposed a simple model for learning in the brain <ref type="bibr" target="#b21">(Hebb, 1949)</ref>. The main advantage of this kind of learning algorithm is requiring lower memory resources. Some works are biologically inspired <ref type="bibr" target="#b36">(Krotov and Hopfield, 2019;</ref><ref type="bibr" target="#b41">Lindsey and Litwin-Kumar, 2020)</ref>, while others focus solely on efficiency <ref type="bibr">Toyoizumi, 2016, 2018)</ref>. There is a growing body of work discussing whether the brain does backpropagation <ref type="bibr" target="#b69">(Whittington and Bogacz, 2019;</ref><ref type="bibr" target="#b64">Song et al., 2020)</ref>, with some approximations for training artificial neural networks <ref type="bibr" target="#b47">(Millidge et al., 2020;</ref><ref type="bibr" target="#b56">Salvatori et al., 2021;</ref><ref type="bibr" target="#b40">Lillicrap et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Let E(x; ?) be a neural network with inputs x and trainable parameters ?. The output is a latent vector z x with an arbitrary number of neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature alignment</head><p>The feature alignment encoder consists of a neural network with parameters ? and an arbitrary number of latent variables as the output. From a data set x ? X, z x = E(x; ?) is the output from an input x. With the same network, z r = E(r; ?) is the output from a random input r with the same dimension as the input data.</p><p>The featurer of z(x) is obtained by minimizing a distance function L(z x , z r ) with respect to the random inputs r. By minimizing this distance, the random input will evolve continually according to a gradient flow:</p><formula xml:id="formula_0">?r ?t = ? ?L ?r .</formula><p>(1) <ref type="figure">Figure 1</ref>: In feature alignment, we have two encoders that share the same parameters. Left: at training time, the network is trained to approximater ? x . Right: during inference, we sample z x from either the data or a known distribution to obtain the features.</p><p>Since z(x) is fixed, r will evolve such as the function of the random variables will approximate z(x) as much as possible (See appendix A.1). We want to solve equation <ref type="formula">(1)</ref> as efficiently as possible in time and memory. By discretizing the gradient flow, we obtain an approximation for the feature:</p><formula xml:id="formula_1">r t = r t?1 ? ? ?L ?r ,<label>(2)</label></formula><p>with ? being a hyperparameter that weights the contribution of the gradient. Equation <ref type="formula" target="#formula_1">(2)</ref> is similar to activation maximization, except that we are minimising for the neurons to have a target activation. These updates are done in T time steps. Properly optimized, the solution to equation (2) converges to a feature by approximating the inverse of the weights (see Appendix 1). So then, optimizing the parameters of the network, the weight matrix between layers will have the orthogonal property w T w = I, which implies in approximated reversibility (see Appendix 2). After we extract the featurer, we measure how similar it is to the inputs x by a new loss function L(x,r). This second loss function is used for training the encoder by optimizing its parameters. As the neural network is trained, the encoder learns, not only to map the inputs to the latent variables, but also the reconstruction of the inputs from the latent vector. Following training, we can reconstruct the inputs by knowing only the latent vector. <ref type="figure">Figure 1</ref> and Algorithm 1 summarize the feature alignment technique. Notice that we initialize r with the same shape as x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational autoencoders with feature alignment (VFA)</head><p>One issue with the encoder, and autoencoders in general, is its inability to generate new samples with the same statistical distribution as the training data. The latent variables from the data are associated to a distribution of variables which might be too complicated for an effective sampling. To solve this problem, we use a variational autoencoder (VAE) formulation, except by removing the decoder network. In the VAE, the output of the encoder is coupled with two layers that return the mean value ? x and variance ? 2</p><p>x of the Algorithm 1 Training with feature alignment</p><formula xml:id="formula_2">z x = E(x; ?) initialize r = x.shape t = 0 while t &lt; T do z r = E(r; ?) L = ||z x ? z r || 2 2 r = r ? ? ?C ?r t = t + 1 end whil? r = r C = ||x ?r|| 2 2 update ? by optimizing C</formula><p>data. We constrain the latent vector to have a distribution that is easy to sample (usually a Gaussian distribution), by comparing two probability distributions via a metric such as the Kullback-Leibler divergence. So, the cost function:</p><formula xml:id="formula_3">L = ||x ?r|| 2 2 ? ?D KL (q ? (z|x)||p(z)),<label>(3)</label></formula><p>is used to train the feature encoder with a constraint to the output latent variables from a known random probability distribution p(z), from which we can easily sample. The constant ? is a hyper-parameter that regularizes the latent vector for better disentanglement representation of the data <ref type="bibr" target="#b23">(Higgins et al., 2016;</ref><ref type="bibr" target="#b9">Burgess et al., 2018;</ref><ref type="bibr" target="#b62">Sikka et al., 2019)</ref>. We choose the distribution p(z) according to the principle of maximum entropy: since the latent variables are in the range (??, +?), the Gaussian distribution is best suited in this case. We then constrain each latent variable as to come from a Gaussian distribution with zero mean value and variance equal to one. Just as the VAE, we can not train the encoder by sampling directly from the mean and variance of the latent vector. Instead, we use the renormalization trick: we sample a random vector from a normal distribution, the latent vector is then: z x = ? x + ? 2 x , with the element-wise product. Note that we do not have an random normal vector for z r , since its purpose is solely to reconstruct x. <ref type="figure" target="#fig_1">Figure 3</ref> summarizes training a VAE with feature alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improving the quality of the features</head><p>As will be shown in the results section, the features extracted using the feature alignment trained with VAE are blurry, and this is due to the variational autoencoder nature (Rezende and Viola, 2018). To improve the quality of the generated images, we couple a generator network G and a discriminator network D to the features. In this way, the feature acts as a second latent vector, with a more complex distribution sampled from the simpler one. This generator network is similar to the refiner network presented in Ref. <ref type="bibr" target="#b4">Atapattu and Rekabdar (2019)</ref>, which take an image as input and outputs an improved version of it.</p><p>The generator takes the feature as input and produces a new output which is compared to the input x. The discriminator is trained as a generative adversarial network, evaluating the probability that G(r) is real or fake. The generator is updated by receiving gradients from the discriminator. For more stable training, we use the least square loss for the discriminator <ref type="bibr" target="#b46">(Mao et al., 2017)</ref>. Alternatively, we can use the Wasserstein GAN formulation <ref type="bibr" target="#b3">(Arjovsky et al., 2017)</ref>, which replaces the discriminator by a critic network that measures the score of the "realness" of an image. To reduce possible effects due to the posterior collapse problem in VAEs <ref type="bibr" target="#b67">(Takida et al., 2021;</ref><ref type="bibr" target="#b20">Havrylov and Titov, 2020;</ref><ref type="bibr" target="#b44">Lucas et al., 2019)</ref> and to balance with the reconstruction loss, we propose a random schedule for the ? variable. For each example during training, we sample ? from a uniform distribution U(0, 1).</p><p>While the reconstruction of images is optimized usually with pixel level loss, we can also consider high level properties of the data. This kind of measure, called perceptual loss <ref type="bibr" target="#b28">(Johnson et al., 2016)</ref>, compares the output of the reconstruction with the original image at high level neurons (presented near the end of the network). Here we consider the perceptual loss using the mean and variance layers from the encoder network, enforcing the reconstruction to have the same statistical properties of the original input. The final losses, for the encoder, generator, and discriminator are, respectively:</p><formula xml:id="formula_4">L E = ||x ?r|| 2 2 + ?D KL (q ? (z|x)||p(z)),<label>(4)</label></formula><formula xml:id="formula_5">L G = ||1 ? D(G(r))|| 2 2 + ?(||?(x) ? ?(G(r))|| 2 2 + ||? 2 (x) ? ? 2 (G(r))|| 2 2 ),<label>(5)</label></formula><formula xml:id="formula_6">L D = ||1 ? D(x)|| 2 2 + || ? 1 ? D(G(r))|| 2 2 .<label>(6)</label></formula><p>with ? a hyperparameter that weights the perceptual loss contribution. If a training data has additional information, such as labels, we can train a neural network jointly with supervised training for specific related tasks such as conditional generation, where we want samples that correspond to a desired class. We can condition the latent variables to have different distributions from different classes. The output of the network is trained from the Gaussian distribution z ? Q(z i , c i ), with c i being a one-hot vector containing the class information. Similar to the work done in Ref. <ref type="bibr" target="#b2">Ardizzone et al. (2019)</ref>, we couple a linear classification layer on top of the network, parallel to the mean and variance layers. Since the class layer is linear, at inference time we can choose a greater value for the one-hot vector, which puts emphasis on the chosen class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Local feature alignment</head><p>The rules for feature alignment were presented as a global rule: the auxiliary loss is defined with the output layer and the loss is defined with the input layer, so we have full communication between all layers. However, we can reformulate this rule with local losses, similar to target propagation rules <ref type="bibr" target="#b14">(Farias and Maziero, 2018;</ref><ref type="bibr" target="#b8">Bengio, 2014;</ref><ref type="bibr" target="#b51">Ororbia et al., 2018)</ref>: the auxiliary loss and loss are defined as the interaction between two connected layers only (or even individual neurons), as follows: for each layer from the first to the last, we activate it from the its inputs and store a second activation from a random input with the same dimension, we then extract the feature by optimizing the random input with an auxiliary loss between activation of the random output and the true input. Finally, the parameters of the chosen layer is updated by optimizing the loss between the reconstruction and true input. This technique of local training is summarized in the Algorithm 2 and illustrated at FIG. 4.</p><p>The neural network becomes a predictive machine, where each layer trains its parameters by trying to predict the inputs by knowing the outputs. The constraint of the local learning has a more pronounced effect on the non-linearity of a neural network trained in this way. While backpropagation can adjust all the parameters of a network so that the feature reconstructs the input, local rules can only rely on very strict information content available. Non reversible functions, such as the ReLU function, propagate loss of information, which will lead to reconstructions with low fidelity. Thus, a non-linear function must be carefully chosen in order to preserve as much information as possible. The function inverse hyperbolic sine (arsinh(x) = ln(x + ? 1 + x 2 ))), is similar to the hyperbolic tangent near zero and logarithmic at large (absolute) values. This function has the properties of being fully invertible, zero-centered mean, unbounded, continuously differentiable and its gradient does not vanish as fast as tanh. These properties make arsinh a good candidate function for local training.  At inference time, we need to propagate the information backwards from the output to the input layer by layer just as one would do normally with non-local feature extraction. However, the non-linear function will play a major role here, it being reversible will be required to approximate reversibility. This is done by, after inputting the latent vector, applying the inverse of the non-linear function after each layer that uses the function. This process is summarized in the Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>The encoder network consists of a series of convolutional layers, similar to the AlexNet architecture <ref type="bibr" target="#b35">(Krizhevsky et al., 2012)</ref>, but with stride one and two for down-scaling, instead of maxpool, with LeakyReLU activation. The generator network has three convolutional layers. The discriminator network has the same architecture as the encoder, except for z l x =r 16: end for the last layer that outputs a single value for each example. Only the generator uses batch normalization after each convolution. All convolutions have kernel size k = 3.</p><p>We use the Adam optimizer (Kingma and Ba, 2017) with learning rate ? = 0.00001 and batch size 128. The parameters of the encoder and generator networks are initialized with orthogonal initialization <ref type="bibr" target="#b57">(Saxe et al., 2014;</ref><ref type="bibr" target="#b24">Hu et al., 2020)</ref>. We set the hyperparameter ? = 0.01 and sample ? from a uniform distribution, with a different random value for each training example.</p><p>From Appendix 2, the loss become unstable when the weights w 2 &gt; 2, so we restrict the weights to the range ? ? 2 ? w ? ? 2 by clamping then, i.e.:</p><formula xml:id="formula_7">w = ? ? ? ? ? ? ? 2 if w &lt; ? ? 2, ? 2 if w &gt; ? 2, w otherwise.<label>(7)</label></formula><p>For the image data sets, the generator is composed by a few convolutional layers, with same number of filters and kernel sizes.</p><p>We also report the results for the modified feature alignment for local training as a proof of concept by training a encoder for reconstruction from a latent vector.</p><p>For GAN, when used for reconstruction, we search the latent space that leads to most similar images by optimizing arg min z ||x ? G(z)|| 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We compare the results against traditional variational autoencoders and generative adversarial networks. The results show reconstruction of the inputs with feature and generator. We also show random samples from the generator network. We evaluate the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b22">(Heusel et al., 2018;</ref><ref type="bibr" target="#b59">Seitzer, 2020)</ref> for each data set as a measure of the quality of the samples. The FID score is calculated by extracting the activation of the global spatial pooling layer of a pre-trained Inception V3 model <ref type="bibr" target="#b66">(Szegedy et al., 2016)</ref>, for equally numbered images from the data set (here we choose 10k images) and sampled from a generator model:</p><formula xml:id="formula_8">F ID = ||? 1 ? ? 2 || 2 2 + tr(? 1 + ? 2 ? 2 ? 1 ? 2 ),<label>(8)</label></formula><p>with ? the mean of activations, ? the covariance matrix and tr the trace function. The results are shown in <ref type="table">Table 1</ref>.  <ref type="table">Table 1</ref>: FID scores across three image data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MNIST</head><p>The MNIST data set <ref type="bibr" target="#b7">(Bengio and Haffner, 1998</ref>) is a set of 60000 grayscale images with size 28 ? 28 pixels that contains hand drawings of digits from zero to nine. <ref type="figure" target="#fig_5">Figure 5</ref> shows the reconstruction of images by the features and with the generator applied to them, compared with traditional AE, VAE and GAN. While the features tend to be noisy, the generator can sharpen the images to resemble better the original inputs. <ref type="figure" target="#fig_6">Figure 6</ref> shows random samples from the generator with a class layer coupled to the encoder. The same image also shows the interpolation of the latent vector between four pairs of images to demonstrate the continuity of transitions on the latent space.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CelebA</head><p>The CelebA data set <ref type="bibr" target="#b43">(Liu et al., 2015)</ref> is a set of 202600 images of celebrity faces. We resize the images to 64 ? 64 pixels. <ref type="figure" target="#fig_7">Figures 7 and 8</ref> show the reconstruction and sampling with interpolation between samples, respectively. Without the perceptual loss (with ? = 1), we noticed a failure on the convergence of the generator network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CIFAR-10</head><p>The CIFAR-10 data set <ref type="bibr" target="#b34">(Krizhevsky and Hinton, 2009</ref>) contains 70000 natural images with size 32 ? 32 pixels across 10 different classes. <ref type="figure" target="#fig_9">Figure 9</ref> shows the reconstruction of images from the data set. While the features do approximate the original inputs, the transformation of the generator tends to be more dissimilar due its loss being dependent only on the adversarial contribution (? = 0). Just as before, <ref type="figure" target="#fig_10">Figure 10</ref> shows random samples and interpolation, which show a diversity of images, albeit less perceptual similar to the original data set.  It is important to note that we do not expect feature alignment to exceed the reconstruction and sample qualities of VAEs and GANs. The condition of reversibility constrains the optimization of a neural network and thus has to balance the reversibility cost with other losses. Nevertheless we compare the reconstruction L 2 loss with other networks to analyse how different each network is compared to same metric. These results are shown in <ref type="figure">FIG. 11.</ref> As expected, autoenconders have the lowest loss, given that it is optimized directly for reconstruction. The features should be compared to VAEs and the generator with GAN, since they are optimized with similar losses. Here we can see that absence of perceptual loss on the generator network decreases the reconstruction (without optimizing the latent vector) ability.  <ref type="figure">Figure 11</ref>: Comparison of the reconstruction L 2 loss for AE, VAE, GAN, r, and G(r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Local feature alignment</head><p>Here we show results from an encoder trained for reconstruction with the local feature alignment training. <ref type="figure" target="#fig_0">Figures 12, 13, and 14</ref> show reconstruction pairs for the MNIST, CIFAR-10, and CelebA data sets respectively. We can see that local training can reconstruct images despite the layers not receiving information from the reconstruction loss of images, which can be explained with the same reason of non-local feature alignment: the weights form a orthogonal matrix which tries to reverse information between layers as much as possible, which is limited only by the network capacity, directly related to the number of neurons.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented feature alignment, a technique that can approximately reverse neural networks. We trained an encoder to predict its own input by optimizing the features to match the inputs. By coupling a probabilistic layer with the same formulation as the variational autoencoders, we have the ability to generate new samples with the same statistical distribution of the training data. To improve the quality of the generated samples, that suffer from noise effects, we combined the generative adversarial network method by coupling a generator and a discriminator network to the features. We also showed that the technique can be modified to a local training rule, as opposed to backpropagation, which offers the advantage of lowering memory resources for training and extracting gradients of neural networks. Mathematical analysis on the convergence of the proposed technique shows that the weights converge to a pseudo-inverse matrix, which justifies the convergence of a network trained in this way to map its outputs back to its own inputs. The restriction is the network's architecture itself, since the bottlenecks do not allow for a one-to-one relationship.</p><p>The results show that the features can approximately match the inputs. Despite not being able to improve on the sampling quality of other current generative techniques, reversibility can offer advantages when a mapping of the outputs back to its inputs is desired. The data set CIFAR-10 is notoriously difficult because of the small image size and high variance, which led to samples with high FID measure. By coupling a classification layer on the encoder network, we can use label information for conditional generation of samples. Additionally, the results of local training suggests that we can train neural networks without feedback of global loss functions.</p><p>The proposed technique can be seen as a compromise between VAEs and GANs. The full architecture has crisper samples than VAEs and a latent vector that can be exploited. Feature extraction can be implemented across many different neural network architectures. Thus, it can potentially be improved to further increase the quality of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Mathematical analysis</head><p>This section studies the numerical analysis of the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Convergence of the features</head><p>For two fully connected layers, we have a (x) j =? j = i w ij x i and a (r) j = i w ij r i . The feature is obtained by optimizing the squared L 2 loss between the two activations:</p><formula xml:id="formula_9">L = 1 2 (? j ? a (r) j ) 2 ,<label>(9)</label></formula><p>which follows that r will evolve with the gradient flux:</p><formula xml:id="formula_10">?r i ?t = ? ?L ?r i ? r t i = r t?1 i ? ?C ?r t?1 .<label>(10)</label></formula><p>Then we can evaluate r t at each time step t:</p><formula xml:id="formula_11">r 1 i = r 0 i ? w ij (? j ? w ij r 0 ) = r 0 i (1 ? w 2 ij ) + w ij?j ,<label>(11)</label></formula><formula xml:id="formula_12">r 2 i = r 1 i (1 ? w 2 ij ) + w ij?j = r 0 i (1 ? w 2 ij ) 2 + [1 + (1 ? w 2 ij )]w ij?j ,<label>(12)</label></formula><formula xml:id="formula_13">r 3 i = r 0 i (1 ? w 2 ij ) 3 + [1 + (1 ? w 2 ij ) + (1 ? w 2 ij ) 2 ]w ij?j ,<label>(13)</label></formula><formula xml:id="formula_14">r 4 i = r 0 i (1 ? w 2 ij ) 4 + [1 + (1 ? w 2 ij ) + (1 ? w 2 ij ) 2 + (1 ? w 2 ij ) 3 ]w ij?j .<label>(14)</label></formula><p>With the pattern above, we generalize r t for any time step:</p><formula xml:id="formula_15">r p i = r 0 i (1 ? w 2 ij ) p + p?1 q=0 (1 ? w 2 ) q w ij?j .<label>(15)</label></formula><p>Under the restriction of w 2 ij ? 2, we have at limit as t grows:</p><formula xml:id="formula_16">lim p?? r 0 i (1 ? w 2 ij ) p = 0 ? lim p?? p?1 q=0 (1 ? w 2 ij ) q w ij?j =? j w ij .<label>(16)</label></formula><p>So the loss L ? 0 as p ? ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Convergence of the weights</head><p>The L 2 loss that update the parameters is:</p><formula xml:id="formula_17">L = 1 2 (x i ? r t i ) 2 = 1 2 ? ? x i ? r 0 i (1 ? w 2 ij ) t ? t?1 q=0 (1 ? w 2 ij ) q w ij?j ? ? 2 .<label>(17)</label></formula><p>For one-shot, T = 1, training, we have:</p><formula xml:id="formula_18">L = 1 2 x i ? r 0 i (1 ? w 2 ij ) ? w ij?j 2 = 1 2 x i ? r 0 i (1 ? w 2 ij ) ? w ij w ij x i 2 .<label>(18)</label></formula><p>We can rewrite the equation above in vector notation as follows:</p><formula xml:id="formula_19">L = 1 2 x ? r 0 (I ? w T w) ? w T wx 2 .<label>(19)</label></formula><p>For any r 0 , the equation above has roots for w 2 ii = 0. We can see then that the loss is minimal when the weight matrix product is orthogonal, i.e. w T w = I, which has the consequence that the transposed weight matrix is also its generalized Moore-Penrose inverse or pseudo-inverse w T = w ?1 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Variational autoencoder with feature alignment. Left: training the encoder to reconstruct the inputs x. Right: we sample a random normal vector to generate new data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Training in the VFA setting, with the addition of generator and discriminator networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Training with local feature alignment 1: for l=0, L do for ? l by optimizing C l 14:x = z l x .detach 15: end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The local training rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3</head><label>3</label><figDesc>Reconstruction with local feature alignment 1: sample z L x 2: for l=L, 0 do 3: initialize r = x.shape with x as z l x = E(x; ? l ) layer = non-linear function f then 13:r = f ?1 (r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Reconstruction of images of the MNIST data set from four different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Left: random samples. Right: interpolation among four images reconstructed from the data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Reconstruction of images of the CelebA data set from four different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Two sets of interpolation among four random sampled images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Reconstruction of images on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Left: random samples. Right: interpolation among four images reconstructed from the data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Local feature alignment on the MNIST data set. Each pair of images contains the reconstruction and original, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Local feature alignment on the CIFAR-10 data set. Each pair of images contains the reconstruction and original, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Local feature alignment on the CelebA data set. Each pair of images contains the reconstruction and original, respectively.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep Learning using Rectified Linear Units (ReLU)</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Raquel Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04585</idno>
		<title level="m">The reversible residual network: Backpropagation without storing activations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lynton Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wirkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Rahner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><forename type="middle">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Klessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04730</idno>
		<title level="m">Analyzing Inverse Problems with Invertible Neural Networks</title>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the realism of synthetic images through a combination of adversarial and perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charith</forename><surname>Atapattu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banafsheh</forename><surname>Rekabdar</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2019.8852449</idno>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="2161" to="4407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Theory of Local Learning, the Learning Channel, and the Optimality of Backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2016.07.006</idno>
		<idno type="arXiv">arXiv:1506.06472</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="51" to="74" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
	</analytic>
	<monogr>
		<title level="j">Invertible Residual Networks</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Yann Lecun Leon Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7906</idno>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding disentangling in beta-vae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on Variational Autoencoders</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
	</analytic>
	<monogr>
		<title level="j">Adversarial Feature Learning</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02644</idno>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengzhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02522</idno>
		<title level="m">On Interpretability of Artificial Neural Networks: A Survey</title>
		<imprint>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souza</forename><surname>Tiago De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maziero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09284</idno>
		<title level="m">Gradient target propagation</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Study on the Large Batch Size Training of Neural Networks Based on the Second Order Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huicai</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08795</idno>
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Explaining Explanations: An Overview of Interpretability of Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilani</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayesha</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalana</forename><surname>Kagal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00069</idno>
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06937</idno>
	</analytic>
	<monogr>
		<title level="j">A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</title>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Preventing Posterior Collapse with Levenshtein Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14758</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The organization of behavior; a neuropsychological theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Olding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hebb</forename></persName>
		</author>
		<idno type="DOI">10.1002/cne.900930310</idno>
		<imprint>
			<date type="published" when="1949" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
	</analytic>
	<monogr>
		<title level="j">Local Nash Equilibrium</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05992</idno>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Local Learning Rule for Independent Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Isomura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Toyoizumi</surname></persName>
		</author>
		<idno>2045-2322. doi: 10.1038/ srep28073</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28073</biblScope>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Error-Gated Hebbian Rule: A Local Learning Rule for Principal and Independent Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Isomura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Toyoizumi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-20082-0</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1835</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03591</idno>
		<title level="m">A Survey on Neural Network Language Models</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08155</idno>
		<title level="m">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</title>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative Flow with Invertible 1x1 Convolutions</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An Introduction to Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000056</idno>
		<idno type="arXiv">arXiv:1906.02691</idno>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Normalizing Flows: An Introduction and Review of Current Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brubaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09257</idno>
		<idno>doi: 10. 1109/TPAMI.2020.2992934</idno>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning by competing hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1820458116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="7723" to="7731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><forename type="middle">Kaae</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2001269</idno>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">One-step neural network inversion with PDF learning and emulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smalenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leemon</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ingkiriwang</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2005.1555983</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2161" to="4407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Backpropagation and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Marris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41583-020-0277-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Litwin-Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09549</idno>
		<title level="m">Learning to Learn with Feedback and Local Plasticity</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Taylor expansion of the accumulated rounding error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seppo</forename><surname>Linnainmaa</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01931367</idno>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1976-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Don&apos;t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02469</idno>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0911-8.arXiv:1512.02017</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="255" />
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
		<idno>doi: 10.1109/ ICCV.2017.304</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2380" to="7504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tschantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05359</idno>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09304</idno>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03616</idno>
		<title level="m">Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00007</idno>
	</analytic>
	<monogr>
		<title level="j">Feature visualization. Distill</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01834</idno>
		<title level="m">Conducting Credit Assignment by Aligning Local Representations</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13233</idno>
		<title level="m">Neural Density Estimation and Likelihood-free Inference</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00597</idno>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
	<note type="report_type">Taming VAEs.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks (GANs): An Overview of Theoretical Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolah</forename><surname>Chalechale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Taghizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13178</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluation Metrics, and Recent Developments</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03725</idno>
		<title level="m">Predictive Coding Can Do Exact Backpropagation on Convolutional and Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">Tibor</forename><surname>Schirrmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonio</forename><surname>Ball</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01610</idno>
	</analytic>
	<monogr>
		<title level="j">Training Generative Reversible Networks</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<ptr target="https://github.com/mseitzer/pytorch-fid" />
		<title level="m">pytorch-fid: FID Score for PyTorch</title>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>Version 0.1.1</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01228-7.arXiv:1610.02391</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1573" to="1405" />
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atefeh</forename><surname>Shahroudnejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01792</idno>
		<title level="m">A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A Closer Look at Disentangling in $\beta$-VAE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshvardhan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05127</idno>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Low-Memory Neural Network Training: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nimit Sharad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">Richard</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Leszczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10631</idno>
		<imprint>
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Can the Brain Do Backpropagation? -Exact Implementation of Backpropagation in Predictive Coding Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22566" to="22579" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for Simplicity: The All Convolutional Net</title>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhta</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hsiang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshimitsu</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusuke</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08663</idno>
		<title level="m">Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE</title>
		<imprint>
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Velichko</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics9091432</idno>
		<idno type="arXiv">arXiv:2006.02824</idno>
		<title level="m">Neural Network for Low-Memory IoT Devices and MNIST Image Recognition Using Kernels Based on Logistic Map. Electronics</title>
		<imprint>
			<date type="published" when="2020-09" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Theories of Error Back-Propagation in the Brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bogacz</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2018.12.005</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="250" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Large Batch Training of Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang Yujun Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00926</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9240" to="9249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02518</idno>
		<title level="m">A New Method to Visualize Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
