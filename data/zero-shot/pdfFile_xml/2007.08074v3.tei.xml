<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Suppress and Balance: A Simple Gated Network for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comppolyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Suppress and Balance: A Simple Gated Network for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Salient Object Detection ? Gated Network ? Dual Branch ? Fold-ASPP</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most salient object detection approaches use U-Net or feature pyramid networks (FPN) as their basic structures. These methods ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control between them, the other is without considering the disparity of the contributions of different encoder blocks. In this work, we propose a simple gated network (GateNet) to solve both issues at once. With the help of multilevel gate units, the valuable context information from the encoder can be optimally transmitted to the decoder. We design a novel gated dual branch structure to build the cooperation among different levels of features and improve the discriminability of the whole network. Through the dual branch design, more details of the saliency map can be further restored. In addition, we adopt the atrous spatial pyramid pooling based on the proposed "Fold" operation (Fold-ASPP) to accurately localize salient objects of various scales. Extensive experiments on five challenging datasets demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Salient object detection aims to identify the visually distinctive regions or objects in a scene and then accurately segment them. In many computer vision applications, it is used as a pre-processing step, such as scene classification <ref type="bibr" target="#b38">[39]</ref>, visual tracking <ref type="bibr" target="#b31">[32]</ref>, person re-identification <ref type="bibr" target="#b40">[41]</ref>, light field image segmentation <ref type="bibr" target="#b51">[52]</ref> and image captioning <ref type="bibr" target="#b16">[17]</ref>, etc.</p><p>With the development of deep learning, salient object detection has gradually evolved from the traditional method based on manual design features to the deep learning method. In recent years, U-shape based structures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> have received the most attention due to their ability to utilize multilevel information to reconstruct high-resolution feature maps. Therefore, most state-of-the-art saliency detection networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33</ref>] adopt U-shape as the encoderdecoder architecture. And many methods aim at combining multilevel features in either the encoder <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b58">59]</ref> or the decoder <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b58">59]</ref>. For each convolutional block, they separately formulate the relationships of internal features for forward update. It is well known that the high-quality saliency maps predicted in the decoder rely heavily on the effective features provided by the encoder. Nevertheless, the aforementioned methods directly use an all-pass skiplayer structure to concatenate the features of the encoder to the decoder, and the effectiveness of feature aggregation at different levels is not quantified. These restrictions not only introduce misleading context information into the decoder but also result in that the really useful features can not be adequately utilized. In cognitive science, Yang et al. <ref type="bibr" target="#b63">[64]</ref> show that inhibitory neurons play an important role in how the human brain chooses to process the most important information from all the information presented to us. And inhibitory neurons ensure that humans respond appropriately to external stimuli by inhibiting other neurons and balancing excitatory neurons that stimulate neuronal activity. Inspired by this work, we think that it is necessary to set up an information screening unit between each pair of encoder and decoder blocks in saliency detection. It can help distinguish the most intense features of salient regions and suppress background interference, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, in which these images have easily-confused backgrounds or low-contrast objects. Moreover, due to the limited receptive field, a single-scale convolutional kernel is difficult to capture context information of size-varying objects. This motivates some efforts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b66">67]</ref> to investigate multiscale feature extraction. These methods directly equip an atrous spatial pyramid pooling module <ref type="bibr" target="#b5">[6]</ref> (ASPP) in their networks. However, when using a convolution with a large dilation rate, the information under the kernel seriously lacks correlation due to inserting too many zeros. This may be detrimental to the discrimination of subtle image structures.</p><p>In this paper, we propose a simple gated network (GateNet) for salient object detection. Based on the feature pyramid network (FPN), we construct multilevel gate units to combine the features from the decoder and the encoder. We use con-volution operation and nonlinear functions to calculate the correlations among features and assign gate values to different blocks. In this process, a partnership is established between different blocks by using weight distribution and the decoder can obtain more efficient information from the encoder and pay more attention to the salient regions. Since the top-layer features of the encoder network contain rich contextual information, we construct a folded atrous spatial pyramid pooling (Fold-ASPP) module to gather multiscale high-level saliency cues. With the "Fold" operation, the atrous convolution is implemented on a group of local neighborhoods rather than a group of isolated sampling points, which can help generate more stable features and more adequately depict finer structure. In addition, we design a parallel branch by concatenating the output of the FPN branch and the features of the gated encoder, so that the residual information complementary to the FPN branch is supplemented to generate the final saliency map.</p><p>Our main contributions can be summarized as follows.</p><p>-We propose a simple gated network to adaptively control the amount of information that flows into the decoder from each encoder block. With multilevel gate units, the network can balance the contribution of each encoder block to the the decoder block and suppress the features of non-salient regions. -We design a Fold-ASPP module to capture richer context information and localize salient objects of various sizes. By the "Fold" operation, we can obtain more effective feature representation. -We build a dual branch architecture. They form a residual structure, complement each other through the gated processing and generate better results.</p><p>We compare the proposed model with seventeen state-of-the-art methods on five challenging datasets. The results show that our method performs much better than other competitors. And, it achieves a real-time speed of 30 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Salient Object Detection</head><p>Early saliency detection methods are based on low-level features and some heuristics prior knowledge, such as color contrast <ref type="bibr" target="#b0">[1]</ref>, background prior <ref type="bibr" target="#b61">[62]</ref> and center prior <ref type="bibr" target="#b21">[22]</ref>. Most of them using hand-crafted features, and more details about the traditional methods are discussed in <ref type="bibr" target="#b53">[54]</ref>.</p><p>With the breakthrough of deep learning in the field of computer vision, a large number of convolutional neural networks-based salient object detection methods have been proposed and their performance had been improved gradually. Especially, fully convolutional networks (FCN), which avoid the problems caused by the fully-connected layer, become the mainstream for dense prediction tasks. Wang et al. <ref type="bibr" target="#b49">[50]</ref> use weight sharing methods to iteratively refine features and promote mutual fusion between features. Hou et al. <ref type="bibr" target="#b19">[20]</ref> achieve efficient feature expression by continuously blending features from deep layers into shallow layers. However, the single-scale feature cannot roundly characterize various objects as well as image contexts. How to get multiscale features and integrate context information is an important problem in saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiscale Feature Extraction</head><p>Recently, the atrous spatial pyramid pooling module (ASPP) <ref type="bibr" target="#b5">[6]</ref> is widely applied in many tasks and networks. The atrous convolution can enlarge the receptive field to obtain large-scale features and does not increase the computational cost. Therefore, it is often used in saliency detection networks. Zhang et al. <ref type="bibr" target="#b66">[67]</ref> insert several ASPP modules into the encoder blocks of different levels, while Deng et al. <ref type="bibr" target="#b11">[12]</ref> install it on the highest-level encoder block. Nevertheless, the repeated stride and pooling operations already make the top-layer features lose much fine information. With the increase of atrous rate, the correlation of sampling points further degrades, which leads to difficulties in capturing the changes of image details (e.g., lathy background regions between adjacent objects or spindly parts of objects). In this work, we propose a folded ASPP to alleviate these issues and achieve a local-in-local effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gated Mechanisms</head><p>The gated mechanism plays an important role in controlling the flow of information and is widely used in the long short term memory (LSTM). In <ref type="bibr" target="#b1">[2]</ref>, the gate unit combines two consecutive feature maps of different resolutions from the encoder to generate rich contextual information. Zhang et al. <ref type="bibr" target="#b66">[67]</ref> adopt gate function to control the message passing when combining feature maps at all levels of the encoder. Due to the ability to filter information, the gated mechanism can also be seen as a special kind of attention mechanism. Some saliency methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b56">57]</ref> employ attention networks. Zhang et al. <ref type="bibr" target="#b69">[70]</ref> apply both spatial and channel attention to each layer of the decoder. Wang et al. <ref type="bibr" target="#b56">[57]</ref> exploit the pyramid attention module to enhance saliency representations for each layer in the encoder and enlarge the receptive field. The above methods all unilaterally consider the information interaction between different levels either in the encoder or in the decoder. We integrate the features from the encoder and the decoder to formulate gate function, which plays the role of block-wise attention and model the overall distribution of all blocks in the network from the global perspective. While previous methods actually utilize the block-specific feature to compute dense attention weights for the corresponding block. Moreover, in order to take advantage of rich contextual information in the encoder, these methods directly feed the encoder features into the decoder and do not consider their mutual interference. Our proposed gate unit can naturally balance their contributions, thereby suppressing the response of the encoder to non-salient regions. Experimental results in <ref type="figure" target="#fig_2">Fig. 4</ref> and <ref type="figure">Fig. 9</ref> intuitively demonstrate the effect of multilevel gate units on the above two aspects, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>The gated network architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, in which encoder blocks, transition layers, decoder blocks and gate units are respectively denoted as E i , T i , D i and G i (i ? {1, 2, 3, 4, 5} indexes different levels). And their output feature maps are denoted as E i , T i , D i and G i , respectively. The final prediction is obtained by combining the FPN branch and the parallel branch. In this section, we first describe the overall architecture, then detail the gated dual branch structure and the folded atrous spatial pyramid pooling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Overview</head><p>Encoder Network. In our model, the encoder is based on a common pretrained backbone network, e.g., the VGG <ref type="bibr" target="#b42">[43]</ref>, ResNet <ref type="bibr" target="#b18">[19]</ref> or ResNeXt <ref type="bibr" target="#b59">[60]</ref>. We take the VGG-16 network as an example, which contains thirteen Conv layers, five maxpooling layers and two fully connected layers. In order to fit saliency detection task, similar to most previous approaches <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b66">67]</ref>, we cast away all the fully-connected layers of the VGG-16 and remove the last pooling layer to retain details of last convolutional layer. Decoder Network. The decoder comprises three main components. i) The FPN branch, which continually fuses different level features from T 1 ? T 5 by element-wise addition. ii) The parallel branch, which combines the saliency map of the FPN branch with the feature maps of transition layers by cross-channel <ref type="figure">Fig. 3</ref>. Detailed illustration of the gate unit. E i , D i+1 indicates feature maps of the current encoder block and those of the previous decoder block, respectively. S is sigmoid function. concatenation. At the same time, multilevel gate units (G 1 ? G 5 ) are inserted between the transition layer and the decoder layer. iii) The Fold-ASPP module, which improves the original atrous spatial pyramid pooling (ASPP) by using a "Fold" operation. It can take advantage of semantic features learned from E 5 to provide multiscale information to the decoder.</p><formula xml:id="formula_0">E i E i D i+1 D i+1 F i F i S G i G i S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated Dual Branch</head><p>The gate unit can control the message passing between scale-matching encoder and decoder blocks. By combining the feature maps of the previous decoder block, the gate value also characterizes the contribution that the current block of the encoder can provide. <ref type="figure">Fig. 3</ref> shows the internal structure of the proposed gate unit. In particular, encoder feature E i and decoder feature D i+1 are integrated to obtain feature F i , and then it is fed into two branches, which includes a series of convolution, activation and pooling operations, to compute a pair of gate values G i . The entire gated process can be formulated as,</p><formula xml:id="formula_1">G i = P (S(Conv(Cat(E i , D i+1 )))) if i = 1, 2, 3, 4 P (S(Conv(Cat(E i , T i )))) if i = 5<label>(1)</label></formula><p>where Cat(?) is the concatenation operation among channel axis, Conv(?) refers to the convolution layer, S(?) is the element-wise sigmoid function, and P (?) is the global average pooling. The output channel of Conv(?) is 2. The resulted gate vector G i has two different elements which correspond to two gate values in <ref type="figure">Fig. 3</ref>. Given the gate values, they are applied to the FPN branch and the parallel branch for weighting the transition-layer features T 1 ? T 5 , which are generated by exploiting 3 ? 3 convolution to reduce the dimension of E 1 ? E 4 and the Fold-ASPP to finely process E 5 (Please see <ref type="figure" target="#fig_1">Fig. 2</ref> for details). Through multilevel gate units, we can suppress and balance the information flowing from different encoder blocks to the decoder.</p><p>In <ref type="figure" target="#fig_2">Fig. 4</ref>, we statistically demonstrate the curves of gate value with a convolutional level as the horizontal axis. It can be seen that the high-level encoder features contribute more contextual guidance to the decoder than the low-level encoder features in the FPN branch. This trend is just the opposite in the parallel branch. It is because the FPN branch is responsible to predict the main body of the salient object by progressively combining multilevel features, which needs more high-level semantic knowledge. While the parallel branch, as a residual structure, aims to fill in the details, which are mainly contained in the low-level features. In addition, some visual examples are shown in <ref type="figure">Fig. 9</ref> demonstrate that multilevel gate units can significantly suppress the interference from each encoder block and enhance the contrast between salient and non-salient regions. Since the proposed gate unit is simple yet effective, a raw FPN network with multilevel gate units can be viewed as a new baseline for saliency detection task. Most existing models either use progressive decoder <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b56">57]</ref> or parallel decoder <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b71">72]</ref>, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The progressive structure begins with the top layer and gradually utilizes the output of the higher layer as prior knowledge to fuse the encoder features. This mechanism is not conducive to the recovery of details because the high-level features lack fine information. While the parallel structure easily results in inaccurate localization of objects since the low-level features without semantic information directly interfere with the capture of global structure cues. In this work, we mix the two structures to build a dual branch decoder to overcome the above restrictions. We briefly describe the FPN branch. Taking D i as an example, we firstly apply bilinear interpolation to upsample the higher-level feature D i+1 to the same size as T i . Next, to decrease the number of parameters, T i is reduced to 32 channels and fed into gate unit G i . Lastly, the gated feature is fused with the upsampled feature of D i+1 by element-wise addition and convolutional layers. This process can be formulated as follows:</p><formula xml:id="formula_2">D i = Conv(G i 1 ? T i + U p(D i+1 )) if i = 1, 2, 3, 4 Conv(G i 1 ? T i ) if i = 5,<label>(2)</label></formula><p>where D 1 is a single-channel feature map with the same size as the input image.</p><p>In the parallel branch, we firstly upsample T 1 ? T 5 to the same size of D 1 . Next, the multilevel gate units are followed to weight the corresponding transition-layer features. Lastly, we combine D 1 and the gated features by crosschannel concatenation. The whole process is written as follows:</p><formula xml:id="formula_3">F Cat = Cat(D 1 , U p(G 1 2 ? T 1 ), U p(G 2 2 ? T 2 ), U p(G 3 2 ? T 3 ), U p(G 4 2 ? T 4 ), U p(G 5 2 ? T 5 )).<label>(3)</label></formula><p>The final saliency map S F is generated by integrating the predictions of the two branches with a residual connection as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>(c),</p><formula xml:id="formula_4">S F = S(Conv(F Cat ) + D 1 )),<label>(4)</label></formula><p>where S(?) is the element-wise sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Folded Atrous Spatial Pyramid Pooling</head><p>In order to obtain robust segmentation results by integrating multiscale information, atrous spatial pyramid pooling (ASPP) is proposed in Deeplab <ref type="bibr" target="#b5">[6]</ref>. And some works <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b11">12]</ref> also show its effectiveness in saliency detection. The ASPP uses multiple parallel atrous convolutional layers with different dilation rates. The sparsity of atrous convolution kernel, especially when using a large dilation rate, results in that the association relationships among sampling points are too weak to extract stable features. In this paper, we apply a simple "Fold" operation to effectively relieve this issue. We visualize the folded convolution structure in <ref type="figure">Fig. 6</ref>, which not only further enlarges the receptive field but also extends each valid sampling position from an isolate point to a 2 ? 2 connected region. Let X represent feature maps with the size of N ? N ? C (C is the channel number). We slide a 2 ? 2 window on X in stride 2 and then conduct atrous convolution with kernel size K ? K in different dilation rates. <ref type="figure">Fig. 6</ref> shows the computational process when K = 3 and dilation rate is 2. Firstly, we collect 2 ? 2 ? C feature points in each window from X and then it is stacked by channel direction, we call this operation "Fold", which is shown in <ref type="figure">Fig. 6</ref> 1 . After the fold operation, we can get new feature maps with the size of N/2 ? N/2 ? 4C. A point on the new feature maps corresponds to a 2 ? 2 area on the original feature maps. Secondly, we adopt an atrous convolution with a kernel size of 3 ? 3 and dilation rate is 2. Followed by the reverse process of "Fold" which is called "Unfold" operation, the final feature maps are obtained. By  <ref type="figure">Fig. 6</ref>. Illustration of the folded convolution. We use 1 , 2 and 3 to respectively indicate "Fold" operation, atrous convolution and "Unfold" operation. <ref type="bibr" target="#b3">4</ref> shows the comparison between atrous convolution (Left) and the folded atrous convolution (Right).</p><p>using the folded atrous convolution, in the process of information transfer across convolution layers, more contexts are merged and the certain local correlation is also preserved, which provides the fault-tolerance capability for subsequent operations.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the Fold-ASPP is only equipped on the top of the encoder network, which consists of three folded convolutional layers with dilation rates <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> to fit the size of feature maps. Just as group convolution <ref type="bibr" target="#b59">[60]</ref> is a trade-off between depthwise convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref> and vanilla convolution in the channel dimension, the proposed folded convolution is a trade-off between atrous convolution and vanilla convolution in the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Supervision</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we use the cross-entropy loss for both the intermediate prediction from the FPN branch and the final prediction from the dual branch. In the dual branch decoder, since the FPN branch gradually combines all-level gated encoding and decoding features, it has very powerful prediction ability. We expect that it can predict salient objects as accurately as possible under the supervision of ground truth. While the parallel branch only combines the gated encoding features, which is helpful to remedy the ignored details with the design of residual structure. Moreover, the supervision on D 1 can drive gate units to learn the weight of the contribution of each encoder block to the final prediction. We use the cross-entropy loss. The total loss L could be written as:</p><formula xml:id="formula_5">L = l s1 + l sf ,<label>(5)</label></formula><p>where l s1 and l sf are respectively used to regularize the output of the FPN branch and the final prediction. The cross-entropy loss could be computed as:</p><formula xml:id="formula_6">l = Y logP + (1 ? Y )log(1 ? P ),<label>(6)</label></formula><p>where P and Y denote the predicted map and ground-truth, respectively. Evaluation Metrics. For quantitative evaluation, we adopt four widely-used metrics: precision-recall (PR) curve, F-measure score, mean absolute error (MAE) and S-measure score. Precision-Recall curve: The pairs of precision and recall are calculated by comparing the binary saliency maps with the ground truth to plot the PR curve, where the threshold for binarizing slides from 0 to 255. The closer the PR curve is to the upper right corner, the better the performance is. Fmeasure: It is an overall performance measurement that synthetically considers both precision and recall:</p><formula xml:id="formula_7">F ? = 1 + ? 2 ? precision ? recall ? 2 ? precision + recall ,<label>(7)</label></formula><p>where ? 2 is set to 0.3 as suggested in <ref type="bibr" target="#b0">[1]</ref> to emphasize the precision. In this paper, we report the maximum F-measure score across the binary maps of different thresholds. Mean Absolute Error : As the supplement of the PR curve and F-measure, it computes the average absolute difference between the saliency map and the ground truth pixel by pixel. S-measure: It is more sensitive to foreground structural information than the F-measure. It considers the region-aware structural similarity S r and the object-aware structural similarity S o :</p><formula xml:id="formula_8">S m = ? * S o + (1 ? ?) * S r ,<label>(8)</label></formula><p>where ? is set to 0.5 <ref type="bibr" target="#b13">[14]</ref>. Implementation Details. We follow most state-of-the-art saliency detection methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b66">67]</ref> to use the DUTS-TR as the training dataset which contains 10, 553 images. Our model is implemented based on the Pytorch repository and the hyper-parameters are set as follows: We train the GateNet on a PC with GTX 1080 Ti GPU for 40 epochs with mini-batch size 4. For the optimizer, we adopt the stochastic gradient descent (SGD). The momentum, weight decay, and learning rate are set as 0.9, 0.0005 and 0.001, respectively. The "poly" policy <ref type="bibr" target="#b29">[30]</ref> with the power of 0.9 is used to adjust the learning rate. We adopt some data augmentation techniques to avoid overfitting and make the learned model more robust, which include random horizontally flipping, random rotation, random brightness, saturation and contrast changing. In order to preserve the integrity of the image semantic information, we only resize the image to 384 ? 384 instead of using a random crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison with State-of-the-art</head><p>We compare the proposed algorithm with seventeen state-of-the-art saliency detection methods, including the DCL <ref type="bibr" target="#b25">[26]</ref>, DSS <ref type="bibr" target="#b19">[20]</ref>, Amulet <ref type="bibr" target="#b68">[69]</ref>, SRM <ref type="bibr" target="#b50">[51]</ref>, DGRL <ref type="bibr" target="#b52">[53]</ref>, RAS <ref type="bibr" target="#b6">[7]</ref>, PAGRN <ref type="bibr" target="#b69">[70]</ref>, BMPM <ref type="bibr" target="#b66">[67]</ref>, R3Net <ref type="bibr" target="#b11">[12]</ref>, HRS <ref type="bibr" target="#b65">[66]</ref>, MLMS <ref type="bibr" target="#b57">[58]</ref>, PAGE <ref type="bibr" target="#b56">[57]</ref>, ICNet <ref type="bibr" target="#b54">[55]</ref>, CPD <ref type="bibr" target="#b58">[59]</ref>, BANet <ref type="bibr" target="#b44">[45]</ref>, BASNet <ref type="bibr" target="#b36">[37]</ref> and Capsal <ref type="bibr" target="#b67">[68]</ref>. For fair comparisons, all the saliency map of these methods are directly provided by their respective authors or computed by their released codes. To further show the effectiveness of our GateNet, we test its performance in both RGBD SOD and Video Object Segmentation tasks and include the results in appendix. Quantitative Evaluation. Tab. 1 shows the experimental comparison results in terms of the F-measure, S-measure and MAE scores, from which we can see that the GateNet can consistently outperform other approaches across all five datasets and different metrics. In particular, the GateNet achieves significant performance improvement in terms of the F-measure compared to the second best method BANet <ref type="bibr" target="#b44">[45]</ref> on the challenging DUTS-test (0.870 vs 0.852 and 0.888 vs 0.872) and PASCAL-S (0.882 vs 0.866 and 0.883 vs 0.877) datasets. This clearly demonstrates its superior performance in complex scenes. Moreover, some methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12]</ref> apply the post-processing techniques to refine their saliency maps. Our GateNet still performs better than them without any postprocessing. We evaluate different algorithms using the standard PR curves in <ref type="figure">Fig. 7</ref>. It can be seen that our PR curves are significantly higher than those of other methods on five datasets.</p><p>Qualitative Evaluation. <ref type="figure" target="#fig_0">Fig. 1</ref> and <ref type="figure">Fig. 8</ref> illustrate some visual comparisons. In <ref type="figure" target="#fig_0">Fig. 1</ref>, other methods are severely disturbed by branches and weeds while ours can precisely identify the whole objects. And the GateNet can significantly suppress the background with similar shapes to salient objects (see the 1 st row in <ref type="figure">Fig. 8</ref>). Since the Fold-ASPP can obtain more stable structural features, it can help to accurately locate objects and separate adjacent objects well, but some competitors make adjacent objects stick together (see the 3 th and 4 th rows in <ref type="figure">Fig. 8)</ref>. Besides, the proposed parallel branch can restore more details, therefore, the boundary information is retained well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We detail the contribution of each component to the overall network.</p><p>Effectiveness of Backbones. Tab. 1 demonstrates that the performance of the gated network can be significantly improved by using better backbones such as ResNet-50, ResNet-101 or ResNeXt-101.</p><p>Effectiveness of Components. We quantitatively show the benefit of each component in Tab. 2. We take the results of the VGG-16 backbone with the FPN branch as the baseline. Firstly, the multilevel gate units are added to the <ref type="table">Table 1</ref>. Quantitative comparisons. Blue indicates the best performance under each backbone setting, while red indicates the best performance among all settings. The subscript in the first column regards the publication year. " ?", "S" and "X" mean using the post-processing, ResNet-101 and ResNeXt-101 backbone, respectively. "-" represents that the results are not available. ? and ? indicate that the larger and smaller scores are better, respectively.  baseline network. The performance is significantly improved with the gain of 2.94%, 2.17% and 11.67% in terms of the F-measure, S-measure and MAE, respectively. To show the effect of the gate units more intuitively, we visualize the features of different levels in <ref type="figure">Fig. 9</ref>. It can be observed that even if the dog has a very low contrast with the chair or the billboard (see the 1 st ? 4 th rows), through using multilevel gate units, the high contrast between the object region and the background is always maintained at each layer while the detail information is continually regained, thereby making salient objects be effectively distinguished. Besides, the gate units can avoid excessive suppression for the slender parts of objects (see the 5 th ? 8 th rows). The corners of the poster, the limbs and even tentacles of the mantis are retained well. Secondly, based on the gated baseline network, we design a series of experimental options to verify the effectiveness of the folded convolution and Fold-ASPP. Tab. 3 illustrates the results in detail. We adopt the atrous convolution with dilation rates of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> and the same dilation rates are also applied to the folded convolution. It can be observed that the folded convolution consistently yields significant performance improvement at each dilation rate than the corresponding atrous convolution in terms of all three metrics. And the single-layer Fold(6) already performs better than the ASPP of aggregating three atrous convolution layers. The Fold-ASPP also naturally outperforms the ASPP with the gain of 1.17% and 8.0% in terms of the F-measure and MAE, respectively. Finally, we add the parallel branch to further restore the details of objects. In this process, the gate units, Fold-ASPP and parallel branch complement each other without repulsion.</p><formula xml:id="formula_9">Method DUTS-test DUT-OMRON PASCAL-S HKU-IS ECSSD F? ? Sm ? MAE? F? ? Sm ? MAE? F? ? Sm ? MAE? F? ? Sm ? MAE? F? ? Sm ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel gated network architecture for saliency detection. We first adopt multilevel gate units to balance the contribution of each encoder block and suppress the activation of the features of non-salient regions, which can provide useful context information for the decoder while minimizing interference. The gate unit is simple yet effective, therefore, a gated FPN network can be used as a new baseline for dense prediction tasks. Next, we use the Fold-ASPP to gather multiscale semantic information for the decoder. By the folded operation, the atrous convolution achieves a local-in-local effect, which not only expands the receptive field but also retains the correlation among local sampling points. Finally, to further supplement the details, we combine all encoder features in parallel and construct a residual structure. Experimental results on five benchmark datasets demonstrate that the proposed model outperforms seventeen state-of-the-art methods under different evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>We expand our GateNet to other tasks including RGB-D Salient Object Detection (SOD) and Video Object Segmentation (VOS) to further demonstrate its effectiveness.</p><p>A.1 Network Architecture </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 RGB-D Salient object detection</head><p>Dataset. There are five main RGB-D SOD datasets which are NJUD <ref type="bibr" target="#b22">[23]</ref>, RGBD135 <ref type="bibr" target="#b8">[9]</ref> NLPR <ref type="bibr" target="#b33">[34]</ref>, SSD <ref type="bibr" target="#b73">[74]</ref> and SIP <ref type="bibr" target="#b15">[16]</ref>. We adopt the same splitting way as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b35">36]</ref> to guarantee a fair comparison. We split 1,485 samples from NJUD and 700 samples from NLPR for traing a new model. The remaining images in these two datasets and other three datasets are all for testing to verify the generalization ability of saliency models. Evaluation Metrics. We adopt several metrics widely used in RGB-D SOD for quantitative evaluation: F-measure score, mean absolute error (MAE, M), the recently released S-measure (S m ) <ref type="bibr" target="#b13">[14]</ref> and E-measure (E m ) <ref type="bibr" target="#b14">[15]</ref> scores. The lower value is better for the MAE and higher is better for others.</p><p>Comparison with State-of-the-art Results. The performance of the proposed model is compared with ten state-of-the-art approaches on five benchmark datasets, including the DES <ref type="bibr" target="#b8">[9]</ref>, DCMC <ref type="bibr" target="#b10">[11]</ref>, CDCP <ref type="bibr" target="#b74">[75]</ref>, DF <ref type="bibr" target="#b37">[38]</ref>, CTMF <ref type="bibr" target="#b17">[18]</ref>, PCA <ref type="bibr" target="#b2">[3]</ref>, MMCI <ref type="bibr" target="#b4">[5]</ref>, TANet <ref type="bibr" target="#b3">[4]</ref>, CPFP <ref type="bibr" target="#b70">[71]</ref> and DMRA <ref type="bibr" target="#b35">[36]</ref>. For fair comparisons, all the saliency maps of these methods are directly provided by authors or computed by their released codes. And we take the VGG-16 as the backbone for each stream. Tab. 4 shows performance comparisons in terms of the maximum F-measure, mean F-measure, weighted F-measure, S-measure, E-measure and MAE scores. It can be seen that our GateNet is very competitive. We believe that future works based on GateNet can further improve performance and easily become the state-of-the-art RGB-D SOD model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Video Object Segmentation</head><p>According to whether the mask of the first frame of the video is provided during the test, video object segmentation (vos) can be divided into zero-shot vos and one-shot vos. In this paper, we mainly use the dual-branch GateNet structure as shown in <ref type="figure" target="#fig_0">Fig. 10</ref> for zero-shot vos. Dataset and Metrics. DAVIS-16 <ref type="bibr" target="#b34">[35]</ref> is one of the most popular benchmark datasets for video object segmentation tasks. It consists of 50 high-quality video sequences (30 for training and 20 for validation) in total. We follow the training strategy as AGS <ref type="bibr" target="#b55">[56]</ref>, COSNet <ref type="bibr" target="#b30">[31]</ref>, PDB <ref type="bibr" target="#b43">[44]</ref> and MATNet <ref type="bibr" target="#b72">[73]</ref> to use extra datasets. We use the image saliency datasets: MSRA10K <ref type="bibr" target="#b7">[8]</ref> and DUT-OMRON <ref type="bibr" target="#b62">[63]</ref> to pretrain our RGB branch, then train the whole model with the training videos in DAVIS16. For quantitative evaluation, we adopt two metrics, namely region similarity J and boundary accuracy F. Comparison with State-of-the-art Results. The performance of the proposed model is compared with ten state-of-the-art approaches on the DAVIS-16 dataset, including the LVO <ref type="bibr" target="#b46">[47]</ref>, ARP <ref type="bibr" target="#b23">[24]</ref>, PDB <ref type="bibr" target="#b43">[44]</ref>, LSMO <ref type="bibr" target="#b47">[48]</ref>, MotAdapt <ref type="bibr" target="#b41">[42]</ref>, EPO <ref type="bibr" target="#b12">[13]</ref>, AGS <ref type="bibr" target="#b55">[56]</ref>, COSNet <ref type="bibr" target="#b30">[31]</ref>, AnDiff <ref type="bibr" target="#b64">[65]</ref> and MATNet <ref type="bibr" target="#b72">[73]</ref>. We follow most methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48]</ref> to take the ResNet-101 as the backbone. Tab. 5 shows performance comparisons in terms of the J and F. It should be noted that our method only performs feature extraction on the optical flow map generated by PWCNet <ref type="bibr" target="#b45">[46]</ref> in order to supplement the motion information of the current frame. Without adding more cross-modal fusion techniques, or using other tracking or detection models, our GateNet can achieve competitive performance with most zero-shot vos methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Corresponding author. ? These authors contributed equally to this work. arXiv:2007.08074v3 [cs.CV] 27 Jul 2020 Visual comparison of different CNN based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the gated network. It consists of the VGG-16 encoder (E 1 ? E 5 ), five transition layers (T 1 ? T 5 ), five gate units (G 1 ? G 5 ), five decoder blocks (D 1 ? D 5 ) and the Fold-ASPP module. We employ twice supervision in this network. Once acts at the end of the FPN branch D 1 . The other is used to guide the fusion of the two branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The distributions of the gate weights on five datasets. We calculate the average gate values for each level of the FPN branch and the parallel branch across all images in every dataset. For the FPN branch, the low-level gate values are significantly smaller than the high-level ones. For the parallel branch, the gate values gradually decrease with the promotion of levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of different decoder architectures. (a) Progressive structure, (b) Parallel structure and (c) Our dual branch structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .Fig. 9 .</head><label>789</label><figDesc>GateNet S 0.893 0.889 0.038 0.821 0.844 0.054 0.883 0.862 0.067 0.937 0.920 0.031 0.951 0.930 0.035 GateNet X 0.898 0.895 0.035 0.829 0.848 0.051 0.888 0.865 0.065 0.943 0.925 0.029 0.952 0.929 0Precision (vertical axis) recall (horizontal axis) curves on six popular rgb-salient object datasets. Visual comparison between our results and state-of-the-art methods. Visual comparison of feature maps for showing the effect of the multilevel gate units. D5 ? D1 represent the feature maps of each decoder block from high level to low level. Odd rows and even rows are the results of the FPN baseline without or with multilevel gate units, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 showsFig. 10 .</head><label>1010</label><figDesc>our proposed dual-branch gated FPN network for RGB-D SOD and VOS. Compared with the RGB SOD network, we only add an extra encoder to extract features of other modals such as depth or optical flow. This dual-branch GateNet is easy to follow and can be used as a new baseline. Network pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We evaluate the proposed model on five benchmark datasets. EC-SSD [61] contains 1, 000 semantically meaningful and complex images with pixel-accurate ground truth annotations. HKU-IS<ref type="bibr" target="#b24">[25]</ref> has 4, 447 challenging images with multiple disconnected salient objects, overlapping the image boundary.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Experimental Setup</cell></row><row><cell>Dataset. PASCAL-S [27] contains 850 images selected from the PASCAL VOC 2009 seg-</cell></row><row><cell>mentation dataset. DUT-OMRON [63] includes 5, 168 challenging images, each</cell></row><row><cell>of which usually has complicated background and one or more foreground ob-</cell></row><row><cell>jects. DUTS [49] is the largest salient object detection dataset, which contains</cell></row><row><cell>10, 553 training and 5, 019 test images. These images contain very complex sce-</cell></row><row><cell>narios with high-diversity contents.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Ablation analysis on the DUTS dataset. Evaluation of the folded convolution and Fold-ASPP. (x) stands for different sampling rates of atrous convolution.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>F ?</cell><cell>Sm MAE</cell></row><row><cell></cell><cell></cell><cell cols="4">Baseline (F P N ) 0.816 0.829 0.060</cell></row><row><cell></cell><cell></cell><cell cols="2">+ Gate U nits</cell><cell cols="2">0.840 0.847 0.053</cell></row><row><cell></cell><cell></cell><cell cols="2">+ F old-ASP P</cell><cell cols="2">0.866 0.863 0.047</cell></row><row><cell></cell><cell></cell><cell cols="4">+ P arallel Branch 0.870 0.869 0.045</cell></row><row><cell></cell><cell cols="6">Atrous(2) Atrous(4) Atrous(6) Fold(2) Fold(4) Fold(6) ASPP Fold-ASPP</cell></row><row><cell>F ?</cell><cell>0.840</cell><cell>0.845</cell><cell>0.848</cell><cell cols="2">0.853 0.856 0.860 0.856</cell><cell>0.866</cell></row><row><cell cols="2">MAE 0.055</cell><cell>0.053</cell><cell>0.051</cell><cell cols="2">0.051 0.050 0.048 0.051</cell><cell>0.047</cell></row><row><cell>Sm</cell><cell>0.847</cell><cell>0.849</cell><cell>0.851</cell><cell cols="2">0.856 0.858 0.859 0.860</cell><cell>0.863</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparison. ? and ? indicate that the larger and smaller scores are better, respectively. Among the CNN-based methods, the best results are shown in red. The subscript in each model name is the publication year.</figDesc><table><row><cell cols="2">Metric</cell><cell></cell><cell></cell><cell cols="10">Traditional Methods DES14 DCMC16 CDCP17 DF17 CTMF18 PCANet18 MMCI19 TANet19 CPFP19 DMRA19 GateNet CNNs-Based Models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[9]</cell><cell>[11]</cell><cell>[75]</cell><cell>[38]</cell><cell>[18]</cell><cell>[3]</cell><cell>[5]</cell><cell>[4]</cell><cell>[71]</cell><cell>[36]</cell><cell>Ours</cell></row><row><cell></cell><cell>F max ?</cell><cell cols="2">?</cell><cell>0.260</cell><cell>0.750</cell><cell>0.576</cell><cell>0.763</cell><cell>0.755</cell><cell>0.844</cell><cell>0.823</cell><cell>0.835</cell><cell>0.801</cell><cell>0.858</cell><cell>0.868</cell></row><row><cell>SSD [74]</cell><cell cols="2">F mean ? F w ? ? Sm ?</cell><cell cols="2">? 0.073 0.172 0.341</cell><cell>0.684 0.480 0.706</cell><cell>0.524 0.429 0.603</cell><cell>0.709 0.536 0.741</cell><cell>0.709 0.622 0.776</cell><cell>0.786 0.733 0.842</cell><cell>0.748 0.662 0.813</cell><cell>0.767 0.727 0.839</cell><cell>0.726 0.709 0.807</cell><cell>0.821 0.787 0.856</cell><cell>0.822 0.785 0.870</cell></row><row><cell></cell><cell>Em ?</cell><cell></cell><cell></cell><cell>0.475</cell><cell>0.790</cell><cell>0.714</cell><cell>0.801</cell><cell>0.838</cell><cell>0.890</cell><cell>0.860</cell><cell>0.886</cell><cell>0.832</cell><cell>0.898</cell><cell>0.901</cell></row><row><cell></cell><cell>M ?</cell><cell></cell><cell></cell><cell>0.500</cell><cell>0.168</cell><cell>0.219</cell><cell>0.151</cell><cell>0.100</cell><cell>0.063</cell><cell>0.082</cell><cell>0.063</cell><cell>0.082</cell><cell>0.059</cell><cell>0.055</cell></row><row><cell></cell><cell>F max ?</cell><cell cols="2">?</cell><cell>0.328</cell><cell>0.769</cell><cell>0.661</cell><cell>0.789</cell><cell>0.857</cell><cell>0.888</cell><cell>0.868</cell><cell>0.888</cell><cell>0.890</cell><cell>0.896</cell><cell>0.914</cell></row><row><cell>NJUD [23]</cell><cell cols="2">F mean ? F w ? ? Sm ? Em ?</cell><cell cols="2">? 0.165 0.234 0.413 0.491</cell><cell>0.715 0.497 0.703 0.796</cell><cell>0.618 0.510 0.672 0.751</cell><cell>0.744 0.545 0.735 0.818</cell><cell>0.788 0.720 0.849 0.866</cell><cell>0.844 0.803 0.877 0.909</cell><cell>0.813 0.739 0.859 0.882</cell><cell>0.844 0.805 0.878 0.909</cell><cell>0.837 0.828 0.878 0.900</cell><cell>0.871 0.847 0.885 0.920</cell><cell>0.879 0.849 0.902 0.922</cell></row><row><cell></cell><cell>M ?</cell><cell></cell><cell></cell><cell>0.448</cell><cell>0.167</cell><cell>0.182</cell><cell>0.151</cell><cell>0.085</cell><cell>0.059</cell><cell>0.079</cell><cell>0.061</cell><cell>0.053</cell><cell>0.051</cell><cell>0.047</cell></row><row><cell></cell><cell>F max ?</cell><cell cols="2">?</cell><cell>0.800</cell><cell>0.311</cell><cell>0.651</cell><cell>0.625</cell><cell>0.865</cell><cell>0.842</cell><cell>0.839</cell><cell>0.853</cell><cell>0.882</cell><cell>0.906</cell><cell>0.919</cell></row><row><cell>RGBD135 [9]</cell><cell cols="2">F mean ? F w ? ? Sm ? Em ?</cell><cell cols="2">? 0.695 0.301 0.632 0.817</cell><cell>0.234 0.169 0.469 0.676</cell><cell>0.594 0.478 0.709 0.810</cell><cell>0.573 0.392 0.685 0.806</cell><cell>0.778 0.687 0.863 0.911</cell><cell>0.774 0.711 0.843 0.912</cell><cell>0.762 0.650 0.848 0.904</cell><cell>0.795 0.740 0.858 0.919</cell><cell>0.829 0.787 0.872 0.927</cell><cell>0.867 0.843 0.899 0.944</cell><cell>0.891 0.838 0.905 0.966</cell></row><row><cell></cell><cell>M ?</cell><cell></cell><cell></cell><cell>0.289</cell><cell>0.196</cell><cell>0.120</cell><cell>0.131</cell><cell>0.055</cell><cell>0.050</cell><cell>0.065</cell><cell>0.046</cell><cell>0.038</cell><cell>0.030</cell><cell>0.030</cell></row><row><cell></cell><cell>F max ?</cell><cell cols="2">?</cell><cell>0.695</cell><cell>0.413</cell><cell>0.687</cell><cell>0.752</cell><cell>0.841</cell><cell>0.864</cell><cell>0.841</cell><cell>0.876</cell><cell>0.884</cell><cell>0.888</cell><cell>0.904</cell></row><row><cell>NLPR [34]</cell><cell cols="2">F mean ? F w ? ? Sm ? Em ?</cell><cell cols="2">? 0.583 0.254 0.582 0.760</cell><cell>0.328 0.259 0.550 0.685</cell><cell>0.592 0.501 0.724 0.786</cell><cell>0.683 0.516 0.769 0.840</cell><cell>0.724 0.679 0.860 0.869</cell><cell>0.795 0.762 0.874 0.916</cell><cell>0.730 0.676 0.856 0.872</cell><cell>0.796 0.780 0.886 0.916</cell><cell>0.818 0.807 0.884 0.920</cell><cell>0.855 0.840 0.898 0.942</cell><cell>0.854 0.838 0.910 0.942</cell></row><row><cell></cell><cell>M ?</cell><cell></cell><cell></cell><cell>0.301</cell><cell>0.196</cell><cell>0.115</cell><cell>0.100</cell><cell>0.056</cell><cell>0.044</cell><cell>0.059</cell><cell>0.041</cell><cell>0.038</cell><cell>0.031</cell><cell>0.032</cell></row><row><cell></cell><cell>F max ?</cell><cell cols="2">?</cell><cell>0.720</cell><cell>0.680</cell><cell>0.544</cell><cell>0.704</cell><cell>0.720</cell><cell>0.861</cell><cell>0.840</cell><cell>0.851</cell><cell>0.870</cell><cell>0.847</cell><cell>0.894</cell></row><row><cell>SIP [16]</cell><cell cols="2">F mean ? F w ? ? Sm ?</cell><cell cols="2">? 0.644 0.342 0.616</cell><cell>0.645 0.414 0.683</cell><cell>0.495 0.397 0.595</cell><cell>0.673 0.406 0.653</cell><cell>0.684 0.535 0.716</cell><cell>0.825 0.768 0.842</cell><cell>0.795 0.712 0.833</cell><cell>0.809 0.748 0.835</cell><cell>0.819 0.788 0.850</cell><cell>0.815 0.734 0.800</cell><cell>0.856 0.810 0.874</cell></row><row><cell></cell><cell>Em ?</cell><cell></cell><cell></cell><cell>0.751</cell><cell>0.787</cell><cell>0.722</cell><cell>0.794</cell><cell>0.824</cell><cell>0.900</cell><cell>0.886</cell><cell>0.894</cell><cell>0.899</cell><cell>0.858</cell><cell>0.914</cell></row><row><cell></cell><cell>M ?</cell><cell></cell><cell></cell><cell>0.298</cell><cell>0.186</cell><cell>0.224</cell><cell>0.185</cell><cell>0.139</cell><cell>0.071</cell><cell>0.086</cell><cell>0.075</cell><cell>0.064</cell><cell>0.088</cell><cell>0.057</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparison of Zero-shot VOS methods on the DAVIS-16 validation set. ? and ? indicate that the larger and smaller scores are better, respectively. The best results are shown in red. The subscript in each model name is the publication year.</figDesc><table><row><cell cols="2">Metric</cell><cell cols="11">LVO17 ARP17 PDB18 LSMO19 MotAdapt19 EPO20 AGS19 COSNet19 AnDiff19 MATNet20 GateNet [47] [24] [44] [48] [42] [13] [56] [31] [65] [73] Ours</cell></row><row><cell></cell><cell>Mean?</cell><cell>75.9</cell><cell>76.2</cell><cell>77.2</cell><cell>78.2</cell><cell>77.2</cell><cell>80.6</cell><cell>79.7</cell><cell>80.5</cell><cell>81.7</cell><cell>82.4</cell><cell>80.9</cell></row><row><cell>J</cell><cell cols="2">Recall? 89.1</cell><cell>91.1</cell><cell>90.1</cell><cell>89.1</cell><cell>87.8</cell><cell>95.2</cell><cell>91.1</cell><cell>93.1</cell><cell>90.9</cell><cell>94.5</cell><cell>94.3</cell></row><row><cell></cell><cell>Decay?</cell><cell>0.0</cell><cell>7.0</cell><cell>0.9</cell><cell>4.1</cell><cell>5.0</cell><cell>2.2</cell><cell>1.9</cell><cell>4.4</cell><cell>2.2</cell><cell>5.5</cell><cell>3.3</cell></row><row><cell></cell><cell>Mean?</cell><cell>72.1</cell><cell>70.6</cell><cell>74.5</cell><cell>75.9</cell><cell>77.4</cell><cell>75.5</cell><cell>77.4</cell><cell>79.5</cell><cell>80.5</cell><cell>80.7</cell><cell>79.4</cell></row><row><cell>F</cell><cell cols="2">Recall? 83.4</cell><cell>83.5</cell><cell>84.4</cell><cell>84.7</cell><cell>84.4</cell><cell>87.9</cell><cell>85.8</cell><cell>89.5</cell><cell>85.1</cell><cell>90.2</cell><cell>89.2</cell></row><row><cell></cell><cell>Decay?</cell><cell>1.3</cell><cell>7.9</cell><cell>-0.2</cell><cell>3.5</cell><cell>3.3</cell><cell>2.4</cell><cell>1.6</cell><cell>5.0</cell><cell>0.6</cell><cell>4.5</cell><cell>2.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by the National Natural Science Foundation of China #61876202, #61725202, #61751212 and #61829102, the Dalian Science and Technology Innovation Foundation #2019J12GX039, and the Fundamental Research Funds for the Central Universities # DUT20ZD212.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Internet Multimedia Computing and Service</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploiting geometric constraints on dense trajectories for motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13258</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10421</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking rgb-d salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Submodular salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency-based discriminant tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9413" to="9422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Region-based saliency detection and its application in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video object segmentation using teacher-student adaptation in a human robot interaction (hri) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selectivity or invariance: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning for light field saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An iterative and cooperative top-down and bottom-up inference network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multi-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Saliency detection via graphbased manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Saliency detection via graphbased manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A dendritic disinhibitory circuit mechanism for pathway-specific gating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12815</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Capsal: Leveraging captioning to boost semantics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multilevel convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Motion-attentive transition for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
