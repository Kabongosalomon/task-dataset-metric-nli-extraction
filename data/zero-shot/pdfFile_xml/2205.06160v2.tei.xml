<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localized Vision-Language Matching for Open-vocabulary Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar?a</forename><forename type="middle">A</forename><surname>Bravo</surname></persName>
							<email>bravoma@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
							<email>mittal@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localized Vision-Language Matching for Open-vocabulary Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Open-vocabulary Object Detection</term>
					<term>Image-caption Match- ing</term>
					<term>Weakly-supervised Learning</term>
					<term>Multi-modal Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose an open-vocabulary object detection method that, based on image-caption pairs, learns to detect novel object classes along with a given set of known classes. It is a two-stage training approach that first uses a location-guided image-caption matching technique to learn class labels for both novel and known classes in a weakly-supervised manner and second specializes the model for the object detection task using known class annotations. We show that a simple language model fits better than a large contextualized language model for detecting novel objects. Moreover, we introduce a consistencyregularization technique to better exploit image-caption pair information. Our method compares favorably to existing open-vocabulary detection approaches while being data-efficient. Source code is available at https://github.com/lmb-freiburg/locov.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in deep learning have rapidly advanced the state-of-the-art object detection algorithms. The best mean average precision score on the popular COCO [24] benchmark has improved from 40 mAP to over 60 mAP in less than 4 years. However, this success required large datasets with annotations at the bounding box level and was a achieved in a closed-world setting, where the number of classes is assumed to be fixed. The closed-world setting restricts the object detector to only discover known annotated objects and annotating all possible objects in the world is infeasible due to high labeling costs. Therefore, research of open-world detectors, which can also discover unmarked objects, has recently come into focus specially using textual information together with images for open-vocabulary detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">41,</ref><ref type="bibr" target="#b28">44]</ref>.</p><p>To learn a visual concept, humans receive the majority of the supervision in the form of narrations rather than class tags and bounding boxes. Consider the example of <ref type="figure" target="#fig_0">Figure 1</ref> together with the annotations of mouse and tv only. Even after learning to detect these objects, finding and identifying the keyboard without any other source of information is ambitious. Instead, if we consider the image together with the caption -"A mouse, keyboard, and a monitor on a desk", it is possible to identify that the other salient object in the image is very likely a keyboard. This process involves successful localization of the objects in the scene, identification of different nouns in the narrated sentence, and matching the two together. Exploiting the extensive semantic knowledge contained in natural language is a reasonable step towards learning such openvocabulary models without expensive annotation costs.</p><p>In this work, we aim to learn novel objects using image-caption pairs. Along with image-caption pairs, the detector is provided with box annotations for a limited set of classes. We follow the problem setting as introduced by Zareian et al. <ref type="bibr" target="#b25">[41]</ref>. They refer to this problem as Open-vocabulary Object Detection. There are two major challenges to this problem: First, image-caption pairs themselves are too weak to learn localized object-regions. Analyzing previous works, we find that randomly sampled feature maps provide imprecise visual grounding for foreground objects, therefore they receive insufficient supervisory signals to learn object properties. Second, the granularity of the information captured by imageregion features should align with the level of information captured by the text representation for an effective matching. For example, it would be ill-suited to match a text representation that captures global image information with image features that capture localized information.</p><p>In this work, we propose a method that improves the matching between image and text representations. Our model is a two-stage approach: in the first stage, Localized Semantic Matching (LSM), it learns semantics of objects in the image by matching image-regions to the words in the caption; and in the second stage, Specialized Task Tuning (STT), it learns specialized visual features for the target object detection task using known object annotations. We called our method LocOv for Localized Image-Caption Matching for Open-vocabulary.</p><p>For the given objects in an image, our goal is to project them to a feature space where they can be matched with their corresponding class in the form of text embeddings. We find that simple text embeddings are better candidates for matching object representations than contextualized embeddings produced by large-scale language models.</p><p>Using image-caption pairs as weak supervision for object detection requires the understanding of both modalities in a fine and a coarse way. This can be obtained by processing each modality independently in a uni-modal fashion and then matching, or using cross-modal attention to process them together. To ensure consistent training between the uni-modal and cross-modal methods, we propose a consistency-regularization between the two matching scores. To summarize, our contributions are: (1) We introduced localized-regions during the image-caption matching stage to improve visual feature learning of objects. <ref type="bibr" target="#b1">(2)</ref> We show that simplified text embeddings match better with identified object features as compared to contextualized text embeddings. <ref type="formula" target="#formula_3">(3)</ref> We propose a consistency regularization technique to ensure effective cross-modal training.</p><p>These three contributions allow LocOv to be not only competitive against state-of-the-art models but also data-efficient by using less than 0.6 million image-caption pairs for training, ?700 times smaller than CLIP-based methods. Additionally, we define an open-vocabulary object detection setup based on the VAW [29] dataset, which offers challenging learning conditions like few-instances per object and a long-tailed distribution. Based on the above mentioned three contributions, we show that our method achieves state-of-the-art performance on both open-vocabulary object detection benchmarks, COCO and VAW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection with limited supervision Semi-supervised (SSOD) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">25,</ref><ref type="bibr">35]</ref> and weakly-supervised (WSOD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> object detection are two widely explored approaches to reduce the annotation cost. WSOD approaches aim to learn object localization using image-level labels only. Major challenges in WSOD approaches include differentiation between object instances [33] and precisely locating the entire objects. SSOD approaches use a small fully-annotated set and a large set of unlabeled images. Best SSOD <ref type="bibr">[25,</ref><ref type="bibr">35]</ref> methods are based on pseudo-labeling, which usually suffers from foreground-background imbalance and overfitting on the labeled set of images. In this work, we address a problem which shares similar challenges with the WSOD and SSOD approaches, however they are limited to a closed-world setting with a fixed and predefined set of classes. Our method addresses a mixed semi-and weakly-supervised object detection problem where the objective is open-vocabulary object detection.</p><p>Multi-modal visual and language models. Over the past years, multiple works have centered their attention on the intersection of vision and language by exploiting their consistent semantic information contained in matching pairs. The success of using this pairwise information has proved to be useful for pretraining transformer-like models for various vision-language tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">26,</ref><ref type="bibr">36,</ref><ref type="bibr">Fig. 2</ref>: Overview of LocOv . It is a two-stage model: (1) Localized Semantic Matching stage trains a Faster R-CNN-based model to match corresponding image-caption pairs using a grounding loss L G . We exploit the multi-modal information by using a cross-attention model and an Image-Caption matching loss L ICM , the mask language modeling loss L M LM and a consistency-regularization loss L Cons . (2) Specialized Task Tuning stage tunes the model using the known class annotations and specializes the model for object detection. See Section 3. 37, <ref type="bibr" target="#b26">42,</ref><ref type="bibr" target="#b29">45]</ref> which process the information jointly using cross-attention. Other approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">27,</ref><ref type="bibr">30,</ref><ref type="bibr">38]</ref>, centered on the vision and language retrieval task use separate encoders for each modality, in a uni-modal fashion. These models give the flexibility to transfer the knowledge learned by the pairwise information to single modality tasks, which is the case of object detection. In particular Miech et al. <ref type="bibr">[27]</ref> showed that combining a cross-attention model with two uni-modal encoders is beneficial for large-scale retrieval tasks. In this paper, we combine the strengths of both types of approaches to train a model using different consistency losses that exploit the information contained in image-caption pairs.</p><p>Language-guided object detection. Zero-shot object detection methods learn to align proposed object-region features to the class-text embeddings. Bansal et al. <ref type="bibr" target="#b1">[2]</ref> is among the first to propose the zero-shot object detection problem. They identified that the main challenge in ZSD is to separate the background class from the novel objects. Zhu et al. <ref type="bibr" target="#b30">[46]</ref> trained a generative model to "hallucinate" (synthesize visual features) unseen classes and used these generated features during training to be able to distinguish novel objects from background. Rahman et al.</p><p>[31] proposed a polarity loss to handle foregroundbackground imbalance and to improve visual-semantic alignment. However, such methods fail to perform well on the novel classes since the detection model has never seen these novel objects, and semantics learned by matching known objecttext embeddings does not extrapolate to novel classes.</p><p>To learn the semantics of novel classes, recent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">41,</ref><ref type="bibr" target="#b28">44]</ref> have simplified the problem by providing image-caption pairs as a weak supervision signal. Such pairs are cheap to acquire and make the problem tractable. Imagecaption pairs allow the model to observe a large set of object categories along with object labels. These methods either use this model to align image-regions with captions and generate object-box pseudo labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">44]</ref> or as region-image feature extractor to classify the regions <ref type="bibr" target="#b12">[13]</ref>. Many weakly-supervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b27">43]</ref> approaches have been proposed to perform such object grounding. Due to the large performance gap between zero-shot/weakly-supervised and fullysupervised approaches for object detection, Zareian et al. <ref type="bibr" target="#b25">[41]</ref> introduced an open-vocabulary problem formulation. It utilizes extra image-caption pairs to learn to detect both known and novel objects. Their approach matches all parts of the image with the caption, whereas we emphasize object localized regions and a consistency loss to enforce more object-centric matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a two-stage approach for the task of open-vocabulary object detection as shown in <ref type="figure">Figure 2</ref>. The first stage, Localized Semantic Matching (LSM), learns to match objects in the image to their corresponding class labels in the caption in a weakly-supervised manner. The second stage, Specialized Task Tuning (STT) stage, includes specialized training for the downstream task of object detection. We consider two sets of object classes: known classes O K and novel classes O N . Bounding box annotations, including class labels, are available for known classes whereas there are no annotations for the novel classes.</p><p>The LSM receives image-caption pairs (I, C) as input, where the caption provides the weak supervision to different image-regions. Captions contain rich information which often include words corresponding to object classes from both known and novel sets. Captions are processed using a pre-trained text-embedding model (e.g.BERT <ref type="bibr" target="#b9">[10]</ref> embedding) to produce word or part-of-word features. Images are processed using an object detection network (Faster R-CNN [32]) to obtain object region features. We propose to utilize an object proposal generator OLN <ref type="bibr" target="#b17">[18]</ref> to provide regions as pseudo-labels to train the Faster R-CNN. This helps obtaining object-rich regions which improve image region-caption matching. This way, during the LSM our model learns to match all present objects in the image in a class-agnostic way. See Section 3.1 for details. The STT stage tunes the Faster R-CNN using known object annotations primarily to distinguish foreground from background and learns corresponding precise location of the foreground objects. See Section 3.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Localized Semantic Matching (LSM)</head><p>The LSM stage consists of three main components: (1) localized object regiontext matching, (2) disentangled text features and (3) consistency-regularization.</p><p>Localized object region-text matching. Given the sets R I = { r : r is an image-region feature vector from the image I} and W C = { w : w is a word or part-of-word feature vector from the caption C}, we calculate the similarity score between an image and a caption in a fine-grained manner, by comparing imageregions with words, since our final objective is to recognize objects in regions. The image is processed using a Faster R-CNN model and a projection layer that maps image-regions into the text-embedding feature space. The similarity score is calculated by taking an image composed of |R I | region features and a caption composed of |W C | part-of-word features by:</p><formula xml:id="formula_0">sim(I, C) = 1 |R I | |R I | i=1 |W C | j=1 d i,j (r i ? w j )<label>(1)</label></formula><p>where d i,j corresponds to:</p><formula xml:id="formula_1">d(r i , w j ) = d i,j = exp(r i ? w j ) |W C | j =1 exp(r i ? w j )</formula><p>.</p><p>(</p><p>Based on the similarity score (Eq. 1) , we apply a contrastive learning objective to match the corresponding pairs together by considering all other pairs in the batch as negative pairs. We define this grounding loss as:</p><formula xml:id="formula_3">L Gr (I) = ? log exp(sim(I, C)) C ?Batch exp(sim(I, C ))<label>(3)</label></formula><p>We apply this loss in a symmetrical way, where each image in the batch is compared to all captions in the batch (Eq. 3) and each caption is compared to all images in the batch L Gr (C). The subscript r denotes the type of imageregions used for the loss calculation. We consider two types of image-regions: box-regions and grid-regions. Box-region features are obtained naturally using the region of interest pooling (RPN) from the Faster R-CNN. We make use of the pre-trained object proposal generator (OLN) to train the Faster-RCNN network. OLN is a class-agnostic object proposal generator which estimates all objects in the image with a high average recall rate. We train OLN using the known class annotations and use the predicted boxes to train our detection model, shown in <ref type="figure">Figure 2</ref>. Since captions sometimes refer to background context in the image, parallel to the box-region features, we also use grid-region features similar to the OVR <ref type="bibr" target="#b25">[41]</ref> approach. Grid-region features are obtained by skipping the RPN in the Faster R-CNN and simply using the output of the backbone network. We apply the grounding loss to both type of image-region features. Our final grounding loss is given by:</p><formula xml:id="formula_4">L G = L G box (C) + L G box (I) + L G grid (C) + L G grid (I)<label>(4)</label></formula><p>Disentangled text features. Many previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">26</ref>, 36] use contextualized language models to extract text representations of the sentence. Although, this might be suitable for a task that requires a global representation of a phrase or text, this is not ideal for the case for object detection, where each predicted bounding box is expected to contain a single object instance. We show that using a simple text representation, which keeps the disentangled semantics of words in a caption, gives the flexibility to correctly match object boxes in an image with words in a caption. Our method uses only the embedding module <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">28]</ref> of a pre-trained language model to encode the caption and perform matching with the proposed image-regions. For embedding model we refer to the learned dictionary of vector representations of text tokens, which correspond to words or part-of-words. For cases where the text representing an object category is divided into multiple tokens, we consider the average representation of the tokens as the global representation of the object category. We show empirically, in Section 4.4, that using such a lightweight text embedding module has better performance than using a whole large-scale language model.</p><p>Consistency-regularization Miech et al.</p><p>[27] showed that processing multimodal data using cross-attention networks brings improvements in retrieval accuracy over using separate encoders for each modality and projecting over a common embedding space. However, this cross-attention becomes very expensive when the task requires large-scale retrieval. To take the benefit of crossattention models, we consider a model similar to PixelBERT <ref type="bibr" target="#b14">[15]</ref> to process the image-caption pairs. This cross-attention model takes the image-regions R I together with the text embeddings W C and matches the corresponding imagecaption pairs in a batch. The image-caption matching loss (L ICM ) of the crossattention model together with the traditional Masking Language Modeling loss (L M LM ) enforces the model to better project the image-region features to the language semantic space.To better utilize the cross-attention model, we propose a consistency-regularization loss (L Cons ) between the final predicted distribution over the image-caption matching scores in the batch, before and after the crossattention model. We use the Kullback-Leibler divergence loss to impose this consistency. In summary, we use three consistency terms over different imagecaption pairs:</p><formula xml:id="formula_5">L Cons =D KL (p(I box , C)||q(I box , C)) + D KL (p(I grid , C)||q(I grid , C)) + D KL (p(I grid , C)||q(I box , C))<label>(5)</label></formula><p>where p(I * , C) and q(I * , C) correspond to the softmax of the image-caption pairs in a batch before and after the cross-attention model respectively, and the subindex of the image corresponds to the box-or grid-region features. Our final loss for the LSM stage corresponds to the sum of the above defined losses:</p><formula xml:id="formula_6">L LSM = L G + L ICM + L M LM + L Cons<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Specialized Task Tuning (STT)</head><p>In this stage, we fine-tune the model using known class annotations to learn to localize the objects precisely. We initialize the weights from the LSM stage model, and partially freeze part of the backbone and the projection layer to preserve the learned semantics. Freezing the projection layer is important to avoid overfitting on the known classes and generalize on novel classes. To predict the class of an object, we compute the similarity score between the proposed object box-region feature vector (r i ) and all the class embedding vectors c k and apply softmax</p><formula xml:id="formula_7">p(r i , c k ) = exp(r i ? c k ) 1 + c k ?O K exp(r i ? c k ) .<label>(7)</label></formula><p>The scalar 1 included in the denominator corresponds to the background class, which has a representation vector of all-zeros. We evaluate the performance across three setups: (Novel) considering only the novel class set O N , (Known) comparing with the known classes only O K and (Generalized) considering all novel and known classes together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Details</head><p>Datasets. The Common Objects in Context (COCO) dataset <ref type="bibr" target="#b22">[23]</ref> is a large-scale object detection benchmark widely used in the community. We use the 2017 train and val split for training and evaluation respectively. We use the known and novel object class splits proposed by Bansal et al. <ref type="bibr" target="#b1">[2]</ref>. The known set consists of 48 classes while the novel set has 17 classes selected from the total of 80 classes of the original COCO dataset. We remove the images which do not contain the known class instances from the training set. For the localized semantic matching phase, we use the captions from COCO captions <ref type="bibr" target="#b4">[5]</ref> dataset which has the same train/test splits as the COCO object detection task. COCO captions dataset contains 118,287 images with 5 captions each. Additionally in the supplementary material, we test LocOv using Visual Attributes in the Wild (VAW) dataset [29] a more challenging dataset containing fine-grained classes with a long-tailed distribution.</p><p>Evaluation metric. We evaluate our method using mean Average Precision (AP) over IoU scores from 0.5 to 0.95 with a step size of 0.05, and using two fixed thresholds at 0.5 (AP 50 ) and 0.75 (AP 75 ). We compute these metrics separately for novel and known classes, calculating the softmax within the subsets exclusively; and in a generalized version both sets are evaluated in a combined manner, calculating the probability across all classes.</p><p>Implementation details. We base our model on Faster R-CNN C4 [32] configuration, using ResNet50 <ref type="bibr" target="#b13">[14]</ref> backbone pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref>, together with a linear layer (projection layer) to obtain the object feature representations. We use Detectron2 framework <ref type="bibr" target="#b24">[40]</ref> for our implementation. For the partof-word feature representations, we use the embedding module of the pre-trained  <ref type="bibr" target="#b17">[18]</ref>. OLN is trained using only the known classes on COCO training set. We use all the proposals generated for the training images which have an objectness score higher than 0.7. For our cross-attention model, we use a transformer-based architecture with 6 hidden layers and 8 attention heads trained from scratch. We train our LSM stage with a base learning rate of 0.001, where the learning rate is divided by 10 at 45k and 60k iterations. We use a batch size of 32 and train on 8 GeForce-RTX-2080-Ti GPUs for 90k iterations. For the STT stage, we initialize the weights of the Faster R-CNN and projection layer from the LSM stage, freezing the first two blocks of ResNet50 and the projection layer. For object classes that contain more than one part-of-word representation given BERT embedding module, we consider the average of their vector representation. We use a base learning rate of 0.005 with a 10 times drop at 60k iterations and do early stopping to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>OVR. The main baseline approach is proposed by Zareian et al. <ref type="bibr" target="#b25">[41]</ref>. We utilize some components proposed in the work including the two-stage design, grounding loss and usage of a cross-attention model. In this work, we propose new components, which simplify and improve the model performance over OVR. STT-ZSD. Our second baseline uses only the Specialized Task Tuning stage. This resembles a zero-shot object detection setting. The model is initialized with ImageNet <ref type="bibr" target="#b8">[9]</ref> weights with a trainable projection layer. Zero-shot methods. We compare to some zero-shot object detection approaches which do not include the weak supervision provided by the captions. We compare to three background-aware zero-shot detection methods, introduced by Bansal et al. <ref type="bibr" target="#b1">[2]</ref>, which project features of an object bounding box proposal method to word embeddings. The SB method includes a fixed vector for the background class in order to select which bounding boxes to exclude during the object classification, LAB uses multiple latent vectors to represent the different variations of the background class, and DSES includes more classes than the known set as word embedding to train in a more dense semantic space. DELO <ref type="bibr" target="#b30">[46]</ref> method uses a generative model and unknown classes to synthesize visual features and uses them while training to increase background confidence. PL [31] work deals with the imbalance between positive vs. negative instance ratio by proposing a method that maximizes the margin between foreground and background boxes. Faster R-CNN. We also compare with training the classical Faster R-CNN model only using the known classes.</p><p>Open-vocabulary with large data. We compare our method with recent open-vocabulary models. RegionClip <ref type="bibr" target="#b28">[44]</ref> uses the CLIP [30] pre-trained model to produce image-region pseudo labels and train an object detector. CLIP (cropped reg) <ref type="bibr" target="#b12">[13]</ref> uses the CLIP pre-trained model on 400M image-caption pairs on object proposals obtained by an object detector trained on known classes. XP-Mask <ref type="bibr" target="#b15">[16]</ref> learns a class-agnostic region proposal and segmentation model from the known classes and then uses this model as a teacher to generate pseudo masks for selftraining a student model. Finally, we also compare with VILD <ref type="bibr" target="#b12">[13]</ref> which uses CLIP soft predictions to distil semantic information and train an object detector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>COCO dataset. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of our method with several zero-shot and open-vocabulary object detection approaches. LocOv outperforms previous zero-shot detection methods, which show weak performance on detecting novel objects. In comparison to OVR, we improve by 2.53 AP, 3.4 AP 50 for the novel classes and 3.91 AP, 3.92 AP 50 for the known categories. We observe open-vocabulary methods including OVR and our methods have a trade-off between known and novel class performance. Our method finds a better trade-off as compared to the previous work. It reduces the performance gap on known classes as compared to the Faster R-CNN and improves over the novel classes as compared to all previous works. Our method is competitive with recent stateof-the-art methods which use more than ?700 times more image-captions pairs to train, which makes our method data efficient. <ref type="figure" target="#fig_1">Figure 3</ref> shows some qualitative results of our method compared with the STT-ZSD baseline and OVR. Known categories are drawn in green while novel are highlighted in magenta. The columns correspond to the ground truth, STT-ZSD, OVR and our method from left to right. LocOv is able to find novel objects with a high confidence, such as the dogs in the first example, the couch in the second and the umbrella in the third one. We observe that our method sometimes misclassifies objects with plausible ones, such as the case of the chair in the second example which shares a similar appearance to a couch. These examples show a clear improvement of our approach, over the other methods. In the supplementary material we include some examples of our method showing the limitations and main cause of errors of LocOv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Experiments</head><p>Localized objects matter. <ref type="table" target="#tab_1">Table 2</ref> presents the impact of using box-vs gridregion features in the LSM stage. We compare our method using grid-region features R I grid , proposed box-region features R I box , and using box-region features from the known (k) or novel (n) class annotations R I ann . We find that the combination of grid-and box-regions proves to be best, showing a complementary behavior. We also considered two oracle experiments (row 1 and 2) using groundtruth box-region features from both known and novel class annotations instead of proposed box-region features. The best performance is achieved when combined with additional grid regions (row 1). The additional grid-regions help in capturing the background objects beyond the annotated classes while box-regions focus on foreground objects, which improves the image-caption matching.</p><p>Consistency loss and text embedding selection. <ref type="table" target="#tab_2">Table 3</ref>, shows the contribution of our consistency-regularization term. We get an improvement of 1.76 AP by introducing our consistency loss. We compare the performance of using a pre-trained text embedding module vs learning it from scratch, finetuning it or considering the complete contextualized language model during the LSM stage in <ref type="table" target="#tab_2">Table 3</ref>. Using the pre-trained text embedding, results in a better model. We find out that using only the embeddings module is sufficient and better than using the complete contextualized BERT language model for the task of object detection. We argue that this is because objects are mostly represented by single word vectors, using simple disentangled text embeddings is better suited for generating object class features. In the supplementary material we include an ablation study showing that both stages of training are necessary and complementary for the success of LocOv . <ref type="table" target="#tab_3">Table 4</ref> shows the the improvement in performance for each of our contributions. Our baseline method is our implementation of OVR <ref type="bibr" target="#b25">[41]</ref>. Both the consistency-regularization together with the inclusion of the box-regions gives the most increment in performance for both novel and known classes. Using only the BERT Embeddings improves the novel class performance although it affects the known classes. Overall we can see that the three contributions are complementary and improve the method for open-vocabulary detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed an image-caption matching method for open-vocabulary object detection. We introduced a localized matching technique to learn improved labels of novel classes as compared to only using grid features. We also showed that the language embedding model is preferable over a complete language model, and proposed a regularization approach to improve cross-modal learning. In conjunction, these components yield favorable results compared to previous open-vocabulary methods on COCO and VAW benchmarks, particularly considering the much lower amount of necessary data to learn from.</p><p>24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European Conference on Computer Vision (ECCV) (2014) 25. Liu, Y.C., Ma, C.Y., He, Z., Kuo, C.W., Chen, K., Zhang, P., Wu, B., Kira, Z., Vajda, P.: Unbiased teacher for semi-supervised object detection. In: International Conference on Learning Representations (2021) 26. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In: The dataset contains 58,565 images for training, 3,317 images for validation, and 10,392 images for testing. We define the splits for known and novel classes taking approximately 20% of the total classes (2260) to be novel, resulting in 1792 known and 468 novel classes. We make sure that all known and novel classes from COCO split are kept in the same subset for VAW splits. After removing images with no known annotations from the training and splitting into known and novel classes, there are 54,632 images for training spanning over 1790 known classes, 818 known / 200 novel classes for the validation set, and 1020 known / 297 novel classes for the test set. This dataset is much more challenging as compared to COCO since it contains fine-grained classes with a long-tailed distribution. It not only contains more classes as compared to the COCO benchmark, but also poses additional challenges like plural versions defined as different classes, e.g.kites vs kite. In the LSM phase, we use the captions from Visual Genome Region Descriptions <ref type="bibr" target="#b20">[21]</ref> which contain 108,077 images with a total of 4,297,502 region descriptions. We combine these region descriptions for every image to have a single caption per image. VAW dataset results. LocOv successfully generalizes to the VAW benchmark. <ref type="table" target="#tab_5">Table 5</ref> shows the comparison of our approach to both STT-ZSD and OVR baselines on the test set. Our method improves consistently over the other two methods for the novel classes, showing that it can scale to more challenging settings with long-tailed distribution and large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ablation Experiments</head><p>Two-stage model performs best. In <ref type="table" target="#tab_6">Table 6</ref>, we show the extended performance of our method using single stage, either LSM or STT, and fine-tuning different sets of the backbone weights during the STT stage. The last two rows of <ref type="table" target="#tab_6">Table 6</ref> consider our method using only the STT stage (same as our baseline STT-ZSD from Section 4.2 in the main paper) and using only the LSM stage. Individual</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Novel (297) Known (1020) Generalized (2060) AP AP50 AP75 AP AP50 AP75 AP AP50 AP75 STT-ZSD (Ours) 0.14 0.28 0. <ref type="bibr" target="#b14">15</ref>    stage models are not able to detect novel objects well, which shows that both stages are fundamental for the detection of novel objects. We further compare the performance of different model configurations by freezing different number of blocks of the backbone network during the STT stage. Our results show that only freezing the first two blocks and the projection layer leads to the best configuration for the STT. In conclusion we can observe two main results: first, using both stages is crucial to detect novel objects. Second, freezing the backbone weights of the 1st and 2nd ResNet blocks during the STT stage results in the best configuration for both, novel and known, performances. Localized objects matter.  the combination of grid-and box-regions and not simply from more boxes, we trained with an increased number of image regions (100 and 200) for every case explicitly stated in <ref type="table" target="#tab_7">Table 7</ref>. Even though increasing the number of regions results in a better performance the combination of both types of regions proves to be best, showing a complementary behavior. We also considered two oracle experiments (row 1 and 2) using ground-truth box-region features from both known and novel class annotations instead of proposed box-region features. These two experiments improve performance on novel classes showing that object-centered box regions are crucial and the best performance is achieved when combined with additional grid regions (row 1). The additional grid-regions help in capturing the background objects beyond the annotated classes while box-regions focus on precise foreground objects, which improves the image-caption matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Limitations</head><p>Visual features of novel object classes are learned during the Localized Semantic Matching stage using image-caption pairs. We notice that such a weak form of supervision is not sufficient to learn fine-grained classification. Similar classes such as 'dog' and 'cat' or 'knife' and 'fork' are often confused, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, since they can be used exchangeably in the caption description and they sometimes even co-occur in the image (e.g.knife-fork), making the matching process ambiguous. We also observe a clear drop in performance of known object classes when a similar novel object class is detected. A table showing this analysis quantitatively is included in the supplementary. <ref type="figure">Figure 5</ref> presents the difference of AP per class when considering the generalized setup, all classes together, minus the AP for the individual setup, only the novel or only the known classes. Most of the scores present a drop when considering the generalized case. Analyzing cases where this drop is larger than 3.5 AP (the red bars in <ref type="figure">Figure 5</ref>) we can deduce that these classes are mostly confused. Figures 6 <ref type="figure">Fig. 5</ref>: We plot the difference in AP score when considering the generalized setup (all classes together) as compare to considering the individual sets of known and novel separately. Most of the classes present a drop when considering all classes together. Red bars correspond to classes with a drop larger than 3.5 AP. and 7 show some qualitative examples of our method. We show the ground truth image with annotations and results using our method for comparison. In <ref type="figure">Figure  6</ref> we can observe that classes such as bowl and cup are frequently confused, and similar error occurs for classes: fork, knife and spoon. These errors occur due to the fact that these classes look similar or appear together very often. These type of errors are also noticeable between other such classes like cow/sheep/dog and snowboard/skis/skateboard. The class toaster is a special case since it is the class with the least instances present in the dataset (only 9 vs a median of 275), which makes it harder for our method to distinguish this class among the known set and the task becomes harder when considering all 65 classes. Our method is capable of discovering novel classes such as cat, dog, sink, bus with high confidence, specially when there is no ambiguity or similarity with other categories. Similar visual classes such as fork, knife and spoon; cow and sheep; cat and dog; couch and bed; or snowboard, skis, and skateboard or are sometimes confused by our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Per Class Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Qualitative examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Open-vocabulary object detection. (a) Compares our method LocOv with the baseline method (OVR) and our zero-shot baseline STT-ZSD (ZSD). LocOv improves on both novel and known classes without dropping the performance on known classes. The zero-shot method, only trained with known classes, obtains low performance (&lt; 0.5 mAP) on novel classes. (b-d) LocOv is able to detect the novel object 'keyboard' along with known objects, shown infigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results for open-vocabulary object detection on MSCOCO dataset. Novel classes are shown in magenta while known are in green. Methods compared are described in Section 4.2. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Proceedings of the 33rd International Conference on Neural Information Processing Systems (2019) 27. Miech, A., Alayrac, J.B., Laptev, I., Sivic, J., Zisserman, A.: Thinking fast and slow: Efficient text-to-visual retrieval with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 28. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (2014) 29. Pham, K., Kafle, K., Lin, Z., Ding, Z., Cohen, S., Tran, Q., Shrivastava, A.: Learning to predict visual attributes in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748-8763. PMLR (2021) 31. Rahman, S., Khan, S., Barnes, N.: Improved visual-semantic alignment for zeroshot object detection. Proceedings of the AAAI Conference on Artificial Intelligence (2020) 32. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in Neural Information Processing Systems (2015) 33. Ren, Z., Yu, Z., Yang, X., Liu, M.Y., Lee, Y.J., Schwing, A.G., Kautz, J.: Instanceaware, context-focused, and memory-efficient weakly supervised object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 34. Sadhu, A., Chen, K., Nevatia, R.: Video object grounding using semantic roles in language description. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 35. Sohn, K., Zhang, Z., Li, C., Zhang, H., Lee, C., Pfister, T.: A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757 (2020) 36. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of generic visual-linguistic representations. In: International Conference on Learning Representations (2019) 37. Tan, H., Bansal, M.: Lxmert: Learning cross-modality encoder representations from transformers. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (2019) 38. Wang, L., Li, Y., Huang, J., Lazebnik, S.: Learning two-branch neural networks for image-text matching tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2018) 39. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Appendix A VAW dataset Visual Attributes in the Wild (VAW) dataset [29] We use the training, validation and test set of images as defined with the proposed dataset [29].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Failure cases. The method fails to learn fine-grained classification for novel objects. The model confuses between similar classes. For e.g. the model sometimes predicts 'fork' as 'knife'(left image) and 'cat' as 'dog'(right image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figures 6 and 7</head><label>7</label><figDesc>show some random qualitative examples of LocOv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Qualitative results obtained using LocOv on the COCO dataset. Novel classes are shown in magenta while known are in green. (Best viewed in color) Qualitative results obtained using LocOv on the COCO dataset. Novel classes are shown in magenta while known are in green. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing mAP and AP 50 state-of-the-art methods. LocOv outperforms all other methods for Novel objects in the generalized setup while using only 0.6M of image-caption pairs. Training dataset: * ImageNet1k, ? COCO captions, ? CLIP400M, ? Conceptual Captions, Open Images, and c COCO</figDesc><table><row><cell></cell><cell>Img-Cap</cell><cell></cell><cell cols="2">Constrained</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Generalized</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="11">Data Novel (17) Known (48) Novel (17) Known (48) All (65)</cell></row><row><cell></cell><cell>Size</cell><cell cols="10">AP AP50 AP AP50 AP AP50 AP AP50 AP AP50</cell></row><row><cell>Faster R-CNN</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SB [2]</cell><cell></cell><cell>-</cell><cell>0.70</cell><cell>-</cell><cell>29.7</cell><cell>-</cell><cell>0.31</cell><cell>-</cell><cell>29.2</cell><cell>-</cell><cell>24.9</cell></row><row><cell>LAB [2] DSES [2]</cell><cell>-</cell><cell>--</cell><cell>0.27 0.54</cell><cell>--</cell><cell>21.1 27.2</cell><cell>--</cell><cell>0.22 0.27</cell><cell>--</cell><cell>20.8 26.7</cell><cell>--</cell><cell>18.0 22.1</cell></row><row><cell>DELO [46]</cell><cell></cell><cell>-</cell><cell>7.6</cell><cell>-</cell><cell>14.0</cell><cell>-</cell><cell>3.41</cell><cell>-</cell><cell>13.8</cell><cell>-</cell><cell>13.0</cell></row><row><cell>PL [31]</cell><cell></cell><cell>-</cell><cell>10.0</cell><cell>-</cell><cell>36.8</cell><cell>-</cell><cell>4.12</cell><cell>-</cell><cell>35.9</cell><cell>-</cell><cell>27.9</cell></row><row><cell>STT-ZSD (Ours)</cell><cell></cell><cell cols="10">0.21 0.31 33.2 53.4 0.03 0.05 33.0 53.1 24.4 39.2</cell></row><row><cell cols="12">OVR  *  ?c [41] LocOv  7 14.6 27.5 26.9 46.8 -22.8 -46.0 22.8 39.9 0.6M</cell></row><row><cell>XP-Mask  ? ? c [16]</cell><cell>5.7M</cell><cell>-</cell><cell>29.9</cell><cell>-</cell><cell>46.8</cell><cell>-</cell><cell>27.0</cell><cell>-</cell><cell>46.3</cell><cell>-</cell><cell>41.2</cell></row><row><cell cols="2">CLIP (cropped reg)  ? [13] 400M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.3</cell><cell>-</cell><cell>28.3</cell><cell>-</cell><cell>27.8</cell></row><row><cell>RegionCLIP  ? ?c [44]</cell><cell>400.6M</cell><cell>-</cell><cell>30.8</cell><cell>-</cell><cell>55.2</cell><cell>-</cell><cell>26.8</cell><cell>-</cell><cell>54.8</cell><cell>-</cell><cell>47.5</cell></row><row><cell>ViLD  ?c [13]</cell><cell>400M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.6</cell><cell>-</cell><cell>59.5</cell><cell>-</cell><cell>51.3</cell></row></table><note>* ?c (Ours) 17.2 30.1 33.5 53.4 16.6 28.6 31.9 51.3 28.1 45.BERT [10] "base-uncased" model from the HuggingFace implementation [39]. To get the object proposals for the LSM stage, we train a generic object proposal network, OLN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Different image regions for the LSM stage. R I grid -grid-regions, R</figDesc><table><row><cell>I box -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study showing the contribution of our proposed consistencyregularization term (L Cons ) and usage of BERT text embeddings on COCO validation set. We compared using frozen pretrained weights (fz) of the language model and embedding, fine-tuning (ft) or training from scratch</figDesc><table><row><cell>LCons</cell><cell cols="3">BERT BERT Model Emb. AP AP50 AP75 AP AP50 AP75 AP AP50 AP75 Novel (17) Known (48) Generalized</cell></row><row><cell></cell><cell></cell><cell>fz</cell><cell>17.2 30.1 17.5 33.5 53.4 35.5 28.1 45.7 29.6</cell></row><row><cell></cell><cell>fz</cell><cell>fz</cell><cell>16.7 29.7 16.7 33.4 53.5 35.5 28.2 45.9 29.5</cell></row><row><cell></cell><cell></cell><cell>ft</cell><cell>16.9 29.5 16.9 33.4 53.0 35.4 28.1 45.7 29.4</cell></row><row><cell></cell><cell></cell><cell cols="2">scratch 16.0 28.3 16.2 30.4 49.6 31.8 25.8 42.9 26.6</cell></row><row><cell></cell><cell></cell><cell>fz</cell><cell>15.4 27.9 15.2 32.2 52.1 34.1 26.3 43.6 27.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study showing the contributions LocOv . L Cons = consistencyregularization, R I box = inclusion of box-regions together with grid-regions, BERT Emb. only.</figDesc><table><row><cell cols="2">LCons R I box</cell><cell cols="2">BERT Emb. AP AP50 AP75 AP AP50 AP75 AP AP50 AP75 Novel (17) Known (48) Generalized</cell></row><row><cell></cell><cell></cell><cell></cell><cell>17.2 30.1 17.5 33.5 53.4 35.5 28.1 45.7 29.6</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>16.7 29.7 16.7 33.4 53.5 35.5 28.2 45.9 29.5</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>15.5 27.1 15.4 32.2 52.1 33.9 27.1 44.5 28.2</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>15.4 27.9 15.2 32.2 52.1 34.1 26.3 43.6 27.3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>14.3 25.6 14.4 28.1 47.8 29.3 23.7 40.9 24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparing open-vocabulary object detection results on the VAW test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the different stages of the model on the novel object detection. The table also shows different configurations of model update in the STT stage by freezing parts of the backbone network AP AP50 AP75 AP AP50 AP75 AP AP50 AP75 17.17 30.86 16.78 30.79 50.68 32.21 26.14 43.80 27.05 16.77 30.91 16.24 30.10 49.71 31.14 25.44 43.14 25.98 15.96 29.09 15.59 29.14 48.50 30.63 24.82 41.</figDesc><table><row><cell>LSM STT</cell><cell>Freezing blocks 1-4 1-3 1-2 99 25.73 Novel (17) Known (48) Generalized</cell></row><row><cell></cell><cell>0.73 1.89 0.37 0.82 2.06 0.48 0.89 2.27 0.52</cell></row><row><cell></cell><cell>0.21 0.31 0.21 33.23 53.43 35.03 24.38 39.19 25.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Different image regions for the LSM stage.</figDesc><table><row><cell>R I grid -grid-regions, R I box -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc>presents the impact of using box-vs grid-region features in the LSM stage. We compare our method using grid-region features R I grid , proposed box-region features R I box , and using box-region features from the known (k) or novel (n) class annotations R I ann . When training the LSM stage, we only consider a fixed amount of image regions to calculate the losses and drop the rest of the regions. To illustrate that the improvement comes from</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by Deutscher Akademischer Austauschdienst -German Academic Exchange Service (DAAD) Research Grants -Doctoral Programmes in Germany, 2019/20; grant number: 57440921.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised object detection and retrieval using unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cobe: Contextualized object embeddings from narrated instructional video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly-supervised spatio-temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coot: Cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lL3lnMbR4WU" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12698</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning open-world object proposals without learning to classify</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5453" to="5460" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object-aware instance labeling for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Where does it exist: Spatio-temporal video grounding for multi-form sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09106</idno>
		<title level="m">Regionclip: Region-based language-image pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Don&apos;t even look once: Synthesizing features for zero-shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
