<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Entity Linking by Modeling Latent Entity Type Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiang</surname></persName>
							<email>fjiang@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Entity Linking by Modeling Latent Entity Type Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing state of the art neural entity linking models employ attention-based bag-of-words context model and pre-trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility. However, the latent entity type information in the immediate context of the mention is neglected, which causes the models often link mentions to incorrect entities with incorrect type. To tackle this problem, we propose to inject latent entity type information into the entity embeddings based on pre-trained BERT. In addition, we integrate a BERT-based entity similarity score into the local context model of a state-of-the-art model to better capture latent entity type information. Our model significantly outperforms the state-of-the-art entity linking models on standard benchmark (AIDA-CoNLL). Detailed experiment analysis demonstrates that our model corrects most of the type errors produced by the direct baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Entity Linking (EL) is the task of disambiguating textual mentions to their corresponding entities in a reference knowledge base (e.g., Wikipedia). An accurate entity linking system is crucial for many knowledge related tasks such as question answering <ref type="bibr">(Yih et al. 2015)</ref> and information extraction <ref type="bibr" target="#b13">(Hoffmann et al. 2011)</ref>.</p><p>Traditional entity linkers mainly depend on manually designed features to evaluate the local context compatibility and document-level global coherence of referent entities <ref type="bibr" target="#b0">(Cheng and Roth 2013;</ref><ref type="bibr" target="#b4">Durrett and Klein 2014)</ref>. The design of such features requires entity-specific domain knowledge. These features can not fully capture relevant statistical dependencies and interactions. One recent notable work <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref> instead pioneers to rely on pre-trained entity embeddings, learnable context representation and differentiable joint inference stage to learn basic features and their combinations from scratch. Such model design allows to learn useful regularities in an end-to-end fashion and eliminates the need for extensive feature engineering. It also substantially outperforms In Milwaukee , Marc Newfield homered off Jose Parra  leading off the bottom of the 12th as the Brewers rallied for a 5-4 victory over the Minnesota Twins .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Title</head><p>Local context score Golden  <ref type="figure">Figure 1</ref>: One error case on AIDA-CoNLL development set of the full model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. (a) Immediate context; (b) Attended contextual words sorted by attention weights. The preposition "In" is a strong cue predictive of the type of mention "Milwaukee" which is not captured by the local context model. the traditional methods on standard benchmark (e.g., <ref type="bibr">AIDA-CoNLL)</ref>. A line of follow-up work <ref type="bibr" target="#b17">(Le and Titov 2018;</ref><ref type="bibr" target="#b18">2019a;</ref><ref type="bibr" target="#b19">2019b)</ref> investigate potential improvement solution or other task settings based on that. Such state-of-the-art entity linking models <ref type="bibr" target="#b6">(Ganea and Hofmann 2017;</ref><ref type="bibr" target="#b17">Le and Titov 2018)</ref> employ attention-based bag-of-words context model and pre-trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility. However, the latent entity type information in the immediate context of the mention is neglected. We suspect this may sometimes cause the models link mentions to incorrect entities with incorrect type. To verify this, we conduct error analysis of the well known DeepED 1 model <ref type="bibr">(Ganea and</ref> Hofmann 2017) on the development set of AIDA-CoNLL <ref type="bibr" target="#b12">(Hoffart et al. 2011)</ref>, and found that more than half of their error cases fall into the category of type errors where the predicted entity's type is different from the golden entity's type, although some predictive contextual cue for them can be found in their local context. As shown in <ref type="figure">Fig. 1</ref>, the full model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> incorrectly links the mention "Milwaukee" to the entity MILWAUKEE BREWERS. However, the prepo-sition "In" is a strong cue predictive of the type (location) of mention "Milwaukee" which is helpful for disambiguation. The reason why the local context model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> couldn't capture such apparent cue is two folds. On one hand, the context encoding module adopts a bag-of-words encoding scheme which is position agnostic. As shown in <ref type="figure">Fig. 1(b)</ref>, the attention mechanism is helpful for selecting predictive words (e.g. "Milwaukee", "games" etc.), but does not capture the pattern that the previous word "In" of the mention "Milwaukee" which very likely refers to an entity with location type. On the other hand, the pre-trained entity embedding of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> is not very sensitive to entity types. For example, as shown in <ref type="table" target="#tab_13">Table 8</ref>, when we query the most similar entities with the entity STEVE JOBS, the top one returned entity is APPLE INC., which is a different type but releated at topic level. So it is natural for the model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> to make type errors when it is trained to fit such entity embeddings.</p><p>As is argued in <ref type="bibr" target="#b32">(Zhou et al. 2018)</ref>, "context consistency is a strong proxy for type compatibility". Based on this claim, the mention's immediate context is a proxy of its type. For example, we consider the following context from Wikipedia linking to the entity APPLE in which the mention is replaced with the [MASK] token.</p><p>Fruits that tend to be more popular in this area are [MASK] , pears , and berries .</p><p>By reading the context surrounding the [MASK] token, we can easily determine that the entities fitting this context should be a kind of fruit.</p><p>In this paper, we propose to inject latent entity type information into the entity embeddings by modeling the immediate context surrounding the mention. Specifically, we apply pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> to represent the entity context and build a shared entity representation by aggregating all the entity contexts linking to the same entity via average pooling. Pre-trained BERT models naturally fit our purpose to represent the entity context surrounding the [MASK] token as it is trained with masked language model objective. What's more, we integrate a BERT-based entity similarity feature into the local model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> to better capture entity type information. This can leverage both the pre-trained entity embeddings from BERT and the domain adaption capability of BERT via fine-tuning.</p><p>We conduct entity linking experiments on standard benchmark datasets: AIDA-CoNLL and five out-domain test sets. Our model achieves an absolute improvement of 1.32% F1 on AIDA-CoNLL test set and average 0.80% F1 on five out-domain test sets over five different runs. In addition, we conduct detailed experiment analysis on AIDA-CoNLL development set which shows our proposed model can reduce 67.03% type errors of the state-of-the-art model <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref> and more than 90% of the remaining type error cases are due to over estimation of prior and global modeling problem which we leave as the further work.</p><p>Our contributions can be summarized as follows. ? We show current state-of-the-art (SOTA) neural entity linking models based on attention-based bag-of-words context model often produce type errors and analyze the possible causes. ? We propose a novel entity embedding method based on pre-trained BERT to better capture latent entity type information. ? We integrate a BERT-based entity similarity into the local model of a SOTA model <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref>. ? We verify the effectiveness of our model on standard benchmark datasets and achieve significant improvement over the baseline. And the detailed experiment analysis demonstrates that our method truly corrects most of the type errors produced by the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Entity Linking Problem</head><p>Formally, given a document D consisting of a list of entity mentions m 1 , ..., m n . The goal of an entity linking system is to assign each m i a KB entity e i or predict that no corresponding entity in the KB (i.e., e i =NIL). Due to potentially very large entity space (e.g. Wikipedia has more than 4 million entities), standard entity linking is often divided into two stages: candidate generation which chooses potential candidates C i = (e i1 , ..., e ili ) using a heuristic and entity disambiguation which learns to select the best entity from the candidates using a statistical model. In this work, we focus on the second stage entity disambiguation. As for entity disambiguation, two different kinds of information can be leveraged: local context compatibility and document-level global coherence which respectively corresponds to the local model and the global model. Next, we introduce the general formulation of entity linking problem with a focus on the well known DeepED model <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref>.</p><p>General Formulation An entity linking model integrating both local and global features can be formulated as a conditional random field. Formally, we can define a scoring function g to evaluate the entity assignment e 1 , ..., e n to mentions m 1 , ..., m n in a document D.</p><p>g(e 1 , ..., e n |D)</p><formula xml:id="formula_0">= n i=1 ?(e i |D) + j =i ?(e i , e j |D) (1)</formula><p>where the first term scores how well an entity fits its local context and the second one measures the global coherence.</p><p>Local Model Following <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, we instantiate the local model as an attention model based on pre-trained word and entity embeddings. Specifically, for each mention m i , a pruned candidate set C i = (e i1 , ..., e ili ) is identified in the candidate generation stage. We compute a local context score for each e ? C i based on the K-word (in practice, K is set to 100 and stop words are removed.) local context c = {w 1 , ..., w K } surrounding m i .</p><formula xml:id="formula_1">? long (e, c) = x e B h(c)<label>(2)</label></formula><p>where B is a learnable diagonal matrix, x e is the embeddings of entity e, h(c) applies a hard attention mechanism to context words in c to obtain the representation of the context.</p><p>Besides, <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> combined this context score with the priorp(e|m) (computed by mixing mention-entity hyperlink count statistics from Wikipedia, a large Web corpus and YAGO. 2 ) using a two-layer feedforward neural network in the local model.</p><p>?(e, m, c) = f (? long (e, c), logp(e|m))</p><p>(3)</p><p>Global Model The second term in Equation 1 is given by:</p><formula xml:id="formula_2">?(e, e ) = 2 n ? 1 x e C x e<label>(4)</label></formula><p>where C is a diagonal matrix. The model defined by Equation 1 is a fully-connected pairwise conditional random field. Exact maximum-a-posteriori inference on this CRF, needed both at training and testing phrase, is NP-hard <ref type="bibr">(Wainwright, Jordan, and others 2008)</ref>. So they used maxproduct loopy belief propagation (LBP) to estimate the maxmarginal probabilit?</p><formula xml:id="formula_3">g i (e|D) ? max e1,.</formula><p>..,ei?1 ei+1,...,en g(e 1 , ..., e n |D)</p><p>for each mention m i . The final score for m i is given by:</p><formula xml:id="formula_5">? i (e) = f (? i (e|D),p(e|m i ))<label>(6)</label></formula><p>where f is another two-layer neural network andp(e|m i ) is the prior feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our work focuses on improving entity linking by capturing latent entity type information with BERT. Specifically, our work related to previous approaches in three aspects.</p><p>Entity Embedding The entity linking task is essentially a zero-shot task where the answer of test cases may not exist in the training data. 3 So we need to build a shared entity embedding space for all entities which allows neural entity linking models to generalize to both seen and unseen entities during test time. Based on the distributional hypothesis <ref type="bibr" target="#b11">(Harris 1954)</ref>, an entity is characterized by its contexts. Different methods to characterize an entity's context result in different information its entity embedding can capture. Previous work <ref type="bibr" target="#b30">(Yamada et al. 2016</ref>; Ganea and Hofmann 2017) on learning entity representation are mostly extensions of the embedding methods proposed by <ref type="bibr" target="#b23">(Mikolov et al. 2013</ref>). An entity's context is a bag-ofwords representation which mainly captures topic level entity relatedness rather than entity type relatedness. In contrast, we propose a simple method to build entity embeddings directly from pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> which can better capture entity type information.</p><p>Type Information Previous work attempt to integrate type information into the entity linking task mostly by jointly modeling named entity recognition and entity linking. Specifically, a line of work <ref type="bibr" target="#b4">(Durrett and Klein 2014;</ref><ref type="bibr" target="#b21">Luo et al. 2015;</ref><ref type="bibr" target="#b25">Nguyen, Theobald, and Weikum 2016)</ref> jointly model entity linking and named entity recognition to capture the mutual dependency between them using structured CRF. These methods mainly differ in the design of hand-engineered features. Recently, Martins, Marinho, and Martins (2019) perform multi-task learning using learned features by extending Stack-LSTM <ref type="bibr" target="#b5">(Dyer et al. 2015)</ref>. However, all of these work rely on extensive annotation of the type of mentions which are difficult to obtain on most of the entity linking datasets. In contrast, based on the assumption that "context consistency is a strong proxy for type compatibility" from <ref type="bibr" target="#b32">Zhou et al. (2018)</ref>, we propose to model a mention's immediate context using BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> to capture its contextual latent entity type information.</p><p>Applications of BERT Since the advent of the wellknown BERT models <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref>, it has been applied successfully to and has achieved state-of-the-art performance on many NLP tasks. The main challenges which the entity linking task has over other tasks e.g. sentence classification, named entity recognition, where BERT has been applied are: (1) a very large label space, i.e. every mention has many target entities and (2) the zero-shot nature of the entity linking task. Training label embeddings from a small labeled dataset could not generalize to cover unseen entities in test time. To tackle this problem, we introduce a novel method to build entity embeddings from BERT by modeling the immediate context of an entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Our model consists of two phrases: (1) Build entity embeddings from BERT (2) Add a BERT-based entity similarity component to the local model. Next we will describe each phrase in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Embeddings from BERT</head><p>Given lists of mention context 4 {c i1 , c i2 , ..., c iN } in Wikipedia for every entity e i ? E, we build the entity embeddings map B : E ? R d . Here, the anchor context</p><formula xml:id="formula_6">c ij = (lctx ij , m ij , rctx ij )</formula><p>where m ij is the mention, lctx ij is the left context and rctx ij is the right context.</p><p>Context Representation A mention's immediate context is a proxy for its type. Here, the mention's immediate context is a sequence of tokens where the mention m ij is replaced with a single [MASK] token. Then, we represent the immediate entity context by extracting the upper most layer representation of pre-trained BERT <ref type="bibr" target="#b3">(Devlin et al. 2019)</ref> corresponding to the [MASK] token.</p><formula xml:id="formula_7">c ij = BERT({lctx ij , [MASK], rctx ij })<label>(7)</label></formula><p>Entity Representation For each entity e i ? E, we randomly sample at most N anchor contexts {c i1 , c i2 , ..., c iN } from Wikipedia. Then the entity representation of e i is computed by aggregating all the context representation {c i1 , c i2 , ..., c iN } via average pooling.</p><formula xml:id="formula_8">B ei = 1 N N j=1 c ij (8)</formula><p>As will be shown in the analysis section, the entity embeddings from BERT better capture entity type information than those from <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-based Entity Similarity</head><p>The local context model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> mainly captures the topic level entity relatedness information based on a long range bag-of-words context. To capture latent entity type information, we design a BERT-based entity similarity score ? BERT (e, c). Specifically, given a short range (the immediate context where the mention m lies) context c = (lctx, m, rctx), we firstly encode c using the same method defined by Equation <ref type="formula" target="#formula_7">7</ref>.</p><formula xml:id="formula_9">c = BERT({lctx, [MASK], rctx})<label>(9)</label></formula><p>Then we define the BERT-based entity similarity as the cosine similarity 5 between the context representation c and the entity representation B e .</p><formula xml:id="formula_10">? BERT (e, c) = cosine(B e , c)<label>(10)</label></formula><p>Finally, as for the local disambiguation model, we integrate the BERT-based entity similarity ? BERT (e, c) with the local context score ? long (e, c) (defined in Equation 2) and the priorp(e|m i ) with two fully connected layers of 100 hidden units and ReLU non-linearities following the same feature composition methods as <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>.</p><formula xml:id="formula_11">? local (e, m, c) = f (? long (e, c), ? BERT (e, c), (11) logp(e|m))</formula><p>As for the global disambiguation model, we firstly define the local context score ? localctx (e, c) by combining ? long (e, c) and ? BERT (e, c). 6</p><formula xml:id="formula_12">? localctx (e, c) = f (? long (e, c), ? BERT (e, c))<label>(12)</label></formula><p>Then we adopt exactly the same global model as <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> which is already introduced in the Background section. Specifically, we adopt loopy belief propagation (LBP) to estimate the max-marginal probability? i (e|D) and then combine it with the priorp(e|m i ) using a two-layer neural network to get the final score ? i (e) for m i .</p><formula xml:id="formula_13">?(e, e ) = 2 n ? 1 x e C x e<label>(13)</label></formula><formula xml:id="formula_14">g i (e|D) ? max e1,...,ei?1 ei+1,...,en g(e 1 , ..., e n |D)<label>(14)</label></formula><formula xml:id="formula_15">? i (e) = f (? i (e|D),p(e|m i ))<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>We minimize the following max-margin ranking loss:</p><formula xml:id="formula_16">L(?) = D?D mi?D e?Ci h(m i , e) + ?||?|| 2 2 (16) h(m i , e) = max 0, ? ? s(e * i ) + s(e) s(e) = ? local (e, m, c) if local model only ? i (e) local &amp; global model</formula><p>In order to discourage the model from biasing toward a particular feature, we add a L2 regularization term (?||?|| 2 2 ) w.r.t parameters ? in feature composition function f to the loss function in <ref type="figure">Equation 16</ref>, where ? is set 10 ?7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>In order to verify the effectiveness of our model, we conduct experiments on standard benchmark datasets considering both in-domain and out-domain settings. For in-domain setting, we use AIDA-CoNLL dataset <ref type="bibr" target="#b12">(Hoffart et al. 2011)</ref> for training, validation and testing. For out-domain setting, we evaluate the model trained with AIDA-CoNLL on five popular out-domain test sets: MSNBC, AQUAINT, ACE 2004 datasets cleaned and updated by <ref type="bibr" target="#b10">Guo and Barbosa (2016)</ref> and WNED-CWEB (CWEB), WNED-WIKI (WIKI) automatically extracted from ClueWeb and Wikipedia <ref type="bibr" target="#b10">(Guo and Barbosa 2016)</ref>. Following previous work (Ganea and Hofmann 2017), we only consider in-KB mentions. Besides, our candidate generation strategy follows that of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> to make our results comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>The main goal of this work is to introduce a BERT-based entity similarity to capture latent entity type information which is supplementary to existing SOTA local context model <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref>. So we evaluate the performance when integrating the BERT-based entity similarity into the local context model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. We also evaluate our model with or without global modeling method of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. In addition, we further compare our methods with other state-of-theart models <ref type="bibr" target="#b30">(Yamada et al. 2016;</ref><ref type="bibr" target="#b17">Le and Titov 2018)</ref>. To verify the contribution of our proposed BERT-based entity embeddings, we also compare with a straightforward baseline which directly replaces the encoder of Ganea and Hofmann (2017) utilizing pre-trained BERT. To do so, we introduce a 768 ? 300 dimensional matrix W which projects BERT-based context representation c into Ganea and Hofmann (2017)'s entity embeddings space when calculating the similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter Setting</head><p>The resources (word and entity embeddings) used to train the local context model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> are obtained from DeepED 7 . For each entity, we randomly sam-7 https://github.com/dalab/deep-ed/ Methods AIDA-B Local models priorp(e|m) 71.9 Lazic et al. <ref type="bibr">(2015)</ref> 86.4 <ref type="bibr" target="#b9">Globerson et al. (2016)</ref> 87.9 <ref type="bibr" target="#b30">Yamada et al. (2016)</ref> 87.2 <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> 88.8 Ganea and Hofmann (2017) (reproduce) 88.75 ? 0.30 BERT-Entity-Sim (local)</p><p>90.06 ? 0.22 Local &amp; Global models <ref type="bibr" target="#b14">Huang, Heck, and Ji (2015)</ref> 86.6 <ref type="bibr" target="#b7">Ganea et al. (2016)</ref> 87.6 <ref type="bibr" target="#b1">Chisholm and Hachey (2015)</ref> 88.7 <ref type="bibr" target="#b10">Guo and Barbosa (2016)</ref> 89.0 <ref type="bibr" target="#b9">Globerson et al. (2016)</ref> 91.0 <ref type="bibr" target="#b30">Yamada et al. (2016)</ref> 91.5 <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> 92.22 ? 0.14 <ref type="bibr" target="#b17">Le and Titov (2018)</ref> 93 <ref type="formula">.</ref>  ple at most 100 anchor contexts from Wikipedia 8 to build the entity representation from BERT. We discard any articles appearing in WIKI dataset when building the entity representation from BERT. We take the anchor context as the surrounding sentence where the mention lies and replace the mention with a single [MASK] token. Each context is truncated to 128 tokens after WordPiece tokenization. We use the PyTorch implementation of pre-trained BERT models 9 and choose the BERT-base-cased version. We adopt the Adam (Kingma and Ba 2014) implemented by BERT with ? 1 = 0.9, ? 2 = 0.999. Empirically, we found that it is helpful to set parameters in BERT a small initial learning rate and not BERT related parameters a larger initial learning rate to avoid the whole model biasing toward the BERT feature and disregarding other model components. In our experiments, pre-trained BERT model is fine-tuned with initial learning rate 10 ?5 whereas not BERT related parameters are trained with 10 ?3 . Similar learning rate usage can be found in the recent work by <ref type="bibr" target="#b15">(Hwang et al. 2019</ref>). Similar to <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, all the entity embeddings are fixed during fine-tuning. We randomly initialize the not BERT related parameters using Gaussian distribution N (0.0, 0.02) and the bias term is zeroed. Note that all the hyper-parameters used in the local context and global model of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> were set to the same values as theirs for direct comparison purpose. Detailed hyper-parameters setting is described in the appendices. Our model is trained with 4 NVIDIA Tesla P100 GPUs. We run each of our model five times with different random seeds, and the performance is reported in the form of average ? standard deviation. <ref type="table" target="#tab_2">Table 1</ref> shows the micro F1 scores on in-domain AIDA-B dataset of the SOTA methods and ours, which all use Wikipedia and YAGO mention-entity index. The models are divided into two groups: local models and local &amp; global models. As we can see, our proposed model, BERT-Entity-Sim, outperforms all previous methods. Our local model achieves a 1.31 improvement in terms of F1 over its corresponding baseline <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref>, yielding a very competitive local model with an average 90.06 F1 score even surpassing the performance of four local &amp; global models. Equipped with the global modeling method of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, the performance of our model further increase to 93.54 with an average 1.32 improvement in terms of F1 over <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. In addition, our method outperforms <ref type="bibr" target="#b17">Le and Titov (2018)</ref> model by 0.47 point. The model of <ref type="bibr" target="#b17">Le and Titov (2018)</ref> is a multirelational extension of Ganea and Hofmann (2017)'s global modeling method while keeps exactly the same local context model. Our better local context model should be orthogonal with them and has potential more applications on short texts (e.g. tweets) where global modeling has little benefits. Moreover, BERT+G&amp;Hs embeddings performs significantly worse than the baseline (Ganea and Hofmann 2017) and our proposed BERT-Entity-Sim model. The reason is that BERT-based context representation space and Ganea and Hofmanns entity embeddings space are heterogeneous. Ganea and Hofmanns entity embeddings are bootstrapped from word embeddings which mainly capture topic level entity relatedness, while BERT-based context representation is derived from BERT which naturally captures type information. The non-parallel information in both context and entity sides makes it difficult to learn the alignment parameter W and results in the poor generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>To evaluate the robustness of our model, <ref type="table" target="#tab_5">Table 2</ref> shows the performance of our method and SOTA methods on five out-domain test sets. On average, our proposed model (BERT-Entity-Sim) outperforms the local &amp; global version of Ganea and Hofmann; <ref type="bibr">Le and Titov (2017;</ref> by an average 0.80 and 0.51 on F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We conduct experiment analysis to answer the following questions:</p><p>? Do the entity embeddings from BERT better capture latent entity type information than that of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>?</p><p>? Does the proposed model correct the type errors in the baseline (Ganea and Hofmann 2017)?</p><p>? Can straightforward integration of state-of-the-art fine grained entity typing systems improve entity linking performance?</p><p>? Can better global model further boost the performance of the proposed model?</p><p>Effectiveness of BERT-based and <ref type="bibr" target="#b6">Ganea &amp; Hofmann (2017)</ref>   <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> 93.7 ? 0.1 88.5 ? 0.4 88.5 ? 0.3 77.9 ? 0.1 77.5 ? 0.1 85.22 <ref type="bibr" target="#b17">Le and Titov (2018)</ref> 93.9 ? 0.2 88.3 ? 0.6 89.9 ? 0.8 77.     <ref type="table">Table 5</ref>: Performance of two state-of-the-art fine grained entity typing systems on AIDA-CoNLL development set order to verify our claim that the entity embeddings from BERT better capture entity type information than those from <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, we carry out an entity type prediction task based on its entity embedding. Specifically, we randomly sample 100K entities from Wikipedia, and randomly split them into training set (80K), development set (10K) and test set (10K). For each entity, we obtain its entity types from three typing systems: FIGER (Ling and Weld 2012), BBN <ref type="bibr" target="#b29">(Weischedel and Brunstein 2005)</ref> and OntoNotes fine <ref type="bibr" target="#b8">(Gillick et al. 2014</ref>) via the entity type mapping provided by <ref type="bibr" target="#b32">Zhou et al. (2018)</ref>. The entity type prediction model is a simple linear classification model 10 using the entity embedding of an entity as features; limiting its capacity enables us to focus on whether type information can be easily extracted from the entity embeddings. We evaluate the model using standard entity typing metrics: Strict Accuracy (Acc.), Micro F1 (F 1 mi ) and Macro F1 (F 1 ma ).</p><p>As shown in <ref type="table" target="#tab_6">Table 3</ref>, our proposed entity embedding from BERT significantly outperforms the entity embedding proposed by <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> on three typing sys-tems FIGER, BBN and OntoNotes fine . Specifically, our method improves over the baseline with an absolute 8.31, 10.43 and 9.33 F 1 mi point than the baseline on three typing systems respectively. This demonstrates that our proposed entity embeddings from BERT indeed capture better latent entity type information than <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>.</p><p>Type Errors Correction As we have mentioned in the introduction section, more than half of the baseline model's errors on the AIDA-A dataset are type errors. Type errors are error cases 11 where (1) the predicted entity's type is different from the golden entity's type; (2) contextual cue predictive of the type of the mention exists; (3) errors are not due to annotation errors. By doing so, we collect 185 type error cases which cover 57.45% of all (322) error cases. This indicates that <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> produces many type errors due to its inability to consider the entity type information in mention context. By integrating the BERTbased entity similarity, our proposed model can correct 124 out of 185 (67.03%) type error cases of the baseline model which demonstrates that we correct more than two third of the type errors produced by the baseline. We have further examined and categorized the remaining 61 type error cases into three categories: (i) Due to prior: golden entities with very lowp(e|m i ) prior, (ii) Due to global: both the local context score and prior score support predicting the golden entity, but the overall score supports predicting incorrect entity due to global modeling, (iii) Due to local context: the local context score misleads the model predicting the wrong entity, this is potentially due to the mention context can be misleading, e.g. a document discussing cricket will favor resolving the mention "Australian" in context "impressed by the positive influence of Australian coach Dave Gilbert" to Methods AIDA-B MSNBC AQUAINT ACE2004 CWEB WIKI Avg <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> 92.22 ? 0.14 93.7 ? 0.1 88.5 ? 0.4 88.5 ? 0.3 77.9 ? 0.1 77.5 ? 0.1 85.22 <ref type="bibr" target="#b17">Le and Titov (2018)</ref> 93.07 ? 0.27 93.9 ? 0.2 88.3 ? 0.6 89.9 ? 0.8 77.5 ? 0.1 78.0 ? 0.1 85.51 <ref type="bibr" target="#b31">Yang et al. (2019)</ref> 94 <ref type="formula">.</ref>  the entity AUSTRALIA NATIONAL CRICKET TEAM instead of the gold entity AUSTRALIA. As shown in <ref type="table" target="#tab_7">Table 4</ref>, 67.21% of the remaining type error cases are due to prior problem which are hard to solve in the current feature combination framework. We argue that prior should be considered as the final resort, only relying on it when the model can not make decision based on other features. Besides, there are 22.95% remaining type errors which are due to global modeling problem which shows the limitation of the global modeling method of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. Finally, 9.84% type error cases are due to local context problem that our BERT-based solution cannot address. We leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Explicit Entity Types</head><p>We have shown that our BERT-based local context model which implicitly captures entity type information and is effective in correcting two third of type error cases. It is nature to conjecture that we can also correct type errors by incorporating explicit type information into <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>. We investigate this approach in this section. Assuming that we have types for each mention and candidate entity, we calculate the Jaccard similarity between them and use it as a feature for local disambiguation model.</p><formula xml:id="formula_17">JaccardSim(e, m, c) = |T m ? T e | |T m ? T e |<label>(17)</label></formula><p>where T m and T e are the type sets of the mention m and candidate entity e respectively. The new local context score function considering explicit type information is defined as:</p><p>? localctx (e, m, c) = f ? long (e, c), JaccardSim(e, m, c)</p><p>We consider both Oracle setting and Predict setting. In the oracle setting, the mention's types are set as the golden entity's types. <ref type="bibr">12</ref> As for the entity's types, we use two sources: one is the ultra-fine type sets from <ref type="bibr" target="#b2">Choi et al. (2018)</ref> consisting of more than 10,000 ultra-fine grained types; the other one is the FIGER type sets <ref type="bibr" target="#b20">(Ling and Weld 2012)</ref> consisting of 112 fine grained types. In the predict setting, we use two state-of-the-art fine grained entity typing systems: 1) Ultrafine <ref type="bibr" target="#b2">(Choi et al. 2018)</ref> which predicts types in ultra-fine type sets; 2) ZOE <ref type="bibr" target="#b32">(Zhou et al. 2018</ref>) which can predict types in FIGER type sets. As we can see from both <ref type="table" target="#tab_2">Table 1 and Table 2</ref>, in the oracle setting, the best model outperforms all the state-of-theart entity linking models by a large margin, even surpass <ref type="bibr" target="#b17">Le and Titov (2018)</ref> by 3.28 F1 points on AIDA-CoNLL test set. This result shows that a better type prediction system can further improve upon the state-of-the-state entity linking systems. However, in the predict setting, the type injection models have worse performance than the baseline. The degradation might be attributed to the poor performance of the two state-of-the-art fine grained entity typing systems. To verify this, we measure the performance of the two typing systems on AIDA-CoNLL development set. 13 As shown in <ref type="table">Table 5</ref>, the ultra-fine entity typing system <ref type="bibr" target="#b2">(Choi et al. 2018)</ref> only achieves 26.52% F 1 mi score while the ZOE system <ref type="bibr" target="#b32">(Zhou et al. 2018</ref>) achieves 66.12% F 1 mi score 14 which are insufficient to improve state-of-the-art entity linking system with more than 92% F1 score.</p><p>Better Global Model In order to investigate whether better global model can further boost the performance of our model, we incorporate the recent proposed Dynamic Context Augmentation (DCA) 15 <ref type="bibr" target="#b31">(Yang et al. 2019)</ref>. DCA is a global entity linking model featuring better efficiency and effectiveness than that of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> by breaking the "all-mention coherence" assumption. Compared to BERT-Entity-Sim equipped with Ganea and Hofmann (2017)'s global model in <ref type="table" target="#tab_13">Table 8</ref>  <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>). We found that <ref type="bibr" target="#b31">Yang et al. (2019)</ref> includes an explicit type similarity which is based on a typing system 16 trained with AIDA-train NER annotation. This explicit type similarity feature is tailored for AIDA-CoNLL data set and doesn't achieve good generalization performance on out-domain test sets. In contrast, our BERT-Entity-Sim model capturing latent type information has potential better generalization performance with an average 2.10 F1 improvement over them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>We demonstrate the effectiveness of our proposed model by retrieving the nearest neighbours in both the context representation space and entity representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Contexts</head><p>We follow <ref type="bibr">Papernot and Mc-Daniel (2018)</ref>     <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> and BERT based entity representation space neighbour in the context representation space. For our model, we use the context representation c defined by Equation <ref type="formula" target="#formula_9">9</ref>. For the baseline model, we use the attentionbased context representation h(c) defined in Equation 2. This can reveal which training instances support the prediction of a model. As shown in <ref type="table" target="#tab_12">Table 7</ref>, for the example in <ref type="figure">Figure 1</ref>, the most similar contexts retrieved by our model's context representation are all with preposition "In" ahead of the mention and the golden entities of them are all American cities. In contrast, the baseline's local context is a bag-of-words representation which we denote using the top 10 attended contextual words sorted by attention weights. The most similar contexts retrieved by baseline's context representation share common words like "games", "victory" and the golden entities of them are all baseball teams. This explains why the baseline model incorrectly links the mention "Milwaukee" to MILWAUKEE BREWERS while our model can link to the correct entity MILWAUKEE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Entities</head><p>We also retrieve nearest entities in the embedding space of <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> and ours. As we can see, we query STEVE JOBS, the nearest entity in <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> is APPLE INC. which is a different type. In contrast, all the entities retrieved by our approach share the same types like person, entrepreneur etc. Another example is when we query NA-TIONAL BASKETBALL ASSOCIATION, the most similar entities in <ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> are NBA teams which are topically related, while the entities retrieved by our approach are all basketball leagues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose to improve entity linking by capturing latent entity type information with BERT. Firstly, we build entity embeddings from BERT by averaging all the context representation extracted from pre-trained BERT.</p><p>Then we integrate a BERT-based entity similarity into the local model of the state-of-the-art method by <ref type="bibr" target="#b6">(Ganea and Hofmann 2017)</ref>. The experiment results show that our model significantly outperforms the baseline with an absolute improvement of 1.32% F1 on in-domain AIDA-CoNLL test set and average 0.80% F1 on five out-domain test datasets. The detailed experiment analysis shows that our method corrects most of the type errors produced by the baseline. In the future, we would like to design global modeling methods which can take advantage of the BERT architecture and investigate other ways to use the prior feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices Classification Model of Entity Prediction Task</head><p>Given an entity e, we firstly retrieve its entity embedding e, then compute the probability for each type in the typeset T :</p><formula xml:id="formula_18">p e j = ?(w j e + b)<label>(18)</label></formula><p>where ? is the sigmoid function, w j and b are respectively the weight and bias parameter. For each entity e, it is labeled with t e , a binary vector of all types where t e j = 1 if the j th type is in the set of gold types of e and 0 otherwise. We optimize a multi-label binary cross entropy objective:</p><p>L type = ? j t e j log p e j + (1 ? t e j ) log(1 ? p e j )</p><p>We optimize the model with Adam with an initial learning rate of 1e-3. Each model is trained for up to 200 epoches and training stops when the performance on the development set does not improve for 6 consecutive epoches.</p><p>Detailed Hyper-parameters Setting (  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>F1 scores on AIDA-B (test set).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>F1 scores on five out-domain test sets. Underlined scores denote the corresponding model outperforms the baseline.</figDesc><table><row><cell></cell><cell></cell><cell>FIGER</cell><cell></cell><cell></cell><cell>BBN</cell><cell></cell><cell cols="2">OntoNotesfine</cell><cell></cell></row><row><cell>Entity Embedding</cell><cell>F 1mi</cell><cell>F 1ma</cell><cell>Acc.</cell><cell>F 1mi</cell><cell>F 1ma</cell><cell>Acc.</cell><cell>F 1mi</cell><cell>F 1ma</cell><cell>Acc.</cell></row><row><cell>Ganea and Hofmann (2017)</cell><cell cols="9">80.38 82.65 53.30 80.87 84.10 69.34 81.41 83.54 57.54</cell></row><row><cell cols="10">BERT based Entity Embedding 88.69 90.98 69.07 91.30 93.35 85.36 90.74 92.52 73.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results of type classification task on three typing systems: FIGER, BBN, OntoNotes fine</figDesc><table><row><cell>Error Type</cell><cell cols="2"># Cases Percentage (%)</cell></row><row><cell>Due to prior</cell><cell>41</cell><cell>67.21</cell></row><row><cell>Due to global</cell><cell>14</cell><cell>22.95</cell></row><row><cell>Due to local context</cell><cell>6</cell><cell>9.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Remaining type error cases categorization</figDesc><table><row><cell>Typing System</cell><cell>F 1mi</cell><cell>F 1ma</cell><cell>Acc.</cell></row><row><cell cols="3">Choi et al. (2018) 26.52% 26.60%</cell><cell>0.36%</cell></row><row><cell cols="4">Zhou et al. (2018) 66.12% 67.98% 46.08%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>? 0.3 90.8 ? 0.4 78.2 ? 0.2 81.0 ? 0.3 86.72</figDesc><table><row><cell></cell><cell>64 ? 0.20</cell><cell cols="2">94.6 ? 0.2 87.4 ? 0.5</cell><cell>89.4 ? 0.4</cell><cell>73.5 ? 0.1</cell><cell>78.2 ? 0.1</cell><cell>84.62</cell></row><row><cell>BERT-Entity-Sim (local &amp; global)</cell><cell>93.54 ? 0.12</cell><cell cols="3">93.4 ? 0.1 89.8 ? 0.4 88.9 ? 0.7</cell><cell>77.9 ? 0.4</cell><cell>80.1 ? 0.4</cell><cell>86.02</cell></row><row><cell>BERT-Entity-Sim (local &amp; DCA global)</cell><cell>93.66 ? 0.17</cell><cell>94.5 ? 0.3</cell><cell>89.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>F1 scores of BERT-Entity-Sim equipped with the DCA global model<ref type="bibr" target="#b31">(Yang et al. 2019</ref>) on six test sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>to retrieve training examples using nearest In Cleveland , Kevin Seitzer 's two-out single ... 0.968 CLEVELAND (2) In Boston , Troy O'Leary homered off the right-field ... 0.956 BOSTON (3) In Houston , Jeff Bagwell homered and Donne Wall ... 0.951 HOUSTON (4) In Los Angeles , Greg Gagne had a run-scoring single ... 0.949 LOS ANGELES (5) In Houston , Andy Benes allowed two runs over seven innings ... 0.947 HOUSTON Baseline ninth ninth seventh seventh Milwaukee Milwaukee inning games games victory ... -MILWAUKEE (1) eighth ninth Milwaukee inning league runs victory fourth rallied earned ...</figDesc><table><row><cell>Model</cell><cell>Context</cell><cell>Context Sim</cell><cell>Golden Entity</cell></row><row><cell></cell><cell>In Milwaukee , Marc Newfield homered off Jose Parra ( 5-4 ) ...</cell><cell>-</cell><cell>MILWAUKEE</cell></row><row><cell>Our</cell><cell cols="2">(1) 0.941</cell><cell>MILWAUKEE BREWERS</cell></row><row><cell></cell><cell>(2) Denny runs fifth fifth allowed game fourth San win win ...</cell><cell>0.940</cell><cell>PHILADELPHIA PHILLIES</cell></row><row><cell></cell><cell>(3) Royals league games runs game won won win Minnesota straight ...</cell><cell>0.938</cell><cell>MINNESOTA TWINS</cell></row><row><cell></cell><cell>(4) Cleveland fifth games sixth inning innings Friday extra Sox month ...</cell><cell>0.934</cell><cell>CLEVELAND INDIANS</cell></row><row><cell></cell><cell>(5) games streak stay Reynoso runs runs second run won fourth ...</cell><cell>0.926</cell><cell>MIAMI MARLINS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Nearest contexts for the example in Fig. 1 in BERT's and baseline's context representation space</figDesc><table><row><cell>Model</cell><cell>STEVE JOBS</cell><cell>NATIONAL BASKETBALL ASSOCIATION</cell><cell>BEIJING</cell></row><row><cell></cell><cell>APPLE INC.</cell><cell>SACRAMENTO KINGS</cell><cell>SEOUL</cell></row><row><cell>Ganea and Hofmann (2017)</cell><cell cols="2">STEVE WOZNIAK GOLDEN STATE WARRIORS</cell><cell>SHANGHAI</cell></row><row><cell></cell><cell>BILL GATES</cell><cell>LOS ANGELES CLIPPERS</cell><cell>CHINA</cell></row><row><cell></cell><cell cols="2">STEVE WOZNIAK AMERICAN BASKETBALL ASSOCIATION</cell><cell>GUANGZHOU</cell></row><row><cell>BERT based Entity Embedding</cell><cell>BILL GATES</cell><cell>WOMEN'S NATIONAL BASKETBALL ASSOCIATION</cell><cell>SHANGHAI</cell></row><row><cell></cell><cell cols="3">STEVE BALLMER NATIONAL BASKETBALL LEAGUE (UNITED STATES) NANJING</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Examples of nearest entities in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 )</head><label>9</label><figDesc></figDesc><table><row><cell>Hyper-parameters</cell><cell>Value</cell></row><row><cell>BERT-based entity embedding dims</cell><cell>768</cell></row><row><cell>dumping factor</cell><cell>0.5</cell></row><row><cell>number of LBP loops</cell><cell>10</cell></row><row><cell>batch size</cell><cell>1 document</cell></row><row><cell></cell><cell>(? 64 mentions)</cell></row><row><cell>? (margin)</cell><cell>0.01</cell></row><row><cell>epoch (local model)</cell><cell>2</cell></row><row><cell>epoch (local &amp; global model)</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Values of hyper-parameters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/dalab/deep-ed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See<ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> for more details.3  Only 58.6% answers of test cases in AIDA-CoNLL dataset are existent in the training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A mention context of an entity is the surrounding text of an anchor text, i.e. mention, pointing to the entity page in Wikipedia.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We also investigated calculating the similarity using a parameterized formula by adding a diagonal matrix between them, but found no significant improvements over Eq. 10.6  Following<ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref>, we did not integrate the prior scorep(e|m) into the local scoring module of the global disambiguation model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Using the same Wikipedia dump (Feb 2014) as the one which<ref type="bibr" target="#b6">Ganea and Hofmann (2017)</ref> used to train their entity embeddings. 9 https://github.com/huggingface/pytorch-pretrained-BERT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Due to space limitation, we put the detailed description of this model and training hyper-parameters in the Appendices.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We discard the error cases due to candidate generation problem (i.e., gold entities that do not appear in mentions' candidate list) which cover 2.98% mentions of AIDA-A dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">In practice, this setting is unachievable due to potentially insufficient context and the imperfect entity typing system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">The mention's golden types are set as its linked entity's types. 14 The size of type sets of ultra-fine typing system is much larger than that of ZOE. 15 https://github.com/YoungXiyuan/DCA/ 16 It yields 95% accuracy on AIDA-A according to their paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partly funded by National Key Research and Development Program of China via grant 2018YFC0806800 and 2018YFC0832105.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relational inference for wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Entity disambiguation with web links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chisholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hachey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic bag-of-hyperlinks model for entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Context-dependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collective entity resolution with multi-focal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ringaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust named entity disambiguation with random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07678</idno>
		<title level="m">Leveraging deep neural networks and knowledge graphs for entity disambiguation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01069</idno>
		<title level="m">A comprehensive exploration on wikisql with table-aware word contextualization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Plato: A selective context model for entity resolution. TACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving entity linking by modeling latent relations between mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosting entity linking performance by leveraging unlabeled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distant learning for entity linking with automatic noise detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint learning of named entity recognition and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Student Research Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to link with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">J-nerd: joint named entity recognition and disambiguation with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="215" to="229" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04765</idno>
		<title level="m">Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Learning dynamic context augmentation for global entity linking</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zero-shot open entity typing as type-compatible grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
