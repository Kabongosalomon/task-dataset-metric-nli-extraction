<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification-Regression for Chart Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Levy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OriginAI</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classification-Regression for Chart Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Chart Question Answering, Multimodal Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chart question answering (CQA) is a task used for assessing chart comprehension, which is fundamentally different from understanding natural images. CQA requires analyzing the relationships between the textual and the visual components of a chart, in order to answer general questions or infer numerical values. Most existing CQA datasets and models are based on simplifying assumptions that often enable surpassing human performance. In this work, we address this outcome and propose a new model that jointly learns classification and regression. Our language-vision setup uses co-attention transformers to capture the complex real-world interactions between the question and the textual elements. We validate our design with extensive experiments on the realistic PlotQA dataset, outperforming previous approaches by a large margin, while showing competitive performance on FigureQA. Our model is particularly well suited for realistic questions with out-of-vocabulary answers that require regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Figures and charts play a major role in modern communication, help to convey messages by curating data into an easily comprehensible visual form, highlighting the trends and outliers. However, despite tremendous practical importance, chart comprehension has received little attention in the computer vision community. Documents ubiquitously contain a variety of plots. Using computer vision to parse these visualizations can enable extraction of information that cannot be gleaned solely from a document's text. Recently, with the rise of multimodal learning methods, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">21,</ref><ref type="bibr">23,</ref><ref type="bibr">25,</ref><ref type="bibr">26,</ref><ref type="bibr">30]</ref>, interest in chart understanding has increased <ref type="bibr">[5, 13-15, 20, 27]</ref>.</p><p>Studies on figure understanding (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>), commonly involve answering questions, a task known as Chart Question Answering (CQA). This task is closely related to Visual Question Answering (VQA), which is usually applied on natural ? Part of this research was conducted at IBM Research AI, Israel. Year 0.000e+0 5.000e+6</p><p>2.000e+7 2.500e+7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.000e+7</head><p>Aid flow (current US$) <ref type="bibr">1973 1974 1975 1976 1978 1979 1980 1981 1977</ref>  <ref type="figure">Fig. 1</ref>: Interactions marked on a sample from the PlotQA dataset <ref type="bibr" target="#b19">[20]</ref>, alongside with our CRCT prediction. We highlight the interacting parts/tokens with matching colors. Note the complexity of attention between the different modalities needed to correctly answer the question. The result predicted by CRCT and the ground truth answer are indicated by green and purple arrows.</p><p>images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">26,</ref><ref type="bibr">30]</ref>. VQA is typically treated as a classification task, where the answer is a category, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">30]</ref>. In contrast, answering questions about charts often requires regression. Furthermore, a small local change in a natural image typically has limited effect on the visual recognition outcome, while in a chart, the impact might be extensive. Previous works have demonstrated that standard VQA methods perform poorly on CQA benchmarks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. A chart comprehension model must consider the interactions between the question and the various chart elements in order to provide correct answers. The complexity of such interactions is demonstrated in <ref type="figure">Fig. 1</ref>. For example, failing to correctly associate a line with the correct legend text would yield an erroneous answer.</p><p>Several previous CQA studies suggest a new dataset along with a new processing model, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. CQA datasets differ in several ways: (1) type and diversity of figures, <ref type="bibr" target="#b1">(2)</ref> type and diversity of questions, (3) types of answers (e.g., discrete or continuous). While previous methods have recently reached a saturation level on some datasets, e.g., 94.9% on FigureQA <ref type="bibr" target="#b14">[15]</ref>, 92.2% on LEAF-QA++ <ref type="bibr">[27]</ref>, and 97.5% on DVQA <ref type="bibr" target="#b12">[13]</ref>, Methani et al . <ref type="bibr" target="#b19">[20]</ref> attribute this to the limitations of these datasets. Hence, they propose a new dataset (PlotQA-D), which is the largest and the most diverse dataset to date, with an order of magnitude more images/figures and ?4, 000 different answers. PlotQA-D further contains more challenging and realistic reasoning and data retrieval tasks, with a new model (PlotQA-M) achieving 22.5% accuracy on this dataset, while human performance reached 80.47% <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this paper we further explore the cause behind the saturation of various methods on previous data sets. We argue that similarly to early stages of VQA <ref type="bibr" target="#b7">[8]</ref>, several common datasets and benchmarks suffer from bias, oversimplicity and classification oriented Q&amp;A, allowing some methods to surpass human performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">27]</ref>. Next, we introduce a novel method called Classification -Regression Chart Transformer (CRCT) for CQA. We start with parsing the chart with a detector that extracts all of its textual and visual elements, which are then passed, along with the question text, to a dual branch transformer for bimodal learning. Our model features the following novelties: 1) In contrast to previous methods that encode only the question, our language model jointly processes all textual elements in the chart, allowing inter and intra relations between all textual and visual elements. 2) We show high generalization by dropping the common 'string matching' practice (replacing question tokens with certain textual chart elements), and accommodating a co-transformer with pretrained BERT <ref type="bibr" target="#b6">[7]</ref>. 3) We introduce a new chart element representation learning, fusing multiple inputs from different domains. 4) Finally, a new hybrid prediction head is suggested, allowing unification of classification and regression into a single model. By jointly optimizing our model end-to-end for all types of questions, we further leverage the multi-task learning regime <ref type="bibr">[31]</ref>. We test our model on the challenging and more realistic dataset of PlotQA-D, as well as on FigureQA. Our results show that CRCT outperforms the previous method by a large margin on PlotQA-D (76.94% vs. 53.96% total accuracy), capable of matching previous results with 10% of the training data. We further analyze our model via explainability visualizations, revealing its limitations as well as strong capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review existing CQA models, while focusing on the datasets in Sec. 3. In particular, we find that previous methods are often over-fitted to the type of datasets and corresponding questions/answers (Q&amp;A).</p><p>Some CQA methods take the entire chart image as input to the model <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, while others first parse the image to extract visual elements using a detector <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">27</ref>]. An example of chart elements and their corresponding class name, obtained from a detector, are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The pioneering model of Kahou et al . <ref type="bibr" target="#b14">[15]</ref> outputs binary (Yes/No) answers using a backbone pretrained on ImageNet fed into a Relation Network (RN) <ref type="bibr">[24]</ref>, in parallel to an LSTM <ref type="bibr" target="#b10">[11]</ref> used for question encoding. Removing the strong limitation to binary Q&amp;A, Kafle et al . <ref type="bibr" target="#b12">[13]</ref> proposed a new dataset (DVQA) and a model referred to as SANDY. The dataset introduces new question types with out-of-vocabulary (OOV) answers. These answers are chart specific (e.g., Which item sold the most units in any store?) and do not necessarily appear in the training set. The SANDY model is a classification network (SAN [30]) with DYnamic encoding. In their approach, each text element in the chart is associated with a unique token in a dynamic encoding dictionary, based on the text location. These elements are then added to the dynamic list of answer classes. Kafle et al . <ref type="bibr" target="#b13">[14]</ref> later introduced PReFIL, another detector-free model with two branches: a visual branch based on DenseNet <ref type="bibr" target="#b11">[12]</ref>, and a text branch based on LSTM to encode the question. For bimodal fusion, they apply a series of 1?1 convolutions on concatenated visual and question features.</p><p>Singh and Shekar [27] introduced STL-CQA, a new detector-based approach, combining transformers followed by co-transformers <ref type="bibr" target="#b2">[3]</ref>. Their method however, relies on replacement of tokens from the question with their string match in the chart, therefore tailored to the dataset question generator and is trained on its dictionary. As also claimed by the authors, STL-CQA is likely to fail in real cases where entities are addressed through their variations, which is the case in a reality as represented also in the PlotQA-D dataset.</p><p>All the above methods use only a classification head, without a regression capability, strongly limiting the generalization of these methods to realistic charts. OOV answers are therefore limited only to values appearing in the chart's image or seen in train set and added a-priori to the answer classes (see Tab. 1, Sec. 3). They commonly overlook the lingual relations between the chart's text, such as the relations between the content of the title, the legend, and the question. Instead, they only rely on the position of the text in the chart as a hint for its class. Nevertheless, PReFIL showed overall accuracy above 93% on FigureQA and DVQA surpassing human performance. Recent results shown in <ref type="bibr" target="#b19">[20]</ref> imply that these datasets are strictly "forgiving" with respect to regression capability and lingual interactions between the questions and chart text (see Sec. 3).</p><p>Recently, Methani et al . <ref type="bibr" target="#b19">[20]</ref> introduced a new method (PlotQA-M) and dataset (PlotQA-D). To the best of our knowledge, this is the first model to address the regression task, suggesting a solution for reasoning on realistic charts. PlotQA-M uses a visual detector and two separate pipelines. In a staging structure, a trained classifier switches between the pipelines, one handling fixed vocabulary classification, and the other for dealing with OOV and regression. In its OOV branch, <ref type="table" target="#tab_1">PlotQA-M first converts the chart to a table and uses a standard  table question-answering [22]</ref>, to generate an answer. This pipeline branching complicates the model requiring each pipeline to be optimized separately and trained on a separate subset of the data, missing the impact of multi-task learning, which we further show as a strong advantage. Furthermore, PlotQA-M inter and intra visual-text interactions from the chart image are only determined through question encoding and a preprocessing stage using prior assumption on proximity between chart elements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>In this section we discuss the properties of existing CQA datasets, emphasizing the bias they introduce into the models and the evaluation methodologies that were proposed. Tab. 1 presents various properties of these datasets that may strongly impact the realism and generalization of the results to a real world application. This is an extended version of a table shown by Methani et al . <ref type="bibr" target="#b19">[20]</ref>.</p><p>Probably the most popular CQA datasets/benchmarks are FigureQA <ref type="bibr" target="#b14">[15]</ref> and DVQA <ref type="bibr" target="#b12">[13]</ref>, both of which are publicly available. FigureQA consists of line plots, bar charts, pie plots, and dot line plots, with question templates that require binary answers. The plot titles and the axes label strings are constant; the axes range is mostly in [0, 100] with low variation; and the legends are chosen from a small set of color names (see example in supplementary material). These properties detract from the realism of this dataset.</p><p>DVQA <ref type="bibr" target="#b12">[13]</ref> contains a single type of charts (bar charts), but offers more complexity in Q&amp;A. The answers are no longer only binary, and may be out of vocabulary (OOV). Questions are split to three conceptual types: Structural, Data retrieval and Reasoning. Structural questions refer to the chart's structure (e.g., How many bars are there? ). Data retrieval questions require the retrieval of information from the chart (e.g., What is the label of the third bar from the bottom? ). Reasoning questions demand a higher level of perceptual understanding from the chart and require a combination of several sub-tasks (e.g., Which algorithm has the lowest accuracy across all datasets? ). Yet, this dataset suffers from lack of semantic relations between the text elements (e.g., bar and legend labels are randomly selected words), and the range of values on the Y-axis is limited. About 46 out of 1.5K unique answers are numeric, consisting of integers with the same values in the train and test sets, allowing a classification head to handle data retrieval and reasoning.</p><p>Two more datasets LEAF-QA <ref type="bibr" target="#b4">[5]</ref> and LEAF-QA++ [27], have fewer Q&amp;A pairs than DVQA, but several types of charts, and use a real world vocabulary with semantic relations (see Tab. 1). However, they are both proprietary. All the mentioned datasets share a strong limitation, lack of regression Q&amp;A, indicated by their question templates and their discrete answer set. PlotQA-D <ref type="bibr" target="#b19">[20]</ref> is, however, the largest and most comprehensive publicly released dataset to date. This dataset consists of charts generated from real-world data, thereby exhibiting realistic lingual relations between textual elements. The questions and answers are based on multiple crowd-sourced templates. PlotQA-D consists of three different chart types: line-plots, bar-charts (horizontal and vertical), and dot line plots. The range of the Y-axis values is orders of magnitudes larger (up to [0, 3.5 ? 10 15 ]) with non-integer answers generally not seen in training, resulting over 5.7M of different answers. In contrast to previous datasets, PlotQA-D often requires a regressor for correctly answering questions. Nearly 30% and 90% of questions require regression in PlotQA-D1 and PlotQA-D2 respectively (see Tab. 1). To the best of our knowledge, PlotQA-D is currently the most realistic publicly available dataset. PlotQA-D offers two benchmarks, the first version of the dataset PlotQA-D1, and its extended version PlotQA-D2, which contains the former as a subset (28% of the Q&amp;A pairs on the charts). The majority of PlotQA-D2 question types require regression (see the suppl. material). We believe that saturated performance on DVQA (97.5%), probably attributed to a single plot type and having only 1.5K unique in contrast to 5.7M answers in PlotQA-D, makes it inappropriate for regression benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We present an overview of our CRCT architecture for CQA in <ref type="figure">Fig. 3</ref>. In our approach, the image is first parsed by a trained object detector (see object classes in <ref type="figure" target="#fig_1">Fig. 2</ref>). The output of the parsing stage are object classes, positions (bounding boxes), and visual features. All of the above are projected into a single representation per visual element, then stacked to form the visual sequence. Similarly, each textual element is represented by fusing its text tokens, positional encoding and class. Together with the question text tokens, we obtain the text sequence. The two sequences are fed in parallel to a bimodal co-attention-transformer (cotransformer). The output of the co-transformer are pooled visual and textual representations that are then fused by Hadamard product and concatenation, and fed into our unified classification-regression head. In the next sections we describe the train and test configurations in detail.</p><p>Visual Encoding: The visual branch encodes all the visual elements in the chart, e.g., line segments or legend markers. For visual encoding we train a Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> with a ResNet-50 <ref type="bibr" target="#b9">[10]</ref> backbone. Object representations are then extracted from the penultimate layer in the classification branch. In our detection scheme objects are textual elements (e.g., title, xlabel) as well as visual elements (e.g., plot segment) as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We create a single representation per visual element by a learnable block as shown in <ref type="figure" target="#fig_4">Fig. 4a</ref>. This block takes as input the 4D vector describing the bounding box (normalized top-left and bottomright coordinates), the class label and the object representation produced by the detector (encapsulating e.g., the line direction), and projects them to an embedding space (1024D).  <ref type="figure" target="#fig_1">Fig. 2</ref>). These features, along with the question text, enable the co-transformers in the second stage (right) to fuse both visual and textual information into a pooled tuple of two single feature vectors {h v0 , h w0 }. Next, our hybrid prediction head containing two different MLPs, outputs a classification score and a regression result. co i /self i : co/self attention.  Object colors are generally encoded in the representation output from the detector. However the actual colors are often important for linking the legend marker to the legend label (text), allowing the connection between the question and the target line or bar in the chart. Our observation shows that training the detector with decomposition of graphs to colors, boosts the performance. Finally, our visual element representations form a sequence, is denoted by v 1 , ..., v k . We further add the global plot representation (v 0 ) as [CLS] token.</p><p>Text Encoding: Raw text is handled with a pretrained BERT <ref type="bibr" target="#b6">[7]</ref>. The textual features are derived from the question and the text contained within the chart, such as the axes labels, legends and title. In contrast to VQA where the lingual part includes only the question, in CQA there are additional text elements that are essential for chart comprehension. Text position in the chart carries important information. In this study, we encode the textual elements in a concatenated version, separated with the special [SEP] token, followed by the question and an answer with the special token [CLS] on top (t 0 ). In contrast to previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">27,</ref><ref type="bibr">30]</ref>, where only the question (or question + answer) was encoded, here the text encoder is generalized to include all textual elements enriched with their spatial location and class. This approach allows free data-driven interaction between different visual and textual elements, e.g., the legend marker and its corresponding text, as well as interactions between text subelements, e.g., the answer and part of the Y-axis label or title. To this end, we create a new representation from all the textual elements in the chart by fusing the word embedding, the positional encoding, the text location in the chart and the text class embedding. This fusion is carried out through a MLP layer, including projection and summation as shown in <ref type="figure" target="#fig_4">Fig. 4b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Associating Visual and Textual Elements</head><p>For multi-modal interaction we rely on the co-attention architecture that was first suggested for machine translation in <ref type="bibr" target="#b2">[3]</ref>. This model contains two different sequence to sequence branches: visual and textual, as shown in <ref type="figure">Fig. 3</ref>. The information in the two streams is fused through a set of attention block exchanges, called co-attention. We use a transformer with 6 blocks of two encoders with co-and self-attention. Each encoder computes a query Q, key K, and value V matrices, followed by feed-forward layer, skip connections and normalization <ref type="bibr">[28]</ref>. In order to exchange the information between the modalities, the co-transformer's keys and values at each stream are mutually exchanged resulting a cross-modality attention. Finally, the resulting {h v0 , h w0 } pooling tokens (indicated by [CLS] special token) are forwarded to the classification and regression heads (see <ref type="figure">Fig. 3</ref>). For more details, see suppl. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Answering Stage</head><p>Similar to previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">27]</ref> and in order to allow fair comparison, we use an oracle to recognize the extracted text elements. The oracle is a perfect text recognition machine, and is used to disentangle the impact of OCR accuracy. Previous work frequently assume a perfect text detector, e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">27]</ref>. In this work however, we explicitly account for inaccuracies in the detector by considering only text elements from the oracle with IoU &gt; 0.5. We then create the set of possible answers for classification, composed of in-vocabulary (e.g., Yes / No) and out-of-vocabulary (OOV) answers (e.g., the title or specific legend label). OOV additional classes (dynamically added) allow dealing with chart specific answers that has not been seen during training. To predict the correct answer, we train the model with binary cross-entropy loss. To this end, we concatenate the answer to the question in the textual branch, pass it through the model and evaluate a score in [0, 1] range (see <ref type="figure">Fig. 3</ref>). This score indicates the model's certainty whether the answer is aligned with the question (correct) or not (wrong).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unified Prediction</head><p>Previous works frequently use only a classification head, overlooking regression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">27]</ref>, or use a totally separate pipeline for the regression task <ref type="bibr" target="#b19">[20]</ref>. In classification based methods, the answers are restricted to discrete values, that are part of the numeric values appearing on the chart. This approach strongly limits the generalization, lacking the capability to predict unseen numeric values or charts with unseen ranges. In this work, we propose a novel hybrid prediction head allowing unified classification-regression. To this end, we add a regression soft decision flag ?R? as an answer class, followed by a regressor. During training the model learns which type of questions require regression by choosing the ?R? class as the correct answer. A separate and consequent regression is then applied to generate the answer (see <ref type="figure">Fig. 3</ref>). Note that during training, the loss changes dynamically from BCE loss for classification and L1 loss for regression, so the network is jointly optimized for classification and regression. During train, we vanish the regression loss when the correct class is not ?R?. The hybrid prediction allows joint training on all types of Q&amp;As, leveraging multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>For training the CRCT we use two stages. We first train a Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> from which the visual features are derived, using Detectron2 [29] library. We then train the co-transformer model for 20 epochs with linear learning rate scheduler. We use binary cross entropy loss for the classification component and L1 loss for regression. For answer alignment prediction (as described in Sec. 4.2), we generate negative examples by randomly assigning wrong answers to questions. Training our model on PlotQA-D1 took 3.5 days on two Nvidia RTX-6000 GPUs. The inference computational cost is proportional to the size of candidate answers. In our experiments the inference time took 0.23 seconds per question. Our code and models are publicly available at https://github.com/levymsn/CQA-CRCT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>As evaluation benchmark we opted for PlotQA-D and FigureQA datasets, being fully annotated to train a detector (DVQA lacks the important annotation of legend markers). Yet, we focus our analysis on PlotQA-D for several reasons: (1) Publicly available to allow benchmarking. (2) The scale: Over ?10 larger Q&amp;A pairs and over ?1000 more unique answers, than the predecessors (see Tab. 1); (3) Highly variable axis scale; (4) Having diverse and realistic questions/answers with rich vocabulary titles, legend labels, X and Y labels including initials gathered from real figures; (5) Most importantly, question types that require regression and therefore reflect a realistic case for CQA.</p><p>In terms of methods to compare with, we searched for publicly available code or assessments on the chosen datasets. To allow a fair comparison to previous methods, in addition to PlotQA-M, we further test PReFIL <ref type="bibr" target="#b13">[14]</ref> on PlotQA-D. To this end, we trained PReFIL on PlotQA-D1. We chose PreFIL due to it's high performance on DVQA and FigureQA and as a representative candidate for previous methods that rely on classification and lack a regression capability. Since PReFIL has only a classification head we quantized the numeric values into Y-ticks and added them to the dynamic classification head in training and also at test (a common practice, also performed in PReFIL <ref type="bibr" target="#b13">[14]</ref>). For sake of analysis and to allow a fair comparison we show the PReFIL results for numeric evaluation with various error tolerances (see <ref type="figure">Fig. 5b</ref>).</p><p>To handle the wide range of Y-axis values in PlotQA-D, we normalize values to [?1, 1] (by detecting X-Y axes and their values). This improves convergence and enables scale invariant prediction. We output answers in the same range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We train our model on PlotQA-D1 dataset, that consists of one third of PlotQA-D2 in questions, while testing on both PlotQA-D1 and PlotQA-D2 test sets. We show significant improvements on both test sets. Results are shown as average accuracy over the test set and accuracy breakdown per-question category.</p><p>Comparison to previous methods: Tab. 2a summarizes the results on PlotQA-D1 test set. In general, we outperform PlotQA-M in all categories by a large margin. For instance, the gaps for Data Retrieval and Reasoning are 48.8% (94.52% vs. 45.68%) and 23.7% (54.87% vs. 31.20%) absolute points, respectively. Finally, on average we achieve 76.94% accuracy, compared to 53.96% of PlotQA-M. While outperforming PlotQA-M when trained on the same train set, in the next experiment we show the extent of train data reduction that can be allowed to match the previous results of PlotQA-M. This experiment shows that as little as 10% of training data (randomly selected) are already sufficient to reach this goal. (see CRCT-10% in 2a).</p><p>With respect to PReFIL, while we show comparable results on the Structural question category, containing classification type questions, CRCT is superior to PReFIL in all other categories. As expected, PReFIL performs poorly on Data Retrieval and particularly Reasoning Q&amp;As (only 31.66% vs 54.87% for our CRCT) due to lack of regression capability. In total average accuracy we surpass both PlotQA-M and PReFIL by 23% and 19% absolute points, respectively. Interestingly, with our quantization scheme training of PReFIL, it outperforms PlotQA-M, in all categories.</p><p>Due to extreme computational demand for train on PlotQA-D2, in the next experiment we train PReFIL and CRCT on PlotQA-D1 train set and report the results on PlotQA-D2 test set in Tab. 2b. Note that for PlotQA-M we report the result from <ref type="bibr" target="#b19">[20]</ref> with the model trained on whole PlotQA-D2. These results show that even when we train on PlotQA-D1 dataset we are able to outperform PlotQA-M trained on ?3 larger size data, in all categories, often with significant margin. Our CRCT is superior here also to PReFIL with average accuracy of 34.44% vs. 10.37%. Note the poor performance of PReFIL on Reasoning category, from which many questions require regression, reaching 3.9% comparing 25.81% in CRCT. These results show the significance of our hybrid classification-regression capability.</p><p>Regression Performance: The accuracy of regression errors are often measured by L 2 or L 1 differences or by ER-error rate. In PlotQA-D <ref type="bibr" target="#b19">[20]</ref>, a regression answer is considered correct if it falls within ?5% tolerance from the ground truth value. This measure, however, is proportional to the true value, vanishing (no tolerance) for true values near zero. We therefore suggest the tick-based error measure as more appropriate for extraction of numerical values. To this end we suggest a constant gap per-chart, defined as a fraction of units between two consecutive sub-ticks (see <ref type="figure" target="#fig_1">Fig. 2</ref>) e.g., 1 /4 sub-tick.</p><p>In PlotQA-D1, 29% of the questions require regression. Following PlotQA <ref type="bibr" target="#b19">[20]</ref>, we show in <ref type="figure" target="#fig_7">Fig. 5a</ref> CRCT accuracy distribution considering the error rate (ER) measure. We observe that 44.37% of the answers are within ?5% of the true value. The prevalence of errors decreases in higher tolerance ranges except the outlier in the tail, indicating that 11.3% of the answers were over 100% off the true value. As expected, we observe that CRCT error distribution indeed accumulates near zero true values (see suppl. material), justifying the advantage of value invariant error measure. <ref type="figure">Fig. 5b</ref> shows the variation of regression accuracy with increased tolerance (as sub-tick fraction) for CRCT and PReFIL. CRCT achieves over 85% total accuracy and 78% regression accuracy for 1 sub-tick tolerance. Note the large gap w.r.t PReFIL through all the range as well as the drop in CRCT-10% that obtained similar accuracy to PlotQA-M ( <ref type="figure" target="#fig_7">Fig. 5a</ref>). This dataset contains two families of validation and test sets. The first family is the Val-1/Test-1 sets, that was generated using the original color schemes as in the train set. On the contrary, Val-2/Test-2 sets consist of alternate color scheme that was not seen in the train set at all. Tab. 3 presents a comparison on FigureQA dataset. CRCT shows comparable performance to SoTA on the original color scheme. While we outperform previous methods on the alternate color scheme sets, we reach an inferior performance w.r.t PReFIL. This test indicates a color sensitivity for our detector-based approach as we discuss in Sec. 8.   The prevalence of predictions that fall in certain error range (b) Accuracy for different sub-tick error range (tolerance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5:</head><p>Model regressor performances on PlotQA-D1. In 5a, the green column shows the "correct" answers i.e. fall in 5% tolerance. 11.3% of the answers (red) miss the target by more than 100%. In 5b, x = 0 indicates exact match between prediction and ground truth (zero tolerance). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Study</head><p>Tab. 4 shows an ablation study of our method using different configurations. First we examine the impact of the legend marker (see <ref type="figure" target="#fig_1">Fig. 2</ref>) as key element.</p><p>Removing it from the input in the visual branch prevents the model to associate the question to the specific plots/bar in multi-graph chart. The results show drop in performance in all categories with total accuracy dropping from 57.75% to 50.45%. In the next two tests we show the impact of representation architecture on the end results. To this end we remove the class label embeddings from the visual and textual representation (e.g., 'line 23' or 'x ticklabel' in <ref type="figure" target="#fig_4">Fig. 4</ref>). Although noisy, these inputs derived from the detector, positively impact the results. Removing them, causes regression accuracy to drop from 20.74% to 17.35%, for visual and 15.51% for text. We observe the best classification performance is achieved without the visual class embedding. However, this embedding is just one component of the visual representation (see <ref type="figure" target="#fig_4">Fig. 4a</ref> -Class-Emb). In some cases Class-Emb is redundant to the visual representation, and removing it can slightly improve certain classification Q&amp;As, resulting in this outcome (e.g., where only textual elements are addressed). However, as Tab. 4 shows, the slight improvement in classification task (? 1%) is traded with large degradation in regression accuracy (? 3%), resulting a lower total accuracy. When removing all features except the bounding box coordinates, from the visual representation, the total accuracy drops by 3.6%. This shows the importance of all elements in our chart element representation model (see <ref type="figure" target="#fig_4">Fig.  4</ref>). Finally, we examine the importance of the multi-tasking regime inherent in our unified classification-regression network. To this end we train our classification and regression network separately (similar to <ref type="bibr" target="#b19">[20]</ref>). Assuming an oracle for routing classification and regression type questions to the proper network, we report the outcome accuracies. We observe performance drop on all categories emphasizing the importance of combining both regression and classification in CRCT's learning process. Our detector achieved AP50=0.90. Testing our model with ground truth detections had a negligible effect on the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Explainability</head><p>We provide visualizations for CRCT attention using the Captum package <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Often, relatively few units in a NN are highly influential towards a particular class <ref type="bibr" target="#b16">[17]</ref>.</p><p>Considering the true answer, we integrate over the input gradients to find the most influential features. We then color code the image to indicate the regions in the chart, visual or textual, that the network found influential in answering the posed question. <ref type="figure" target="#fig_9">Fig. 6</ref> shows such visualization maps over charts, on examples from the test set. In <ref type="figure" target="#fig_7">Fig. 6a</ref> CRCT correctly "looks" at the xtick at the global minimum in the plot and on the corresponding x-label, when asked about the minimum argument. <ref type="figure" target="#fig_9">Fig. 6b</ref> shows an example of a bar chart. Note that CRCT's attention is driven toward the dark-green bars due to the question asking about the average for a certain category (secondary education).</p><p>As observed, CRCT attends intuitive features and spatial locations according to the questions asked. For more examples see the suppl. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary and Discussion</head><p>In this paper we argue that the simplicity of Chart Question Answering (CQA) associated with lack of realistic chart content and question types, has lead pre-  vious methods to omit the regression task. The recent PlotQA work <ref type="bibr" target="#b19">[20]</ref> addresses these shortcomings, suggesting a remedy via a new large scale and diverse dataset, as well as a new model. We hereby suggest a bimodal framework for CQA that leverages the natural lingual inter-relations between different chart elements and introduce a novel unified classification-regression head. Our explainability visualizations shed light on question-chart understanding of our model.</p><p>We evaluate our method on the PlotQA and FigureQA datasets, significantly outperforming the PlotQA model. We further compare our method to a previous classification based method of PReFIL, that reached SoTA results on FigureQA (also high performing on DVQA) observing a strong drop in performance when tested on more challenging datasets such as PlotQA-D. We argue that the edge of our method is not in classification but rather on the combined classification regression tasks with natural lingual relations that exist in real CQA case.</p><p>However, some limitations still remains, such as sensitivity to color combinations and non-linear axis scales. Although we reach a comparable result to PReFIL on FigureQA, we noticed deterioration in results when the test and train colors are different. We relate this limitation to the detector representation learning, including the color attributes from the charts and relying on them to distinguish between the plots in a chart. In practice, this limitation can be overcome by extending the (synthetic) dataset to contain more colors.</p><p>In future work we intend to relax the need for full chart annotations, and tackle the efficiency of the training. With PlotQA opening the door again toward chasing human performance in chart comprehension, we hope this paper will encourage researchers to take this challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In the supplementary material we elaborate on some results shown in the main paper, as well as present new ones. We start with further elaboration on the characteristics of Chart Question Answering (CQA) benchmarks in Appendix B, showing the shortcomings of previous public datasets such as FigureQA <ref type="bibr" target="#b14">[15]</ref> and DVQA <ref type="bibr" target="#b12">[13]</ref> by examples. Next, we elaborate on PlotQA <ref type="bibr" target="#b19">[20]</ref> question type distribution, presenting the richness, realistic lingual relations, as well as regression requirement in this dataset. In Appendix D we show further justifications for the new accuracy metric. We show additional explainability results in Appendix E to strengthen the reasoning process in our results. Next, we show our results on DVQA, emphasizing the shortcomings of this dataset to showcase our method.</p><p>In Appendix G, we show a new experiment for language robustness and compare our CRCT model with the PReFIL <ref type="bibr" target="#b13">[14]</ref> which showed high performance in previous benchmarks. Finally, In Appendix H, we run our model on a newly generated example (not from PlotQA) as a single demo case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Architecture</head><p>In this section we provide some equations in order to further clarify our model descriptions in the paper. Let us denote the Query, Key and Value for each branch at certain block as Q v , K v , V v ? R nv?d and Q t , K t , V t ? R nt?d corresponding to the visual and textual branch respectively. Each branch is attended by the other, as the following:</p><formula xml:id="formula_0">z t = attn d (Q v , K t , V t ) := softmax ( Q v K t T ? d )V t ? R nt?d (1) z v = attn d (Q t , K v , V v ) := softmax ( Q t K v T ? d )V v ? R nv?d<label>(2)</label></formula><p>The co-encoder output is followed by a regular self-attention encoder, namely:</p><formula xml:id="formula_1">?i ? {t, v} O i = attn d (Q(z i ), K(z i ), V (z i ))<label>(3)</label></formula><p>Note that Q t , Q v are exchanged, to allow interaction between different modalities. Then the outputs of each branch, O t , O v , are fed to the next co-transformers block in their proper branch. Finally, the resulting h v0 , h w0 pooling tokens from the last layer are used for predicting if the concatenated answer is aligned (C) and the answer numeric value, R:</p><formula xml:id="formula_2">Loss CLS = BCELoss(M CLS (h w0 * h v0 ), C)<label>(4)</label></formula><formula xml:id="formula_3">Loss REG = L1(M REG ([h w0 ; h v0 ]), R)<label>(5)</label></formula><p>We train our model with the combined loss:</p><formula xml:id="formula_4">Loss = ? 1 ? Loss CLS + ? 2 ? Loss REG<label>(6)</label></formula><p>We find ? 1 = ? 2 = 1 to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Characteristics of public CQA datasets</head><p>In this section we elaborate on the characteristics of different chart datasets previously used for Chart Question Answering (CQA) methods and further justify the choice of PlotQA as our main benchmark dataset. To this end, we present examples of charts from FigureQA, DVQA, and PlotQA in <ref type="figure" target="#fig_11">Fig. 7</ref>. This figure demonstrates the fixed templates and degenerate lingual forms used in two previous datasets. In FigureQA, the title, x-axis label and y-axis label are fixed in all the charts. Additional template pattern in FigureQA includes, the legend markers of the plots (e.g., bars or scatter plots) named after their color. This pattern of naming is redundant throughout the entire dataset and is strictly used in the associated questions (see <ref type="figure" target="#fig_7">Fig. 7a &amp; 7b</ref>). DVQA alleviates part of these shortcomings, yet with random words used as legend or bar labels, as shown in <ref type="figure" target="#fig_11">Fig. 7c &amp; 7d</ref>. DVQA is limited to a single chart type and further introduces degenerated lingual forms that are unlikely to appear in a realistic chart. Note for instance, the word "Title" appearing as the title of the chart in <ref type="figure" target="#fig_11">Fig. 7d</ref>.</p><p>In <ref type="figure" target="#fig_11">Fig. 7e &amp; 7f</ref> we show examples from the PlotQA dataset. To the best of our knowledge this is the most realistic dataset publicly available to date. In addition to it's size and diversity (see <ref type="table" target="#tab_0">Table 1</ref>, main paper) it is the only dataset that satisfies all the following terms: 1) Publicly available; 2) Fully annotated to train a detector; 3) Includes multiple chart types; 4) Charts with natural language patterns and relations; 5) Questions that demand regression.</p><p>These dataset characteristics are strongly related to the performance drop that was recently reported on PlotQA dataset in <ref type="bibr" target="#b19">[20]</ref>, and discussed in the paper. We further discuss additional factors, such as lack of regression required questions in previous benchmarks, in the main paper. In Tab. 5 we summarize the performance of several recently published methods against the existing datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PlotQA Data Distribution</head><p>The PlotQA dataset suggests two benchmarks, which we refer to as PlotQA-D1 and PlotQA-D2 (see the paper for chart and Q&amp;A breakdown). Both datasets share the same chart images. However, PlotQA-D1 is a subset of PlotQA-D2, with the latter having ?3.5 more Q&amp;As. Tab. 6 shows the question type distributions for each benchmark with <ref type="figure" target="#fig_12">Fig. 8</ref> depicting distributions of question templates in each question category, Structural (S), Data Retreival (D) and Reasoning (R). PlotQA-D1 introduces a relatively uniform distribution over the question templates, while PlotQA-D2 distribution is strongly skewed by a large number of questions requiring regression (with non-integer answers). PlotQA-D2 was designed to showcase the capability of a method on handling regression, a highly practical task and a strong shortcoming of previous datasets. The results reported in the paper demonstrate that CRCT outperforms previous methods on both of these benchmarks.</p><p>S0 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 </p><formula xml:id="formula_5">(b) Data Retrieval A0 A1 A2 A3 A4 A5 A6 A7 A8 A9 C0 C1 C2 C3 C4 C5 C6 CD0 CD1 CD2 CD3 CD4 CD5 CD6 CD7 CD8 CD9 CD10 CD11 D6 D16 D17 D18 M0 M1 M2 M3 M4 M5 M6 M7</formula><p>Questions types </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Accuracy Metric</head><p>In <ref type="figure" target="#fig_7">Fig. 9a</ref> we graphically visualize the dependency of the error tolerance on the ground truth value for the error ratio measure, in contrast to a fixed tolerance in the tick-based error, as suggested in our paper. Note the vanishing of the tolerance as the true value goes to zero (and vice versa).</p><p>This bias in the ratio based measure drives the errors to accumulate near zero as we show in <ref type="figure">Fig 9b.</ref> This figure presents a comparison between the error ratio measure and the suggested tick based error, for CRCT on PlotQA-D1, showing the bias in ratio based tolerance. We observe a relatively uniform error distribution on the ticked based alternative, as desired. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Explainability Examples</head><p>In this section we show more visualization examples on CRCT explainability using Captum visualization tool (see Sec. 7 in the paper). All examples are drawn from the test set. In <ref type="figure">Fig. 10</ref> we present a case with two line-plots in a chart. Note how the model attends to the correct plot among the two (hot bounding boxes) when asked about the revenue. <ref type="figure" target="#fig_14">Fig. 11</ref> shows an example of semantic understanding. Asked about the intersection, CRCT mostly attends to the two intersection points in the chart. <ref type="figure" target="#fig_1">Fig. 12</ref> shows another multi-plot chart. Here, the model correctly finds the private credit line plot as more influential to the question asked. Furthermore, asking about the average value drives CRCT to attend to all the plot elements corresponding to the private credit label.</p><p>In the next example in <ref type="figure" target="#fig_2">Fig. 13</ref> we show a bar chart. Although the bars are very close in their heights (values), the relevant bar, with the minimum value gets the highest attention, leading to the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Result on DVQA</head><p>DVQA dataset is limited by 1) Single chart type (bar charts), 2) Lack of natural lingual text in the chart (see Appendix B), 3) Answers appearing as a-priori known classes, eliminating the need for regression. This dataset further lacks the important legend marker annotation needed to train our detector. The importance of this object is clearly shown in the explainability examples in Appendix E (and in the paper), where legend markers are frequently highlighted, allowing CRCT to correspond to the correct plot/bar in the chart. The results of our CRCT model are shown in <ref type="table" target="#tab_9">Table 7</ref>. Despite the limitations above, and errors involved in our heuristic annotation, we achieve a reasonable performance of 82.14%, ranked 3rd, on this benchmark and far beyond PlotQA-M that achieves 57.99% . Note that to showcase the strong limitation and existing performance saturation on DVQA we evaluate PReFIL, that reaches almost perfect performance on DVQA (96.37%), on the new PlotQA dataset (see results in the paper). (a) A visual comparison between values and their error ranges, according to ?5% ratio metric (red) and ? 1 /2 sub-tick metric (green). The error tolerance of the error ratio measure depends on the ground truth value. Errors according to two different metrics tick metric ?5% metric (b) Distribution of regression errors by two different metrics, the ?5% tolerance, in blue, and our suggested tick metric (Sec. 5 in the paper), in red. Note the peak in errors near zero, while the fixed tick based tolerance results nearly uniform distribution. <ref type="figure">Fig. 9</ref>: Ratio vs Sub-tick metric.   (indicated in green), it is mostly wrong (indicated in red) after question rephrasing. CRCT however is more robust to phrasing for various question types e.g. data retrieval, and regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H New Generated Example</head><p>We conclude by showing a result from an experiment in our study in <ref type="figure" target="#fig_4">Fig. 14.</ref> To this end, we create a new chart showing our accuracy result compared to PReFIL. We now pose the following question to the model: In 2.5% tolerance error, what is the difference between the accuracy of CRCT and PReFIL?. Although this figure was not part of the PlotQA dataset, we obtained an answer that deviates the true result only by 0.47%. The robustness of CRCT is further illustrated here on handling unknown initials of the corresponding methods.  <ref type="figure" target="#fig_14">Fig. 11</ref>. Each variation was manually rephrased and never seen in train, except the original version. Note the high sensitivity of PReFIL to different question phrasings.    <ref type="figure" target="#fig_11">Fig. 7f</ref>. Each variation was manually rephrased and never seen in train, except the original version. Note that this question requires regression. In every variation therefore the ?R? token was chosen in CRCT's hybrid prediction head, leading to the regression value shown as an answer. The values in green and red are correct and wrong answers respectively. Values in blue present the deviation from the true value. Note the high sensitivity of PReFIL to different question phrasings </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2111.14792v2 [cs.CV] 11 Jul 2022 Question: What is the average amount of aid provided by spain per year?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of object annotations in train images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Q:Fig. 3 :</head><label>13</label><figDesc>What is the total amount of methane emissions in the graph ? Our Classification -Regression Chart Transformer (CRCT) network architecture consists of two stages of detection and question answering. The detection stage (left) provides bounding boxes and object representations of the visual and textual elements (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Textual Representation (per token).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Chart element representations. The relevant information for representing each type of element is summed into a single vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For visual examples of our CRCT model on regression assignment see Figures 1, 6b and the suppl. material. Results on FigureQA: Although our model's strength is in general Q&amp;A with regression, we also test our model on the binary answer data set of Fig-ureQA [15]. FigureQA's training set was generated using different 100 colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>The prevalence of CRCT's answers that fall in certain error range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>a) Q: In which year was the use of IMF credit in DoD minimum? GT: 1989, CRCT: 1989. (b) Q: What is the average percentage of labor force who received secondary education per country? GT: 48.05, CRCT: 47.91 (Error: -0.29%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :</head><label>6</label><figDesc>Test set visualizations. Warmer box color means higher influence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>21.Miech, A., Zhukov, D., Alayrac, J., Tapaswi, M., Laptev, I., Sivic, J.: HowTo100M:Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In: ICCV (2019) 1 22. Pasupat, P., Liang, P.: Compositional Semantic Parsing on Semi-Structured Tables.In: ACL (Jul 2015) 4 23. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.:Learning Transferable Visual Models From Natural Language Supervision. In: Meila, M., Zhang, T. (eds.) ICML (2021) 1 24. Santoro, A., Raposo, D., Barrett, D.G.T., Malinowski, M., Pascanu, R., Battaglia, P.W., Lillicrap, T.: A simple neural network module for relational reasoning. In: NIPS (2017) 3 25. Schwartz, I., Yu, S., Hazan, T., Schwing, A.G.: Factor Graph Attention. In: CVPR (2019) 1 26. Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards VQA Models That Can Read. In: CVPR (2019) 1, 2 27. Singh, H., Shekhar, S.: STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering. In: EMNLP (Nov 2020) 1, 2, 3, 4, 5, 7, 8, 18, 22 28. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention Is All You Need. In: NeurIPS (2017) 8 29. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https:// github.com/facebookresearch/detectron2 (2019) 9 30. Yang, Z., He, X., Gao, J., Deng, L., Smola, A.J.: Stacked Attention Networks for Image Question Answering. In: CVPR (2016) 1, 2, 4, 7 31. Zhang, Y., Yang, Q.: A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering (2021) 3 32. Zou, J., Wu, G., Xue, T., Wu, Q.: An Affinity-Driven Relation Network for Figure Question Answering. 2020 ICME (2020) 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 :</head><label>7</label><figDesc>Examples of charts from the FigureQA, DVQA, and PlotQA datasets. FigureQA charts -(a) and (b) lack any diversity in title and axis labels as well as the plot labels. In DVQA -(c) and (d) Random phrases and words are used in the chart text, resulting in lack of natural semantic relations between the different textual elements. These drawbacks are addressed in PlotQA, where the charts are taken from real world data, as shown in (e) and (f). Zoom in for better visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 :</head><label>8</label><figDesc>PlotQA-D1 and D2 question type distributions. While in Structural questions the distribution is similar, in Data Retrieval and Reasoning questions, PlotQA-D2 is skewed towards few specific templates, which require a regression answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 :</head><label>11</label><figDesc>Explainability visualizations for a PlotQA test sample. Q: How many lines intersect with each other?, Ground truth: 3. CRCT: 3. Note the hot spots at the intersection points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 :</head><label>12</label><figDesc>Explainability visualizations for a PlotQA test sample. Q: What is the average percentage of firms listed by private credit bureau per year? Ground truth: 3.379. CRCT: 3.295 (Error: -2.49%). Note how the model attends the correct plot among the two, with "hot" bounding boxes over all the plot due to average request.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>the label or title of the X-axis ? Years Years #1 What's the name of the X-axis? Years 40 #2 What is the label or title of the horizontal axis? Years 0 #3 What is the x label of the plot? Years 2011 #4 The x-label of the figure? 2004 0 #5 What's the figure's x-axis label? Years 2011 #6 Give me the x-axis label Years 2011 #7 What the x-axis represents? No % of total expenditure in tertiary public institutions #8 What is the label of X? Years 2011 #9 X-label? 2011 2011</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 14 :</head><label>14</label><figDesc>years, what is the maximum completion rate in primary schools ? 36.618 (+0.57%) 35 (?3.87%) #1 What's the maximum primary completion? 36.625 (+0.59%) 0 (?100%) #2 What is the maximal rate of primary school completion, over the years? 36.305 (?0.288%) No #3 Across all years, what is the maximum primary school completion rate? 36.613 (+0.56%) 0 (?100%) #4 Over the years, what is the highest primary school completion rate? 36.502 (+0.25%) No #5 What is the maximum completion rate in primary schools across all years? 36.635 (+0.618%) 0 (?100%) #6 In primary schools, what is the highest completion rate across all years? 36.53 (+0.33%) 3 (?91.76%) #7 The maximum completion rate in primary schools is what -across all years? 36.622 (+0.58%) 0 (?100%) #8 Give me the maximum rate of primary completion over the graph 33.581 (?7.77%) 5 (?86.27%) #9 Average the primary completion rate<ref type="bibr" target="#b14">15</ref>We insert into CRCT a result from our paper showing the regression accuracy of our model against PReFIL. We pose the following question: In 2.5% tolerance error, what is the difference between the accuracy of CRCT and PRe-FIL?. Ground truth: 23.978. CRCT: 23.865 (-0.47%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CQA datasets comparison. Real world vocabulary refers to axes variables. Some datasets apply question paraphrasing (par.)</figDesc><table><row><cell>Dataset</cell><cell>#Plot types</cell><cell>#Plot images</cell><cell>#Q&amp;A pairs</cell><cell>Avg. question length</cell><cell>Q&amp;A #Templates</cell><cell>#Unique answers</cell><cell>Open vocab.</cell><cell>Real World Vocabulary</cell><cell>Semantic Relations</cell><cell>Bbox Ann.</cell><cell>Regression answers</cell><cell>Publicly Available</cell></row><row><cell>FigureQA</cell><cell>4</cell><cell cols="2">180k 2.4M</cell><cell>33.39</cell><cell>15 (no variations)</cell><cell>2</cell><cell>?</cell><cell>? (100 colors names)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>DVQA</cell><cell>1</cell><cell cols="2">300k 3.5M</cell><cell>55.22</cell><cell>26 (w\o par.)</cell><cell>1.5k</cell><cell>? (Strings)</cell><cell>? (1K nouns)</cell><cell>?</cell><cell>Partial</cell><cell>?</cell><cell>?</cell></row><row><cell>LEAF-QA</cell><cell>5</cell><cell cols="2">246k 1.9M</cell><cell>-</cell><cell>35 (with par.)</cell><cell>12k</cell><cell>? (Strings)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">LEAF-QA++ 5</cell><cell cols="2">246k 2.6M</cell><cell>65.65</cell><cell>75 (with par.)</cell><cell>25k</cell><cell>? (Strings)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>PlotQA-D1</cell><cell>3</cell><cell cols="2">224k 8.2M</cell><cell>78.96</cell><cell>74 (with par.)</cell><cell>1M</cell><cell>? (Strings, Floats)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>? (29.86%)</cell><cell>?</cell></row><row><cell>PlotQA-D2</cell><cell>3</cell><cell cols="2">224k 29M</cell><cell>105.18</cell><cell>74 (with par.)</cell><cell>5.7M</cell><cell>? (Strings, Floats)</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>? (88.84%)</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracies [%] on PlotQA test sets. Values in each column indicate average accuracy per-question category. CRCT and PReFIL are trained on the PlotQA-D1 subset. PReFIL results are reproduced. 'CRCT-10%' indicates our results with training on 10% of the PlotQA-D1 train set. S, D and R stand for Structural, Data Retrieval and Reasoning question categories, respectively</figDesc><table><row><cell cols="4">(a) Evaluation on PlotQA-D1 test set</cell><cell cols="4">(b) Evaluation on PlotQA-D2 test set</cell></row><row><cell>Method</cell><cell>S</cell><cell>D</cell><cell>R Overall</cell><cell>Method</cell><cell>S</cell><cell>D</cell><cell>R Overall</cell></row><row><cell cols="4">PlotQA-M [20] 86.31 45.68 31.2 53.96</cell><cell cols="4">PReFIL [14] 96.66 21.9 3.9 10.37</cell></row><row><cell>CRCT-10%</cell><cell cols="3">87.15 74.71 29.19 57.75</cell><cell cols="4">PlotQA-M [20] 75.99 58.94 15.77 22.52</cell></row><row><cell cols="4">PReFIL [14] 96.66 58.69 31.66 57.91</cell><cell cols="4">CRCT (ours) 96.23 66.65 25.81 34.44</cell></row><row><cell cols="4">CRCT (ours) 96.13 94.52 54.87 76.94</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on FigureQA dataset<ref type="bibr" target="#b14">[15]</ref>. Second place is coloured in brown</figDesc><table><row><cell cols="3">(a) Original color scheme</cell><cell cols="3">(b) Alternate color scheme</cell></row><row><cell>Model / Acc.</cell><cell>Val.</cell><cell>Test</cell><cell>Model / Acc.</cell><cell>Val.</cell><cell>Test</cell></row><row><cell>RN [15]</cell><cell>-</cell><cell>76.52</cell><cell>RN [15]</cell><cell>72.54</cell><cell>72.40</cell></row><row><cell>LEAF-Net [5]</cell><cell>-</cell><cell>-</cell><cell>LEAF-Net [5]</cell><cell>81.15</cell><cell>-</cell></row><row><cell>Zou et al. [32]</cell><cell>85.48</cell><cell>85.37</cell><cell>Zou et al. [32]</cell><cell>82.95</cell><cell>83.05</cell></row><row><cell>CRCT (ours)</cell><cell cols="2">94.61 94.23</cell><cell>CRCT (ours)</cell><cell cols="2">85.04 84.77</cell></row><row><cell>PReFIL [14]</cell><cell cols="2">94.84 94.88</cell><cell>PReFIL [14]</cell><cell cols="2">93.26 93.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study with different configurations (see alsoFig. 4). All models are trained on 10% of PlotQA-D1 train set, and evaluated on the entire PlotQA-D1 test set. S, D and R stand for Structural, Data Retrieval and Reasoning, respectively</figDesc><table><row><cell>Method</cell><cell cols="2">Regression Classification</cell><cell>S</cell><cell>D</cell><cell>R</cell><cell>Overall</cell></row><row><cell>w/o Legend Marker</cell><cell>14.76</cell><cell>65.02</cell><cell cols="4">81.13 56.01 27.05 50.45</cell></row><row><cell>w/o Textual Class Emb.</cell><cell>15.51</cell><cell>66.86</cell><cell cols="4">81.75 61.73 26.96 51.98</cell></row><row><cell>w/o Visual Class Emb.</cell><cell>17.35</cell><cell>73.68</cell><cell cols="4">85.06 73.09 30.57 57.36</cell></row><row><cell>Only Bbox forVisual Feats.</cell><cell>18.66</cell><cell>68.68</cell><cell cols="4">84.97 72.94 23.75 54.19</cell></row><row><cell>Two Pipelines</cell><cell>14.80</cell><cell>70.19</cell><cell cols="4">84.49 68.65 25.16 53.64</cell></row><row><cell>CRCT</cell><cell>20.74</cell><cell>72.86</cell><cell cols="4">87.15 74.71 29.19 57.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of different methods on existing datasets. Note the significant drop in accuracy on PlotQA dataset (PlotQA-D).</figDesc><table><row><cell>our evaluation of PReFIL</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Distribution over different question categories in PlotQA benchmarks</figDesc><table><row><cell cols="4">Data Ver. Structural Data Retrieval Reasoning</cell></row><row><cell cols="2">PlotQA-D1 30.41%</cell><cell>24.01%</cell><cell>45.58%</cell></row><row><cell>PlotQA-D2</cell><cell>4.3%</cell><cell>13.74%</cell><cell>81.96%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on DVQA dataset. CRCT p indicates the CRCT model with a detector that was trained with partial bounding box annotations.In this section we demonstrate the robustness under natural lingual variations of the CRCT model. Our transformer based model allows initialization with pretrained BERT<ref type="bibr" target="#b6">[7]</ref> on a large language corpus such as Wikipedia. CRCT is further trained downstream on all the textual elements in the chart, without any heuristics, such as string replacements (see the paper). This is in contrast to previous methods, often using LSTM, based only on the chart dataset vocabulary. We further compare the CRCT robustness with PReFIL<ref type="bibr" target="#b13">[14]</ref>, where a LSTM is used for question encoding. InTables 8, 9and 10 we present question rephrasing on test figures. Each variation is a new manual phrasing of the original template. Note that the original template is the only one appearing in train set.Tables 8-10show that while on the template question PReFIL gives the correct answerFig. 10: Explainability visualizations for a PlotQA test sample. Q: Is the amount of revenue collected in 2005 less than that in 2008? ground truth: Yes. CRCT: Yes. Note the high attention on the correct plot between the two. The font sizes are from the dataset source.</figDesc><table><row><cell cols="7">Method SANDY [13] PlotQA-M [20] LEAF-Net [5] CRCT p PReFIL [14] STL-CQA [27]</cell></row><row><cell>Accuracy</cell><cell>56.48</cell><cell>57.99</cell><cell>72.72</cell><cell>82.14</cell><cell>96.37</cell><cell>97.35</cell></row><row><cell cols="3">G Language Robustness</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Question rephrasing, for</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Question rephrasing, for Fig. 7e. Each variation was manually rephrased and never seen in train, except the original version. Note the high sensitivity of PReFIL to different question phrasings.Fig. 13: Explainability visualizations for a PlotQA test sample. Q: In which year was the persistence rate of female students minimum?, Ground truth: 1982. CRCT: 1982. For better visibility, we overlay the visualization as colored bounding box around the bars. Note how green bars related to Female achieve higher attention with the correct bar receiving the highest attention.</figDesc><table><row><cell>Var.</cell><cell>Question</cell><cell cols="2">CRCT PReFIL</cell></row><row><cell cols="2">Original How many different coloured bars are there ?</cell><cell>2</cell><cell>2</cell></row><row><cell cols="2">#1 How many bar colors are there?</cell><cell>2</cell><cell>2</cell></row><row><cell cols="2">#2 How many colors of bars can you see?</cell><cell>2</cell><cell>1</cell></row><row><cell cols="2">#3 Coloured bars?</cell><cell cols="2">No Bolivia</cell></row><row><cell cols="2">#4 How many colors paints each group of bars?</cell><cell>2</cell><cell>Bolivia</cell></row><row><cell cols="2">#5 How many colors are there?</cell><cell>2</cell><cell>2</cell></row><row><cell cols="2">#6 How many different bars exists in each group?</cell><cell>2</cell><cell>0</cell></row><row><cell cols="2">#7 Colors in each group?</cell><cell>No</cell><cell>0</cell></row><row><cell cols="2">#8 How many colors?</cell><cell>2</cell><cell>0</cell></row><row><cell cols="2">#9 Give me the size of each group of bars</cell><cell>Bolivia</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Question rephrasing, for</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We thank Or Kedar and Nir Zabari for their assistance in parts of this research. We thank PlotQA <ref type="bibr" target="#b19">[20]</ref> authors for sharing additional breakdowns. This work was supported in part by the Israel Science Foundation (grant 2492/20).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TallyQA: Answering Complex Counting Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<title level="m">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LEAF-QA: Locate, Encode &amp; Attend for Figure Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maneriker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>In: WACV (2020) 1, 2, 3, 5, 8, 12</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DVQA: Understanding Data Visualizations via Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Answering questions about data visualizations using efficient bimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>In: WACV (2020) 1, 2, 3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">FigureQA: An Annotated Figure Dataset for Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>K?d?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICLRW</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kokhlikyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reblitz-Richardson</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/captum" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>PyTorch Captum</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Influence-Directed Explanations for Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ITC</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PlotQA: Reasoning over Scientific Plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Methani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-03-01" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
