<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent 3D Pose Sequence Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
							<email>linmude@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<email>wangkeze@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Cheng</surname></persName>
							<email>chengh9@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent 3D Pose Sequence Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation. * Corresponding author is Liang Lin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Though quite challenging, recovering the 3D full-body human pose from a monocular RGB image sequence has re- <ref type="figure">Figure 1</ref>: Some visual results of our approach (RPSM) on Human3.6M dataset. The estimated 3D skeletons are reprojected into the images and shown by themselves from the side view (next to the images). The figures from left to right correspond to the estimated 3D poses generated by the 1st-stage, 2nd-stage and 3rd-stage of RPSM, respectively. We can observe that the predicted human 3D joints are progressively corrected along with the multi-stage sequential learning. Best viewed in color.</p><p>cently attracted a lot of research interests due to its huge potentials on high-level applications, which includes humancomputer interaction <ref type="bibr" target="#b9">[10]</ref>, surveillance <ref type="bibr" target="#b13">[14]</ref>, video browsing/indexing <ref type="bibr" target="#b5">[6]</ref> and virtual reality <ref type="bibr" target="#b22">[23]</ref>.</p><p>Besides the challenges shared with 2D image pose estimation (e.g., large variation in human appearance, arbitrary camera viewpoints and obstructed visibilities due to external entities and self-occlusions), 3D articulated pose recovery from monocular imagery is much more difficult since 3D pose is inherently ambiguous from a geometric perspective <ref type="bibr" target="#b39">[40]</ref>, as shown in <ref type="figure">Fig. 1</ref>. To resolve all these issues, a preferable way is to investigate how to simultaneously enforce 2D spatial relationship, 3D geometry constraint and temporal consistency within one single model.</p><p>Recently, notable successes have been achieved for 2D pose estimation based on 2D part models coupled with 2D deformation priors, e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37]</ref>, and the deep learning techniques, e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. However, these meth-ods have not explored the 3D pose geometry that is crucial for 3D pose estimation. There has been some limited attempts on combining the image-based 2D part detectors, 3D geometric pose priors and temporal models for generating 3D poses <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref>. They mainly follow two kinds of pipelines: the first <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref> resorts to the model-based 3D pose reconstruction by using external 3D pose gallery, while the second pipeline <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref> focuses on elaborately designing human body kinematic constraints with the model training. These separate techniques and prior knowledge make their models very sophisticated. Hence, validating the effectiveness of their each component is also not straightforward. In contrast to all these mentioned methods, we introduce a completely data-driven approach that learns to integrate the 2D spatial relationship, 3D geometry and temporal smoothness for the network training in a fully differential way.</p><p>We propose a novel Recurrent 3D Pose Sequence Machine (RPSM) for estimating 3D human poses from a sequence of images. Inspired by the pose machine <ref type="bibr" target="#b21">[22]</ref> and convolutional pose machine <ref type="bibr" target="#b33">[34]</ref> architectures for 2D pose estimation, our RPSM proposes a multi-stage training to capture long-range dependencies among multiple body-parts for 3D pose prediction, and further enforce the temporal consistency between the predictions of sequential frames. Specifically, the proposed RPSM recursively refines the predicted 3D pose sequences by sensing what already achieved in the previous stages, i.e., 2D pose representations and previously predicted 3D poses. At each stage, our RPSM is composed by a 2D pose module, a feature adaption module, and a 3D pose recurrent module. These three modules are constructed by the integration of the advanced convolutional and recurrent neural networks to fully exploit spatial and temporal constraints, which makes our RPSM with multi-stages a differentiable architecture that can be trained in an end-to-end way.</p><p>As illustrated in <ref type="figure">Fig. 1</ref>, our RPSM enables to gradually refine the 3D pose prediction for each frame with multiple sequential stages, contributing to seamlessly learning the image-dependent constraint between multiple body parts and sequence-dependent context from the previous frames. Specifically, at each stage, the 2D pose module takes each frame and 2D feature maps produced in previous stages as inputs and progressively updates the 2D pose representations. Then a feature adaption module is injected to transform learned pose representations from 2D to 3D domain. The 3D pose recurrent module, constructed by a Long-Short Term Memory (LSTM) layer, can thus regress the 3D pose estimation by combining the three lines of information, i.e. the transformed 2D pose representations, 3D joint prediction from the previous stage and the memorized states from past frames. Intuitively, the 2D pose representations are conditioned on the monocular image which captures the spatial appearance and context information. The 3D joint prediction implicitly encodes the 3D geometry structural information by aggregating multi-stage computation. Then temporal contextual dependency is captured by the hidden states of LSTM units, which effectively improves robustness of the 3D pose estimations over time.</p><p>The main contribution of this work is three-fold. i) We propose a novel RPSM model that learns to recurrently integrate rich spatial and temporal long-range dependencies using a multi-stage sequential refinement, instead of relying on specifically manually defined body smoothness or kinematic constraints. ii) Casting the recurrent network models to sequentially incorporate 3D pose geometry structural information is innovative in literature, which may also inspire other 3D vision tasks. iii) Extensive evaluations on the public challenging Human3.6M dataset <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I dataset <ref type="bibr" target="#b24">[25]</ref> show that our approach outperforms existing methods of 3D human pose estimation by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Considerable research has addressed the challenge of 3D human pose estimation. Early research on 3D monocular pose estimation from videos involves frame-to-frame pose tracking and dynamic models that rely on Markov dependencies among previous frames, e.g. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b25">26]</ref>. The main drawbacks of these approaches are the requirement of the initialization pose and the inability to recover from tracking failure. To overcome these drawbacks, more recently approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> focus on detecting candidate poses in each individual frames and a post-processing step attempts to establish temporal consistent poses. Yasin et al. <ref type="bibr" target="#b37">[38]</ref> proposed a dual-source approach for 3D pose estimation from a single image. They combined the 3D pose data from motion capture system with image source annotated with 2D pose. They transformed the estimation to a 3D pose retrieval problem. One major limitation of this approach is the time efficiency. It takes more than 20 seconds to process an image. Sanzari et al. <ref type="bibr" target="#b23">[24]</ref> proposed a hierarchical Bayesian non-parametric model, which relies on a representation of the idiosyncratic motion of human skeleton joints groups and the consistency of the connected group poses is taken into account when reconstructing the full-body pose. Their approach achieved state-of-the-art performance on the Hu-man3.6M <ref type="bibr" target="#b15">[16]</ref> dataset.</p><p>Recently, deep learning has proven its ability in many computer vision tasks, such as the 3D human pose estimation. Li and Chan <ref type="bibr" target="#b18">[19]</ref> firstly used the CNNs to regress the 3D human pose from monocular images and proposed two training strategies to optimize the network. Li et al. <ref type="bibr" target="#b19">[20]</ref> proposed to integrate the structure-learning into deep learning framework, which consists of a convolutional neural network to extract image feature, and two following subnetworks to transform the image features and pose into a joint embedding. Tekin et al. <ref type="bibr" target="#b29">[30]</ref> proposed to exploit  <ref type="figure">Figure 2</ref>: An overview of the proposed Recurrent 3D Pose Sequence Machine architecture. Our framework predicts the 3D human poses for all of the monocular image frames, and then sequentially refines them with multi-stage recurrent learning. At each stage, every frame of the input sequence is sequentially passed into three neural network modules: i) a 2D pose module extracting the image-dependent pose representations; 2) a feature adaption module for transforming the pose representations from 2D to 3D domain; 3) a 3D pose recurrent module predicting the human joints in 3D coordinates. Note that, the parameters of 3D pose recurrent module for all frames are shared to preserve the temporal motion coherence. Given the initial predicted 3D joints and 2D features from the first stage, we perform the multi-stage refinement to recurrently improve the pose accuracy. From the second stage, the previously predicted 17 joints (51 dimensions) and the 2D pose-aware features are also posed as the input of 2D pose module and 3D pose recurrent module, respectively. The final 3D pose sequence results are obtained after recurrently performing the multi-stage refinement.</p><p>motion information from consecutive frames and applied a deep learning network to regress the 3D pose. Zhou et al. <ref type="bibr" target="#b40">[41]</ref> proposed a 3D pose estimation framework from videos that consists of a novel synthesis between a deep-learningbased 2D part detector, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior. Zhou et al. <ref type="bibr" target="#b39">[40]</ref> proposed to directly embed a kinematic object model into the deep learning. Du et al. <ref type="bibr" target="#b8">[9]</ref> introduced an additional built-in knowledge for reconstructing the 2D pose and formulated a new objective function to estimate 3D pose from detected 2D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recurrent 3D Pose Sequence Machines</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, we propose a novel Recurrent 3D Pose Sequence Machine (RPSM) to resolve 3D pose sequence generation for monocular frames, which recurrently refines the predicted 3D poses at multiple stages. At each stage, RPSM consists of three consecutive modules: 1) 2D pose module to extracts 2D pose-aware features; 2) feature adaption module to transform the representation from 2D to 3D domain; 3) 3D pose recurrent module to estimate 3D poses for each frame incorporating temporal dependency in the image sequence. These three modules are combined into a unified framework in each stage. The monocular image sequences are passed into multiple stages to gradually refine the predicted 3D poses. We train the network parameters recurrently at multiple stages in a fully end-to-end way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-stage Optimization</head><p>The 3D human pose is often represented as a set of P joints with 3D location relative to a root joint (e.g., pelvis joint). Some exemplar poses are shown in <ref type="figure">Fig. 1</ref>. Our goal is to learn a mapping function that predicts the 3D pose sequence {S 1 , ..., S T } for the image sequence {I 1 , ..., I T }, where I t is the t-th frame containing a subject and S t ? R 3?P is its corresponding 3D joint locations.</p><p>Aiming at obtaining the 3D pose S k t of the t-th frame at k-th stage, 2D pose module ? p is first employed to extract the 2D pose-aware features f t,k 2D for each image by taking the image I t and the previously 2D pose-aware features f t,k?1 2D as the input. Then the extracted 2D pose-aware features f t,k 2D are fed into the feature adaption module ? a to generate adapted features f t,k 3D . Finally, the 3D pose S k t is predicted according to the input of 3D pose recurrent module ? r , which is composed of f t,k 3D , the previously predicted 3D pose S k?1 t and the hidden states H k t?1 learned from the past frames. Formally, the f t,k 2D , S k t , f t,k 3D of the t-th stage at k-th stage are formulated as,</p><formula xml:id="formula_0">f t,k 2D = ? p (I t , f t,k?1 2D ; W p ), f t,k 3D = ? a (f t,k 2D ; W a ), S k t = ? r (f t,k 3D , H k t?1 , S k?1 t ; W r ),<label>(1)</label></formula><p>where W p , W a , W r are network parameters of ? p , ? a , ? r , respectively. At the first stage, the f t,0 2D , S 0 t are set as   is set to be a vector of zeros. The 3D pose sequence {S K 1 , S K 2 , . . . , S K T } estimated by the last K-th stage stage is the final prediction. The sequential refinement procedure of our RPSM enables the gradually updating of the network status to better learn the mapping between the image sequence and 3D pose sequence.</p><formula xml:id="formula_1">(kernel-stride) 256(3-1) 256(2-2) 512(3-1) 512(3-1) 256(3-1) 256(3-1) 256(3-1) 256(3-1) 128(3-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">2D Pose Module</head><p>The goal of the 2D pose module is to encode each frame in the monocular sequence with a compact representation of the pose information, e.g. the body shape of the human. As a matter of fact, the lower convolution layers often extract the common low-level information, which is a very basic representation of the human image. Hence, we divide our proposed 2D pose module into two parts: the shared convolution layers across all stages and specialized pose-aware convolution layers in each stage. The architecture of 2D pose module is illustrated in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>.</p><p>The shared convolution layers, i.e., those before the concatenation operation shown in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>, consist of 15 convolutional layers and four max-pooling layer. The kernel size of all shared convolutional layers are set to 3 ? 3, and the four max-pooling layers are set to have 2 ? 2 kernel with a stride of 2. The numbers of channels for the shared convolution layers from left to right in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>  Afterwards, the shared convolution features and the extracted 2D pose-aware features at the previous stage are concatenated and then fed into the last two convolution layers to generate the updated 2D pose-aware features in 2D pose module. By combining the previously learned 2D pose-aware features at previous stage, the discriminative capability of the extracted 2D pose-aware features can be gradually enhanced, leading to a better 3D pose prediction. The higher convolution layers (i.e., the last 2 convolution layers in <ref type="figure" target="#fig_0">Fig. 3(a)</ref>) of 2D pose module often capture more structure-sensitive information, which should be specialized for the refinement at each stage. Thus, we train the network parameters of the last 2 layers independently across all stages. Finally, the 2D pose module takes the 368 ? 368 image as the input and outputs 128?46?46 2D pose-aware feature maps for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Adaption Module</head><p>Based on the features extracted by the 2D pose module, the feature adaption module is employed to adapt the 2D pose representations into a adapted feature space for the later 3D pose prediction. As depicted in <ref type="figure" target="#fig_0">Fig. 3(b)</ref>, the proposed feature adaption module consists of two convolutional layers and one fully connected layers. Each convo-lution layer contains 128 different kernels with the size of 5?5, a stride of 2, and a max pooling layer with a 2?2 kernel size and a stride of 2 is appended on the convolutional layers. Finally, the convolution features are fed to a fully connected layer with 1024 units to produce the adapted feature vector. In this way, the feature adapter module transforms the 2D pose-aware features into the adapted feature vector of 1024 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D Pose Recurrent Module</head><p>Given the adapted features for all frames, we propose a 3D pose sequence module to sequentially predict the 3D pose sequence. In this way, the rich temporal motion patterns between frames can be effectively incorporated into the 3D pose prediction. Note that Long Short-Term Memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> has proved better performance on exploiting temporal correlations than vanilla recurrent neural network in many tasks, e.g., speech recognition <ref type="bibr" target="#b11">[12]</ref> and video description <ref type="bibr" target="#b7">[8]</ref>. In our RPSM, the 3D pose recurrent module resorts to the LSTM layers to capture the temporal dependency in monocular sequence for refining the 3D pose prediction for each frame.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig 3(c)</ref>, the 3D pose recurrent module is constructed by one LSTM layer with 1024 hidden cells and an output layer that predicts the location of P = 17 joint points of the human. In particular, the hidden states learned by the LSTM layers are capable of implicitly encoding the temporal dependency across different frames of the input sequence. As formulated in Eq. (1), the adapted features, the previous hidden states and the previous 3D pose predictions are concatenated together as the current input of 3D poses recurrent module. Incorporating the previous 3D pose prediction at each stage endows our RPSM the ability of gradually refining the pose predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Training and Testing</head><p>In the training phase, our RPSM enforces the 3D pose sequence prediction loss for all frames at all stages, which is defined as the Euclidean distances between the prediction for all P joints and ground truth:</p><formula xml:id="formula_2">L = K k=1 ? k T t=1 S k t ? S * t 2 2 ,<label>(2)</label></formula><p>where K is the number of stages, T is the length of an image sequence, S * t is the ground-truth 3D pose for t-th frame, and ? k is the loss weight for each stage.</p><p>The 2D Pose Module is first pretrained with MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref>, since this dataset provides a larger variant of 2D pose data. Specifically, we temporally build up an extra convolution layer upon the public shared layers of 2D Pose Module to generate heat maps (joint confidence) as <ref type="bibr" target="#b30">[31]</ref>, which denote pixel-wise confidence maps of the body joints. Then we exploit the MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> to pretrain the tailored 2D Pose Module via the stochastic gradient decent algorithm. As for the whole framework, the ADAM <ref type="bibr" target="#b16">[17]</ref> strategy is employed for parameter optimization .</p><p>In order to obtain sufficient samples to train the 3D pose recurrent module, we propose to decompose one long monocular image sequence into several small equal clips with C frames. According to the Eq. (2), we integrally fine-tune the parameters of 3D pose recurrent module, the feature adaption module and the specialized convolutional layers of the 2D pose module in a multi-stage optimization manner. In this way, the feature adaption module can learn the adapted feature representation according to the Eq. (2) for the further 3D pose estimation.</p><p>In the testing phase, every frame of the input image sequence is processed by our proposed RPSM in a stage-bystage manner. In the end, after the final stage refinement, we output the 3D pose prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>We perform the extensive evaluations on two publicly available datasets: Human3.6M <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I <ref type="bibr" target="#b24">[25]</ref>.</p><p>Human3.6M dataset. The Human3.6M dataset is a recently published dataset, which provides 3.6 million 3D human pose images and corresponding annotations in a controlled laboratory environment. It captures 11 professional actors performing in 15 scenarios under 4 difference viewpoints. In the following experiments, we strictly follow the same data partition protocol as in previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>. The data from five subjects (S1,S5,S6,S7,S8) is for training and two subjects (S9,S11) is for testing. Note that to increase the number of training samples, the sequences from different viewpoints of the same subject are treated as distinct sequences. Through downsampling the frame rate from 50FPS to 2FPS, 62,437 human pose images (104 images per sequence) are obtained for training while 21,911 images for testing (91 images per sequence). To be more general, our RPSM is trained on training samples from all 15 actions instead of exploiting individual action like <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>HumanEva-I dataset. The HumanEva-I dataset contains video sequences of four subjects performing six common actions(e.g., walking, jogging, boxing etc.), and it also provides the 3D pose annotation for each frame in the video sequences. We train our RPSM on training sequences of the subject 1, 2 and 3 and test on the 'validation' sequence in the same protocol as <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>. Similar as the Human3.6M dataset, the data from different camera viewpoints is also regarded as different training samples. Note that we have not downsampled the video sequences to   <ref type="table">Table 3</ref>: Quantitative comparisons on HumanEva-I dataset using 3D pose errors (in millimeter) for the "Walking", "Jogging" and "Boxing" sequences. '-' indicates the corresponding method has not reported the accuracy on that action. The entries with the smallest 3D pose errors for each category are bold-faced. Our RPSM outperforms all the compared state-of-the-art methods by a clear margin.</p><p>obtain more samples for training.</p><p>Evaluation metric. Following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>, we employ the popular 3D pose error metric <ref type="bibr" target="#b27">[28]</ref> , which calculates the Euclidean errors on all joints and all frames up to translation. In the following section, we will report the 3D pose error metric for all the experimental comparisons and analysises.</p><p>Implementation Details: Our RPSM is implemented by using Torch7 <ref type="bibr" target="#b6">[7]</ref> deep learning toolbox. We follow <ref type="bibr" target="#b11">[12]</ref> to build the LSTM memory cells, except that the peephole connections between cell and gates are omitted. The loss weights ? k for each stage are all set to 1. In total, three-stage refinements are performed for all our experiments since only unnoticeable performance difference is observed using more stages. Following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>, the input image is cropped around the human. To keep the human ratio, we crop a square image of the subject from the image according to the bounding box provided by the dataset. Then we resize the image region inside the bounding box into 368?368 resolution before feeding it into the network. Moreover, we augment the training data only by random scaling with factors in [0.9,1.1]. Note that to transform the absolute locations of joint points into the [0,1] range, max-min normalization strategy is applied. In the testing phase, the predicted 3D pose is transform to the origin scale according to the maximum and minimal value of the pose from training frames. During the training, the Xavier initialization method <ref type="bibr" target="#b10">[11]</ref> was used to initialize the weights of our RPSM. The decay is set as 1e ?4 and base learning rate of 1e ?3 is employed for training. It took about 2 days to train a 3-stage RPSM with 50 epochs on single NVIDIA GeForce GTX TITAN X with 12GB memory. In the testing phase, it takes about 50 ms to process an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with state-of-the-art methods</head><p>Comparison on Human3.6M: We compare our RPSM with the state-of-the art methods on Human3.6M <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I <ref type="bibr" target="#b24">[25]</ref> dataset. These state-of-the-art methods are LinKDE <ref type="bibr" target="#b15">[16]</ref>, Tekin et al. <ref type="bibr" target="#b29">[30]</ref>, Li et al. <ref type="bibr" target="#b19">[20]</ref>, Zhou et al. <ref type="bibr" target="#b40">[41]</ref> (CNN based), Zhou et al. <ref type="bibr" target="#b39">[40]</ref>, Du et al. <ref type="bibr" target="#b8">[9]</ref> and Sanzari et al. <ref type="bibr" target="#b23">[24]</ref>.</p><p>The results are summarized in <ref type="table" target="#tab_5">Table 2</ref>. As one can see from <ref type="table" target="#tab_5">Table 2</ref>, our proposed RPSM model significantly outperforms all compared methods with mean error reduced by 31.85% compared with <ref type="bibr" target="#b39">[40]</ref> and 21.52% compared with <ref type="bibr" target="#b23">[24]</ref>. Note that some compared methods, e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref>, also employ deep learning techniques. Especially, Zhou et al. <ref type="bibr" target="#b39">[40]</ref>'s method has used the recently published Residual Network <ref type="bibr" target="#b12">[13]</ref>. This superior performance achieved by RPSM demonstrates that utilizing multi-  <ref type="bibr" target="#b39">[40]</ref>, our RPSM and the ground truth are illustrated from left to right, respectively. Our RPSM achieves much more accurate estimations than the methods of Zhou et al. <ref type="bibr" target="#b40">[41]</ref> and Zhou et al. <ref type="bibr" target="#b39">[40]</ref>. Best view in color.  stage RPSM is simple yet powerful in capturing complex contextual features within images and learning temporal dependency within image sequences, which are critical for estimating 3D pose sequence.</p><p>Comparison on HumanEva-I: On this dataset, we compare our RPSM against methods which rely on several kinds of separate processing steps. These methods include discriminative regressions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, 2D pose detectors based <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>, CNN-based regressions <ref type="bibr" target="#b29">[30]</ref>. For fair comparison, our RPSM also predicts the 3D pose consisting of 14 joints, i.e., left/right shoulder, elbow, wrist, left/right hip knee, ankle, head top and neck, as <ref type="bibr" target="#b37">[38]</ref>. <ref type="table">Table 3</ref> illustrates the performance comparisons between our RPSM with compared methods. It is obvious that our RPSM model obtains substantially lower 3D pose errors than the compared methods, and achieves new state-of-theart performance on all Walking, Jogging and Boxing sequences. In addition, in terms of the time efficiency, compared with <ref type="bibr" target="#b2">[3]</ref> which takes around three minutes per image and <ref type="bibr" target="#b37">[38]</ref> which takes more than 25 seconds per im-age, our RPSM model only costs 50ms per image. This demonstrate the effectiveness and efficiency of our proposed RPSM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Component Analysis</head><p>Effectiveness of multi-stage refinement: To validate the superiority of the proposed multi-stage refinement of our RPSM, we conduct the following experiment: employing one, two, three stages for human pose estimation and denote them as "RPSM-1-stage", "RPSM-2-stage" and "RPSM-3-stage". The evaluations are performed on the Human3.6M dataset from the qualitative and quantitative aspects. The top five rows of <ref type="table" target="#tab_7">Table 4</ref> illustrates the comparisons of estimating 3D pose errors for using different number of stages. As one can see from <ref type="table" target="#tab_7">Table 4</ref>, the performance increases monotonically within 3 stages. Moreover, the single/multi-stage performance without temporal dependency is also compared in <ref type="table">Table.</ref> 4 (denoted as "RPSM-1stage seq 1" and "RPSM-3stage seq 1", respectively). As illustrated in <ref type="table">Table.</ref> 4, RPSM-3stage seq 1 has achieved <ref type="figure">Figure 5</ref>: Qualitative comparisons of different stage refinement on Human3.6M dataset. The estimated 3D skeletons are reprojected into the images and shown by themselves from the side view (next to the images). The figures from left to right correspond to the estimated 3D poses generated by the 1st-stage, 2nd-stage, 3rd-stage of our RPSM and ground truth, respectively. We can observe that the predicted human 3D joints are progressively corrected along with the multi-stage sequential learning. Best viewed in color. much lower 3D pose errors than RPSM-1stage seq 1 (81.12 vs 90.83). This validates that the effectiveness of multistage refinement even when temporal information is ignored. Thanks to the exploited richer contextual information, our RPSM can learn more robust 2D pose-aware features and the representation of 3D pose sequences. Exemplar visual results on three different stages are shown in <ref type="figure">Fig. 5</ref>. It can be seen that the joint predictions are progressively corrected by performing multi-stage refinement.</p><p>Pre-training and Weight Sharing: To evaluate the performance without pre-training, we have only employed Hu-man3.6m 2D pose data and annotations to train the 2D pose module. We denote this version of our RPSM as "RPSM-3-stage no MPII". The result is reported in the bottom two rows of <ref type="table">Table.</ref> 4. As one can see from <ref type="table">Table.</ref> 4, RPSM-3-stage no MPII performs quite worse than RPSM-3-stage. This may be due to that Human3.6m 2D pose data, compared with MPII dataset, is less challenging for CNN to learn a rich 2D pose presentation. Note that according to the bottom row of Table. 4, the performance of sharing all layers in the 2D pose module (denoted as "RPSM-3stagesharing") is slightly better than the partially sharing one (denoted as "RPSM-3-stage"). However, the training time will significantly increased. Thus, we decide to choose partially sharing manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of temporal dependency:</head><p>To study the effectiveness of incorporating temporal dependency, we also evaluate the variants of our single-stage RPSM using different clip lengths, i.e., 1, 5 and 10, named as "RPSM-1stageseq C", where C denotes the frame number of clips for training. Note that, when C is equal to 1, no temporal information is considered and thus the recurrent LSTM layer in 3D pose errors is replaced with a fully connected layer with the same units as the LSTM. Results of using different clip length are reported in <ref type="table" target="#tab_7">Table 4</ref>. From the comparison results, the importance of temporal dependency is well demonstrated. Considering temporal dependency methods (i.e., RPSM 1stage seq 5 and RPSM 1stage seq 10) all outperform the RPSM 1stage seq 1 in a clear margin (about 10% reduction of the mean joint errors on the Hu-man3.6M dataset). The minor performance difference between RPSM 1stage seq 5 and RPSM 1stage seq 10 may be due to effect of temporal inconsistency, which has higher probability to occur in long clips. Moreover, it also should be noted that "RPSM 1stage seq 1" shows superiority over all state-of-the-art approaches owing to the contribution of the proposed 2D pose module and feature adaption module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel Recurrent 3D Pose Sequence Machines (RPSM) for estimating 3D human pose from a sequence of monocular images. Through the proposed unified architecture with 2D pose, feature adaption and 3D pose recurrent modules, our RPSM can learn to recurrently integrate rich spatio-temporal long-range dependencies in an implicit and comprehensive way. We also proposed to employ multiple sequential stages to refine the estimation results via the 3D pose geometry information. The extensive evaluations on two public 3D human pose dataset validate the effectiveness and superior performance of the our RPSM. In future work, we will extend the proposed framework for other sequence-based human centric analysis such as human action and activity recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Detailed network architecture of our proposed RPSM at the k-th stage. An input frame with the 368 ? 368 size is subsequently fed into 2D pose module, feature adaption module and 3D pose recurrent module to predict the locations of 17 joint points (51 dimensions output). The 2D pose module consists of 15 shared convolution layers across all stages and 2 specialized convolution layers for each stage. The specialized convolution layers take the shared features and the 2D poseaware features at previous stage as the input, and output specialized features to the feature adaption module as well as the next stage. The feature adaption module consists of two convolution layers and one fully-connected layer with 1024 units. Finally, the adapted features, the hidden states of the LSTM layer and previously predicted 3D poses are concatenated together as the input of 3D pose recurrent module to produce the 3D pose of each frame. The symbol ? means the concatenation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Empirical study on the qualitative comparisons on Human3.6M dataset. The 3D pose are visualized from the side view and the camera are also depicted. Zhou et al. [41], Zhou et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Details of the shared convolutional layers in 2D pose module. zero of the same size with those of other stages, and H k 0</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>are 64,64,  128, 128, 256, 256, 256, 256, 512, 512, 256, 256, 256, 256    and 128, respectively (please seeTable 1for more details). Moreover, we append the Rectified Linear Unit(ReLU) layers on all convolution layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method Direction Discuss Eating Greet Phone Pose Purchase Sitting SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg. LinKDE [16] 132.71 183.55 132.37 164.39 162.12 150.61 171.31 151.57 243.03 162.14 205.94 170.69 96.60 177.13 127.88 162.14 Li et al. Tekin et al. [30] 102.39 158.52 87.95 126.83 118.37 114.69 107.61 136.15 205.65 118.21 185.02 146.66 65.86 128.11 77.21 125.28 Zhou et al. [41] 87.36 109.31 87.05 103.16 116.18 106.88 99.78 124.52 199.23 107.42 143.32 118.09 79.39 114.23 97.70 113.01 Zhou et al. [40] 91.83 102.41 96.95 98.75 113.35 90.04 93.84 132.16 158.97 106.91 125.22 94.41 79.02 126.04 98.96 107.26 Du et al. [9] 85.07 112.68 104.90 122.05 139.08 105.93 166.16 117.49 226.94 120.02 135.91 117.65 99.26 137.36 106.54 126.47 Sanzari et al. [24] 48.82 56.31 95.98 84.78 96.47 66.30 107.41 116.89 129.63 97.84 105.58 65.94 92.58 130.46 102.21 93.15 Ours 58.02 68.16 63.25 65.77 75.26 61.16 65.71 98.65 127.68 70.37 93.05 68.17 50.63 72.94 57.74 73.10</figDesc><table><row><cell>[20]</cell><cell>-</cell><cell>136.88 96.94 124.74</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>168.68</cell><cell>-</cell><cell>69.97 132.17</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons on Human3.6M dataset using 3D pose errors (in millimeter) for different actions of subjects 9 and 11. The entries with the smallest 3D pose errors for each category are bold-faced. Our RPSM achieves the significant improvement over all compared state-of-the-art approaches, i.e. reduces mean error by 21.52%.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell></cell><cell cols="2">Jogging</cell><cell></cell><cell></cell><cell cols="2">Boxing</cell><cell></cell></row><row><cell>Methods</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell></row><row><cell>Simo-Serra et al. [28]</cell><cell>99.6</cell><cell>108.3</cell><cell>127.4</cell><cell>111.8</cell><cell>109.2</cell><cell>93.1</cell><cell>115.8</cell><cell>108.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Radwan et al. [21]</cell><cell>75.1</cell><cell>99.8</cell><cell>93.8</cell><cell>89.6</cell><cell>79.2</cell><cell>89.8</cell><cell>99.4</cell><cell>89.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. [33]</cell><cell>71.9</cell><cell>75.7</cell><cell>85.3</cell><cell>77.6</cell><cell>62.6</cell><cell>77.7</cell><cell>54.4</cell><cell>71.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Du et al. [9]</cell><cell>62.2</cell><cell>61.9</cell><cell>69.2</cell><cell>64.4</cell><cell>56.3</cell><cell>59.3</cell><cell>59.3</cell><cell>58.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Simo-Serra et al. [27]</cell><cell>65.1</cell><cell>48.6</cell><cell>73.5</cell><cell>62.4</cell><cell>74.2</cell><cell>46.6</cell><cell>32.2</cell><cell>56.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bo et al. [3]</cell><cell>45.4</cell><cell>28.3</cell><cell>62.3</cell><cell>45.3</cell><cell>55.1</cell><cell>43.2</cell><cell>37.4</cell><cell>45.2</cell><cell>42.5</cell><cell>64.0</cell><cell>69.3</cell><cell>58.6</cell></row><row><cell>Kostrikov et al. [18]</cell><cell>44.0</cell><cell>30.9</cell><cell>41.7</cell><cell>38.9</cell><cell>57.2</cell><cell>35.0</cell><cell>33.3</cell><cell>40.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin et al. [30]</cell><cell>37.5</cell><cell>25.1</cell><cell>49.2</cell><cell>37.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.5</cell><cell>61.7</cell><cell>57.5</cell><cell>56.6</cell></row><row><cell>Yasin et al. [38]</cell><cell>35.8</cell><cell>32.4</cell><cell>41.6</cell><cell>36.6</cell><cell>46.6</cell><cell>41.4</cell><cell>35.4</cell><cell>38.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>26.5</cell><cell>20.7</cell><cell>38.0</cell><cell>28.4</cell><cell>41.0</cell><cell>29.7</cell><cell>29.1</cell><cell>33.2</cell><cell>39.4</cell><cell>57.8</cell><cell>61.2</cell><cell>52.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>68.50 65.64 68.18 78.41 62.82 67.04 100.63 136.72 73.35 96.87 67.96 51.64 77.27 59.31 75.55 RPSM-3-stage 58.02 68.16 63.25 65.77 75.26 61.16 65.71 98.65 127.68 70.37 93.05 68.17 50.63 72.94 57.74 73.10 RPSM 1stage seq 1 70.46 83.36 76.46 80.96 88.14 76.00 92.39 116.62 163.14 85.87 111.46 83.60 65.38 95.10 73.54 90.83 RPSM 3stage seq 1 61.94 75.84 65.25 71.28 79.39 67.73 77.88 105.47 153.58 76.01 101.84 74.12 56.07 85.63 64.78 81.12 RPSM 1stage seq 5 62.89 74.74 67.86 73.33 79.76 67.48 76.19 100.21 148.03 75.95 100.26 75.82 58.03 78.74 62.93 80.15 RPSM 1stage seq 10 66.73 76.82 73.57 76.56 84.80 70.57 75.44 110.70 143.10 80.35 103.61 75.66 58.52 80.55 66.19 82.88 RPSM-3-stage no MPII 91.58 109.35 93.28 98.52 102.16 93.87 118.15 134.94 190.6 109.39 121.49 101.82 88.69 110.14 105.56 111.3 RPSM-3-stage sharing 58.36 66.52 63.37 64.5 72.22 59.39 63.9 90.73 129.99 68.26 93.86 65.22 48.47 70.53 56.26 71.44</figDesc><table><row><cell>Method</cell><cell cols="2">Direction Discuss Eating Greet Phone Pose Purchase Sitting SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg.</cell></row><row><cell>RPSM-1-stage</cell><cell>62.89 74.74 67.86 73.33 79.76 67.48 76.19 100.21 148.03 75.95 100.26 75.82 58.03 78.74</cell><cell>62.93 80.15</cell></row><row><cell>RPSM-2-stage</cell><cell>58.96</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Top five rows: empirical study on different number of refinement stages. Middle two rows: empirical comparisons by different sequence lengths (i.e., 1, 5, 10) for each clip. Note that the results are evaluated by a single-stage RPSM. Bottom two rows: performance of RPSM variants. The entries with the smallest 3D pose errors on Human3.6m dataset for each category are bold-faced.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Merging pose estimates across space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video browsing-a study of user behavior in online vod services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Communication and Networks (IC-CCN)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop, number EPFL-CONF-192376</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Human-computer interaction. An Introduction to Cyberpsychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Errity</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intelligent video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Schenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="83" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Virtual Reality: Exploring the Brave New Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rheingold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Simon &amp; Schuster Adult Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Looselimbed people: Estimating 3d human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aleny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatio-temporal matching for human detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05317</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
