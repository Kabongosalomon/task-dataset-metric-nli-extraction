<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roya</forename><surname>Javadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<settlement>Burnaby</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Many Faces of Anger: A Multicultural Video Dataset of Negative Emotions in the Wild (MFA-Wild)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The portrayal of negative emotions such as anger can vary widely between cultures and contexts, depending on the acceptability of expressing full-blown emotions rather than suppression to maintain harmony. The majority of emotional datasets collect data under the broad label "anger", but social signals can range from annoyed, contemptuous, angry, furious, hateful, and more. In this work, we curated the first in-the-wild multicultural video dataset of emotions, and deeply explored anger-related emotional expressions by asking culture-fluent annotators to label the videos with 6 labels and 13 emojis in a multi-label framework. We provide a baseline multi-label classifier on our dataset, and show how emojis can be effectively used as a language-agnostic tool for annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Anger is a basic emotion suggested to be found around the world. Despite the universal theories that associated prototypical expressions such as bared teeth to anger, depictions of anger come in a variety of social signals and arousal levels <ref type="bibr" target="#b1">[2]</ref>. Furthermore, some research <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b9">[10]</ref> suggests that more collectivist cultures tend to suppress negative emotions to maintain harmony. These reasons imply that correctly recognizing anger in videos is highly dependent on the samples fed into models for emotion recognition tasks.</p><p>Automatic emotion expression recognition for anger samples is relatively low, and deserves more investigation. In <ref type="bibr" target="#b24">[25]</ref> for example, authors found that using both AffectNet and FER+, resulted in accuracy between 45% and 54% for contempt, anger, and disgust on the AffectNet dataset, while happiness had 77% accuracy rate. Similarly in <ref type="bibr" target="#b15">[16]</ref> where images were multi-labeled with 26 categories, after 'aversion' with an average precision of 7.48, anger was the worst recognized basic emotion with an average precision of 9.49, followed by fear <ref type="bibr">(14.14)</ref>, surprise (18.81), sadness <ref type="bibr">(19.66)</ref> and happiness (60.69). Hence, a deeper understanding of the anger emotion is necessary.</p><p>Most of the work in this field is on images, but the dynamics of face and head are also important. Therefore, we decide to focus on dynamic representations of emotions, such as videos. A major challenge in emotion recognition in videos is to find a high-quality dataset, which includes both high-quality videos and their corresponding labels. In particular, it is important to gather in-the-wild datasets, as lab-acted datasets such as <ref type="bibr" target="#b25">[26]</ref> may not well represent the variety of possible expressions. Among in-the-wild datasets,</p><p>We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN/06908-2019. We also thank people who helped us in collection and labeling of the data, especially Emma Hughson. GIFGIF+ <ref type="bibr" target="#b5">[6]</ref> introduced by MIT is a relatively massive and realistic video dataset that consists of more than 23,000 animated GIFs over 17 emotions. The authors' dataset collection method is semi-automatic, meaning that they used both human labour and clustering techniques to label GIF samples. Aff-Wild2 <ref type="bibr" target="#b14">[15]</ref> is another in-the-wild video dataset containing 260 videos that are used for estimation of the valence and arousal emotion dimensions. A variety of subjects, movements, and context is also another key point in video datasets. The Affectiva-MIT Facial Expression Dataset (AM-FED) database <ref type="bibr" target="#b18">[19]</ref> contains 242 facial videos of people watching Super Bowl commercials using their webcam. It has frame-by-frame annotations of 14 Action Units (AU), head movements, and facial landmarks, but there is not much variance in head poses and subjects, as all participants are reacting to videos while sitting in front of a computer.</p><p>Psychological research indicates that people express emotions differently depending on their age, gender or culture <ref type="bibr" target="#b16">[17]</ref>. A few datasets have focused on underrepresented groups in emotion recognition, namely EmoReact <ref type="bibr" target="#b19">[20]</ref>, which contains videos of children in the wild reacting to objects and answering questions about them, and ElderReact <ref type="bibr" target="#b17">[18]</ref> where older adults react to videos and express their opinion. Both of them collected in-the-wild videos from the same channel on YouTube, containing videos of people seated at a desk.</p><p>Annotation schemes in affective computing still have room for improvement. For instance, the broad emotion label of anger is problematic for several reasons. Firstly, there is a range of anger-related phenomena which could be more precisely recognized with a more specific word. Similar to how the word "mammal" can encompass a wide range of animals, more specific words such as "cat" or "dog" can provide better labels. Here, we consider labels such as annoyed, furious, hatred, contempt and disgust. Secondly, emotion "readings" can be highly subjective theory of mind exercises, which can depend on the annotator or their own internal state. An alternate scheme is to provide objective behavioral descriptions of facial expressions (e.g. frowning, rolling eyes), similar to AU recognition but at a higher level. Here, we explore representations of social signals using emojis, which in addition are language-agnostic. Using emojis for labeling has gained popularity recently. Saheb Jam et al. <ref type="bibr" target="#b22">[23]</ref> used emojis to annotate emotional expressions in videos of interaction between a human and a robot. Vemulapalli et al. have used emojis to build an embedding space for facial expressions <ref type="bibr" target="#b26">[27]</ref>. Herein, we used 13 emojis related to 6 emotion labels of 'annoyed', 'anger', 'fury', 'hatred', 'disgust', 'contempt' in order to investigate whether mapping emojis to their emotion category can improve languageagnostic annotations. Thirdly, emotional expressions can be mixed <ref type="bibr" target="#b6">[7]</ref>. While fields such as text, speech, and music emotion processing have readily accepted the multi-label paradigm, video-based approaches still assume one label per sample, throwing out data that does not have high interrater agreement or use single-label classifiers on datasets that have multi-labels for each sample. As noted by <ref type="bibr" target="#b13">[14]</ref>, "ambiguous, subtle expressions of emotion, which often obtain no majority agreement from human annotators, are prevalent in the real world". Since we are working on in-thewild data, a multi-label baseline is an important component in this work.</p><p>To summarize, the main contributions of this study are collecting data from underrepresented cultures in the anger category, as well as annotating them by people from the same culture. The majority of datasets usually focus on Western-Caucasian subjects and we rarely see other cultures or ethnicities such as East-Asian or West-Asian. In this research, we tried to bridge this gap by collecting data from underrepresented cultures from Middle East, which to the best of our knowledge has never been done before. A few works such as <ref type="bibr" target="#b2">[3]</ref> designed feature extractors and a classifier on a multi-cultural (East-Asian and Western-Caucasian) image dataset. Khanh et al. also collected a Korean emotion dataset from Korean movies and used a Multi-Layer Perceptron to classify them into basic emotions. They showed that training the model using Korean videos and testing on English videos and vice-versa yielded the worst result <ref type="bibr" target="#b12">[13]</ref>. Another contribution of ours is a collection of social signals for each video in the dataset, aiming to identify culture-dependent facial expressions in emotions.</p><p>In this paper, we first describe the process of raw data collection. Then, we outline the emotion and emoji annotation procedure. Finally, we present dataset statistics and a baseline classification for future benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA COLLECTION</head><p>The massive amount of videos on YouTube is a great asset for researchers. A major challenge that remains to be addressed is extending videos with in-the-wild or close to in-the-wild emotions. We collected more than 200 videos on YouTube from Persian and North American (NA) cultures. All Persian videos were collected from TV series and movies accessible on YouTube, and NA videos included movies, "vlog" style content, reality television shows (e.g., Dance Moms), and talk shows (e.g., The Late Show with James Corden). Since some videos were too long for our purpose or had multiple actors, we trimmed, split, or removed them. In the end, there were 97 videos for Persian and 104 videos for NA culture, each lasting between 1 to 10 seconds.</p><p>Our main challenge in label collection was that crowdsourcing platforms such as Amazon Mechanical Turk (AMT) do not provide a facility to choose the culture of annotators. Thus, we decided to design and implement a web interface We designed an interface using Flask 1 and an SQLite database. Each annotator registered on the website and accepted a Research Consent form. We only asked for general information about the culture, language, and perceived individualism of people. No directly identifiable information was collected during the study. The participants were allowed to withdraw from the study at any time. An Amazon gift card code corresponding to approximately the minimum wage in Canada was emitted upon completion of or withdrawal from the study, and this study was approved by the university research ethics board. In total, we recruited 10 people from each culture via social media. The audio was removed from the videos. Each user annotated half of the videos of their culture, resulting in 5 annotations per clip.</p><p>1) Emotion Labels: We defined 6 negative emotions for labels: Contemptuous, Annoyed, Anger, Hatred, Furious, and Disgusted. We also added a "None" option. Annotators were allowed to select more than one label and leave their idea or thoughts in a comment box. The annotations were done independently and blindly, meaning that annotators did not have any information about each other's labels.</p><p>2) Emoji Labels: In this study, we also included 14 emoji annotations to extract underlying subtle social signals of each emotion. The emojis that users could select is presented in <ref type="figure">Fig. 2</ref>. Pilot annotations among researchers suggested difficulty in obtaining unanimity in labeling; in many samples, people selected multiple emotions. This gave us a clue that allowing multiple labels for videos would be better than restricting the labels to only one. We accumulated all the emotion and emoji labels belonging to each video and did majority voting to derive final labels. If two or more labels had equal votes, we attached all those labels to the video. For example, a video that has 2 votes for annoyed, 2 votes for anger, and 1 vote for contempt, is assigned both anger and annoyed. <ref type="figure">Fig. 4</ref> shows the distribution of the number of labels.</p><p>3) Social Signals Annotation: In order to explore what social signals can be found in emotional expressions, one researcher from NA cultural background and one from Persian culture extracted visible face and body expressions for each video. Since social signal annotations such as raised brows, arms crossed, turning head, etc. are relatively objective (compared to emotion labels), both researchers reviewed all videos for this annotation step. Overall, they had a consensus for over 90% of videos. The analysis we conducted on these social signals aimed to identify the cultural differences in emotion expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATASET STATISTICS</head><p>Initially, our dataset aimed to cover the emotions of Contempt, Anger, and Disgust (also known as the CAD triad <ref type="bibr" target="#b21">[22]</ref>). The first steps of annotation revealed that annotators perceived some fine-grained emotions like annoyed, as well as complex emotions such as disgust-anger or contemptdisgust. Therefore, we decided to further refine the labels and allow people to choose several emotions. We emphasize that the purpose of this study is not to obtain perfect agreement, but to consider that, similar to <ref type="bibr" target="#b13">[14]</ref>, the distribution over annotators is the ground truth for this challenging in-thewild data. Nevertheless, as a descriptive measure, we used Krippendorff's Alpha (used when multiple labels can be chosen) to calculate the agreement between annotations. The agreement scores for NA and Persian emotion dataset were 0.252 and 0.076, respectively. Since Jeni et al. <ref type="bibr" target="#b11">[12]</ref> indicated that label imbalance can dramatically affect metrics such as Krippendorff's Alpha, and we can see that labels were indeed imbalanced when observing the number of videos for each emotion/emoji in <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 6</ref>.</p><p>We calculated the mean of the annotator's confidence ratings for each label, which you can see in <ref type="figure" target="#fig_0">Fig. 1</ref>. The noticeable difference between the two cultures is the confidence for disgust, which is higher in NA. That is because most disgust videos in NA dataset were reactions to foods, whereas in Persian 'disgust' videos were social disgust, hence more challenging to label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Co-Occurrence of Emoji-Emotions</head><p>We calculated which emojis co-occur with which emotions. <ref type="figure" target="#fig_0">Fig. 13</ref> shows the emojis that appeared in more than 15% of the videos for a specific label. Interestingly, annotators associated red emojis with higher arousal forms of anger, which supports the research that red faces map to higher levels of anger <ref type="bibr" target="#b10">[11]</ref>. Also, annotators chose other emojis like over for contempt. In NA dataset, we see an abundance of 'nauseated' emoji for disgust while in Persian annotations there is none; this is due to the videos in NA that react to food, as we mentioned. Another interesting finding from this analysis is that the label "contempt" is not fine-grained enough; users selected the emoji when it was combined with social disgust, while they preferred for contempt alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Correlation between emotions</head><p>We think the correlation between emotions may shed light on how emotions are related to each other, so we computed the Pearson correlation between the labels given by annotators (i.e. before voting the labels). The results are in <ref type="figure">Fig. 9</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref>, which provides us another view of the relationship between negative emotions. It is not surprising that anger, furious and hatred have a relatively high positive correlation. It is worth noting that disgust and hatred have a positive correlation in the Persian dataset while their correlation is negative in NA dataset. Disgust and contempt show different patterns as well. The correlation between them is positive in Persian and negative in NA. Such differences may suggest that we should distinguish between 'social disgust' and 'physical disgust'. Social disgust might be a mix of hatred, annoyance and contempt according to the result, whereas 'physical disgust' can be considered a basic emotion <ref type="bibr" target="#b7">[8]</ref>. These tables also show the importance of having multiple labels for samples in order to capture compound negative feelings such as 'angry and disgusted' or 'contemptuous and disgusted', which are seen in Persian dataset labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Unit Analysis</head><p>For all videos under each emotion category, we calculated the mean of peaks of each AU (one value per AU, per video) and visualized it using a radar chart in <ref type="table">Table I. This table  can</ref> give a better view of differences between cultures. In our dataset, the AU values for Persian are generally smaller than NA. We see noticeable differences in the activated AUs in contempt, annoyed, hatred, and anger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Co-Occurrence of Social Signals-Emotions</head><p>Another purpose of this study is to identify the key social signals for each emotion. We extracted co-occurrences of facial expressions (and body movements, such as arms crossed) with final labels, presented in <ref type="figure">Fig. 7</ref> for NA and <ref type="figure" target="#fig_4">Fig. 8</ref> for Persian. Each cell in the heatmap is normalized by the number of samples under each emotion class. Mocking in NA videos and Smiling social signal in Persian had 0 occurrences so their corresponding row is empty. Persian videos exhibited a wider variety of social signals except in disgust, which may be attributed to the low number of videos in this class. 'Eyebrows pushed together' was the most prevalent of all social signals in all emotions, though it was more common in Persian. In the Persian results, we notice some similarities between disgust and hatred social signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we describe preprocessing of the data and the baseline classification that we performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Selection and Cleaning</head><p>Our analysis showed that the type of disgust collected in our dataset was heterogeneous, containing both physical and social disgust, therefore, prior to doing classification, we removed the 'disgust' label, its associated emojis ( and ) and videos which only had 'disgust' as label, focusing on the anger affect. We also removed 2 emojis that appeared least of all:</p><p>and . In the end, we performed our experiments on 74 videos from NA and 91 videos from Persian culture.</p><p>Secondly, we checked the annotations for probable errors (e.g. checking the gender of actor to validate the annotation). In a few cases, the video was not properly loaded for the user, hence the actor's gender was inconsistent; we removed those annotations. In one video in which two actors appeared, some users annotated female and some male, so we removed these videos from our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Features</head><p>OpenFace <ref type="bibr" target="#b0">[1]</ref> was used to extract facial landmarks and movements, gaze direction, and head movement for each frame in a given video. We collected 17 Action Unit (AU) attributes (AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU12, AU14, AU15, AU17, AU23, AU25, AU26, AU45) describing relative values for each AU, head rotation values in 3-dimensional space and gaze direction. In addition to these features, success level, confidence level, frame number and timestamp of each frame were also collected.</p><p>During preprocessing, we only kept the frames where the face tracking resulted in a confidence level of above 85% and a success value of 1. We performed min-max feature normalization because the range of values for the aforementioned features differed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-label Classification</head><p>Our dataset analysis showed multiple labels for many videos -especially emoji labels. Work by Du et al. <ref type="bibr" target="#b6">[7]</ref> also showed that emotions can be mixtures, therefore, we used multi-label, multi-class classifiers for our baseline. Traditional classifiers output a single label for each sample. In order to use them for multi-label classification, we can use either of the following approaches: 1) Adapt existing algorithms to output multiple labels, 2) Transform the problem into another form, e.g. performing binary classification for each label. In the latter case, we can use several classifiers that may or may not be independent from each other. For our baselines, we used Multi-Label K-Nearest Neighbors (MLKNN) <ref type="bibr" target="#b27">[28]</ref> for the adaptive approach and Classifier Chains (CC) <ref type="bibr" target="#b20">[21]</ref> for the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classifier Chains</head><p>For a given set of labels L the CC model learns |L| classifiers in which all classifiers are linked in a chain through 22 features. The dataset is transformed in |L| data sets where instances of j-th data set has the form ((x i , l 1 , ..., l j?1 ), l j ), l j ? 0, 1.</p><p>The advantage of classifier chains method is that it is capable of taking correlations between labels into account while maintaining acceptable computational complexity since the output of previous classifiers is fed into the next ones as additional features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training and Testing</head><p>Before the classification step, we separated 25% of the videos for the test phase. We performed 5-fold crossvalidation on the rest to obtain the best order of classifiers for the chain and for the parameter k in MLKNN. In the classifier chains, we used the XGBoost <ref type="bibr" target="#b4">[5]</ref> classifier, since  it showed promising results on similar datasets <ref type="bibr" target="#b17">[18]</ref>. We input all permutations of the order of labels in the classifier chain and selected the order that yielded the best result on the validation set. The inputs to the model were AUs, head rotation, and gaze angle for each frame, and then the model outputs set of labels for that frame. We report the sample average of F1-score for all of our models since it is widely used in multi-label classification models. We computed these  <ref type="figure">9</ref>. Correlation between annotators' assigned labels -NA <ref type="figure" target="#fig_0">Fig. 10</ref>. Correlation between annotators' assigned labels -Persian metrics in two ways: 1) for each frame, and 2) assigning the label to the whole video by taking the majority of predicted labels on the frames. The results on emotion labels are presented in <ref type="table" target="#tab_0">Table II</ref> where CC=Classifier Chains, F-F1 score = Frame-level F1-score, V-F1 score = Video-level F1-score and Comb. = Combined. We also combined NA and Persian datasets for all classification models to investigate the result of a culturally heterogeneous dataset. The results are reported under the Combined column in <ref type="table" target="#tab_0">Table II to Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Baseline Classification</head><p>We performed three types of classification as follows: 1) Multi-label Classification Using Emotion Words: First, we used word labels of emotions (annoyed, anger, hatred, furious, contempt, none) to perform classification.</p><p>2) Multi-label Classification Using Emojis: In this experiment, we used 10 emoji labels to perform a multi-label classification. The results are in <ref type="table" target="#tab_0">Table II to Table IV.</ref> 3) Hierarchical Classification Using Emojis: To explore the role of non-lexical labels, we conducted a hierarchical classification using emoji labels. First, we fed in our training and test set into the classifiers similar to what we did with emoji labels, and then mapped each predicted emoji label to its emotion class. The training and test sets were the same as our emotion classification experiment. We used <ref type="figure" target="#fig_0">Fig. 13</ref> to create a mapping of each emoji to its corresponding emotion.</p><p>We mapped or to contempt, or or to annoyed, to anger, or to hatred, to furious and to none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Results</head><p>Overall, NA scores were better than Persian. Video-level scores did not follow a pattern; in some cases they were more than frame-level scores and in some cases less than framelevel. This change in results from frame-level to video-level is highly dependent on test videos. If a test video had a short length (e.g. 50 frames) and the model misclassified a large portion of its frames, it affected video-level scores more than frame-level scores since the number of frames is far greater than the number of videos (2000 frames vs 15 videos in test set). In emotion classification where word labels are used, the difference between NA and Persian is more than emoji and hierarchical classification. In hierarchical and emoji label classification, Persian's video-level F1-score is highest of all. Combining the two datasets did not yield better results, usually with results between the two scores of Persian and NA, or less than both, except in emoji classification using the MLKNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND LIMITATIONS</head><p>As shown by the result of data collection, there is always some ambiguity between different emotions. We should also note that the object to which people are reacting may affect the way it is expressed. The result of emoji and hierarchical classification suggests that sometimes non-verbal labels may be better alternative for word labels, especially in crosscultural research where language might be a barrier. This is very useful in crowd-sourcing platforms like AMT, where labels usually are in English, but annotators' first language may not be. In this study, we cannot completely eliminate the effect of the questionnaire's language. Although people who participated in Persian dataset were competent in English, they likely lacked a deep emotional connection with it and their affective processing may have been weaker <ref type="bibr" target="#b3">[4]</ref>. It was also more difficult to collect high-quality, emotionally-rich videos in Persian due to lack of resources. For example,  reaction videos akin to YouTube React channel are almost non-existent for Persian, which makes it an obstacle for this type of research on low-resource cultures. Additionally, we had label-imbalance in our dataset that can negatively affect the result. Another technical limitation is the features that we extracted through OpenFace <ref type="bibr" target="#b0">[1]</ref>. OpenFace only extracts 17 AUs out of 30 main AUs. Finally, emotional expressiveness may affect the result of classifications. Research suggests that more conservative cultures do not express negative emotions such as anger blatantly. We found supporting evidence in the radar charts of Table I that activated AUs in Persian have smaller values. This adds to the complexity of predictive models training on more conservative cultures like Persian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conclusion</head><p>The main contribution of this paper is collection of a multi-cultural dataset of videos annotated with (a) affect labels under anger category, (b) their associated emojis, (c) social signals for building more robust emotion recognition models for underrepresented cultures. We conducted statistical analyses to find the underlying expressions of each emotion and compare NA and Persian cultures in terms of emotion expression. Moreover, we provided multi-label classification baseline models that demonstrated how emojis can be used instead of or in addition to word-labels. In order to examine the effectiveness of non-verbal labels, we built a similar model, this time with emoji labels. This opens the opportunity to language-agnostic labels, especially in crosscultural emotion studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Future Work</head><p>We collected the presented dataset with intention to classify them as a set of features varying over time (i.e. a multifeature timeseries). However, there is no machine learning model that is adaptable to multi-label classification of multifeature timeseries. Hence, a great improvement on the computational aspect of this field would be designing multi-label classifiers that are capable of handling 3-dimensional data (time, features and samples). We are also interested to extend the approach presented here to positive affects such as joy and related emotion such as happiness, surprise and cheerful. Future work could compare the result using English labels fully translated to Persian (and back-translated to ensure accuracy). Augmenting the dataset with more videos (synthetically or generated) will allow us to use transfer learning and deep neural network algorithms and investigate their effectiveness. The purpose of this study was not applying state of the art deep learning models for two main reasons. First, they are black boxes and extracted feature embeddings are harder to interpret (e.g. compared to AUs). Secondly, our dataset is much smaller than the aforementioned datasets and even transfer learning may lead to overfitting on training data. Future research may investigate the accuracy of these models. We also collected videos from Filipino culture, but due to the low number of annotators we omitted them. We look forward to applying the method to Filipino and other cultural datasets. Training the model on one culture and testing it on another culture can also be a matter of investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Annotators' confidence for labels Fig. 2. Emoji labels and collect culturally fluent annotators in Canada. Annotators were 19+ years old residents of Canada who were fluent in English, as well as Farsi if annotating Persian videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Frequency of emotion labels Distribution of the number of emotion labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Distribution of the number of emoji labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Frequency of emoji labelsTABLE I RADAR CHARTS OF THE MEAN OF THE PEAK OF AUS -NUMBERS AROUND EACH CHART INDICATE AU INDEX. Co-occurrences of social signals for each emotion in NA videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Co-occurrences of social signals for each emotion in Persian videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Examples of NA dataset with their labels Examples of Persian dataset with their labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 13 .</head><label>13</label><figDesc>Most common emojis for each emotion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="7">MULTI-LABEL CLASSIFICATION RESULTS USING 6 EMOTION</cell></row><row><cell></cell><cell></cell><cell cols="2">CATEGORIES</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>F-F1 score</cell><cell></cell><cell></cell><cell>V-F1 score</cell><cell></cell></row><row><cell>Model</cell><cell>NA</cell><cell cols="2">Persian Comb.</cell><cell>NA</cell><cell cols="2">Persian Comb.</cell></row><row><cell cols="2">CC -XGB 0.42</cell><cell>0.28</cell><cell>0.33</cell><cell>0.40</cell><cell>0.33</cell><cell>0.36</cell></row><row><cell>MLKNN</cell><cell>0.42</cell><cell>0.31</cell><cell>0.34</cell><cell>0.42</cell><cell>0.40</cell><cell>0.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III MULTI</head><label>III</label><figDesc></figDesc><table><row><cell cols="7">-LABEL CLASSIFICATION USING 10 EMOJI CATEGORIES</cell></row><row><cell></cell><cell></cell><cell>F-F1 score</cell><cell></cell><cell></cell><cell>V-F1 score</cell><cell></cell></row><row><cell>Model</cell><cell>NA</cell><cell cols="2">Persian Comb.</cell><cell>NA</cell><cell cols="2">Persian Comb.</cell></row><row><cell>CC -XGB</cell><cell>0.24</cell><cell>0.22</cell><cell>0.20</cell><cell>0.28</cell><cell>0.31</cell><cell>0.29</cell></row><row><cell>MLKNN</cell><cell>0.27</cell><cell>0.22</cell><cell>0.25</cell><cell>0.28</cell><cell>0.27</cell><cell>0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV BASELINE</head><label>IV</label><figDesc>OF HIERARCHICAL CLASSIFICATION USING 10 EMOJI LABELS GROUPED INTO 6 LABELS</figDesc><table><row><cell></cell><cell></cell><cell>F-F1 score</cell><cell></cell><cell></cell><cell>V-F1 score</cell><cell></cell></row><row><cell>Model</cell><cell>NA</cell><cell cols="2">Persian Combined</cell><cell>NA</cell><cell cols="2">Persian Combined</cell></row><row><cell cols="2">CC -XGBoost 0.34</cell><cell>0.28</cell><cell>0.21</cell><cell>0.32</cell><cell>0.36</cell><cell>0.28</cell></row><row><cell>MLKNN</cell><cell>0.38</cell><cell>0.30</cell><cell>0.28</cell><cell>0.39</cell><cell>0.32</cell><cell>0.34</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://flask.palletsprojects.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OpenFace: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multicultural facial expression recognition based on differences of western-caucasian and east-asian facial expressions of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Benitez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotionality differences between a native and foreign language: Implications for everyday life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Caldwell-Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<title level="m">Xgboost. International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gifgif+: Collecting emotional animated gifs with clustered multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; emotion</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differences between cultures in emotional verbal and nonverbal reactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>P?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Candia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psicothema</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dimensionalizing cultures: The Hofstede model in context. Online Readings in Psychology and Culture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hofstede</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Influence of color on emotion recognition is not bidirectional: An investigation of the association between color and emotion using a stroop-like task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ikeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological reports</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Facing imbalanced datarecommendations for the use of performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Korean video dataset for emotion recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-T</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-like emotion recognition: Multi-label learning from noisy labeled audio-visual expressive speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Aff-wild2: Extending the aff-wild database for affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context based emotion recognition using emotic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cultural differences in emotion: differences in emotional arousal level between the east and the west</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integrative Medicine Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ElderReact: a multimodal dataset for recognizing emotional response in aging adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected &quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaliouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EmoReact: A multimodal approach and dataset for recognizing emotional responses in children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>W. Buntine, M. Grobelnik, D. Mladeni?, and J. Shawe-Taylor</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The cad triad hypothesis: A mapping between three moral emotions (contempt, anger, disgust) and three moral codes (community, autonomy, divinity)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Developing a data-driven categorical taxonomy of emotional expressions in real world human robot interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saheb Jam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rhim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cultural differences in emotion suppression in belgian and japanese couples: A social functional model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirchner-H?usler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient facial feature learning with wide ensemble-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siqueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Moving faces, looking places: validation of the amsterdam dynamic facial expression set (adfes). Emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Schalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Hawk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doosje</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A compact embedding for facial expression similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ML-KNN: A lazy learning approach to multi-label learning. Pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
