<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatiotemporal CNN for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD Digits</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
							<email>liguorong@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
							<email>liefeng.bo@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD Digits</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">UCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Big Data Mining and Knowledge Management</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inst. of Computi. Tech</orgName>
								<orgName type="laboratory">Key Laboratory of Intell. Info. Process. (IIP)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatiotemporal CNN for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a unified, end-to-end trainable spatiotemporal CNN model for VOS, which consists of two branches, i.e., the temporal coherence branch and the spatial segmentation branch. Specifically, the temporal coherence branch pretrained in an adversarial fashion from unlabeled video data, is designed to capture the dynamic appearance and motion cues of video sequences to guide object segmentation. The spatial segmentation branch focuses on segmenting objects accurately based on the learned appearance and motion cues. To obtain accurate segmentation results, we design a coarse-to-fine process to sequentially apply a designed attention module on multi-scale feature maps, and concatenate them to produce the final prediction. In this way, the spatial segmentation branch is enforced to gradually concentrate on object regions. These two branches are jointly fine-tuned on video segmentation sequences in an end-to-end manner. Several experiments are carried out on three challenging datasets (i.e., DAVIS-2016, DAVIS-2017 and Youtube-Object)  to show that our method achieves favorable performance against the stateof-the-arts. Code is available at https://github. com/longyin880815/STCNN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) becomes a hot topic in recent years, which is a crucial step for many video analysis tasks, such as video summarization, video editing, and scene understanding. It aims to extract foreground objects from video clips. Existing VOS methods can be divided into two settings based on the degrees of human involvement, namely, unsupervised and semi-supervised. The unsupervised VOS methods <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref> do not require any manual annotation, while the semi-supervised * Corresponding author. methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18]</ref> rely on the annotated mask for objects in the first frame. In this paper, we are interested in the semi-supervised VOS task, which can be treated as the label propagation problem through the entire video. To maintain the temporal associations of object segments, optical flow is usually used in most of previous methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> to model the pixel consistency across the time for smoothness. However, optical flow annotation requires significant human effort, and estimation is challenging and often inaccurate, and thus it is not always helpful in video segmentation. To that end, Li et al. <ref type="bibr" target="#b32">[33]</ref> design an end-to-end trained deep recurrent network to segment and track objects in video simultaneously. Xu et al. <ref type="bibr" target="#b50">[51]</ref> present a sequence-to-sequence network to fully exploit long-term spatial-temporal information for VOS.</p><p>In contrast to the aforementioned methods, we design a spatiotemporal convolutional neural network (CNN) algorithm (denoted as STCNN, for short) for VOS, which is a unified, end-to-end trainable CNN. STCNN is formed by two branches, i.e., the temporal coherence branch and the spatial segmentation branch. The features in both branches are able to obtain useful gradient information during backpropagation. Specifically, the temporal coherence branch focuses on capturing the dynamic appearance and motion cues to provide the guidance of object segmentation, which is pre-trained in an adversarial manner from unlabeled video data following <ref type="bibr" target="#b23">[24]</ref>. The spatial segmentation branch is a fully convolutional network focusing on segmenting objects based on the learned appearance and motion cues from the temporal coherence branch. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we design a coarse-to-fine process to sequentially apply a designed attention module on multi-scale feature maps, and concatenate them to produce the final accurate prediction. In this way, the spatial segmentation branch is enforced to gradually concentrate on the object regions, which benefits both training and testing. These two branches are jointly fine-tuned on the video segmentation sequences (e.g., the training set in DAVIS-2016 <ref type="bibr" target="#b38">[39]</ref>) in an end-to-end man-ner. We conduct several experiments on three challenging datasets, i.e., DAVIS-2016 <ref type="bibr" target="#b38">[39]</ref>, DAVIS-2017 <ref type="bibr" target="#b39">[40]</ref> and Youtube-Object <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>, to demonstrate the effectiveness of the proposed method against the state-of-the-art methods. Specifically, our STCNN method produces 0.838 in mIOU for semi-supervised task on the DAVIS-2016 <ref type="bibr" target="#b38">[39]</ref>, and achieves the state-of-the-art results with 0.796 in mIoU on Youtube-Object <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Contributions. (1) We present a unified, end-to-end trainable spatiotemporal CNN algorithm for VOS without relying on optical flow, which is formed by two branches, i.e., spatial segmentation branch and temporal coherence branch. <ref type="bibr" target="#b1">(2)</ref> The temporal coherence branch is designed to capture the dynamic appearance and motion cues across the time to guide object segmentation, which is pre-trained in an adversarial manner from unlabeled video data. <ref type="formula" target="#formula_3">(3)</ref> We design a coarse-to-fine process to sequentially apply a designed attention module on multi-scale features maps, and concatenate them to produce the final accurate prediction. (4) Extensive experiments are conducted on three datasets, namely, DAVIS-2016, DAVIS-2017, and Youtube-Object, to demonstrate that the proposed method achieves favorable performance compared to the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-supervised video object segmentation.</p><p>Semisupervised VOS aims to segment video objects based on the preliminarily provided foreground regions, and propagates them to the remaining frames. In <ref type="bibr" target="#b0">[1]</ref>, a patch-based probabilistic graphical model is presented for semi-supervised VOS, which uses a temporal tree structure to link patches in adjacent frames to exactly infer the pixel labels in video. Jain et al. <ref type="bibr" target="#b19">[20]</ref> design a higher-order supervoxel label consistency potential for foreground region propagation, which leverages bottom-up supervoxels to guide the estimation towards long-range coherent regions. Wen et al. <ref type="bibr" target="#b47">[48]</ref> integrate the multi-part tracking and segmentation into a unified energy objective to handle the VOS, which is efficiently solved by a RANSAC-style approach. Tsai et al. <ref type="bibr" target="#b45">[46]</ref> jointly optimize VOS and optical flow estimation in a unified framework using an iterative scheme to exploit mutually bootstrapping information between the two tasks for better performance.</p><p>Recently, the deep neural network based methods dominate the VOS task. Khoreva et al. <ref type="bibr" target="#b25">[26]</ref> describe a CNNbased algorithm, which combines offline and online learning strategies, where the former produces a refined mask from the estimation of previous frame, and the latter aims to capture the appearance of the specific object instance. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> presents an end-to-end trainable network for simultaneously predicting pixel-wise object segmentation and optical flow in videos, which is pre-trained offline to learn a generic notion, and fine-tuned online for spe-cific objects. Caelles et al. <ref type="bibr" target="#b2">[3]</ref> design the one-shot video object segmentation (OSVOS) approach based on a fullyconvolutional neural network to transfer generic semantic information to tackle the video object segmentation task. After that, Voigtlaender et al. <ref type="bibr" target="#b46">[47]</ref> improve the OSVOS method by updating the network online using training examples selected based on the confidence of the network and the spatial configuration. The online updating strategy noticeably improves the accuracy but sacrifices the running efficiency. To tackle time-consuming finetuning stage in the first frame, Cheng et al. <ref type="bibr" target="#b5">[6]</ref> propose a fast VOS approach, which is formed by three modules, i.e., the part-based tracking, region-of-interest segmentation, and similarity-based aggregation. This method is able to immediately start to segment a specific object through the entire video fast and accurately. In <ref type="bibr" target="#b15">[16]</ref>, a recurrent neural net approach is proposed to fuse the outputs of a binary segmentation net providing a mask and a localization net providing a bounding box for each object instance in each frame, which is able to take advantage of long-term temporal structures of the video data as well as rejecting outliers. Bao et al. <ref type="bibr" target="#b1">[2]</ref> propose a spatio-temporal Markov Random Field (MRF) model for VOS, which uses a CNN to encode the spatial dependencies among pixels, and optical flow to establish the temporal dependencies. An efficient CNN-embedded algorithm is presented to perform approximate inference in the MRF to complete the VOS task.</p><p>Unsupervised video segmentation. Some unsupervised video segmentation algorithms use the bottom-up strategy to group spatial-temporal coherent tubes without any prior information. Xu et al. <ref type="bibr" target="#b49">[50]</ref> implement a graph-based hierarchical segmentation method within the streaming framework, which enforces a Markovian assumption on the video stream to approximate full video segmentation. Yu et al. <ref type="bibr" target="#b52">[53]</ref> propose an efficient and robust video segment algorithm based on parametric graph partitioning, that identifies and removes between-cluster edges to generate node clusters to complete video segmentation.</p><p>Several other unsupervised video segmentation methods upgrade bottom-up video segmentation to object-level segments. Lee et al. <ref type="bibr" target="#b29">[30]</ref> use the static and dynamic cues to identify object-like regions in any frame, and discover hypothesis object groups with persistent appearance and motion. Then, each ranked hypothesis is used to estimate a pixel-level object labeling across all frames. Li et al. <ref type="bibr" target="#b30">[31]</ref> track multiple holistic figure-ground segments simultaneously to generate video object proposals, which trains an online non-local appearance models for each track using a multi-output regularized least squares formulation. Papazoglou et al. <ref type="bibr" target="#b35">[36]</ref> present a fast unsupervised VOS method, which simply aggregates the pixels in video by combining two kinds of motion boundaries extracted from optical flow to generate the proposals. In <ref type="bibr" target="#b48">[49]</ref>, a series of easy- <ref type="figure">Figure 1</ref>: Overview of the network architecture of our STCNN algorithm. The part above the dashed line is the temporal coherence branch, and the part below the dashed line is the spatial segmentation branch. The red lines indicate the attention mechanism used in our model, and the hexagon indicates the attention module. Notably, each convolution layer is followed by a batch normalization layer <ref type="bibr" target="#b18">[19]</ref> and a ReLU layer.</p><p>to-group instances of an object are discovered, and the appearance model of the instances are iteratively updated to detect harder instances in temporally-adjacent frames. Tokmakov et al. <ref type="bibr" target="#b43">[44]</ref> use a fully convolutional network to learn motion patterns in videos to handle VOS, which designs an encoder-decoder style architecture to first learn a coarse representation of the optical flow field features, and then refine it iteratively to produce motion labels at high-resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatiotemporal CNN for VOS</head><p>As described above, we design a spatiotemporal CNN for VOS. Specifically, given a video sequence X = {X 1 , ? ? ? , X i , ? ? ? }, we aim to use our STCNN model to generate the segmentation results, i.e., S = {S 1 , ? ? ? , S i , ? ? ? }, where S i is the segmentation mask corresponding to X i . At time t, STCNN takes the previous ? frames, i.e., X t?? , ? ? ? , X t?1 , and the current frame X t , to predict the segmentation results at current frame S t 1 . As shown in <ref type="figure">Figure 1</ref>, STCNN is formed by two branches, i.e., the temporal coherence branch and the spatial segmentation branch. The temporal coherence branch learns the spatiotemporal discriminative features to capture the dynamic appearance and motion cues of video sequences instead of using optical flow. Meanwhile, the spatial segmentation branch is a fully convolutional network designed to segment objects with temporal constraints from the temporal coherence branch. In the following sections, we will describe these two branches in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal Coherence Branch</head><p>Architecture. As shown in <ref type="figure">Figure 1</ref>, we construct the temporal coherence branch based on the backbone ResNet-101 network <ref type="bibr" target="#b13">[14]</ref>, with the input number of channels 3?. That is, we concatenate the previous ? frames and feed them into the temporal coherence branch for prediction. After that, we use three deconvolution layers with the kernel size 3?3. To preserve spatiotemporal information in each resolution, we use three skip connections to concatenate low layer features. The convolution layer with kernel size 1?1 is used to compact features for efficiency. Notably, each convolution or deconvolution layer is followed by a batch normalization layer <ref type="bibr" target="#b18">[19]</ref> and a ReLU layer for non-linearity.</p><p>Pretraining. Motivated by <ref type="bibr" target="#b23">[24]</ref>, we use the adversarial manner to train the temporal coherence branch by predicting future frames from unlabeled video data. Specifically, we set the temporal coherence branch as the generator G, and construct a discriminator D to identify the generated video frames from G and the real video frames. Here, we use the Inception-v3 network <ref type="bibr" target="#b42">[43]</ref> pretrained on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b41">[42]</ref>. We replace the last fully connected (FC) layer by a randomly initialized 2-class FC layer as the discriminator D.</p><p>At time t, we use the generator G to produce the predic-tionX t of the current frame, based on previous ? frames</p><formula xml:id="formula_0">X t?? , ? ? ? , X t?1 , i.e.,X t = G({X t?i } ? i=1 )</formula><p>. Then, the discriminator D is adopted to distinguish the generated fram? X t from the real one X t . The generator G and discriminator D are trained iteratively in an adversarial manner <ref type="bibr" target="#b10">[11]</ref>. That is, for the fixed parameter W G of the generator G, we aims to optimize the discriminator D to minimize the probability of making mistakes, which is formulated as:</p><formula xml:id="formula_1">min W D ? log 1 ? D(X t ) ? log D(X t ) (1) whereX t = G({X t?i } ? i=1 )</formula><p>is the generated frame from G based on previous ? frames, and X t is the real video frame. Meanwhile, for the fixed parameter W D of the discriminator D, we expect the generator G to generate a video frame more like a real one, i.e.,</p><formula xml:id="formula_2">min W G X t ?X t 2 ? ? adv ? log D(X t )<label>(2)</label></formula><p>where the first term is the mean square error, penalizing the differences between the fake frameX t and the real frame X t , the second term is the adversarial term used to maximize the probability of D making a mistake, and ? adv is the predefined parameter used to balance these two terms. In this way, the discriminator D and generator G are optimized iteratively to make the generator G capturing the discriminative spatiotemporal features in the video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Segmentation Branch</head><p>The spatial segmentation branch is constructed based on the ResNet-101 network <ref type="bibr" target="#b13">[14]</ref> by replacing the convolution layers in the last two residual blocks (i.e., res4 and res5) with the dilated convolution layers <ref type="bibr" target="#b3">[4]</ref> of stride 1, which aims to preserve the high resolution for segmentation accuracy. Then, we use the PPM module <ref type="bibr" target="#b53">[54]</ref> to exploit the global context information by different-region-based context aggregation, followed by three designed attention modules to refine the predictions. That is, we apply the attention modules sequentially on multi-scale feature maps to help the network focus on object regions and ignore the background regions. After that, we concatenate the multi-scale feature maps, followed by a 3 ? 3 convolution layer to produce the final prediction, see <ref type="figure">Figure 1</ref>.</p><p>Notably, we design the attention module to focus on object regions for accurate results. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we first use the element-wise addition to exploit high-level context, and concatenate the temporal coherence features to integrate temporal constraints. After that, we use the predicted mask from the previous coarse scale feature map to guide the attention of the network, i.e., use the element-wise multiplication to mask the feature map in the current stage. Let? t to be the predicted mask at current stage. We mul-tiply? t on the feature map in element-wise and add it to the concatenated features for prediction. In this way, the features around the object regions are enhanced, which enforces the network gradually to concentrate on object regions for accurate results.</p><p>The pixel-wise binary cross-entropy with the softmax function P (?) is used in multi-scale feature maps to guide the network training, see <ref type="figure">Figure 1</ref>, which is defined as, where * i,j,t and i,j,t are the labels of the ground-truth mask S * t and the predicted mask S t at the coordinate (i, j), i,j,t = 1 indicates that the prediction is foreground at the coordinate (i, j), and i,j,t = 0 indicates that the prediction is background at the coordinate (i, j).</p><formula xml:id="formula_3">L(S t , S * t ) = ? * i,j,t =1 log P ( i,j,t = 1) ? * i,j,t =0 log P ( i,j,t = 0)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Implementation and Training</head><p>We implement our STCNN algorithm in Pytorch <ref type="bibr" target="#b36">[37]</ref>.</p><p>All the training and testing codes and the trained models are available at https://github.com/ longyin880815/STCNN. In training phase, we first pretrain the temporal coherence branch and the spatial segmentation branch individually, and iteratively update the models of both branches. After that, we finetune both models on each sequence for online processing. Pretraining temporal coherence branch. We pretrain the temporal coherence branch in the adversarial manner on the training and validation sets of the ILSVRC 2015 VID dataset <ref type="bibr" target="#b41">[42]</ref>, which consists of 4, 417 video clips in total, i.e., 3, 862 video clips in the training set and 555 video clips in the validation set. The backbone ResNet-101 network in our generator G is initialized by the pretrained model on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b41">[42]</ref>, and the other convolution and deconvolution layers are randomly initialized by the method <ref type="bibr" target="#b12">[13]</ref>. While the discriminator D is initialized by the pretrained model on the ILSVRC CLS-LOC dataset <ref type="bibr" target="#b41">[42]</ref>, with the last 2-class FC layer initialized by the method <ref type="bibr" target="#b12">[13]</ref>. Meanwhile, we randomly flip all frames in a video clip horizontal to augment the training data, and resize all frames to the size (480, 854) for training. The batch size is set to 3, and the Adam optimization algorithm <ref type="bibr" target="#b26">[27]</ref> is used to train the model. We set ? to 4, and use the learning rates 10 ?7 and 10 ?4 to train the generator G and the discriminator D, respectively. The adversarial weight ? adv is set to 0.001 in training phase. Pretraining spatial segmentation branch. We use the MSRA10K salient object dataset <ref type="bibr" target="#b7">[8]</ref> and the PASCAL VOC 2012 segmentation dataset <ref type="bibr" target="#b9">[10]</ref> to pretrain the spatial segmentation branch. The MSRA10K dataset contains 10, 000 images, and the PASCAL VOC 2012 dataset contains 11, 355 images. Meanwhile, we randomly flip the images horizontally, and rotate the images to augment the data for training. Each training image is resized to (300, 300).</p><p>The SGD algorithm with the batch size 8 and learning rate 10 ?3 is used to optimize the model. In addition, we directly add the cross-entropy losses on multi-scale predictions (see <ref type="figure">Figure 1</ref>) to compute the overall loss for training. Iterative offline training for VOS. After pretraining, we jointly finetune the model on the training set of DAVIS-2016 <ref type="bibr" target="#b38">[39]</ref> for VOS, which includes 30 video clips. Specifically, we train the temporal coherence branch and the spatial segmentation branch iteratively. When optimizing the temporal coherence branch, we freeze the weights of the spatial segmentation branch, and use the learning rates 10 ?8 and 10 ?4 to train the generator G and the discriminator D, respectively. The Adam algorithm is used to optimize the weights in temporal coherence branch with the batch size 1.</p><p>For training the spatial segmentation branch, similarly we fix the weights in the temporal coherence branch and only update the weights in the spatial segmentation branch using the SGD algorithm with the learning rate 10 ?4 . For better training, we randomly flip horizontally, rotate and rescale to augment the training data. For this iterative learning process, each branch in the network is able to obtain useful information from another branch through backpropagation. In this way, the spatial segmentation branch can receive useful temporal information from the temporal coherence branch, while the temporal coherence branch can learn more effective spatiotemporal features for accurate segmentation.</p><p>Online training for VOS. To adapt the network to a specific object for VOS, we finetune the network on the first frame for each video clip. Since we only have the annotation mask in the first frame, only the spatial segmentation branch is optimized. Each mask in the first frame is augmented to generate multiple training samples to increase the diversity. Specifically, we use the "lucid dream" strategy <ref type="bibr" target="#b24">[25]</ref> to generate in-domain training data based on the provided annotation in the first frame, including 5 steps, i.e., illumination changing, foreground-background splitting, object motion simulating, camera view changing, and foregroundbackground merging. Notably, in contrast to <ref type="bibr" target="#b24">[25]</ref>, we do not generate the optical flow since our STCNN do not require the optical flow for video segmentation. The SGD algorithm with the learning rate 10 ?4 and batch size 1 is used to train the network online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate the proposed algorithm against state-of-theart VOS methods on three challenging datasets, namely the DAVIS-2016 <ref type="bibr" target="#b38">[39]</ref>, DAVIS-2017 <ref type="bibr" target="#b39">[40]</ref>, and Youtube-Object <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>. All the experiments are conducted on a workstation with a 3.6 GHz Intel i7-4790 CPU, 16GB RAM, and a NVIDIA Titan 1080ti GPU. The quantitative results are presented in <ref type="table" target="#tab_0">Table 1</ref> and 2. Some qualitative segmentation results are shown in <ref type="figure">Figure 3</ref>, and more video segmentation results can be found in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DAVIS-2016 Dataset</head><p>The DAVIS-2016 dataset <ref type="bibr" target="#b38">[39]</ref> comprises of 50 sequences, 3, 455 annotated frames with a binary pixel-level foreground/background mask. Due to the computational complexity being a major bottleneck in video processing, the sequences in the dataset have a short temporal extent (about 2-4 seconds), but include all major challenges typically found in longer video sequences, such as background clutter, fast-motion, edge ambiguity, camera-shake, and out-of-view. We tested the proposed method on the 480p resolution set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Evaluation</head><p>For comprehensive evaluation, we use three measures provided by the dataset, i.e., region similarity J , contour accuracy F and temporal instability T . Specifically, region similarity J measures the number of mislabeled pixels, which is defined as the intersection-over-union (IoU) of the estimated segmentation and the ground-truth mask. Given a segmentation mask S and the ground-truth mask S * , J is calculated as J = S ?S * S ?S * . The contour accuracy F computes the F-measure of the contour-based precision P c and recall R c between the contour points of estimated segmentation S and the ground-truth mask S * , defined as F = 2PcRc</p><p>Pc+Rc . In addition, the temporal instability T measures oscillations and inaccuracies of the contours, which is calculated by following <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation Study</head><p>To comprehensively understand the proposed method, we conduct several ablation experiments. Specifically, we construct three variants and evaluate them on the validation set of DAVIS-2016, to validate the effectiveness of different components (i.e., the "Lucid dream" augmentation, the attention module, and the temporal coherence branch) in the proposed method, shown in <ref type="table" target="#tab_2">Table 3</ref>. Meanwhile, we also conduct experiments to analyze the importance of different training phases in <ref type="table" target="#tab_4">Table 5</ref>. For a fair comparison, we use the same parameter settings except for the specific declaration. Lucid dream augmentation. To demonstrate the effect of the "Lucid Dream" augmentation, we remove it from our STCNN model (see the forth column in <ref type="table" target="#tab_2">Table 3</ref>). As shown in <ref type="table" target="#tab_2">Table 3</ref>, we find that the region similarity J is reduced from 0.838 to 0.832. This decline (i.e., 0.006) demonstrate that the "Lucid dream" data augmentation is useful to improve the performance. Attention module. To validate the effectiveness of the attention module, we construct an algorithm by further re-moving the attention mechanism in the spatial segmentation branch. That is, we remove the red lines in <ref type="figure">Figure 1</ref> to directly generate the output mask. In this way, the object region is not specifically concentrated by the network. The segmentation results of the model is reported in the third column in <ref type="table" target="#tab_2">Table 3</ref>. We compare the third and forth columns in <ref type="table" target="#tab_2">Table 3</ref>, and find that the attention module improves 0.01 region similarity J , and 0.015 contour accuracy F, which demonstrates that the attention module is critical to the performance. The main reason is that the attention module is gradually applied on multi-scale features maps, enforcing the network to focus on the object regions to generate more accurate results. Temporal coherence branch. We construct a network based on the spatial segmentation branch without the attention module and report its results in the second column in <ref type="table" target="#tab_2">Table 3</ref>. Comparing the results between the second and third columns in <ref type="table" target="#tab_2">Table 3</ref>, we observe that the temporal coherence branch is critical to the performance of video segmentation, i.e., it improves 0.01 mean region similarity J (0.812 vs. 0.822) and 0.013 mean contour accuracy F (0.807 vs. 0.820). Most importantly, the temporal coherence branch significantly reduces the temporal instability, i.e., it reduces relative 13.4% temporal instability T (0.231 vs. 0.200). The results demonstrate that the temporal coherence branch is effective to capture the dynamic appearance and motion cues of video sequences to help generate accurate and consistent segmentation results. Training analysis. As described in Section 3.3, we first iteratively update the pretrained temporal coherence branch and the spatial segmentation branch offline. After that, we finetune both branches on each sequence for online processing. We evaluate the proposed STCNN method with different training phase on the validation set of DAVIS-2016 to analyze their effects on performance in <ref type="table" target="#tab_4">Table 5</ref>. As shown in <ref type="table" target="#tab_4">Table 5</ref>, we find that without online training phase, the mean region similarity J of STCNN drops 0.096 (i.e., 0.838 vs. 0.742), while without offline training phase, J <ref type="figure">Figure 3</ref>: The qualitative segmentation results of STCNN on the DAVIS-2016 (first two rows) and Youtube-Objects (last row) datasets. The output on the pixel level are indicated by the red mask. The results show that our method is able to segment objects under several challenges, such as occlusions, deformed shapes, fast motion, and cluttered backgrounds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison with State-of-the-Arts</head><p>We compare the proposed method with 7 state-of-the-art semi-supervised methods, i.e., CRN <ref type="bibr" target="#b14">[15]</ref>, OnAVOS <ref type="bibr" target="#b46">[47]</ref>, OSVOS <ref type="bibr" target="#b2">[3]</ref>, MSK <ref type="bibr" target="#b37">[38]</ref>, CTN <ref type="bibr" target="#b22">[23]</ref>, SegFlow <ref type="bibr" target="#b4">[5]</ref>, and VPN <ref type="bibr" target="#b21">[22]</ref>, and 4 state-of-the-art unsupervised methods, namly ARP <ref type="bibr" target="#b27">[28]</ref>, LVO <ref type="bibr" target="#b44">[45]</ref>, FSEG <ref type="bibr" target="#b20">[21]</ref>, and LMP <ref type="bibr" target="#b43">[44]</ref> in <ref type="table" target="#tab_0">Table  1</ref>.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, our algorithm outperforms the existing semi-supervised algorithms (e.g., OSVOS <ref type="bibr" target="#b2">[3]</ref> and MSK <ref type="bibr" target="#b37">[38]</ref>) and unsupervised algorithms (e.g., ARP <ref type="bibr" target="#b27">[28]</ref> and LVO <ref type="bibr" target="#b44">[45]</ref>) with 0.838 mean region similarity J , 0.838 mean contour accuracy F, and 0.191 temporal stability T , except CRN <ref type="bibr" target="#b14">[15]</ref> and OnAVOS <ref type="bibr" target="#b46">[47]</ref>. The OnAVOS algorithm <ref type="bibr" target="#b46">[47]</ref> updates the network online using training examples selected based on the confidence of the network and the spatial configuration, which requires heavy consumption of time and computation resource. Our algorithm is much more efficient and do not require the optical flow in both training and testing phase. The online updating mechanism in OnAVOS <ref type="bibr" target="#b46">[47]</ref> is complementary to our method. We believe that is can be used in our STCNN to further improve the performance. In addition, in contrast to CRN <ref type="bibr" target="#b14">[15]</ref> relying on optical flow to render temporal coherence in both training and testing, our method uses a self-supervised strategy to implicitly exploit the temporal coherence without relying on the expensive human annotations of optical flow. The temporal coherence branch is able to capture the dynamic appearance and motion cues of video sequences, pretrained in an adversarial manner from nearly unlimited unlabeled video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Runtime Performance</head><p>We present the inference time of STCNN and the state-ofthe-art methods on the validation set of DAVIS-2016 in the last row of <ref type="table" target="#tab_0">Table 1</ref>. Since different algorithms are developed and evaluated on different platforms (e.g., different algorithms are evaluated on different types of GPUs), it is difficult to compare the running time efficiency fairly. We report the running speed for reference. Meanwhile, we also analyze the influence of the number of iterations in the online training phase of STCNN to the segmentation accuracy and running speed in <ref type="table" target="#tab_3">Table 4</ref>. With the number of iterations increasing, the mean region similarity J increases to reach a maximal value 0.838. Continue training is not able to obtain the accuracy gain, but slows down the inference speed. Thus, we set the number of iterations in online training to 400 in our experiments. Compared to the state-of-the-art methods such as OSVOS (9.24 s/f), OnAVOS (15.57 s/f), our method achieves impressive results with much faster running speed.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">DAVIS-2017 Dataset</head><p>We evaluate our STCNN on the DAVIS-2017 validation set <ref type="bibr" target="#b39">[40]</ref>, which consists of 30 video sequences with various challenging cases including multiple objects with similar appearance, heavy occlusion, large appearance variation, clutter background, etc. The mean region similarity J and contour accuracy F are used to evaluate the performance in <ref type="table" target="#tab_5">Table 6</ref>. Our STCNN performs favorably against most of the semi-supervised methods, e.g., OSVOS <ref type="bibr" target="#b2">[3]</ref>, FAVOS <ref type="bibr" target="#b6">[7]</ref>, and MSK <ref type="bibr" target="#b37">[38]</ref>, with 58.7% mean region similarity J and 64.6% contour accuracy F. The results demonstrate that our STCNN is effective to segment objects in more complex scenarios with similar appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Youtube-Objects Dataset</head><p>The Youtube-Objects dataset <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref> contains web videos from 10 object categories. 126 video sequences with more than 20, 000 frames and ground-truth masks provided by <ref type="bibr" target="#b19">[20]</ref> are used for evaluation, where a single object or a group of objects of the same category are separated from the background. The videos in Youtube-Objects have a mix of static and moving objects, and the number of frames in each video clip ranges from 2 to 401. The mean IoU between the estimated results and the ground-truth masks in all video frames is used to evaluate the performance of the algorithms.</p><p>We compare the proposed STCNN method to 11 stateof-the-art semi-supervised algorithms, namely BVS <ref type="bibr" target="#b33">[34]</ref>, JFS <ref type="bibr" target="#b34">[35]</ref>, SCF <ref type="bibr" target="#b19">[20]</ref>, MRFCNN <ref type="bibr" target="#b1">[2]</ref>, LT <ref type="bibr" target="#b24">[25]</ref>, OSVOS <ref type="bibr" target="#b2">[3]</ref>, MSK <ref type="bibr" target="#b37">[38]</ref>, OFL <ref type="bibr" target="#b45">[46]</ref>, CRN <ref type="bibr" target="#b14">[15]</ref>, DRL <ref type="bibr" target="#b11">[12]</ref>, and OnAVOS <ref type="bibr" target="#b46">[47]</ref> in <ref type="table" target="#tab_1">Table 2</ref>. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we observe that the STCNN method produces the best results with 0.796 mean IoU, which surpasses the state-of-the-art results, i.e., MR-FCNN <ref type="bibr" target="#b1">[2]</ref> (0.784 mean IoU), with 0.012 mIoU. Compared to the optical flow based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b37">38]</ref>, our STCNN method performs well on fast moving objects, such as car and cat. The estimation of optical flow for fast moving objets is inaccurate, affecting the segmentation accuracy. Our STCNN relies on the temporal coherence branch to capture discriminative spatiotemporal features, which is effective to tackle such scenario. Meanwhile, the algorithm <ref type="bibr" target="#b19">[20]</ref> use long-term supervoxels to capture the temporal coherence. Only the superpixels are used in segmentation, causing the inaccurate boundaries of objects. In contrast, our algorithm design a coarse-to-fine process to sequentially apply the attention module on multi-scale feature maps, enforcing the network to focus on object regions to generate accurate results, especially for the non-rigid objects, e.g., cat and horse. The qualitative results are shown in the last three rows in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present an end-to-end trained spatiotemporal CNN for VOS, which is formed by two branches, i.e., the temporal coherence branch and the spatial segmentation branch. The temporal coherence branch is pretrained in an adversarial fashion, and used to predict the appearance and motion cues in the video sequence to guide object segmentation without using optical flow. The spatial segmentation branch is designed to segment object instance accurately based on the predicted appearance and motion cues from the temporal coherence branch. In addition, to obtain accurate segmentation results, a coarse-to-fine process is iteratively applied on multi-scale feature maps in the spatial segmentation branch to refine the predictions. These two branches are jointly trained in an end-to-end manner. Extensive experimental results on three challenging datasets, i.e., DAVIS-2016, DAVIS-2017, and Youtube-Object, demonstrate that the proposed method achieves favorable performance against the state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the attention module.? t denotes the segmented mask in the current stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on the validation set of DAVIS-2016. The performance of the semi-supervised VOS methods are shown in the left part, while the performance of the unsupervised VOS methods are shown in the right part. The symbol ? means higher scores indicate better performance, while ? means lower scores indicate better performance. In the last row, the numbers in parentheses are running time reported in the original papers of the corresponding methods.</figDesc><table><row><cell>Metric</cell><cell cols="12">Semi-supervised Ours CRN[15] OnAVOS[47] OSVOS[3] MSK[38] CTN[23] SegFlow[5] VPN [22] ARP[28] LVO[45] FSEG[21] LMP[44] Unsupervised</cell></row><row><cell cols="2">J Mean (?) 0.838</cell><cell>0.844</cell><cell>0.861</cell><cell>0.798</cell><cell>0.797</cell><cell>0.735</cell><cell>0.761</cell><cell>0.750</cell><cell>0.762</cell><cell>0.759</cell><cell>0.707</cell><cell>0.700</cell></row><row><cell cols="2">Recall (?) 0.961</cell><cell>0.971</cell><cell>0.961</cell><cell>0.936</cell><cell>0.931</cell><cell>0.874</cell><cell>0.906</cell><cell>0.901</cell><cell>0.911</cell><cell>0.891</cell><cell>0.835</cell><cell>0.850</cell></row><row><cell cols="2">Decay (?) 0.049</cell><cell>0.056</cell><cell>0.052</cell><cell>0.149</cell><cell>0.089</cell><cell>0.156</cell><cell>0.121</cell><cell>0.093</cell><cell>0.007</cell><cell>0.000</cell><cell>0.015</cell><cell>0.013</cell></row><row><cell cols="2">F Mean (?) 0.838</cell><cell>0.857</cell><cell>0.849</cell><cell>0.806</cell><cell>0.754</cell><cell>0.693</cell><cell>0.760</cell><cell>0.724</cell><cell>0.706</cell><cell>0.721</cell><cell>0.653</cell><cell>0.659</cell></row><row><cell cols="2">Recall (?) 0.915</cell><cell>0.952</cell><cell>0.897</cell><cell>0.926</cell><cell>0.871</cell><cell>0.796</cell><cell>0.855</cell><cell>0.842</cell><cell>0.835</cell><cell>0.834</cell><cell>0.738</cell><cell>0.792</cell></row><row><cell cols="2">Decay (?) 0.064</cell><cell>0.052</cell><cell>0.058</cell><cell>0.150</cell><cell>0.090</cell><cell>0.129</cell><cell>0.104</cell><cell>0.136</cell><cell>0.079</cell><cell>0.013</cell><cell>0.018</cell><cell>0.025</cell></row><row><cell>T (?)</cell><cell>0.191</cell><cell>-</cell><cell>0.190</cell><cell>0.376</cell><cell>0.189</cell><cell>0.198</cell><cell>0.182</cell><cell>0.300</cell><cell>0.359</cell><cell>0.255</cell><cell>0.295</cell><cell>0.688</cell></row><row><cell>Time(s/f)</cell><cell>3.90</cell><cell>(0.73)</cell><cell>(15.57)</cell><cell>(9.24)</cell><cell>(12.0)</cell><cell>(1.3)</cell><cell>(7.9)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results on the Youtube-Objects dataset. The mean intersection-over-union is used to evaluate the performance of methods. The results are directly taken from the original paper. The symbol ? means higher scores indicate better performance. Bold font indicates the best result.Method BVS<ref type="bibr" target="#b33">[34]</ref> JFS<ref type="bibr" target="#b34">[35]</ref> SCF<ref type="bibr" target="#b19">[20]</ref> MRFCNN<ref type="bibr" target="#b1">[2]</ref> LT<ref type="bibr" target="#b24">[25]</ref> OSVOS<ref type="bibr" target="#b2">[3]</ref> MSK<ref type="bibr" target="#b37">[38]</ref> OFL<ref type="bibr" target="#b45">[46]</ref> CRN<ref type="bibr" target="#b14">[15]</ref> DRL<ref type="bibr" target="#b11">[12]</ref> OnAVOS<ref type="bibr" target="#b46">[47]</ref> Ours</figDesc><table><row><cell>aeroplane</cell><cell>0.868</cell><cell>0.890</cell><cell>0.863</cell><cell>-</cell><cell>-</cell><cell>0.868</cell><cell>0.845</cell><cell>0.899</cell><cell>-</cell><cell>0.852</cell><cell>-</cell><cell>0.869</cell></row><row><cell>bird</cell><cell>0.809</cell><cell>0.816</cell><cell>0.810</cell><cell>-</cell><cell>-</cell><cell>0.851</cell><cell>0.837</cell><cell>0.842</cell><cell>-</cell><cell>0.868</cell><cell>-</cell><cell>0.879</cell></row><row><cell>boat</cell><cell>0.651</cell><cell>0.742</cell><cell>0.686</cell><cell>-</cell><cell>-</cell><cell>0.754</cell><cell>0.774</cell><cell>0.740</cell><cell>-</cell><cell>0.799</cell><cell>-</cell><cell>0.786</cell></row><row><cell>car</cell><cell>0.687</cell><cell>0.709</cell><cell>0.694</cell><cell>-</cell><cell>-</cell><cell>0.709</cell><cell>0.640</cell><cell>0.809</cell><cell>-</cell><cell>0.672</cell><cell>-</cell><cell>0.859</cell></row><row><cell>cat</cell><cell>0.559</cell><cell>0.677</cell><cell>0.589</cell><cell>-</cell><cell>-</cell><cell>0.676</cell><cell>0.698</cell><cell>0.683</cell><cell>-</cell><cell>0.746</cell><cell>-</cell><cell>0.772</cell></row><row><cell>cow</cell><cell>0.699</cell><cell>0.791</cell><cell>0.686</cell><cell>-</cell><cell>-</cell><cell>0.762</cell><cell>0.767</cell><cell>0.798</cell><cell>-</cell><cell>0.746</cell><cell>-</cell><cell>0.781</cell></row><row><cell>dog</cell><cell>0.685</cell><cell>0.703</cell><cell>0.618</cell><cell>-</cell><cell>-</cell><cell>0.779</cell><cell>0.745</cell><cell>0.766</cell><cell>-</cell><cell>0.827</cell><cell>-</cell><cell>0.800</cell></row><row><cell>horse</cell><cell>0.589</cell><cell>0.678</cell><cell>0.540</cell><cell>-</cell><cell>-</cell><cell>0.714</cell><cell>0.641</cell><cell>0.726</cell><cell>-</cell><cell>0.736</cell><cell>-</cell><cell>0.738</cell></row><row><cell>motorbike</cell><cell>0.605</cell><cell>0.615</cell><cell>0.609</cell><cell>-</cell><cell>-</cell><cell>0.582</cell><cell>0.892</cell><cell>0.481</cell><cell>-</cell><cell>0.737</cell><cell>-</cell><cell>0.680</cell></row><row><cell>train</cell><cell>0.652</cell><cell>0.782</cell><cell>0.663</cell><cell>-</cell><cell>-</cell><cell>0.746</cell><cell>0.744</cell><cell>0.763</cell><cell>-</cell><cell>0.830</cell><cell>-</cell><cell>0.796</cell></row><row><cell>Mean (?)</cell><cell>0.680</cell><cell>0.740</cell><cell>0.676</cell><cell>0.784</cell><cell>0.762</cell><cell>0.744</cell><cell>0.717</cell><cell>0.776</cell><cell>0.766</cell><cell>0.781</cell><cell>0.774</cell><cell>0.796</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effectiveness of various components in the proposed method. All models are evaluated on the DAVIS-2016 dataset. The symbol ? means high scores indicate better result, while ? means lower scores indicate better result.</figDesc><table><row><cell>Component Temporal Coherence Branch? Attention Module? Lucid Dream?</cell><cell></cell><cell cols="3">STCNN ! ! ! ! ! !</cell></row><row><cell>J Mean (?)</cell><cell>0.812</cell><cell>0.822</cell><cell>0.832</cell><cell>0.838</cell></row><row><cell>F Mean (?)</cell><cell>0.807</cell><cell>0.820</cell><cell>0.835</cell><cell>0.838</cell></row><row><cell>T (?)</cell><cell>0.231</cell><cell>0.200</cell><cell>0.192</cell><cell>0.191</cell></row><row><cell cols="5">of STCNN drops 0.052 (i.e., 0.838 vs. 0.786). In sum-</cell></row><row><cell cols="5">mary, both training phases are extremely important to our</cell></row><row><cell cols="4">STCNN, especially for the online training phase.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance and running speed of the proposed STCNN with different number of iterations in online training phase.</figDesc><table><row><cell>#Iter</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell></row><row><cell>mIOU</cell><cell>0.830</cell><cell>0.834</cell><cell>0.836</cell><cell>0.838</cell><cell>0.838</cell><cell>0.838</cell></row><row><cell>time(s/f)</cell><cell>1.11</cell><cell>2.04</cell><cell>2.97</cell><cell>3.90</cell><cell>4.83</cell><cell>5.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance on DAVIS-2016 for different training phases of STCNN.</figDesc><table><row><cell></cell><cell></cell><cell>Offline</cell><cell>Online</cell><cell></cell></row><row><cell></cell><cell>Metric</cell><cell>Training</cell><cell>Training</cell><cell>All</cell></row><row><cell>J</cell><cell>Mean (?)</cell><cell>0.742</cell><cell>0.786</cell><cell>0.838</cell></row><row><cell></cell><cell>Recall (?)</cell><cell>0.854</cell><cell>0.921</cell><cell>0.961</cell></row><row><cell></cell><cell>Decay (?)</cell><cell>-0.004</cell><cell>0.075</cell><cell>0.049</cell></row><row><cell>F</cell><cell>Mean (?)</cell><cell>0.743</cell><cell>0.79</cell><cell>0.838</cell></row><row><cell></cell><cell>Recall (?)</cell><cell>0.806</cell><cell>0.871</cell><cell>0.915</cell></row><row><cell></cell><cell>Decay (?)</cell><cell>0.018</cell><cell>0.089</cell><cell>0.064</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The results on the DAVIS-2017 dataset. The symbol ? means higher scores indicate better performance. Bold font indicates the best result.</figDesc><table><row><cell>Metric</cell><cell cols="2">[46] [38] [52]</cell><cell>[7]</cell><cell>[3]</cell><cell>[47] Ours</cell></row><row><cell cols="6">J Mean (?) 43.2 51.2 52.5 54.6 56.6 61.6</cell><cell>58.7</cell></row><row><cell>F Mean (?)</cell><cell>-</cell><cell cols="4">57.3 57.1 61.8 63.9 69.1</cell><cell>64.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the time index t &lt; ?, we copy the first frame ? ? t times to get the ? frames for segmentation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised video segmentation using tree structured graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2751" to="2764" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in A cnn-based higher-order spatio-temporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="524" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">J</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement cutting-agent learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9080" to="9089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation using motion saliency-guided spatio-temporal propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="813" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervoxelconsistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7474" to="7483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequential clique optimization for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Yoon</forename><surname>Yeong Jun Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="537" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Keysegments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised video object segmentation with motion-based bilateral networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="215" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Marki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3235" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstraint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop Autodiff</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3491" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The 2017 DAVIS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1704.00675</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3282" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="531" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">JOTS: joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="626" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient video segmentation using parametric graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3155" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
