<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Classification Perspective on Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiang</forename><surname>Cai</surname></persName>
							<email>hongxiang.cai@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sun</surname></persName>
							<email>jun.sun@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Xiong</surname></persName>
							<email>yichao.xiong@media-smart.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Media Intelligence Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Classification Perspective on Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The prevalent perspectives of scene text recognition are from sequence to sequence (seq2seq) and segmentation. Nevertheless, the former is composed of many components which makes implementation and deployment complicated, while the latter requires character level annotations that is expensive. In this paper, we revisit classification perspective that models scene text recognition as an image classification problem. Classification perspective has a simple pipeline and only needs word level annotations. We revive classification perspective by devising a scene text recognition model named as CSTR, which performs as well as methods from other perspectives. The CSTR model consists of CPNet (classification perspective network) and SPPN (separated conv with global average pooling prediction network). CSTR is as simple as image classification model like ResNet [15]  which makes it easy to implement and deploy. We demonstrate the effectiveness of the classification perspective on scene text recognition with extensive experiments. Futhermore, CSTR achieves nearly state-ofthe-art performance on six public benchmarks including regular text, irregular text. The code will be available at https://github.com/Media-Smart/vedastr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text is one of the most important carrier of information and knowledge, it can be found almost everywhere in our life such as books, newspapers, road signs, billboards, etc. The objective of scene text recognition (STR) is to translate a cropped text instance image into a target string sequence, which is very useful in a range of applications, such as ID card scan, image-based machine translation, industrial automation, self-driving, to name a few.</p><p>With the development of deep learning, a lot of neural network based methods significantly boosted the performance of STR in which two categories can be divided: * Equal contribution. ? Corresponding author.</p><p>segmentation-based methods <ref type="bibr" target="#b23">[24]</ref> and seq2seq-based methods. Seq2seq-based methods can be roughly classified into CTC-based <ref type="bibr" target="#b12">[13]</ref> methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>, attention-based <ref type="bibr" target="#b1">[2]</ref> methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48]</ref>, transformer-based <ref type="bibr" target="#b38">[39]</ref> methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Segmentation-based <ref type="bibr" target="#b23">[24]</ref> methods usually include two steps: character segmentation, and character recognition. Methods in this category attempt to locate the position of each character on the input text instance image, apply a character classifier to recognize each character, and group characters into text line to obtain the final recognition result.</p><p>Seq2seq-based methods recognize the text line as a whole and focus on mapping the entire text instance image into a target string sequence directly by an encoder-decoder framework, thus avoiding character segmentation. This approach contains two stages: image encoding and sequence decoding. In image encoding phase, the input text instance image will be encoded into a feature sequence by Convolutional Neural Network (CNN) <ref type="bibr" target="#b20">[21]</ref>, Recurrent Neural Network (RNN), transfomer <ref type="bibr" target="#b38">[39]</ref> encoder or the combinations. The feature sequence then will be decoded by CTC <ref type="bibr" target="#b12">[13]</ref>, attention based RNN or transformer <ref type="bibr" target="#b38">[39]</ref> decoder into text line.</p><p>Segmentation-based methods <ref type="bibr" target="#b23">[24]</ref> are simple, but they require expensive character level annotations. Many systems based on seq2seq have achieved state-of-the-art performance, however seq2seq-based methods are complex and have a long pipeline.</p><p>In addition to above prevalent two perspectives, there are also a minority in the literature which model STR as an image classification problem which can be named as classification-based method. CHAR <ref type="bibr" target="#b16">[17]</ref> assumes that there is a maximum number k of characters per word that can be recognized. The body of the CHAR model consists of four convolutional layers and two fully connected layers followed by k independent multi-class classification heads, each of which predicts the character at each position. An end token ? is introduced to handle the problem that words have variable length which is unknown at test time. The CHAR model is easy to train like an image classification model, but its accuracy is very low.</p><p>Classification-based methods <ref type="bibr" target="#b16">[17]</ref> are really simple which makes it very attractive. However, CHAR performs badly which leads to few following works from classification perspective <ref type="bibr" target="#b3">[4]</ref>. In this paper, we revisit classificationbased methods and demonstrate that this perspective can achieve comparable performance with seq2seq-based methods and segmentation-based methods through our carefully designed CSTR.</p><p>Specifically, we devise a dedicated prediction network, SPPN (separated conv with global average pooling prediction network), which incorporates global semantic information to implicitly encode character position of text sequence. This design match the mechanism behind classification perspective that i-th classification head predicts i-th character of the word sequence in the input image, where the classification head should know the i characters from the left. SPPN is the key component that makes classification-based methods work better than CTC-based methods with low back-propagating computation burden under the same number of parameters.</p><p>Furthermore, we design a new backbone network, CP-Net (classification perspective network), which not only improves the network's ability to focus on important features and suppress unnecessary one but also has larger valid receptive field. This powerful backbone network, which coordinates classification perspective, brings a large improvement for CSTR.</p><p>CSTR achieves nearly state-of-the-art performance on a variety of standard scene text recognition benchmarks such as ICDAR03, ICDAR13, ICDAR15, IIIT5k, SVT, SVTP.</p><p>Our main contributions are highlighted as follows:</p><p>? We demonstrate that cross entropy (CE) loss perform better than CTC loss when prediction network is de-vised properly by thorough experiments.</p><p>? We propose a novel method named CSTR which is the first classification-based method on STR that works as well as segmentation-based methods and seq2seqbased methods.</p><p>? CSTR achieves nearly state-of-the-art accuracy on benchmarks for scene text recognition.</p><p>? The proposed model is as simple as image classification model and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the past few years, the research of scene text recognition has made significant progress with the development of deep learning. In this section, we will review some recent scene text recognition methods, and group the prevalent of them into two categories: seq2seq-based and segmentationbased. Besides, the classification-based methods which are nearly ignored by the community and some popular modules will also be involved.</p><p>Seq2seq-based methods recognize the text line as a whole and focus on mapping the entire text instance image into a target string sequence directly by a encoder-decoder framework. This approach contains two stages: image encoding and sequence decoding. In image encoding phase, the input text instance image will be encoded into a feature sequence by CNN, RNN, transfomer <ref type="bibr" target="#b38">[39]</ref> encoder or their combinations. The feature sequence then will be decoded by CTC <ref type="bibr" target="#b12">[13]</ref>, attention <ref type="bibr" target="#b1">[2]</ref> based RNN or transformer <ref type="bibr" target="#b38">[39]</ref> decoder into text line.</p><p>CRNN <ref type="bibr" target="#b34">[35]</ref>, one of the first seq2seq-based scene text recognition methods, uses CNN to extract features which are encoded into a feature sequence by RNN, then CTC <ref type="bibr" target="#b12">[13]</ref> is introduced to decode the feature sequence into text line.</p><p>The idea of using a recurrent neural network to predict a character sequence has since been extended by various methods that incorporate an attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> into the text sequence decoding <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48]</ref>. This line of work extracts visual features by a convolutional neural network which is encoded into a feature sequence by a recurrent neural network, then another recurrent neural network equipped with attention mechanism is adopted to decode the feature sequence into text line.</p><p>Recently, the structure of transformer <ref type="bibr" target="#b25">[26]</ref> has been proposed to capture global dependencies. Transformer has been proved to be effective in many tasks of computer vision <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b5">6]</ref> and natural language processing <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref>. A lot of work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b46">47]</ref> incorporates transformer into scene text recognition and achieves good results. They use transformer encoder to encode visual features into a feature sequence which is then decoded into text line by transformer decoder. Seq2seq-based methods are the most popular in scene text recognition community, however they are complex.</p><p>Spatial transformer <ref type="bibr" target="#b17">[18]</ref> not only selects regions of an image that are most relevant, but also transforms those regions to a canonical, expected pose (horizontally aligned characters of uniform heights and widths) to simplify recognition in the following layers.</p><p>The spatial transformer is split into three parts: localisation network, grid generator and sampler. First, a localisation network takes the input image, and outputs the parameters of the spatial transformation that should be applied to the input image. Then, the predicted transformation parameters are used to create a sampling grid, a set of points where the input image should be sampled to produce the transformed output image, which is done by the grid generator. Finally, the input image and the sampling grid are taken as inputs to the sampler, producing the output rectified image sampled from the input image at the grid points.</p><p>ASTER <ref type="bibr" target="#b36">[37]</ref> adds a spatial transformer before the scene text recognition model to eliminate the negative effects of perspective distortion and distribution curvature which boosts performance on irregular texts. Since the proposal of spatial transformer in <ref type="bibr" target="#b27">[28]</ref>, spatial transformer has been a default module on scene text recognition <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Segmentation-based methods attempt to locate the position of each character on the input text instance image, apply a character classifier to recognize each character, and group characters into text line to obtain the final recognition results. CA-FCN <ref type="bibr" target="#b23">[24]</ref> exploits extra character level annotations in addition to the word level annotations to supervise the training process which can overcome the disadvantage of seq2seq-based methods that missing or superfluous char-acters will cause misalignment between the ground truth strings and the predicted sequences.</p><p>While segmentation-based methods are simple, they require character level annotations which are expensive.</p><p>Classification-based methods model scene text recognition as an image classification problem which is simpler than seq2seq-based methods. Furthermore, classificationbased methods don't require character level annotations like segmentation-based methods, which are hard to obtain.</p><p>CHAR <ref type="bibr" target="#b16">[17]</ref> assumes that there is a maximum number k of characters per word that can be recognized. The body of the CHAR model consists of four convolutional layers and two fully connected layers followed by k independent multi-class classification heads, each of which predicts the character at each position. An end token ? is introduced to handle the problem that words have variable length which is unknown at test time. The CHAR model is easy to train like an image classification model, but it has a very low accuracy <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we delve into classification-based methods and devise CSTR which demonstrates that classificationbased methods can perform as well as seq2seq-based methods and segmentation-based methods with simpler architecture and no need for character level annotations. Moreover, CSTR abandons the spatial transformer which simplifies scene text recognition pipeline further more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Classification Perspective on STR</head><p>Our aim is to design a simple and effective scene text recognition method which can be easily implemented and deployed like CTC which is widely used and practical in production <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>CTC is a way to get around not knowing the alignment between the input and the output which is an intermediate process. We believe that the intermediate process is not necessary which is elaborated in Section 5.4.1. We model the scene text recognition as image classification problem to avoid the alignment between input and output as is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Borisyuk et al. <ref type="bibr" target="#b3">[4]</ref> conducted experiments to compare CHAR <ref type="bibr" target="#b16">[17]</ref> and CTC which shows that CHAR is inferior to CTC. We argue that CHAR's bad performance doesn't imply that classification-based methods can not work well. We propose CSTR, a classification-based method, which achieves comparable performance with seq2seq-based methods and segmentation-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CSTR</head><p>As is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the network architecture of CSTR is as simple as image classification models. It consists of two main parts: backbone network and prediction network. Specifically, the backbone network extracts fea-tures from an input image and the prediction network uses the features to predict the character at each position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backbone Network</head><p>We use ResNet proposed by Cheng et al. <ref type="bibr" target="#b6">[7]</ref> as our basic backbone network. For each residual block, we use the projection shortcut (using 1 ? 1 convolutions) when the dimensions between input and output are different, and use the identity shortcut when they are the same. Due to different modeling method, we have to redesign the backbone network from classification perspective which leads to the birth of CPNet. The detailed architecture of CPNet is shown in <ref type="table" target="#tab_0">Table 1</ref> without FPN.</p><p>Depth, Width and Resolution. From the point of view of network depth, we deepen stage-3 and stage-4 and insert additional residual blocks in stage-5. With respect to network width, we widen it by 1.5 times. In order to better exploit the model capacity, input image resolution is enlarged from 32 ? 128 to 48 ? 192 as described in <ref type="bibr" target="#b37">[38]</ref>.</p><p>Feature Pyramid Network. Following Yu et al. <ref type="bibr" target="#b46">[47]</ref>, we also use FPN <ref type="bibr" target="#b24">[25]</ref> to aggregate hierarchical feature maps from stage-3, stage-4 and stage-5 which is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Thus the output feature map width and height are 1/4 of the input image, and the channel number is 512. CBAM. Attention <ref type="bibr" target="#b43">[44]</ref> has been proved effective in classification. Since our proposed method is based on classification perspective, we insert attention modules <ref type="bibr" target="#b43">[44]</ref> in all residual blocks in order to focus on important features and suppress unnecessary ones.</p><p>Semantic-Aware Downsampling Module. In addition to the above classic modules, we propose the semanticaware downsampling module (SADM) to reduce discriminative information loss in feature downsampling procedure.</p><p>We equip traditional downsampling module with nonlocal <ref type="bibr" target="#b42">[43]</ref> unit which can effectively capture global spatial interaction between features to make the network aware of which features should be discarded and what should be preserved when downsampling. <ref type="figure" target="#fig_2">Figure 3</ref> depicts SADM details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prediction Network</head><p>A prediction network in CSTR is responsible for predicting the text line from output feature of backbone network. There are many choices of prediction network design. We survey the current popular methods in STR and classifica-  </p><formula xml:id="formula_0">: 2 ? 2 Conv/s1 : 96 ? 3 ? 3 ? 192 Conv/s1 : 192 ? 3 ? 3 ? 192 ? 1 Stage 2 Conv/s1 : 192 ? 3 ? 3 ? 192 SADM -A Conv/s1 : 192 ? 3 ? 3 ? 384 Conv/s1 : 384 ? 3 ? 3 ? 384 ? 4 Stage 3 Conv/s1 : 384 ? 3 ? 3 ? 384 SADM -A Conv/s1 : 384 ? 3 ? 3 ? 768 Conv/s1 : 768 ? 3 ? 3 ? 768 ? 7 Conv/s1 : 768 ? 3 ? 3 ? 768 Stage 4 Conv/s1 : 768 ? 3 ? 3 ? 768 Conv/s1 : 768 ? 3 ? 3 ? 768 ? 5 SADM -B Conv/s1 : 768 ? 3 ? 3 ? 768 Conv/s1 : 768 ? 3 ? 3 ? 768 ? 3 Stage 5</formula><p>Conv/s1 : 768 ? 2 ? 2 ? 768 tion, and design three types of prediction network which are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Shared conv prediction network (SHPN) uses a convolutional layer followed with backbone network which is shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>. If the dimension of the feature from backbone network is C in ? H ? W , then the maximum length of prediction word is W in this manner. This prediction network is widely used in CTC based architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Separated conv prediction network (SEPN) uses separated convolutional layers in parallel after backbone network and we display it in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. If the dimension of the feature from backbone network is C in ? H ? W , the number of convolutional layers is W . It has the same property like SHPN that it can only predict W length words. Compared with SHPN, SEPN has more parameters.</p><p>Separated conv with global average pooling prediction network (SPPN) utilizes a global average pooling layer after backbone network followed by separated convolutional layers. The potential design philosophy of SPPN is to introduce global semantic information to the next stage which has been proved effective by <ref type="bibr" target="#b46">[47]</ref>. The architecture of SPPN is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>(c).</p><p>We implement these three types of prediction network in CSTR respectively. Compared these three types of prediction network, SHPN and SEPN encode the character with positional information explicitly by splitting the input feature into W features of which dimension is C in ?H?1. Due to we modeling STR from classification perspective, each split feature should contain global semantic information to know the character order in the text line. Nevertheless, restricted by the receptive field and the various distributions of character in image, SHPN and SEPN cannot make sure that the features satisfy the above requirements. However, with global average pooling we can incorporate global semantic information to the feature which implicitly encodes character position of text sequence.</p><p>It's worth noting that we don't adopt the prediction network used in CHAR <ref type="bibr" target="#b1">[2]</ref>, which used several fully connected layers in prediction network. The fully connected layers will result in lots of parameters which will make network suffer from overfitting. The two main advantages of SPPN compared with prediction network in CHAR <ref type="bibr" target="#b1">[2]</ref> are integrating global semantic information and avoiding huge number of parameters. We adopt SPPN as the prediction network for CSTR with overall consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Training</head><p>We only use two common synthetic text datasets as the training data in our experiments:</p><p>MJSynth (MJ) <ref type="bibr" target="#b16">[17]</ref> is a synthetic text in image dataset which contains 9 million word box images, generated from a lexicon of 90K English words.</p><p>SynthText (ST) <ref type="bibr" target="#b13">[14]</ref> is a synthetic text in image dataset, designed for scene text detection and recognition. It contains 8 million text boxes from 800K synthetic scene images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Testing</head><p>All experiments are evaluated on the six Latin scene text benchmarks described bellow, which contain both regular and irregular text:</p><p>ICDAR 2003 (IC03) <ref type="bibr" target="#b29">[30]</ref> was created for the ICDAR 2003 Robust Reading competition for reading cameracaptured scene texts. It contains 1110 images for evaluation. Following previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>, we ignore all words that are either too short(less than 3 characters) or ones that contain non-alphanumeric characters. Thus, we got 867 images for evaluation.</p><p>ICDAR 2013 (IC13) <ref type="bibr" target="#b19">[20]</ref> contains 1095 images for evaluation, where pruning words with non-alphanumeric characters results in 1015 images.</p><p>ICDAR 2015 (IC15) <ref type="bibr" target="#b18">[19]</ref> was created for the ICDAR 2015 Robust Reading competitions and contains 2077 images for evaluation. The images are taken with Google Glasses without careful position and focusing.</p><p>IIIT 5K-Words (IIIT5k) <ref type="bibr" target="#b30">[31]</ref> is collected from the website. It contains 3000 test images for evaluation.</p><p>Street View Text (SVT) <ref type="bibr" target="#b40">[41]</ref> has 647 testing images cropped form Google Street View. Many images are severely corrupted by noise, blur, and low resolution.</p><p>Street View Text-Perspective (SVTP) <ref type="bibr" target="#b31">[32]</ref> is also cropped form Google Street View. There are 645 test images in this set and many of them are perspectively distorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>General Setting. The size of input images is set to 48 ? 192. The number of character classes is set to 37, including 26 alphabets, 10 digitals and 1 end token. And the max length of output sequence k is set to 25.</p><p>Data Augmentation. We adopt some common image processing operations, such as motion blur, gaussian noise and color jitter, and randomly add them to the training images.</p><p>Model Training. The proposed model is trained from scratch without finetuning on other datasets. Label smoothing and warming up are used for all experiments. The batch size is set to 192. Adadelta optimizer is adopted with the initial learning rate 1. The model is totally trained for 420k iterations and the learning rate is decreased 10 ?1 and 10 ?2 at 150k iterations and 250k iterations. All experiments are implemented on a workstation with 10 NVIDIA 1080Ti graphical cards.</p><p>Model Testing. At runtime, images are rescaled to 48 ? 192 without keeping ratio. Unlike <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>, we do not use test time augmentation like beam-search or rotating images whose height is larger than width. The predicted text line is calculated by taking the highest probability character at each head and remove the end token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons with State-of-the-Arts</head><p>The comparisons of our method with previous methods on various scene text recognition benchmarks are shown in <ref type="table" target="#tab_1">Table 2</ref>. We only compare the results without any lexicon, because the lexicon is always unknown before recognition in practical use.</p><p>As for classification-based methods, it is evident that our approach outperforms CHAR <ref type="bibr" target="#b16">[17]</ref> by a large margin Compared with seq2seq-based and segmentation-based methods, our approach also achieves comparable performance although CSTR is just a simple image classification method without STN <ref type="bibr" target="#b36">[37]</ref>. Actually, our model performs the best on 1 of the 6 evaluated text settings which shows the feasibility of perspective from image classification. The advantage of our model is obvious, it is easy to implement and simple for both training and testing phases. Furthermore, STN-CSTR performs the best on two benchmarks (SVT, IC15) and the second best on three benchmarks (IC03, IC13, SVTP) compared with previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>We perform a thorough ablation study to present our design concept. For metric evaluation, we provide the average accuracy like <ref type="bibr" target="#b0">[1]</ref> does on the unified test dataset involving all subsets of the above mentioned benchmarks. We use the same codebase to run all experiments. We set training iterations to 300k in this section for efficiency.</p><p>First, we compare our classification-based method with CTC which is widely used and practical in production <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Second, we design CPNet to suit the image classification method which is demonstrated to be very important. Third, we find that data augmentation is crucial to scene text recognition which has not received the attention it deserves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">From CTC Loss to CE Loss</head><p>CTC is a way to get around not knowing the alignment between the input and the output which is an intermediate process. However, is this intermediate process necessary?</p><p>To answer this question, we conduct the first experiment where we substitute the CTC loss with cross entropy (CE) loss directly and use SHPN as prediction network without other settings changed on the basic backbone network <ref type="bibr" target="#b6">[7]</ref> as mentioned in Section 4.1, the result of which is shown in the first row of <ref type="table" target="#tab_2">Table 3</ref>. Here we set input resolution to 32 ? 100 and no data augmentation is used for fair comparison, other settings are the same as described in Section 5.2. The average accuracy of CE loss is almost the same as CTC loss which indicates that the intermediate process of alignment between the input and the output is unnecessary.</p><p>The mechanism behind CE loss is that i-th classification head predicts i-th character of the word sequence in the input image, where the classification head should know the i characters from the left. Intuitively, global receptive field is needed for the classification heads in CE loss based model. We perform the second experiment where the features output by backbone are fed into SPPN which is depicted in the third row of <ref type="table" target="#tab_2">Table 3</ref>. Considering that there are two gaps between SHPN and SPPN, i.e. number of parameters and the presence of average pooling layer, we conduct a third experiment (SEPN) to eliminate the influence of parameters. The above three experiments prove our intuition: global receptive field is needed for the prediction network in CE loss based model. <ref type="table" target="#tab_2">Table 3</ref>, CTC gets the best accuracy 83.8% with SHPN. While for CE, SPPN achives best accuracy 84.1%. This clearly reveals that CE can work better than CTC when the prediction network design is proper. We use the basic backbone network and SPPN as the base model for all the remaining experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">CPNet</head><p>The design of backbone architecture varies according to different perspectives and has direct impact on performance.</p><p>Since the backbone network in the base model is designed under the seq2seq perspective, which is not suitable for our modeling method, it is necessary for us to redesign the backbone network. Inspired by works in image classification and object detection, we consider to adapt the backbone network through model scaling, receptive field, attention mechanism, downsampling accordingly. Since there exist various effective methods and modules <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>, we directly use some of them (EM as mentioned in Section 4.1) to scale model capacity, enlarge receptive field and increase complexity of feature interaction. Moreover, to alleviate the loss of discriminative information in downsampling, we propose the new module named as SADM.</p><p>To evaluate the effectiveness of the two parts in our model, we conduct a series of experiments with/without them. As shown in <ref type="table" target="#tab_3">Table 4</ref>, with general enhanced modules, we achieve 3.1% (84.1% to 87.2%) improvement in average accuracy. Equipped with SADM, we get further boost in performance. <ref type="table" target="#tab_3">Table 4</ref> clearly reveals the importance and effectiveness of our redesigned network CPNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Data Augmentation</head><p>Data augmentation is widely used in the training of deep CNN. It is an explicit form of regularization and aims to artificially enlarge the training dataset from existing data using data transforms. As is known to all, data augmentation plays an important role in many computer vision tasks, such as image classification, object detection, semantic segmentation and so on. Some researchers are dedicated to proposing specific data augmentation methods. Although there are a significant amount of synthetic data for training in scene text recognition, the influence of data augmentation in it has been ignored in previous works. Therefore, to evaluate how data augmentation influences performance, we conduct the experiment and choose some common operations as mentioned in Section 5.2. As depicted in <ref type="table">Table 5</ref>, without data augmentation, the average accuracy drops 1.7%, which demonstrates that data augmentation is crucial to scene text recognition like other tasks. To the best of our knowledge, we are the first one to reveal the importance of data augmentation for scene text recognition. We hope this finding may inspire researchers to do further explorations on data augmentation in this field. <ref type="table">Table 5</ref>. Ablation Study of data augmentation. "Base" means CE loss with CPNet and SPPN. "DA" means data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average Base 87.3 Base + DA 89.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Spatial Transformer Network</head><p>When a spatial transformer network is added upon our CSTR to rectify the input image, which is called STN-CSTR, 0.4% perfomance boost can be achieved, as is showed in <ref type="table" target="#tab_4">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we revisit classification perspective on scene text recognition, which models the scene text recognition as an image classification problem. Based on the image classification perspective, we design a scene text recognition model, which is named as CSTR. The CSTR model consists of CPNet and SPPN which is composed of a global average pooling layer and independent convolutional layers, each of which predicts the corresponding character in the text line. CSTR is as simple as image classification models such as ResNet <ref type="bibr" target="#b14">[15]</ref> which makes it easy to implement, and the fully convolutional neural network architecture makes it easy to train and deploy. Furthermore, CSTR achieves comparable performance and STN-CSTR achieves the state-of-the-art performance. In the future, we are interested in further improving the performance of classificationbased methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The model architecture of CSTR. ? indicates the end token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The model architecture of FPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The model architecture of SADM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of the prediction network. (a) SHPN. (b) SEPN. (c) SPPN. Cin represents the input channels, Cout represents the output channel, H represents the height of the feature map, W represents the width of the feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CPNet body architecture. Residual blocks are highlighted with gray background.</figDesc><table /><note>Stage name Type / Stride : Filter Shape Conv/s1 : 1 ? 3 ? 3 ? 48 Stage 1 Conv/s1 : 48 ? 3 ? 3 ? 96 P ool/s2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of scene text recognition performance with previous methods on several benchmarks. ? means the method evaluate with beam search, means the method evaluate with TTA. * means the corresponding measured dataset is unclear.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Data</cell><cell>Annos</cell><cell cols="2">IIIT5K SVT</cell><cell cols="2">IC03</cell><cell></cell><cell>IC13</cell><cell cols="2">IC15</cell><cell cols="2">SVTP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3000</cell><cell>647</cell><cell cols="8">860 867 857 1015 1811 2077 639 645</cell></row><row><cell></cell><cell>AON [8]</cell><cell>MJ + ST</cell><cell>word</cell><cell>87.0</cell><cell>82.8</cell><cell>-</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">68.2 73.0</cell><cell>-</cell></row><row><cell></cell><cell>FAN [7]</cell><cell>MJ +ST</cell><cell>word</cell><cell>87.4</cell><cell>85.9</cell><cell>-</cell><cell>94.2</cell><cell>-</cell><cell cols="2">93.3 70.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Baek et al.[1]</cell><cell>MJ+ST</cell><cell>word</cell><cell>87.9</cell><cell cols="6">87.5 94.9 94.4 93.6 92.3 77.6</cell><cell>71.8</cell><cell>-</cell><cell>79.2</cell></row><row><cell></cell><cell>SEED [33]  ?</cell><cell>MJ + ST</cell><cell>word</cell><cell>93.8</cell><cell>89.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.8</cell><cell>-</cell><cell>80  *</cell><cell>-</cell><cell>81.4</cell></row><row><cell>Seq2seq</cell><cell>SRN [47] ASTER[37]  ?</cell><cell>MJ + ST MJ + ST</cell><cell>word word</cell><cell>94.8 93.4</cell><cell cols="2">91.5 89.5 94.5 -</cell><cell>--</cell><cell>--</cell><cell cols="2">95.5 82.7 91.8 -</cell><cell cols="2">-76.1  *  78.5 85.1</cell><cell>--</cell></row><row><cell></cell><cell>SAR[23]  ?</cell><cell>MJ + ST</cell><cell>word</cell><cell>91.5</cell><cell>84.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.0</cell><cell>-</cell><cell cols="2">69.2 76.4</cell><cell>-</cell></row><row><cell></cell><cell>SATRN [22]</cell><cell>MJ+ST</cell><cell>word</cell><cell>92.8</cell><cell>91.3</cell><cell>-</cell><cell>96.7</cell><cell>-</cell><cell>94.1</cell><cell>-</cell><cell>79.0</cell><cell>-</cell><cell>86.5</cell></row><row><cell></cell><cell>DAN [42]</cell><cell>MJ+ST</cell><cell>word</cell><cell>94.3</cell><cell>89.2</cell><cell>-</cell><cell>95.0</cell><cell>-</cell><cell>93.9</cell><cell>-</cell><cell cols="2">74.5 80.0</cell><cell>-</cell></row><row><cell></cell><cell>CA-FCN [24]</cell><cell>ST</cell><cell>word,char</cell><cell>91.9</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Segmentation</cell><cell cols="3">TextScanner[40] MJ + ST word,char</cell><cell>93.9</cell><cell>90.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell cols="2">79.4 84.3</cell><cell>-</cell></row><row><cell></cell><cell>CHAR[17]</cell><cell>MJ</cell><cell>word</cell><cell>-</cell><cell>68.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Classification</cell><cell>CSTR</cell><cell>MJ + ST</cell><cell>word</cell><cell>93.7</cell><cell>90.1</cell><cell>95</cell><cell cols="4">94.8 95.3 93.2 85.6</cell><cell>81.6</cell><cell>-</cell><cell>85</cell></row><row><cell></cell><cell>STN-CSTR</cell><cell>MJ + ST</cell><cell>word</cell><cell>94.2</cell><cell cols="6">92.3 95.3 95.4 96.3 94.1 86.1</cell><cell>82.0</cell><cell>-</cell><cell>86.2</cell></row><row><cell cols="5">which demonstrates our superiority over CHAR. In particu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">lar, our approach gives accuracy increases of 22.1% (68.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">to 90.1%) on SVT and 13.7% gains (79.5% to 93.2%) on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IC13.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of the performance between CTC and CE with different prediction network.</figDesc><table><row><cell></cell><cell cols="2">Method Average</cell></row><row><cell>SHPN</cell><cell>CTC CE</cell><cell>83.8 83.6</cell></row><row><cell>SEPN</cell><cell>CTC CE</cell><cell>83.2 83.2</cell></row><row><cell>SPPN</cell><cell>CTC CE</cell><cell>82.4 84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of CPNet. "Base" means the basic CEbased model with SPPN. "EM" means the enhanced module. "SADM" means the semantic-aware downsampling module. CP-Net = Base + EM + SADM.</figDesc><table><row><cell>Method</cell><cell>Average</cell></row><row><cell>Base</cell><cell>84.1</cell></row><row><cell>Base + EM</cell><cell>87.2</cell></row><row><cell>Base + EM + SADM</cell><cell>87.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation Study of STN.</figDesc><table><row><cell>Method</cell><cell>Average</cell></row><row><cell>CSTR</cell><cell>89.0</cell></row><row><cell>CSTR + STN</cell><cell>89.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghun</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geewook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Kiss: Keeping it simple for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08400</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viswanath</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pp-ocr: A practical ultra lightweight ocr system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoting</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Dang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09941</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recurrent calibration network for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunze</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07145</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gtc: Guided training of ctc towards efficient and accurate scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11005" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning, NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Fernandez</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<meeting><address><addrLine>Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On recognizing texts of arbitrary shapes with 2d self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghun</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="546" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scatter: selective context attentional scene text recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Star-net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Master: Multi-aspect non-local network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02562</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions: entries, results, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC-British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nrtr: A norecurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenfen</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Textscanner: Reading characters in order for robust scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12120" to="12127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Decoupled attention network for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiang</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12216" to="12224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A simple and strong convolutional-attention network for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanning</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01375</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Symmetry-constrained rectification network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushuo</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9147" to="9156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
