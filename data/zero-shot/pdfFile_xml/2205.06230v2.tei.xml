<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Open-Vocabulary Object Detection with Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
							<email>agritsenko@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Open-Vocabulary Object Detection with Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>open-vocabulary detection</term>
					<term>transformer</term>
					<term>vision transformer</term>
					<term>zero-shot detection</term>
					<term>image-conditioned detection</term>
					<term>one-shot object detec- tion</term>
					<term>contrastive learning</term>
					<term>image-text models</term>
					<term>foundation models</term>
					<term>CLIP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many recent works aim to transfer the language capabilities of these models to object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b19">20]</ref>. These methods, for example, use distillation against embeddings of image crops <ref type="bibr" target="#b11">[12]</ref>, weak supervision with image-level labels <ref type="bibr" target="#b45">[46]</ref>, or self-training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>. Here, we provide a simple architecture and end-to-end training recipe that achieves strong open-vocabulary detection without these methods, even on categories not seen during training.</p><p>We start with the Vision Transformer architecture <ref type="bibr" target="#b21">[22]</ref>, which has been shown to be highly scalable, and pre-train it contrastively on a large image-text dataset <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b18">19]</ref>. To transfer the model to detection, we make a minimal set of changes: We remove the final token pooling layer and instead attach a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model <ref type="bibr" target="#b1">[2]</ref>  <ref type="figure" target="#fig_8">(Figure 1</ref>). We fine-tune the pre-trained model on standard detection datasets using a bipartite matching loss <ref type="bibr" target="#b5">[6]</ref>. Both the image and the text model are fine-tuned end-to-end.</p><p>We analyze the scaling properties of this approach and find that increasing model size and pre-training duration continue to yield improvements in detection performance beyond 20 billion image-text pairs. This is important since imagetext pairs, in contrast to detection data, are abundant and allow further scaling.</p><p>A key feature of our model is its simplicity and modularity. Since the image and text components of our model are not fused, our model is agnostic to the source of query representations. We can therefore use our model without modification as a one-shot detection learner simply by querying it with imagederived embeddings. One-shot object detection is the challenging problem of detecting novel objects solely based on a query image patch showing the object <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>. The image-conditioned one-shot ability is a powerful extension to text-conditioned detection because it allows detecting objects that are difficult to describe through text (yet easy to capture in an image), such as specialized technical parts. Despite using a generic architecture not specialized for this problem, we improve the state of the art for one-shot detection on unseen COCO categories (held out during training) from 26.0 to 41.8 AP50, an improvement of 72%.</p><p>For open-vocabulary text-conditioned detection, our model achieves 34.6% AP overall and 31.2% AP rare on unseen classes on the LVIS dataset.</p><p>In summary, we make the following contributions:</p><p>1. A simple and strong recipe for transferring image-level pre-training to openvocabulary object detection. 2. State-of-the-art one-shot (image conditional) detection by a large margin. <ref type="bibr" target="#b2">3</ref>. A detailed scaling and ablation study to justify our design. We believe our model will serve as a strong baseline that can be easily implemented in various frameworks, and as a flexible starting point for future research on tasks requiring open-vocabulary localization. We call our method Vision Transformer for Open-World Localization, or OWL-ViT for short.   <ref type="figure" target="#fig_8">Fig. 1</ref>. Overview of our method. Left: We first pre-train an image and text encoder contrastively using image-text pairs, similar to CLIP <ref type="bibr" target="#b32">[33]</ref>, ALIGN <ref type="bibr" target="#b18">[19]</ref>, and LiT <ref type="bibr" target="#b43">[44]</ref>. Right: We then transfer the pre-trained encoders to open-vocabulary object detection by removing token pooling and attaching light-weight object classification and localization heads directly to the image encoder output tokens. To achieve open-vocabulary detection, query strings are embedded with the text encoder and used for classification. The model is fine-tuned on standard detection datasets. At inference time, we can use text-derived embeddings for open-vocabulary detection, or image-derived embeddings for few-shot image-conditioned detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Contrastive Vision-Language Pre-Training. The idea of embedding images and text into a shared space has been used to achieve "zero-shot" generalization for a long time <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>. Thanks to innovations in contrastive losses and better architectures, recent models can learn consistent visual and language representations from web-derived image and text pairs without the need for explicit human annotations. This vastly increases the available training data and has led to large improvements on zero-shot classification benchmarks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32]</ref>. While any of the recent image-text models are compatible with our approach, our model and dataset are most similar to LiT <ref type="bibr" target="#b43">[44]</ref> and ALIGN <ref type="bibr" target="#b18">[19]</ref>.</p><p>Closed-Vocabulary Object Detection. Object detection models have been traditionally formulated for closed-vocabulary settings. Initially, "one-stage" and "two-stage" detectors, such as SSD <ref type="bibr" target="#b27">[28]</ref> and Faster-RCNN <ref type="bibr" target="#b33">[34]</ref> respectively, proliferated. More recently, DETR <ref type="bibr" target="#b5">[6]</ref> showed that object detection can be framed as a set prediction problem, trained with bipartite matching, and achieve competitive results. Notably, such architectures do not require region proposal generation or non-maximum suppression. Follow-up works have proposed more efficient variants of DETR <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37]</ref>, including architectures without a "decoder-stage" <ref type="bibr" target="#b8">[9]</ref>. Our work also simplifies DETR, in that we do not use a decoder. Compared to <ref type="bibr" target="#b8">[9]</ref>, which uses additional "detection" tokens, we further simplify the model by predicting one object instance directly from each image token.</p><p>Long-Tailed and Open-Vocabulary Object Detection. To go beyond a closed vocabulary, fixed classification layers can be replaced by language em-beddings to create open-vocabulary detectors <ref type="bibr" target="#b1">[2]</ref>. Open-vocabulary object detection has recently seen much progress from combining contrastively trained image-text models and classic object detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b41">42]</ref>. The main challenge in this task is how to transfer the image-level representations of the image-text backbone to detection despite the scarcity of localized annotations for rare classes. Making efficient use of the image-text pre-training is crucial since it allows for scaling without the need for expensive human annotations. Various approaches have been proposed. ViLD <ref type="bibr" target="#b11">[12]</ref> distills embeddings obtained by applying CLIP or ALIGN to cropped image regions from a class-agnostic region proposal network (RPN). The RPN, however, limits generalization performance on novel objects, which is exacerbated by ViLD's two-step distillationtraining process. Multistage training is also used by RegionCLIP, which generates pseudo-labels on captioning data, followed by region-text contrastive pretraining, and transfer to detection. In contrast, our method fine-tunes both image and text models end-to-end on publicly available detection datasets, which simplifies training and improves generalization to unseen classes. MDETR <ref type="bibr" target="#b19">[20]</ref> and GLIP <ref type="bibr" target="#b25">[26]</ref> use a single text query for the whole image and formulate detection as the phrase grounding problem. This limits the number of object categories that can be processed per forward pass. Our architecture is simpler and more flexible in that it performs no image-text fusion and can handle multiple independent text or image-derived queries. OVR-CNN <ref type="bibr" target="#b41">[42]</ref> is most similar to our approach in that it fine-tunes an image-text model to detection on a limited vocabulary and relies on image-text pre-training for generalization to an open vocabulary. However, we differ in all modelling and loss function choices. We use ViT <ref type="bibr" target="#b21">[22]</ref> instead of their ResNet <ref type="bibr" target="#b14">[15]</ref>, a DETR-like model instead of their Faster-RCNN <ref type="bibr" target="#b33">[34]</ref> and image-text pre-training as in LiT <ref type="bibr" target="#b43">[44]</ref> instead of their PixelBERT <ref type="bibr" target="#b17">[18]</ref> and visual grounding loss. Orthogonal to our approach, Detic <ref type="bibr" target="#b45">[46]</ref> improves long-tail detection performance with weak supervision by training only the classification head on examples where only image-level annotations are available. We note that in our definition of open-vocabulary detection, object categories may overlap between detection training and testing. When we specifically refer to detecting categories for which no localized instances were seen during training, we use the term zero-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Conditioned Detection.</head><p>Related to open-vocabulary detection is the task of image-conditioned detection, which refers to the ability to detect objects matching a single query image which shows an object of the category in question <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>. This task is also called one-shot object detection because the query image is essentially a single training example. Image-based querying allows openworld detection when even the name of the object is unknown, e.g. for unique objects or specialized technical parts. Our model can perform this task without modifications by simply using image-derived instead of text-derived embeddings as queries. Recent prior works on this problem have focused mainly on architectural innovations, for example using sophisticated forms of cross-attention between the query and target image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>. Our approach instead relies on a simple but large model and extensive image-text pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to create a simple and scalable open-vocabulary object detector. We focus on standard Transformer-based models because of their scalability <ref type="bibr" target="#b21">[22]</ref> and success in closed-vocabulary detection <ref type="bibr" target="#b5">[6]</ref>. We present a two-stage recipe:</p><p>1. Contrastively pre-train image and text encoders on large-scale image-text data. 2. Add detection heads and fine-tune on medium-sized detection data.</p><p>The model can then be queried in different ways to perform open-vocabulary or few-shot detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>Architecture. Our model uses a standard Vision Transformer as the image encoder and a similar Transformer architecture as the text encoder ( <ref type="figure" target="#fig_8">Figure 1</ref>). To adapt the image encoder for detection, we remove the token pooling and final projection layer, and instead linearly project each output token representation to obtain per-object image embeddings for classification ( <ref type="figure" target="#fig_8">Figure 1</ref>, right). The maximum number of predicted objects is therefore equal to the number of tokens (sequence length) of the image encoder. This is not a bottleneck in practice since the sequence length of our models is at least 576 (ViT-B/32 at input size 768 ? 768), which is larger than the maximum number of instances in today's datasets (e.g., 294 instances for LVIS <ref type="bibr" target="#b12">[13]</ref>). Box coordinates are obtained by passing token representations through a small MLP. Our setup resembles DETR <ref type="bibr" target="#b5">[6]</ref>, but is simplified by removing the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-vocabulary object detection.</head><p>For open-vocabulary classification of detected objects, we follow prior work and use text embeddings, rather than learned class embeddings, in the output layer of the classification head <ref type="bibr" target="#b1">[2]</ref>. The text embeddings, which we call queries, are obtained by passing category names or other textual object descriptions through the text encoder. The task of the model then becomes to predict, for each object, a bounding box and a probability with which each query applies to the object. Queries can be different for each image. In effect, each image therefore has its own discriminative label space, which is defined by a set of text strings. This approach subsumes classical closedvocabulary object detection as the special case in which the complete set of object category names is used as query set for each image.</p><p>In contrast to several other methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref>, we do not combine all queries for an image into a single token sequence. Instead, each query consists of a separate token sequence which represents an individual object description, and is individually processed by the text encoder. In addition, our architecture includes no fusion between image and text encoders. Although early fusion seems intuitively beneficial, it dramatically reduces inference efficiency because encoding a query requires a forward pass through the entire image model and needs to be repeated for each image/query combination. In our setup, we can compute query embeddings independently of the image, allowing us to use thousands of queries per image, many more than is possible with early fusion <ref type="bibr" target="#b25">[26]</ref>.</p><p>One-or Few-Shot Transfer. Our setup does not require query embeddings to be of textual origin. Since there is no fusion between image and text encoders, we can supply image-instead of text-derived embeddings as queries to the classification head without modifying the model. By using embeddings of prototypical object images as queries, our model can thus perform image-conditioned oneshot object detection. Using image embeddings as queries allows detection of objects which would be hard to describe in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Image-Level Contrastive Pre-Training. We pre-train the image and text encoder contrastively using the same image-text dataset and loss as in <ref type="bibr" target="#b43">[44]</ref>  <ref type="figure" target="#fig_8">(Figure 1</ref>, left). We train both encoders from scratch with random initialization with a contrastive loss on the image and text representations. For the image representation, we use multihead attention pooling (MAP) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43]</ref> to aggregate token representation. The text representation is obtained from the final end-ofsequence (EOS) token of the text encoder. Alternatively, we use publicly available pre-trained CLIP models <ref type="bibr" target="#b32">[33]</ref> (details in Appendix A1. <ref type="bibr" target="#b2">3</ref>).</p><p>An advantage of our encoder-only architecture is that nearly all of the model's parameters (image and text encoder) can benefit from image-level pre-training. The detection-specific heads contain at most 1.1% (depending on the model size) of the parameters of the model.</p><p>Training the Detector. Fine-tuning of pre-trained models for classification is a well-studied problem. Classifiers, especially large Transformers, require carefully tuned regularization and data augmentation to perform well. Recipes for classifier training are now well established in the literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>. Here, we aim to provide a similar fine-tuning recipe for open-vocabulary detection.</p><p>The general detection training procedure of our model is almost identical to that for closed-vocabulary detectors, except that we provide the set of object category names as queries for each image. The classification head therefore outputs logits over the per-image label space defined by the queries, rather than a fixed global label space.</p><p>We use the bipartite matching loss introduced by DETR [6], but adapt it to long-tailed/open-vocabulary detection as follows. Due to the effort required for annotating detection datasets exhaustively, datasets with large numbers of classes are annotated in a federated manner <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. Such datasets have nondisjoint label spaces, which means that each object can have multiple labels. We therefore use focal sigmoid cross-entropy <ref type="bibr" target="#b47">[48]</ref> instead of softmax cross-entropy as the classification loss. Further, since not all object categories are annotated in every image, federated datasets provide both positive (present) and negative (known to be absent) annotations for each image. During training, for a given image, we use all its positive and negative annotations as queries. Additionally, we randomly sample categories in proportion to their frequency in the data and add them as "pseudo-negatives" to have at least 50 negatives per image <ref type="bibr" target="#b46">[47]</ref>.</p><p>Even the largest federated detection datasets contain only ? 10 6 images, which is small in contrast to the billions of image-level weak labels which exist for pre-training <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19]</ref>. It is known that large Transformers trained on datasets of this size (such as ImageNet-1k) require carefully-tuned regularization and data augmentation to perform well <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>. We found the same to be true for detection training and provide a detailed breakdown of the augmentations and regularizations required to achieve very high performance with large Transformers in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Details</head><p>For the image model, we use standard Vision Transformers <ref type="bibr" target="#b21">[22]</ref>. We follow the nomenclature from <ref type="bibr" target="#b21">[22]</ref> for model size, patch size, and Transformer vs. hybrid architectures. For example, B/32 refers to ViT-Base with patch size 32, while R50+H/32 refers to a hybrid ResNet50 + ViT-Huge with stride 32.</p><p>For the text model, we use a Transformer architecture similar to the image model. Unless otherwise noted, we use a text model with 12 layers, 512 hidden size (D), 2048 MLP size and 8 heads (this is smaller than B).</p><p>Image and text models are first pre-trained on the image level and then finetuned on object-level annotations. Pre-training is performed from scratch as in LiT <ref type="bibr" target="#b43">[44]</ref> (uu in their notation) on their dataset of 3.6 billion image-text pairs.</p><p>After pre-training, token pooling is removed and detection heads are added (see Section 3.1 and <ref type="figure" target="#fig_8">Figure 1</ref>). The model predicts one box for each output token. We add a bias to the predicted box coordinates such that each box is by default centered on the image patch that corresponds to the token from which this box is predicted when arranging the token sequence as a 2D grid. The model therefore predicts the difference from that default location, similar to how Region Proposal Networks <ref type="bibr" target="#b33">[34]</ref> predict offsets with respect to pre-defined anchors. Although there is no strict correspondence between image patches and tokens representations later in the Transformer network, biasing box predictions in this way speeds up training and improves final performance (Section 4.6).</p><p>We use an image size of 224 ? 224 in most models for pre-training (see Appendix A1.3) and larger sizes for detection fine-tuning and evaluation (specified in <ref type="table">Table 1</ref>). To change model input size after pre-training, we resize the image position embeddings with linear interpolation. Models are fine-tuned at a batch size of 256 for at most 140'000 steps (fewer for larger models). We implement our model using JAX <ref type="bibr" target="#b4">[5]</ref> and the Scenic library <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detection Data</head><p>Due to the open-vocabulary design of our model, we can easily combine datasets with different label spaces by replacing integer labels with class name strings. For object-level training, we use publicly available detection datasets with a total of around 2 million images (OpenImages V4 (OI) <ref type="bibr" target="#b23">[24]</ref>, Objects 365 (O365) <ref type="bibr" target="#b34">[35]</ref>, and/or Visual Genome (VG) <ref type="bibr" target="#b22">[23]</ref>, as indicated). Evaluation is performed on the COCO <ref type="bibr" target="#b26">[27]</ref>, LVIS <ref type="bibr" target="#b12">[13]</ref>, and O365. For dataset details, see Appendix A1.2. <ref type="table">Table 1</ref>. Open-vocabulary and zero-shot performance on LVIS v1.0 val. For our models, we remove annotations matching LVIS rare category names from all detection training datasets, such that AP LVIS rare measures zero-shot performance. Gray numbers indicate models trained on the LVIS frequent and common ("base") annotations. For reference, ViT-B/32 is comparable to ResNet50 in inference compute (139.6 vs 141.5 GFLOPs). For our models, we report the mean performance over three fine-tuning runs. Results for COCO and O365 are provided in Appendix A1.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Since OI, VG, O365 and the image-level pre-training data contain images that are also in COCO / LVIS, we use a strict deduplication procedure to remove any COCO or LVIS test and validation images from all datasets we use for training (see Appendix A1.2 for details). Unless otherwise noted, we mix OI and VG randomly at a ratio of 70% to 30% for detection training in our experiments. In <ref type="table">Table 1</ref>, as indicated, we use either LVIS base training (for comparability to prior work), or O365 and VG at a ratio of 80% to 20%. We use a range of image and label augmentations, which we discuss in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Open-Vocabulary Detection Performance</head><p>We use LVIS v1.0 val <ref type="bibr" target="#b12">[13]</ref> as our main benchmark since this dataset has a long tail of rare categories and is therefore well-suited to measure open-vocabulary performance. For evaluation, we use all category names as query for each image, i.e. 1203 queries per image for LVIS. Class predictions are ensembled over seven prompt templates as described in Section 4.6. Some LVIS categories appear in the datasets we use for training. To measure performance on unseen categories, we therefore remove from our training data all box annotations with labels that match any of the LVIS "rare" categories. The AP LVIS rare metric therefore measures In both cases, the highest score is given to instances of the species matching the query. In contrast, text-based querying (not shown) detects the correct species only for the top example ("swallowtail butterfly") but not for the bottom ("luna moth").</p><p>the "zero-shot" performance of our model in the sense that the model has not seen localized annotations for these categories. <ref type="table">Table 1</ref> shows LVIS results for our models and a range of prior work. We compare to open-vocabulary models that do not train on the full LVIS dataset. Results obtained by training on parts of LVIS (e.g. "base" categories <ref type="bibr" target="#b11">[12]</ref>) are shown in gray. Our method is highly competitive across architecture sizes in both open-vocabulary (AP LVIS ) and zero-shot (AP LVIS rare ) scenarios. Our best model achieves 31.2% AP LVIS rare and uses a publicly available CLIP backbone. For comparison to prior work, we also provide results on MS-COCO 2017 and Objects 365. For these evaluations, we train models on OI+VG instead of O365+VG, to measure generalization. However, most COCO and O365 categories are present in the training data and we do not remove them, since they constitute a large fraction of the available annotations. Our COCO and O365 results are therefore not "zero-shot", but test the open-vocabulary transfer ability of our model. Our best model (CLIP L/14; see <ref type="table">Table 1</ref>) achieves 43.5% AP COCO ; a version of the model trained without O365 achieves 15.8% AP O365 (further results in Appendix A1.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Few-Shot Image-Conditioned Detection Performance</head><p>As described in Section 3.1, our model can perform one-or few-shot object detection simply be replacing text-derived query embeddings with image-derived query embeddings. In few-shot detection, we are given a query image with a box around an example object. The goal is to detect objects of the same category as the example in new target images. To get the query embedding, we first run inference on the query image and select a predicted detection which has high box overlap with the query box (after some filtering; see Appendix A1.7 for details). We then use the image embedding of that prediction as query on the test images. <ref type="table">Table 2</ref>. One-and few-shot image-conditioned detection performance on COCO AP50. Our method (R50+H/32 architecture) strongly outperforms prior work and also shows marked improvements as the number of conditioning queries is increased to k = 10. COCO category splits as in <ref type="bibr" target="#b15">[16]</ref>. Because the evaluation is stochastic, for our results, we report the average across 3 runs. For evaluation on this task, we follow the procedure described in <ref type="bibr" target="#b15">[16]</ref>: During detection training, we hold out some COCO categories to evaluate on, and in addition all synonymous and semantically descendant categories that appear in our detection training data. We do not modify the image-text pre-training stage.</p><p>Despite not being designed specifically for this task, our model strongly outperforms the best task-specific prior work by a margin of 72% across the four COCO splits as shown in <ref type="table">Table 2</ref>. Unlike prior work, our model does not entangle query image and target image features during inference, which enables us to run our models on thousands of different image embeddings simultaneously and efficiently, enhancing its practicality.</p><p>To move beyond a single query example (one-shot) to few-shot predictions, we can simply average image embeddings for multiple query examples for each category. This leads to further significant improvements ( <ref type="table">Table 2</ref>, bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scaling of Image-Level Pre-Training</head><p>After establishing that our method achieves strong open-vocabulary, zero-shot, and image-conditioned detection performance, we next analyze its scaling properties and design choices. We focus on image-level pre-training in this section. In Section 4.6, we will describe the fine-tuning methods that are necessary for successful transfer of the pre-trained model to detection.</p><p>To understand how image-level pre-training relates to final detection performance, we systematically explored the dimensions of pre-training duration, model size, and model architecture. For every configuration, we pre-trained and then fine-tuned several models across a range of learning rates and weight decays, since the optimal settings of these parameters vary by configuration (see Appendix A1.3 for a list of covered settings). We first consider how well image-level pre-training transfers to detection in general. <ref type="figure" target="#fig_2">Figure 3</ref> shows the relationship between image-level performance (zeroshot ImageNet accuracy) and object-level performance (zero-shot AP LVIS rare ) for all architecture, size, and pre-training-duration configurations covered by our study (the best result across learning rates and weight decays is shown). We find that, while the best object-level models typically also have good image-level performance, the reverse is not true: many models that do well to the image-level task transfer poorly to detection. In other words, high image-level performance is necessary, but not sufficient, for strong transfer to detection.</p><p>Which factors contribute to strong transfer? Prior work on classification found that pre-training and model size must be scaled together to achieve optimal transfer -over-training small models on large data can even lead to reduced performance <ref type="bibr" target="#b20">[21]</ref>. We find this effect to be even stronger for transfer to detection. As the amount of pre-training is increased, detection performance increases at first but then peaks, while image-level performance continues to increase <ref type="figure" target="#fig_2">(Figure 3</ref>, right). However, the positive trend of detection performance with pre-training can be extended by increasing model size and improving detection fine-tuning <ref type="figure" target="#fig_2">(Figure 3</ref>, right, R50+H/32). Given that increasing model size improves performance, an important question is which architectures have the most favorable scaling properties. For classification, Transformer-based architectures have been found to be more efficient in terms of pre-training compute than ResNets, and hybrid ResNet-Transformer architectures to be the most efficient, at least at smaller computational bud- Colored markers indicate the best model of a given size across all explored hyperparameters; light gray markers indicate the suboptimal hyperparameters. Asterisks ( * ) indicate models trained with random negative labels. Right: Architecture also influences which aspects of the task a model learns: Pure ViTs perform systematically better at zero-shot detection (AP LVIS rare ) than hybrid architectures at a given overall object-level performance (AP LVIS ). We speculate that ViTs are biased towards learning semantic generalization, whereas ResNets/Hybrids are biased towards learning localization of known classes. This difference diminishes as model size and performance increases.</p><p>gets <ref type="bibr" target="#b21">[22]</ref>. In addition, ResNets were found to be better when little pre-training data is available, but were overtaken by Transformers as available data increases <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. We performed a similar analysis for detection. Using detection inference compute as the measure of model size, and choosing the best hyperparameters and pre-training duration for each size, we found that hybrid models tend to be more efficient than pure ViTs at small model sizes, while ResNets perform poorly in our setup <ref type="figure" target="#fig_3">(Figure 4</ref>). However, for large models, pure ViTs overtake hybrids. To start explaining this difference, we compared overall and zero-shot detection performance and found a clear dissociation between hybrids and pure Transformers (at least at small model sizes; <ref type="figure" target="#fig_3">Figure 4</ref>, right). This perhaps indicates that Transformers are more biased than hybrid architectures towards learning semantic generalization (necessary for high zero-shot performance), which might be beneficial when large-scale pre-training is possible. Overall, our findings go beyond those for classification and suggest that further scaling efforts should focus on pure Transformer architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">How to Unlock Pre-Training Potential for Detection</head><p>In Section 4.5, we found that strong image-level performance is necessary, but not sufficient, for strong detection performance. We will now describe our recipe for obtaining strong open-vocabulary detection performance after image-level pre-training. Ultimately, all components of our recipe aim at reducing overfitting on the relatively small number of available detection annotations, and the small semantic label space covered by the annotations. Our approach relies on (i) measures to stabilize optimization, (ii) careful use of the available detection <ref type="table">Table 3</ref>. Ablation study of the main methodological improvements necessary for successful transfer of image-text models to detection. For simplicity, difference in AP to the baseline is shown. Except for the experiment retraining LVIS rare labels (last row), all differences are expected to be negative. To reduce variance, all results are averaged across two replicates. All ablations were carried out for the ViT-R26+B/32 model, and unless otherwise specified used a 70K step training schedule. training data, and (iii) a range of data augmentations. We discuss these ablations in detail below, where numbers in italic (e.g. (15)) refer to individual ablation experiments in <ref type="table">Table 3</ref>. Importantly, the optimal recipe for zero-shot performance (AP LVIS rare ) does not necessarily maximize in-distribution performance (AP OI ). We discuss this finding and further ablations in Appendix A1.9.</p><p>Stabilizing Optimization. The goal of fine-tuning is to learn from the available detection data without destroying the representations learned during pretraining. To this end, we take the following measures. First, we reduce the learning rate of the text encoder to 2 ? 10 ?6 (i.e. 100? smaller than the image encoder learning rate) during fine-tuning (3). This reduces overfitting, possibly by preventing the text encoder from "forgetting" the semantics learned during pre-training while fine-tuning on the small space of detection labels. Interestingly, freezing the text encoder completely yields poor results. Second, we bias predicted box coordinates <ref type="bibr" target="#b10">(11)</ref> to be centred at the position of the corresponding token on the 2D grid, as described in Section 3.1. This speeds up learning and improves final performance, presumably by breaking symmetry during the bipartite matching used in the loss. Third, for larger models, we use stochastic depth regularisation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref> with probability of 0.1 on both the image and text encoders, and shorter training schedules (Section A1.3).</p><p>Careful Use of Available Detection Data. As our ablations show <ref type="table">(Table 3)</ref>, the amount of detection training data is a limiting factor for the performance of our models. Therefore, we combine multiple datasets -OI+VG for most models in our study <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref>, and O365+VG for the largest models as indicated in <ref type="table">Table 1</ref>. Further, we take care to keep the available annotations free of noise:</p><p>We remove "group" annotations and "not exhaustively annotated" categories (14) from datasets indicating such annotations (e.g. OI). These annotations provide conflicting supervision to the model because it cannot learn (except through memorization) which annotations are exhaustive and which are not. Removing them improves performance of larger models. In addition, we remove partial boxes left by random crop augmentation, since these can also provide conflicting supervision if most of an object was actually cropped out. Retaining instances with at least 60% of their original area leads to better results than retaining all <ref type="bibr" target="#b11">(12)</ref> or only uncropped (13) instances.</p><p>Augmentations. Finally, we enrich the available detection labels through augmentation of both images and queries. On the images, we use random cropping (removing partially cropped boxes as described above). Additionally, we use image scale augmentation similar to "large scale jitter" <ref type="bibr" target="#b10">[11]</ref>. However, instead of simply resizing and padding images, we tile several downscaled images into one large "mosaic" image. We randomly sample single images, 2 ? 2 grids, and 3 ? 3 grids with probabilities 0.5, 0.33, and 0.17, respectively (7-9). To augment the queries (category names), we use random prompts during training, and ensemble predictions over several prompts for evaluation <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref>. We use the 80 CLIP prompts for training and ensemble over the 7 "best" CLIP prompts (as defined in <ref type="bibr" target="#b32">[33]</ref>) during evaluation. Finally, we randomly sample pseudonegative labels for each image until there are at least 50 negative labels <ref type="bibr" target="#b46">[47]</ref>. Further implementation details are provided in Appendices A1.5 and A1.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a simple recipe for transferring contrastively trained image-text models to detection. Our method achieves zero-shot detection results competitive with much more complex approaches on the challenging LVIS benchmark and outperforms existing methods on image-conditioned detection by a large margin. Our results suggest that pre-training on billions of image-text examples confers strong generalization ability that can be transferred to detection even if only relatively limited object-level data are available (millions of examples). In our analyses we disentangle the determinants of successful transfer of image-level representations to detection, and show that pre-training simple, scalable architectures on more data leads to strong zero-shot detection performance, mirroring previous observations for image classification tasks. We hope that our model will serve as a strong starting point for further research on open-world detection.</p><p>The appendix provides additional examples, results and methodological details. For remaining questions, please refer to the code at github.com/google-research/ scenic/tree/main/scenic/projects/owl_vit.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1 Qualitative Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2 Detection Datasets</head><p>Five datasets with object detection annotations were used for fine-tuning and evaluation in this work. <ref type="table" target="#tab_5">Table A1</ref> shows relevant statistics for each of these datasets:</p><p>MS-COCO (COCO) <ref type="bibr" target="#b26">[27]</ref>: The Microsoft Common Objects in Context dataset is a medium-scale object detection dataset. It has about 900k bounding box annotations for 80 object categories, with about 7.3 annotations per image. It is one of the most used object detection datasets, and its images are often used within other datasets (including VG and LVIS). This work uses the 2017 train, validation and test splits.</p><p>Visual Genome (VG) <ref type="bibr" target="#b22">[23]</ref> contains dense annotations for objects, regions, object attributes, and their relationships within each image. VG is based on COCO images, which are re-annotated with free-text annotations for an average of 35 objects per image. All entities are canonicalized to WordNet synsets. We only use object annotations from this dataset, and do not train models using the attribute, relationship or region annotations.</p><p>Objects 365 (O365) <ref type="bibr" target="#b34">[35]</ref> is a large-scale object detection dataset with 365 object categories. The version we use has over 10M bounding boxes with about 15.8 object annotations per image.</p><p>LVIS <ref type="bibr" target="#b12">[13]</ref>: The Large Vocabulary Instance Segmentation dataset has over a thousand object categories, following a long-tail distribution with some categories having only a few examples. Similarly to VG, LVIS uses the same images as in COCO, re-annotated with a larger number of object categories. In contrast to COCO and O365, LVIS is a federated dataset, which means that only a subset of categories is annotated in each image. Annotations therefore include positive and negative object labels for objects that are present and categories that are not present, respectively. In addition, LVIS categories are not pairwise disjoint, such that the same object can belong to several categories.</p><p>OpenImages V4 (OI) <ref type="bibr" target="#b23">[24]</ref> is currently the largest public object detection dataset with about 14.6 bounding box annotations (about 8 annotations per image). Like LVIS, it is a federated dataset. De-duplication Our detection models are typically fine-tuned on a combination of OpenImages V4 (OI) and Visual Genome (VG) datasets and evaluated on MS-COCO 2017 (COCO) and LVIS. In several experiments our models are additionally trained on Objects 365 (O365). We never train on COCO and LVIS datasets, but the public versions of our training datasets contain some of the same images as the COCO and LVIS validation sets. To ensure that our models see no validation images during training, we filter out images from OI, VG and O365 train splits that also appear in LVIS and COCO validation and tests splits following a procedure identical to <ref type="bibr" target="#b20">[21]</ref>. De-duplication statistics are given in <ref type="table" target="#tab_6">Table A2</ref>.  <ref type="table">Table A3</ref> provides an exhaustive overview of the hyper-parameter settings used for our main experiments. Beyond this, we used cosine learning rate decay; used focal loss with ? = 0.3 and ? = 2.0; set equal weights for the bounding box, gIoU and classification losses <ref type="bibr" target="#b5">[6]</ref>; used the Adam optimizer with ? 1 = 0.9, ? 2 = 0.999; used per-example global norm gradient clipping (see Section A1.9); limited the text encoder input length to 16 tokens for both LIT and CLIPbased models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.3 Hyper-parameters</head><p>CLIP-based models. The visual encoder of the publicly available CLIP models provides, in addition to the image embedding features, a class token. In order to evaluate whether the information in the class token is useful for detection fine-tuning, we explored to either drop this token, or to merge it into other feature map tokens by multiplying it with them. We found that multiplying the class token with the feature map tokens, followed by layer norm, worked best for the majority of architectures, so we use this approach throughout. Other hyperparameters used in the fine-tuning of CLIP models are shown in <ref type="table">Table A3</ref>. <ref type="table">Table A3</ref>. List of hyperparameters used for all models shown in the paper. Asterisks ( * ) indicate parameters varied in sweeps. MAP and GAP indicate the use of multihead attention pooling and global average pooling for image-level representation aggregation. Where two numbers are given for the droplayer rate, the first is for the image encoder and the second for the text encoder. Baseline models for the ablation study <ref type="table">(Tables 3 and A5)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.4 Pre-Training Image Resolution</head><p>We investigated the effect of the image size used during image-text pre-training, on zero-shot classification and detection performance ( <ref type="figure" target="#fig_2">Figure A3</ref>). To reduce clutter the results are shown for the ViT-B/32 architecture only, but the observed trends extend to other architectures, including Hybrid Transformers. The use of larger images during pre-training consistently benefits zero-shot classification, but makes no significant difference for the detection performance. We thus default to the commonly used 224 ? 224 resolution for pre-training. We used 288 ? 288 for some of our experiments with Hybrid Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.5 Random Negatives</head><p>Our models are trained on federated datasets. In such datasets, not all categories are exhaustively annotated in every image. Instead, each image comes with a number of labeled bounding boxes (making up the set of positive categories), and a list of categories that are known to be absent from the image (i.e., negative categories). For all other categories, their presence in the image unknown. Since the number of negative labels can be small, prior work has found it beneficial to randomly sample "pseudo-negative" labels for each image and add them to the annotations <ref type="bibr" target="#b46">[47]</ref>. We follow the same approach and add randomly sampled pseudo-negatives to the real negatives of each image until there are at least 50 negative categories. In contrast to <ref type="bibr" target="#b46">[47]</ref>, we sample categories in proportion to their frequency in the full dataset (i.e. a weighted combination of OI, VG, and potentially O365). We exclude categories from the sample that are among the positives for the given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.6 Image Scale Augmentation</head><p>To improve invariance of detection models to object size, prior work found it beneficial to use strong random jittering of the image scale during training <ref type="bibr" target="#b10">[11]</ref>. We use a similar approach, but follow a two-stage strategy that minimizes image padding.</p><p>First, we randomly crop each training image. The sampling procedure is constrained to produce crops with an aspect ratio between 0.75 and 1.33, and an area between 33% and 100% of the original image. Bounding box annotations are retained if at least 60% of the box area is within the post-crop image area. After cropping, images are padded to a square aspect ratio by appending gray pixels at the bottom or right edge.</p><p>Second, we assemble multiple images into grids ("mosaics") of varying sizes, to further increase the range of image scales seen by the model. We randomly sample single images, 2 ? 2 mosaics, and a 3 ? 3 mosaics, with probabilities 0.5, 0.33, and 0.17, respectively, unless otherwise noted ( <ref type="figure" target="#fig_3">Figure A4</ref>). This procedure allows us to use widely varying images scales while avoiding excessive padding and/or the need for variable model input size during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.7 One-shot (Image-Conditioned) Detection Details</head><p>Extracting Image Embeddings to Use as Queries. We are given a query image patch Q for which we would like to detect similar patches in a new target image, I. We first run inference on the image from which patch Q was selected, and extract an image embedding from our model's class head in the region of Q. In general, our model predicts many overlapping bounding boxes, some of which will have high overlap with Q. Each predicted bounding box b i has a corresponding class head feature z i . Due to our DETR-style bipartite matching loss, our model will generally predict a single foreground embedding for the object in Q and many background embeddings adjacent to it which should be ignored. Since all the background embeddings are similar to each other and different from the single foreground embedding, to find the foreground embedding, we search for the most dissimilar class embedding within the group of class embeddings whose corresponding box has IoU &gt; 0.65 with Q. We score a class embedding z i 's similarity to other class embeddings as f (z i ) = N ?1 j=0 z i ? z T j . Therefore, we use the most dissimilar class embedding argmin zi f (z i ) as our query feature when running inference on I. In about 10% of the cases, there are no predicted boxes with IoU &gt; 0.65 with Q. In these cases we fall back to using the embedding for the text query "an image of an object".</p><p>Image-Conditioned Evaluation Protocol. We follow the evaluation protocol of <ref type="bibr" target="#b15">[16]</ref>. During evaluation, we present the model with a target image containing at least one instance of a held-out MS-COCO category and a query image patch containing the same held-out category. Both the target image and the query patch are drawn from the validation set. We report the AP50 of the detections in the target image. Note that unlike typical object detection, it is assumed that there is at least one instance of the query image category within the target image. Like prior work, we use Mask-RCNN <ref type="bibr" target="#b13">[14]</ref> to filter out query patches which are too small or do not show the query object clearly. During detection training, we took care to hold out all categories related to any category in the held-out split. We removed annotations for any label which matched a <ref type="table">Table A4</ref>. Open-vocabulary detection performance on COCO and O365 datasets. The results show the open-vocabulary generalization ability of our models to datasets that were not used for training. Results for models trained on the target dataset are shown in gray. Most of our models shown here were not trained directly on COCO or O365 (they are different from the models in <ref type="table">Table 1</ref>). However, we did not remove COCO or O365 object categories from the training data, so these numbers are not "zero-shot". For our models, we report the mean performance over three fine-tuning runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone held-out label or was a descendant of a held-out label (for example, the label "girl" is a descendant label of "person"). Beyond this we also manually removed any label which was similar to a held-out category. We will publish all held-out labels with the release of our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.8 Detection results on COCO and O365</head><p>We present additional evaluation results on the COCO and O365 datasets in <ref type="table">Table A4</ref>. These results show the open-vocabulary generalization ability of our approach. Although we do not train these models directly on COCO or O365 (unless otherwise noted), our training datasets contain object categories overlapping with COCO and O365, so these results are not "zero-shot" according to our definition. The breadth of evaluation setups in the literature makes direct comparison to existing methods difficult. We strove to note the differences relevant for a fair comparison in <ref type="table">Table A4</ref>. <ref type="table">Table A5</ref> extends the ablation results provided in <ref type="table">Table 3</ref> of the main text. It uses the same training and evaluation protocol as outlined in <ref type="table">Table 3</ref>, but goes further in the range of settings and architectures (ViT-B/32 and ViT-R26+B/32) considered in the study. We discuss the additional ablations below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.9 Extended Ablation Study</head><p>Dataset ratios. In the majority of our experiments we use OI and VG datasets for training. In the ablation study presented in the main text <ref type="table">(Table 3)</ref>, we showed that having more training data (i.e. training on both VG and OI) improves zero-shot performance. Here, we further explored the optimal ratio in which these datasets should be mixed and found that a 7:3 = OI:VG ratio worked best. Note that this overweighs VG significantly compared to the relative size of these datasets. Overweighing VG might be beneficial because VG has a larger label space than OI, such that each VG example provides more valuable semantic supervision than each OI example. We also tested the relative value of VG "object" and "region" annotations. In VG, "region" annotations provide free-text descriptions of whole image regions, as opposed to the standard single-object annotations. Interestingly, we found that training on the region annotations hurts the generalization ability of our models, so we do not use them for training.</p><p>Loss normalization and gradient clipping. In its official implementation, DETR <ref type="bibr" target="#b5">[6]</ref> uses local (i.e. per-device) loss normalization and is thus sensitive to the (local) batch size. We found this to be an important detail in practice, which can significantly affect performance. We explored whether normalizing the box, gIoU and classification losses by the number of instances in the image or the number of instances in the entire batch performed better. Our experiments show that per-example normalization performs best, but only when combined with per-example gradient clipping, i.e. when clipping the gradient norm to 1.0 for each example individually, before accumulating gradients across the batch. We found that per-example clipping improves training stability, leads to overall lower losses and allows for training models with larger batch sizes.</p><p>Instance merging. Federated datasets such as OI have non-disjoint label spaces, which means that several labels can apply to the same object, either due to (near-)synonymous labels (e.g. "Jug" and "Mug"), or due to non-disjoint concepts (e.g. "Toy" and "Elephant" labels both apply to a toy elephant). Due to the annotation procedure, in which a single label is considered at a time, one object can therefore be annotated with several similar (but not identical) bounding boxes. We found it helpful to merge such instances into a single multi-label instance. Multi-label annotations are consistent with the non-disjoint nature of federated annotations and we speculate that this provides more efficient supervision to the models, since it trains each token to predict a single box for all appropriate labels. Without this instance merging, the model would be required to predict individual boxes for each label applying to an object, which clearly cannot generalize to the countless possible object labels.</p><p>To merge overlapping instances we use a randomized iterative procedure with the following steps for each image: The picked instances are then removed and the procedure is repeated until no instances with a high enough IoU are left. Having explored multiple IoU thresholds, we note that not merging instances with highly similar bounding boxes is clearly worse than merging them; and that a moderately high threshold of 0.7-0.9 works best in practice.</p><p>Learning rates. In <ref type="table">Table 3</ref> we show that using the same learning rate for the image and text encoders is clearly sub-optimal, and that it is necessary to training the text encoder with a lower learning rate. This may help to prevent catastrophic forgetting of the wide knowledge the model acquired during the contrastive pre-training stage. Here we explore a range of text encoder learning rates and demonstrate that the learning rate for the text encoder needs to be much lower (e.g. 100?) than that of the image encoder to get good zero-shot transfer (AP LVIS rare ). However, freezing the text encoder completely (learning rate 0) does not work well either. AP OI , which measure in-distribution performance, behaves in the opposite way. While using the same learning rate for the image and text encoders results in a big drop in AP LVIS rare , it increases AP OI . This demonstrates that the optimal recipe for zero-shot transfer (AP LVIS rare ) does not necessarily maximize in-distribution performance (AP OI ).</p><p>Cropped bounding box filtering. We use random image crop augmentation when training our models. Upon manual inspection of the resulting images and bounding boxes we noticed a frequent occurrence of instances with degenerate bounding boxes that no longer matched their original instance label (e.g. a bounding box around a hand with label "Person" resulting from cropping most of the person out of the image). To reduce the chance of our models overfitting due to having to memorize such instances, we remove object annotations if a large fraction of their box area falls outside of the random crop area. The optimal area threshold lies between 40% and 60%, and that neither keeping all boxes, nor keeping only uncropped boxes, performs as well (Tables 3 and A1.9).</p><p>Mosaics. As described in Appendix A1.6, we perform image scale augmentation by tiling multiple small images into one large "mosaic". We explored mosaic sizes up to 4 ? 4, and found that while using only 2 ? 2 mosaics in addition to single images is clearly worse than also including larger mosaics, for the considered resolutions and patch sizes the benefits of using larger mosaics (i.e. smaller mosaic tiles) saturates with the inclusion of 3 ? 3 or 4 ? 4 mosaics. We have not performed extensive sweeps of the mosaic ratios, and for mosaics with grid sizes from 1 ? 1 (i.e. a single image) to M ? M we use a heuristic of sampling k ? k girds with probability 2?(M ?k+1) M ?(1+M ) , such that smaller mosaics are sampled more frequently than the larger mosaics proportionally to the mosaic size.</p><p>Prompting. For generating text queries, similar to prior work, we augment object category names with prompt templates such as "a photo of a {}" (where {} is replaced by the category name) to reduce the distribution shift between image-level pre-training and detection fine-tuning. We use the prompt templates proposed by CLIP <ref type="bibr" target="#b32">[33]</ref>. During training, we randomly sample from the list of 80 CLIP prompt templates such that, within an image, every instance of a category has the same prompt, but prompt templates differ between categories and across images. During testing, we evaluate the model for each of the "7 best" CLIP prompts and ensemble the resulting predicted probabilities by averaging them. The results in <ref type="table">Table A5</ref> show that not using any prompting does not perform well, especially on the in-distribution AP OI metric. Perhaps unsurprisingly, test-time prompt ensembling works better in cases when random prompting was also used during training. In some cases, prompting can have different effects on different model architectures. For example, applying random prompt augmentation to the VG dataset tends to improve performance of the B/32 model, but worsens that of the R26+B/32 model. We speculate that this variability is due to the relatively small number of prompt templates; expanding the list of prompt templates might provide more consistent benefits. We thus only use train-time random prompting for the OI dataset, where it yields consistent benefits.</p><p>Location bias. As discussed in the main text, biasing box predictions to the location of the corresponding image patch improves training speed and final performance. The gain is especially large for the pure Transformer architecture (ViT-B/32 in <ref type="table" target="#tab_5">Table A1</ref>.9), where removing the bias reduces performance by almost 3 points on AP LVIS and AP LVIS rare , whereas the hybrid R26+B/32 drops by only slightly more than 1 point. We therefore speculate that the spatial inductive bias of the convolutional component of the hybrid serves a similar function as the location bias. <ref type="table">Table A5</ref>. Additional ablations. VG(obj) and VG(reg) respectively refer to Visual Genome object and region annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ViT-B/32</head><p>ViT-R26+B/32   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Example of one-shot image-conditioned detection. Images in the middle are used as queries; the respective detections on the target image are shown on the left and right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Image-level pre-training transfers to detection. Left: Overview of the relationship between image-level performance (zero-shot ImageNet accuracy after pretraining) and object-level performance (AP LVIS rare after detection fine-tuning) of contrastively trained image-text models. Each dot represents one pre-training configuration and its best detection performance across a range of learning rates and weight decays. Configurations vary in encoder architecture (ViT/Hybrid/ResNet), model size (in order of detection inference compute: R50, B/32, R26+B/32, R101, L/32, B/16, H/32, R50+H/32, L/16), and pre-training duration (billions of examples seen including repetitions; 3.6B unique examples). High image-level performance is necessary, but not sufficient, for high object-level performance (Pearson's r = 0.73; in contrast, image-level transfer performance correlates better with pre-training-task performance: r = 0.98). Right: Across model sizes, longer image-level pre-training translates to higher objectlevel performance. Further gains on detection are possible by scaling up fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Effect of model architecture on detection performance. Left: Hybrid architectures are more efficient than pure transformers for small models. As the model size increases (in terms of detection inference FLOPs), pure ViTs scale better than hybrids both in overall and zero-shot performance. Pure ResNets perform poorly in our setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. A2 .</head><label>A2</label><figDesc>Image conditioning examples. The center column shows the query patches and the outer columns show the detections along with the similarity score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>: B/ 32 2B 16k 3 ?</head><label>323</label><figDesc>10 ?4 1 ? 10 ?5 224 MAP 70k 256 2 ? 10 ?4 0 0.0 768 OI, VG .7/.3 .5/.33/.17 yes R26+B/32 8B 16k 3 ? 10 ?4 1 ? 10 ?5 288 MAP 70k 256 2 ? 10 ?4 0 0.0 768 OI, VG .7/.3 .5/.33/.17 yes Models used in the scaling study (Figures 3 and 4): .3 .5/.33/.17 no R50+H/32 * 12k 7 ? 10 ?4 1 ? 10 ?5 224 GAP 28k 256 2 ? 10 ?4 0 0.0 960 OI, VG .7/.3 .5/.33/.17 yes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. A3 .</head><label>A3</label><figDesc>Effect of image size used during image-level pre-training on zero-shot classification and detection performance shown for the ViT-B/32 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. A4 .</head><label>A4</label><figDesc>Example training images. Ground-truth boxes are indicated in red. From left to right, a single image, a 2 ? 2 mosaic, and a 3 ? 3 mosaic are shown. Non-square images are padded at the bottom and right (gray color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 .</head><label>1</label><figDesc>Pick the two instances with the largest bounding box overlap. 2. If their intersection over union (IoU) is above a given threshold: 2.1. Merge their labels. 2.2. Randomly pick one of the original bounding boxes as the merged instance bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>7 5 3 ?Mosaics.</head><label>753</label><figDesc>Instance merging. Baseline merges instance that overlap with IoU ? 0Text encoder learning rate. Baseline uses image LR 2 ? 10 ?4 and text LR 2 ? 10 ?6 . LR 2 ? 10 ?Cropped box filtering. Baseline retains boxes with ? 60% of their original area. No box area filtering ?Baseline uses 1-to-3-size mosaics at ratio 0.5 : 0.33 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Image-level contrastive pre-trainingTransfer to open-vocabulary detection</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object image embeddings</cell></row><row><cell>'bird</cell><cell></cell><cell>Text embedding</cell><cell></cell><cell>'giraffe'</cell><cell>Text</cell><cell></cell><cell cols="3">Query embeddings</cell><cell></cell><cell></cell></row><row><cell>'sitting</cell><cell></cell><cell></cell><cell></cell><cell>'tree'</cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell></cell></row><row><cell>'on a tree'</cell><cell>Vision Transformer encoder</cell><cell>embedding Image</cell><cell>Contrastive loss over images in a batch.</cell><cell>'car'</cell><cell>encoder Vision Transformer encoder</cell><cell>Linear projection</cell><cell>.9 .8 .2 .1</cell><cell>.1 .4 .8 .0</cell><cell>.1 .0 .0 .1</cell><cell>classes/queries 'giraffe' 'giraffe' 'tree' &lt;no object&gt;</cell><cell>Set prediction loss over objects in an image.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>AblationAP LVIS AP LVIS rare AP COCO AP OI</figDesc><table><row><cell>Baseline</cell><cell>21.0</cell><cell>18.9</cell><cell cols="2">30.9 54.1</cell></row><row><cell>(1) Only use VG for training</cell><cell cols="2">?14.5 ?14.0</cell><cell cols="2">?23.6 ?38.3</cell></row><row><cell>(2) Only use OI for training</cell><cell>?6.9</cell><cell>?5.7</cell><cell>?4.2</cell><cell>0.3</cell></row><row><cell>(3) Same LR for image and text encoders</cell><cell>?3.0</cell><cell>?8.5</cell><cell>?0.5</cell><cell>0.4</cell></row><row><cell>(4) No prompt ensembling at inference</cell><cell>?2.8</cell><cell>?5.5</cell><cell cols="2">?5.9 ?0.1</cell></row><row><cell>(5) No prompts (train or inference)</cell><cell>?1.2</cell><cell>?1.3</cell><cell cols="2">?0.6 ?6.3</cell></row><row><cell>(6) No random negatives</cell><cell>?1.0</cell><cell>?2.8</cell><cell>?0.4</cell><cell>1.0</cell></row><row><cell>(7) No mosaics</cell><cell>?2.3</cell><cell>?1.5</cell><cell cols="2">?1.7 ?0.7</cell></row><row><cell>(8) No mosaics, train 2x longer</cell><cell>?2.9</cell><cell>?2.8</cell><cell cols="2">?1.8 ?0.7</cell></row><row><cell>(9) No mosaics, train 3x longer</cell><cell>?3.4</cell><cell>?3.6</cell><cell cols="2">?1.8 ?0.8</cell></row><row><cell>(10) Do not merge overlapping instances</cell><cell>?0.8</cell><cell>?1.3</cell><cell cols="2">?0.6 ?0.7</cell></row><row><cell>(11) No location bias in box predictor</cell><cell>?1.2</cell><cell>?1.1</cell><cell cols="2">?1.3 ?1.0</cell></row><row><cell>(12) Do not filter out any cropped boxes</cell><cell>?0.1</cell><cell>0.0</cell><cell cols="2">0.1 ?0.1</cell></row><row><cell>(13) Filter out all cropped boxes</cell><cell>?0.1</cell><cell>?0.6</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>(14) Do not remove OI crowd instances</cell><cell>0.0</cell><cell>0.7</cell><cell>?0.4</cell><cell>3.0</cell></row><row><cell>(15) Do not remove LVIS rare labels</cell><cell>0.1</cell><cell>0.2</cell><cell>?0.1</cell><cell>1.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.95 .97 .97 .81 .99 .99 .99 .99 .99 .99 .68 .82 .71 .50</head><label></label><figDesc>Fig. A1. Text conditioning examples. Prompts: "an image of a {}", where {} is replaced with one of bookshelf, desk lamp, computer keyboard, binder, pc computer, computer mouse, computer monitor, chair, drawers, drinking glass, ipod, pink book, yellow book, curtains, red apple, banana, green apple, orange, grapefruit, potato, for sale sign, car wheel, car door, car mirror, gas tank, frog, head lights, license plate, door handle, tail lights.</figDesc><table><row><cell>.95</cell><cell>.95</cell><cell>.95</cell><cell>.98</cell><cell>.98</cell></row><row><cell></cell><cell>.98</cell><cell></cell><cell>.99</cell><cell></cell></row><row><cell></cell><cell>.95</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A1 .</head><label>A1</label><figDesc>Statistics of object detection datasets used in this work.</figDesc><table><row><cell>Name</cell><cell>Train</cell><cell>Val</cell><cell cols="2">Test Categories</cell></row><row><cell>MS-COCO 2017 [27]</cell><cell>118k</cell><cell>5k</cell><cell>40.1k</cell><cell>80</cell></row><row><cell>Visual Genome [23]</cell><cell>84.5k</cell><cell>21.6k</cell><cell>-</cell><cell>-</cell></row><row><cell>Objects 365 [35]</cell><cell>608.5k</cell><cell>30k</cell><cell>-</cell><cell>365</cell></row><row><cell>LVIS [13]</cell><cell>100k</cell><cell>19.8k</cell><cell>19.8k</cell><cell>1203</cell></row><row><cell>OpenImages V4 [24]</cell><cell>1.7M</cell><cell>41.6k</cell><cell>125k</cell><cell>601</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A2 .</head><label>A2</label><figDesc>Train dataset de-duplication statistics. 'Examples' refers to images and 'instances' refers to bounding boxes.</figDesc><table><row><cell></cell><cell cols="2">Original</cell><cell cols="2">Duplicates</cell><cell cols="2">Remaining</cell></row><row><cell>Name</cell><cell cols="6">Examples Instances Examples Instances Examples Instances</cell></row><row><cell>OpenImages V4</cell><cell>1.7M</cell><cell>14.6M</cell><cell>948</cell><cell>6.4k</cell><cell>1.7M</cell><cell>14.6M</cell></row><row><cell>Visual Genome</cell><cell>86.5k</cell><cell>2M</cell><cell>6.7k</cell><cell>156k</cell><cell>79.8K</cell><cell>1.9M</cell></row><row><cell>Objects 365</cell><cell>608.6k</cell><cell>9.2M</cell><cell>147</cell><cell>2.4k</cell><cell>608.5k</cell><cell>9.2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? 10 ?4 1 ? 10 ?5 224 GAP 28k 256 2 ? 10 ?4 0 0.1 960 OI, O365, VG .4/.4/.2 .5/.33/.17 yes</figDesc><table><row><cell></cell><cell>Training duration</cell><cell>Batch size</cell><cell>Learning rate</cell><cell>Weight decay</cell><cell>Image size</cell><cell>Pool type</cell><cell>Training steps</cell><cell>Batch size</cell><cell>Learning rate</cell><cell>Weight decay</cell><cell>Droplayer rate</cell><cell>Image size</cell><cell>Training datasets</cell><cell>Dataset proportions</cell><cell>Mosaic proportions</cell><cell>Random negatives</cell></row><row><cell>Model</cell><cell></cell><cell cols="4">Image-level pre-training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Detection fine-tuning</cell><cell></cell><cell></cell></row><row><cell cols="6">CLIP-based OWL-ViT models from Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B/32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">140k 256 5 ? 10 ?5 0 .2/.1 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell>B/16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">140k 256 5 ? 10 ?5 0 .2/.1 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell>L/14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">70k 256 2 ? 10 ?5 0 .2/.1 840</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell cols="5">LiT-based OWL-ViT models from Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B/32</cell><cell cols="12">16B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 140k 256 2 ? 10 ?4 0 0.0 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell>B/16</cell><cell cols="12">8B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 140k 256 2 ? 10 ?4 0 0.0 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell cols="13">R26+B/32 16B 16k 3 ? 10 ?4 1 ? 10 ?5 288 MAP 140k 256 2 ? 10 ?4 0 0.0 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell>L/16</cell><cell cols="12">16B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 70k 256 5 ? 10 ?5 0 0.0 768</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell>H/14</cell><cell cols="12">12B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 70k 256 5 ? 10 ?5 0 .1/.0 840</cell><cell>O365, VG</cell><cell>.8/.2</cell><cell cols="2">.4/.3/.3 yes</cell></row><row><cell cols="6">Model used for one-shot detection (Table 2):</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">R50+H/32 24B 12k 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>AblationAP LVIS AP LVIS rare AP COCO AP OI AP LVIS AP LVIS rare AP COCO AP OI Gradient clipping. Baseline uses per-example clipping and per-example normalization. Global clip, per-ex. norm</figDesc><table><row><cell>Baseline</cell><cell>15.7</cell><cell>14.1</cell><cell cols="2">24.1 48.5</cell><cell>21.0</cell><cell>18.9</cell><cell cols="2">30.9 54.1</cell></row><row><cell cols="3">Dataset ratio. Baseline uses OI:VG(obj) = 7:3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OI:VG(obj) = 2:8</cell><cell>?1.9</cell><cell>?2.7</cell><cell cols="2">?2.4 ?4.8</cell><cell>?4.2</cell><cell>?4.1</cell><cell cols="2">?4.7 ?4.8</cell></row><row><cell>OI:VG(obj) = 3:7</cell><cell>?1.0</cell><cell>?1.9</cell><cell cols="2">?1.2 ?3.1</cell><cell>?3.0</cell><cell>?3.0</cell><cell cols="2">?3.3 ?2.9</cell></row><row><cell>OI:VG(obj) = 4:6</cell><cell>?0.6</cell><cell>?1.8</cell><cell cols="2">?0.4 ?1.7</cell><cell>?2.2</cell><cell>?3.6</cell><cell cols="2">?2.2 ?1.5</cell></row><row><cell>OI:VG(obj) = 5:5</cell><cell>0.0</cell><cell>?0.5</cell><cell cols="2">0.1 ?0.6</cell><cell>?1.0</cell><cell>?1.1</cell><cell cols="2">?1.0 ?1.1</cell></row><row><cell>OI:VG(obj) = 6:4</cell><cell>0.1</cell><cell>?0.6</cell><cell cols="2">0.1 ?0.3</cell><cell>?0.3</cell><cell>?1.4</cell><cell cols="2">?0.4 ?0.2</cell></row><row><cell>OI:VG(obj) = 8:2</cell><cell>?0.7</cell><cell>?0.9</cell><cell cols="2">?0.6 ?0.1</cell><cell>?0.4</cell><cell>?0.3</cell><cell>0.2</cell><cell>0.4</cell></row><row><cell>OI:VG(obj) = 9:1</cell><cell>?1.8</cell><cell>?1.1</cell><cell>?1.6</cell><cell>0.1</cell><cell>?1.8</cell><cell>?1.8</cell><cell>?1.1</cell><cell>0.3</cell></row><row><cell>OI:VG(obj, reg) = 7:3</cell><cell>?0.6</cell><cell>0.0</cell><cell cols="2">?0.9 ?3.3</cell><cell>?1.2</cell><cell>?0.5</cell><cell cols="2">?0.8 ?3.6</cell></row><row><cell>OI:VG(reg) = 7:3</cell><cell>?2.1</cell><cell>?1.4</cell><cell cols="2">?2.3 ?2.5</cell><cell>?2.9</cell><cell>?2.3</cell><cell cols="2">?2.2 ?2.2</cell></row><row><cell>Only OI</cell><cell>?4.9</cell><cell>?3.2</cell><cell cols="2">?3.5 ?0.5</cell><cell>?6.9</cell><cell>?5.7</cell><cell>?4.2</cell><cell>0.3</cell></row><row><cell>Only VG(obj)</cell><cell>?8.0</cell><cell>?8.4</cell><cell cols="4">?14.2 ?28.5 ?14.5 ?14.0</cell><cell cols="2">?23.6 ?38.3</cell></row><row><cell>Global clip, global norm</cell><cell>?1.0</cell><cell>?2.0</cell><cell cols="2">?1.4 ?4.9</cell><cell>?2.3</cell><cell>?2.9</cell><cell cols="2">?2.8 ?5.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Prompting. Baseline uses train prompting for OI and test ensemble (ens.) prompting.Other. Baseline uses location bias, samples 50 random negatives and removes LVIS rare labels.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1 ?0.3</cell></row><row><cell>No mosaics</cell><cell>?1.4</cell><cell>?1.6</cell><cell cols="2">?1.5 ?0.4</cell><cell>?2.3</cell><cell>?1.5</cell><cell cols="2">?1.7 ?0.7</cell></row><row><cell>No mosaics, 2x train sched.</cell><cell>?1.0</cell><cell>?1.8</cell><cell>?0.3</cell><cell>1.2</cell><cell>?2.9</cell><cell>?2.8</cell><cell cols="2">?1.8 ?0.7</cell></row><row><cell>No mosaics, 3x train sched.</cell><cell>?1.2</cell><cell>?3.4</cell><cell>0.3</cell><cell>1.1</cell><cell>?3.4</cell><cell>?3.6</cell><cell cols="2">?1.8 ?0.8</cell></row><row><cell>Train: none; test: none</cell><cell>0.0</cell><cell>?0.1</cell><cell cols="2">0.8 ?10.2</cell><cell>?1.2</cell><cell>?1.3</cell><cell cols="2">?0.6 ?6.3</cell></row><row><cell>Train: none; test: ens.</cell><cell>?2.6</cell><cell>?2.2</cell><cell cols="2">?7.3 ?11.1</cell><cell>?4.5</cell><cell>?5.0</cell><cell cols="2">?10.0 ?6.6</cell></row><row><cell>Train: OI+VG; test: ens.</cell><cell>0.8</cell><cell>1.3</cell><cell cols="2">0.9 ?0.1</cell><cell>?0.7</cell><cell>?0.7</cell><cell cols="2">?0.4 ?0.2</cell></row><row><cell>Train: VG; test: ens.</cell><cell>?0.8</cell><cell>?1.1</cell><cell cols="2">?2.9 ?7.8</cell><cell>?3.1</cell><cell>?4.0</cell><cell cols="2">?7.8 ?5.6</cell></row><row><cell>No location bias</cell><cell>?2.8</cell><cell>?2.9</cell><cell cols="2">?3.7 ?2.6</cell><cell>?1.2</cell><cell>?1.1</cell><cell cols="2">?1.3 ?1.0</cell></row><row><cell>No random negatives</cell><cell>?1.2</cell><cell>?3.7</cell><cell cols="2">?0.8 ?0.4</cell><cell>?1.0</cell><cell>?2.8</cell><cell>?0.4</cell><cell>1.0</cell></row><row><cell>Keep LVIS rare</cell><cell>0.1</cell><cell>0.9</cell><cell>0.0</cell><cell>0.7</cell><cell>0.1</cell><cell>0.2</cell><cell>?0.1</cell><cell>1.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Sunayana Rane and Rianne van den Berg for help with the DETR implementation, Lucas Beyer for the data deduplication code, and Yi Tay for useful advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting ResNets: Improved training and scaling strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One shot detection with laplacian object and fast matrix cosine similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="546" to="562" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<title level="m">JAX: composable transformations of Python+NumPy programs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="213" to="229" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive image transformer for one-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12242" to="12251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11403</idno>
		<title level="m">SCENIC: A JAX library for computer vision research and beyond</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look at one sequence: Rethinking transformer in vision through object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NeurIPS. vol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. vol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<editor>Mask R-CNN</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One-shot object detection with coattention and co-excitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NeurIPS. vol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="646" to="661" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="4904" to="4916" />
			<date type="published" when="2021" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MDETR -modulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="491" to="507" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Open Images Dataset V4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03857</idno>
		<title level="m">Grounded language-image pre-training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="740" to="755" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="21" to="37" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="185" to="201" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11507</idno>
		<title level="m">One-shot instance segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">OS2D: One-stage one-shot object detection by matching anchor features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lomakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="635" to="652" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10050</idno>
		<title level="m">Combined scaling for zero-shot transfer learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NeurIPS. vol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Objects365: A Large-Scale, High-Quality Dataset for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8429" to="8438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot learning through crossmodal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ViDT: An efficient and effective fully transformer-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">How to train your ViT? data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient detr: improving end-to-end object detector with dense prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<title level="m">Scaling vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07991</idno>
		<title level="m">LiT: Zero-shot transfer with locked-image text tuning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09106</idno>
		<title level="m">RegionCLIP: Region-based language-image pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Detecting twentythousand classes using image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07461</idno>
		<title level="m">Probabilistic two-stage detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
