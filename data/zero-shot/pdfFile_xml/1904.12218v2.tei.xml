<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Kernels: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<email>nikolentzos@lix.polytechnique.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">?cole Polytechnique Palaiseau</orgName>
								<address>
									<postCode>91120</postCode>
									<country>France Ioannis Siglidis</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LIGM,?cole des Ponts</orgName>
								<orgName type="institution" key="instit1">Universit? Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<postCode>77420</postCode>
									<country>France Michalis Vazirgiannis</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">LIX,?cole Polytechnique Palaiseau</orgName>
								<address>
									<postCode>91120</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Kernels: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the amount of data that can be naturally modeled as graphs has increased significantly. Such types of data have become ubiquitous in many application domains, ranging from social networks to biology and chemistry. A large portion of the available graph representations corresponds to data derived from social networks. These networks represent the interactions between a set of individuals such as friendships in a social website or collaborations in a network of film actors or scientists. In chemistry, molecular compounds are traditionally modeled as graphs where vertices represent atoms and edges represent chemical bonds. Biology constitutes another primary source of graph-structured data. Protein-protein interaction networks, metabolic networks, regulatory networks, and phylogenetic networks are all examples of graphs that arise in this domain. Graphs are also well-suited to representing technological networks. For example, the World Wide Web can be modeled as a graph where vertices correspond to webpages and edges to hyperlinks between these webpages. The use of graph representations is not limited to the above application domains. In fact, most complex systems are usually represented as compositions of entities along with their interactions, and can thus be modeled as graphs. Interestingly, graphs are very flexible and rich as a means of data representation. It is not thus surpris-arXiv:1904.12218v2 [stat.ML] 24 Nov 2021 ing that they can also represent data that do not inherently possess an underlying graph structure. For instance, sequential data such as text can be mapped to graph structures <ref type="bibr" target="#b49">(Filippova, 2010)</ref>. From the above, it becomes clear that graphs emerge in many real-world applications, and hence, they deserve no less attention than feature vectors which is the dominant representation in data mining and machine learning.</p><p>The aforementioned abundance of graph-structured data raised requirements for automated methods that can gain useful insights. This often requires applying machine learning techniques to graphs. In chemistry and biology, some experimental methods are very expensive and time-consuming, and machine learning methods can serve as cost-effective alternatives. For example, identifying experimentally the function of a protein with known sequence and structure is a very expensive and tedious process. Therefore, it is often desirable to be able to use computational approaches in order to predict the function of a protein. By representing proteins as graphs, the problem can be formulated as a graph classification problem where the function of a newly discovered protein is predicted based on structural similarity to proteins with known function . Besides the need for more efficient methods, there is also a need for automating tasks that were traditionally handled by humans and which involve large amounts of data. For instance, in cybersecurity, humans used to manually inspect code samples to identify if they contain malicious functionality. However, due to the rapid increase in the number of malicious applications in the past years, humans are no longer capable of meeting the demands of this task <ref type="bibr" target="#b154">(Suarez-Tangil et al., 2014)</ref>. Hence, there is a need for methods that can accumulate human knowledge and experience, and that can successfully detect malicious behavior in code samples. It turns out that machine learning approaches are particularly suited to this task since most of the newly discovered malware samples are variations of existing malware. By representing code samples as function call graphs, detecting such variations becomes less problematic. Hence, the problem of detecting malicious software can be formulated as a graph classification problem where unknown code samples are compared against known malware samples and clean code <ref type="bibr" target="#b6">(Anderson et al., 2011)</ref>. From the above example, it becomes clear that performing machine learning tasks on graph-structured data is of critical importance for many real-world applications.</p><p>A central issue for machine learning is modelling and computation of similarity among objects. In the case of graphs, graph kernels have received a lot of attention in the past years, and have been established as one of the major approaches for learning on graphstructured data. A graph kernel is a symmetric, positive semidefinite function defined on the space of graphs G. This function can be expressed as an inner product in some Hilbert space. Specifically, given a kernel k, there exists a map ? : G ? H into a Hilbert space H such that k(G 1 , G 2 ) = ?(G 1 ), ?(G 2 ) for all G 1 , G 2 ? G. Roughly speaking, a graph kernel is a measure of similarity between graphs. Graph comparison is a fundamental problem with numerous applications in many disciplines <ref type="bibr" target="#b32">(Conte et al., 2004)</ref>. However, the problem is far from trivial and requires considerable computational time. Graph kernels tackle this problem by trying to both capture as much as possible the semantics inherent in the graph but also to remain computationally efficient. One of the most important reasons behind the success of graph kernels is that they allow the large family of kernel methods to work directly on graphs. Therefore, graph kernels can bring to bear several machine learning algorithms to real-world problems on graph-structured data. The field of graph kernels has been intensively developed recently. Interestingly, dozens of graph kernels have been proposed in the past 20 years. Some of these kernels have achieved state-of-the-art results on several datasets. Recently, there has been a significant surge of interest in Graph Neural Network (GNN) approaches for graph representation learning. Most of these models follow a neighborhood aggregation scheme similar to that of many graph kernels, and can be reformulated into a single common framework <ref type="bibr" target="#b58">(Gilmer et al., 2017)</ref>. The main advantage of GNNs over graph kernels is that their complexity is linear to the number of samples, while kernels require quadratic time to compute all kernel values. For a detailed presentation of this important emerging field, the interested reader is referred to <ref type="bibr" target="#b174">Wu et al. (2020)</ref>.</p><p>This paper is a survey of graph kernels, that is kernels that operate on graph-structured data. We present a comprehensive study of these approaches. We begin with well-known kernels that established the foundations of the field, and we proceed with more recent kernels that are considered the state-of-the-art for many graph-related machine learning tasks. Besides the detailed description of the kernels, we also provide an extensive experimental evaluation of most of them. As we show in this survey, graph kernels are powerful tools with a wide range of applications, while their empirical performance is superior to that of graph neural networks for certain types of graphs. We thus expect these methods to gain soon more attention in a wealth of applications due to their attractive properties. Importantly, this study aims to assist both practitioners and researchers who are interested in applying machine learning tasks on graphs. Furthermore, it should be of interest to all researchers who deal with the problems of graph similarity and graph comparison. The abundance of applications related to the above problems stresses the value of the survey. We should note that three similar surveys reviewing work on graph kernels became very recently available <ref type="bibr" target="#b57">(Ghosh et al., 2018;</ref><ref type="bibr" target="#b87">Kriege et al., 2020;</ref><ref type="bibr" target="#b18">Borgwardt et al., 2020)</ref>. One may thus ask the question: why another survey within such a short period of time? The answer is that in contrast to the first two above surveys, this survey is much more thorough and covers a larger number of kernels. Moreover, it presents kernels in a more comprehensive way allowing researchers to identify open problems and areas for further exploration, and practitioners to gain a deeper understanding of kernels so that they can decide which kernel best suits their needs. Specifically, the above two surveys do not go into sufficient details about the mathematical foundations of the different kernels. On the other hand, we provide an indepth discussion of a large number of kernels along with all the mathematical details that are of high importance in this domain. This survey also provides a much more meaningful taxonomy of graph kernels. More specifically, kernels are grouped into classes based on different criteria such as the type of data on which they operate, and the design paradigm that they follow. The third survey <ref type="bibr" target="#b18">(Borgwardt et al., 2020)</ref> is very detailed and wellwritten, and there is a considerable intersection with this survey, especially in terms of the articulation of the presentation of the kernels, however, it lags behind in terms of empirical analysis. To the best of our knowledge, we provide the most complete evaluation in terms of the number of considered graph kernels. <ref type="bibr" target="#b57">Ghosh et al. (2018)</ref> do not perform original graph classification experiments, but they only report results from the kernels' original papers. <ref type="bibr" target="#b87">Kriege et al. (2020)</ref> perform original experiments, however, they only evaluate 9 kernels (and their variants) and 1 framework in total, while <ref type="bibr" target="#b18">Borgwardt et al. (2020)</ref> evaluate 12 kernels and 1 framework. On the other hand, our list of methods includes 16 different kernels and 2 frameworks. Besides classification performance, we also measure and report running times (not provided by <ref type="bibr" target="#b87">Kriege et al., 2020</ref> or by <ref type="bibr" target="#b18">Borgwardt et al., 2020)</ref>. We believe that running times are one of the major reasons behind the choice of a kernel for a practical application. Also, we need to stress that such a wider, and more extensive experimental comparison of graph kernels can provide useful insights into the strengths and weaknesses of the different kernels. Furthermore, we compare graph kernels against graph neural networks which we believe that is an important piece of exploration as to the comparison of two worlds (neural networks and kernels) in the context of graphs. Finally, we empirically compare the expressiveness of the kernels to each other, that is how well the different kernels capture the similarity of graphs, something that is missing from the current literature.</p><p>The rest of this manuscript is organized as follows. In Section 2, we discuss why the use of graphs as a means of object representation is vital and necessary in many domain areas, and we also present the challenges of applying learning algorithms on graphs. In Section 3, we introduce notation and background material that we need for the remainder of the paper, including some fundamental concepts from graph theory and from kernel methods. In Section 4, we discuss the core concepts of graph kernels, and we give an overview of the literature on graph kernels. We begin by describing important kernels that were developed in the early days of the field. We next present kernels that are based on neighborhood aggregation mechanisms. We then describe more recent kernels that do not employ neighborhood aggregation mechanisms. Subsequently, we present kernels that are based on assignment, and methods that can handle continuous node attributes. Finally, we give details about frameworks that work on top of graph kernels and aim to improve their performance. The grouping of the reported studies is designed to make it easier for the reader to follow the analysis of the literature, and to obtain a complete picture of the different graph kernels that have been proposed throughout the years. In Section 5, we provide a short introduction to graph neural networks, the main competitors of graph kernels, and we discuss how the major family of these models is related to graph kernels. In Section 6, we present applications of graph kernels in many different domain areas. In Section 7, we experimentally evaluate the performance of many graph kernels on several widely-used graph classification benchmark datasets. Furthermore, we measure the running times of these kernels. Based on the obtained results, we provide guidelines for the successful application of graph kernels in different classification problems. We also study the expressive power of graph kernels from an empirical standpoint by comparing the obtained kernel values against the similarities that are produced by a well-accepted but intractable graph similarity function. Finally, Section 8 contains the summary of the survey, along with a discussion about future research directions in the field of graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation and Challenges</head><p>In this Section, we present the main reasons that motivate the use of graphs instead of feature vectors as a means of data representation. Furthermore, we describe the problem of learning on graphs which arises in many application domains. We focus on the instance of the problem where each sample is a graph, and highlight its relationship to the graph comparison problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Why Graphs</head><p>Graphs are a powerful and flexible means of representing structured data. The power of graphs stems from the fact that they represent both entities, and the relationships between them. Typically, the vertices of a graph correspond to some entities, and the edges model how these entities interact with each other. It is important to note that several fundamental structures for representing data can be seen as instances of graphs <ref type="bibr" target="#b19">(Borgwardt, 2007)</ref>. This highlights the generality of graphs as a form of representation. For example, a vector can be naturally thought of as a graph where vertices correspond to components of the vector and consecutive components within the vector are joined by an edge. Associative arrays can be modeled as graphs, with keys and values represented as vertices, and directed edges connecting keys to their corresponding values. Strings can also be represented as graphs, with one vertex per character and edges between consecutive characters. Due to the power and the generality of graphs as representational models, in some cases, even data that does not exhibit graph-like structure is mapped to graph representations. A very common example is that of textual data, where graphs are usually employed to model the relationships between sentences or terms <ref type="bibr" target="#b106">(Mihalcea &amp; Tarau, 2004)</ref>.</p><p>In data mining and machine learning, observations traditionally come in the form of vectors. However, vector representations suffer from a series of limitations. Specifically, vectors have limited capability to model complex objects since they are unable to capture relationships that may exist between different entities of an object. Furthermore, all the input objects are usually represented as vectors of the same length, despite their size and complexity. On the other hand, as discussed above, graphs are characterized by increased flexibility which allows them to adequately model a variety of different objects. Graphs model both the entities and the relationships between them. Moreover, they are allowed to vary in the number of vertices and in the number of edges. Therefore, graphs address several of the limitations inherent to vectors. It is thus clear that the need for methods that perform learning tasks on graphs is intense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning on Graphs and Challenges</head><p>Learning on graphs has gained extensive attention in the past years. This is mainly due to the representational power of graphs which has established them as a major structure for modeling data from various disciplines. Hence, it is not surprising that a plethora of learning problems have been defined on graphs. Most of these learning problems focus either on the node level or on the graph level. Node classification belongs to the former set of problems, while graph classification belongs to the latter set of problems. In this survey, we focus exclusively on tasks performed at the graph level. Therefore, all the kernels that are presented correspond to functions between graphs.</p><p>Data representation is a key issue in the fields of data mining and machine learning. Algorithms are mainly designed to handle data in a specific representation. Due to the appealing properties of graphs, one would expect that there would be great progress in the development of algorithms that can handle graph-structured data. However, the combinatorial nature of graphs acts as a "barrier" since it is very likely that algorithms that operate directly on graphs will be computationally expensive and will not scale to large datasets. Thus, research in these areas has mainly focused on algorithms operating on vectors, as vectors possess many desirable mathematical properties and can be dealt with much more efficiently. Hence, it is not surprising that the most popular learning algorithms are designed for data represented as vectors. As a consequence, it has become common practice to represent any type of data as feature vectors. Even in application domains where data is naturally represented as graphs, attempts were made to transform graphs into feature vectors instead of designing algorithms that operate directly on graphs. Ideally, we would like to have a method that runs in polynomial time and is capable of transforming graphs to feature vectors without sacrificing their representational power. Unfortunately, such a method does not exist. Directly representing data as vectors is thus suboptimal since vectors fail to preserve the rich topological information encoded in a graph. Hence, it would be much more preferable to devise algorithms that operate directly on graphs.</p><p>The problem of learning on graphs (at the graph level) is directly related to that of graph comparison. The ability to compute meaningful similarity or distance measures is often a prerequisite to perform machine learning tasks. Such similarity and distance measures are at the core of many machine learning algorithms. Examples include the k-nearest neighbor classifier, and algorithms that learn decision functions in proximity spaces <ref type="bibr" target="#b62">(Graepel et al., 1999)</ref>. These algorithms are very flexible since they require only a distance or similarity function to be defined as the sole mathematical structure on the set of input objects. Hence, by defining a meaningful distance function d : G ? G ? R + between graphs, we can immediately use one of the above algorithms to perform tasks such as graph classification and graph clustering. However, it turns out that graph comparison is a very complex problem. Specifically, graphs lack the convenient mathematical context of vector spaces, and many operations on them, though conceptually simple, are either not properly defined or computationally expensive. Perhaps the most striking example of these operations is to determine if two objects are identical. In the case of vectors, it requires comparing all their corresponding components, and it can thus be accomplished in linear time with respect to the size of the vectors. For the analogous operation on graphs, known as graph isomorphism, no polynomial-time algorithm has been discovered so far <ref type="bibr" target="#b52">(Garey &amp; Johnson, 1979)</ref>. In general, the problem of comparing two objects is much less well-defined on graphs compared to vectors. For vectors, distance can be computed efficiently using the universally accepted Euclidean distance metric. Unfortunately, there exists no such metric on graphs. Several fundamental problems in graph theory related to graph comparison such as the subgraph isomorphism problem and the maximum common subgraph problem are NP-complete <ref type="bibr" target="#b52">(Garey &amp; Johnson, 1979)</ref>. Furthermore, identifying common parts in two graphs is computationally infeasible. Given a graph consisting of n vertices, there are 2 n possible subsets of vertices. Hence, there are exponentially many (in the size of the graphs) pairs of subsets to consider. It becomes thus clear that although graphs offer a very intuitive way of modeling data from diverse sources, their power and flexibility do not come without a price.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Before we delve into the details of graph kernels, we outline some fundamental aspects of graph theory and kernel methods. We first introduce basic concepts from graph theory, and (1, 3, 0)</p><p>(1, 1, 2) (0, 2, 5) (2, 0, 4) (1, 3, 0)</p><p>(1, 1, 2) (0, 2, 5) (2, 0, 4) ? ? ? ? (1, 3, 0)</p><p>(1, 1, 2) (0, 2, 5) (2, 0, 4) <ref type="figure">Figure 1</ref>: Examples of different types of graphs. A simple undirected graph (left), a labeled graph (center), and an attributed graph (right). define our notation. We also provide a short introduction to kernel functions and kernel methods in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definitions and Notations</head><p>Definition 1 (Graph). A graph is a pair G = (V, E) consisting of a set of vertices (or nodes) V and a set of edges E ? V ? V which connect pairs of vertices.</p><p>The size of the graph corresponds to its number of vertices denoted by |V | or n. As regards the number of edges of the graph, we will denote it as |E| or m. An example of a graph is given in <ref type="figure">Figure 1</ref> (left). A graph may have labels on its nodes and edges. This is often necessary for capturing the semantics of complex objects. For instance, most graphs derived from chemistry (e. g., molecules) are annotated with categorical labels from a finite set.</p><p>Definition 2 (Labeled Graph). A labeled graph is a graph G = (V, E) endowed with a function : V ? E ? ? that assigns labels to the vertices and edges of the graph from a discrete set of labels ?.</p><p>A graph with labels on its vertices is called node-labeled. Similarly, a graph with labels on edges is called edge-labeled. A graph with labels on both the vertices and edges is called fully-labeled. An example of a node-labeled graph is given in <ref type="figure">Figure 1</ref> (center). In many settings, vertex and edge annotations are in the form of vectors. For example, vertices and edges may be annotated with multiple categorical or real-valued properties. These graphs are known as attributed graphs.</p><p>Definition 3 (Attributed Graph). An attributed graph is a graph G = (V, E) endowed with a function f : V ? E ? R d that assigns real-valued vectors to the vertices and edges of the graph.</p><p>An example of a node-attributed graph is given in <ref type="figure">Figure 1</ref> (right). Note that labeled graphs are a special case of attributed graphs. We can represent labeled graphs as attributed graphs if we map the discrete labels to one-hot vector representations. A graph G = (V, E) can be represented by its adjacency matrix A.</p><p>Definition 4 (Adjacency Matrix). Let A ij be the element in the i-th row and j-th column of matrix A. Then, the adjacency matrix A of a graph G = (V, E) can be defined as follows</p><formula xml:id="formula_0">A ij = 1 if (v i , v j ) ? E, 0 otherwise</formula><p>The adjacency matrix A consists of n rows and n columns, that is</p><formula xml:id="formula_1">A ? R n?n . The neighborhood N (v i ) of vertex v i is the set of all vertices adjacent to v i . Hence, N (v i ) = {v j : (v i , v j ) ? E} where (v i , v j ) is an edge between vertices v i and v j of V . A concept closely related to the neighborhood of a vertex v i is its degree deg G (v i ).</formula><p>Definition 5 (Degree). Given an undirected graph G = (V, E) and a vertex v i ? V , the degree of v i is the number of edges incident to v i , and is defined as</p><formula xml:id="formula_2">deg(v i ) = |{v j : (v i , v j ) ? E}| = |N (v i )| (1)</formula><p>The maximum of the degrees of the vertices of a graph is denoted by deg * , and deg * = max v?V deg(v). Besides the adjacency matrix A, a graph G = (V, E) can also be represented by its Laplacian matrix L.</p><p>Definition 6 (Laplacian Matrix). Let A be the adjacency matrix of a graph G = (V, E) and D a diagonal matrix with D ii = j A ij . Then, the Laplacian matrix L of a graph G = (V, E) can be defined as follows</p><formula xml:id="formula_3">L = D ? A<label>(2)</label></formula><p>Similarly to the adjacency matrix A, the dimensionality of the Laplacian matrix is n?n. A subgraph of a graph G is a graph whose set of vertices and set of edges are both subsets of those of G. Let G ? G denote that G is a subgraph of G.</p><p>Definition 7 (Induced Subgraph). Given a graph G = (V, E) and a subset of vertices S ? V , the subgraph G(S) = (S, E(S)) induced by S consists of the set of vertices S and the set of edges E(S) that have both end-points in S defined as follows</p><formula xml:id="formula_4">E(S) = {(v i , v j ) ? E : v i , v j ? S}<label>(3)</label></formula><p>The degree of a vertex v i ? S, deg G(S) (v i ), is equal to the number of vertices that are adjacent to v i in G(S). The density of a graph G is ?(G) = m/ n 2 , the number of edges m over the total possible edges. A graph G with density ?(G) = 1 is called a complete graph. In a complete graph, every pair of distinct vertices are adjacent. A clique is a subset of vertices such that every pair of them are connected by an edge, that is, their induced subgraph is complete.</p><formula xml:id="formula_5">Definition 8 (Walk, Path, Cycle). A walk in a graph G = (V, E) is a sequence of vertices v 1 , v 2 , . . . , v k+1 where v i ? V for all 1 ? i ? k + 1 and (v i , v i+1 ) ? E for all 1 ? i ? k.</formula><p>The length of the walk is equal to the number of edges in the sequence, that is k in the above case.  Definition 9 (Shortest Path). A shortest path from vertex v i to vertex v j of a graph G is a path from v i to v j such that there exist no other path between these two vertices with smaller length.</p><formula xml:id="formula_6">A walk in which v i = v j ? i = j is called a path. A cycle is a path with (v k+1 , v 1 ) ? E.</formula><p>The diameter of a graph G is the length of the longest shortest path between any pair of vertices of G. The neighborhood of radius r (or r-hop neighborhood) of vertex v i is the set of vertices whose shortest path distance from v i is less than or equal to r and is denoted by N r (v i ). <ref type="table" target="#tab_1">Table 1</ref> gives a list of the most commonly used symbols along with their definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernel Functions and Kernel Methods</head><p>We next give an introduction to kernel functions and kernel methods.</p><p>Definition 10 (Gram Matrix). Given a set of inputs x 1 , . . . , x N ? X and a function k : X ? X ? R, the N ? N matrix K defined as</p><formula xml:id="formula_7">K ij = k(x i , x j )<label>(4)</label></formula><p>is called the gram matrix (or kernel matrix) of k with respect to the inputs x 1 , . . . , x N .</p><p>In what follows, we will refer to gram matrices as kernel matrices.</p><formula xml:id="formula_8">Definition 11 (Positive Semidefinite Matrix). A real N ?N symmetric matrix K satisfying N i=1 N j=1 c i c j K ij ? 0 (5)</formula><p>for all c i ? R is called positive semidefinite.</p><p>Definition 12 (Positive Semidefinite Kernel). Let X be a nonempty set. A function k : X ? X ? R which for all N ? N and all x 1 , . . . , x N ? X gives rise to a positive semidefinite kernel matrix is called a positive semidefinite kernel, or just a kernel.</p><p>Informally, a kernel function measures the similarity between two objects. Furthermore, kernel functions can be represented as inner products between the vector representations of these objects. Specifically, if we define a kernel k on X ? X , then there exists a mapping ? : X ? H into a Hilbert space with inner product ?, ? , such that:</p><formula xml:id="formula_9">?x i , x j ? X : k(x i , x j ) = ?(x i ), ?(x j )<label>(6)</label></formula><p>A Hilbert space is an inner product space which also possesses the completeness property that every Cauchy sequence of points taken from the space converges to a point in the space. Furthermore, the Hilbert space H has the following property known as the reproducing property:</p><formula xml:id="formula_10">?f ? H, ?x ? X : f (x) = f, k(x, ?)<label>(7)</label></formula><p>By virtue of this property, H is called a reproducing kernel Hilbert space (RKHS) associated with kernel k. It is interesting to note that every kernel function on X ? X is associated with an RKHS and vice versa <ref type="bibr" target="#b9">(Aronszajn, 1950)</ref>. Kernel methods are a class of machine learning algorithms which operate on input data after they have been mapped into an implicit feature space using a kernel function. One of the major advantages of kernel methods is that they can operate on very general types of data <ref type="bibr" target="#b140">(Sch?lkopf &amp; Smola, 2002)</ref>. The input space X does not have to be a vector space, but it can represent any structured domain, such as the space of strings or graphs <ref type="bibr" target="#b53">(G?rtner, 2003)</ref>. Kernel methods can still be applied to such types of data, as long as we can find a mapping ? : X ? H, where H is an RKHS. This mapping is not neccasary to be explicitly determined. These methods implicitly represent data in a feature space and compute inner products between them in that space using a kernel function. These inner products can be interpreted as the similarities between the corresponding objects. Machine learning tasks such as classification and clustering can be carried out by using only the inner products computed in that feature space. Kernel methods are very popular and have been successfully used in a wide variety of applications. Here, we need to stress that the optimization problem of several kernel methods such as the Support Vector Machines is convex only if the employed function is positive semidefinite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Kernels</head><p>In this Section, we give an overview of the graph kernel literature. Our study is not exhaustive, however, we have tried to cover the most representative approaches that have appeared in the literature of graph kernels. We first present some fundamental aspects of graph kernels, and we then proceed by discussing the details of several graph kernel instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Kernels between Graphs</head><p>Kernels on graphs can be divided into two categories: (1) those that compare nodes in a graph, and (2) those that compare graphs. As mentioned above, in this survey, we focus on the second category, that is, kernels between graphs and thus we exclusively use the term graph kernel for describing such kernel functions. As regards the first category, we refer the interested reader to the work of <ref type="bibr" target="#b83">Kondor and Lafferty (2002)</ref> which was later extended by <ref type="bibr" target="#b148">Smola and Kondor (2003)</ref>. Graph kernels have recently emerged as a promising approach for learning on graph-structured data. These methods exhibit several attractive statistical</p><formula xml:id="formula_11">G 1 G 2 G 3 G H ?(G 1 ) ?(G 2 ) ?(G 3 )</formula><p>Figure 2: Feature space and map defined by graph kernels. Any kernel on a space of graphs G can be represented as an inner product after graphs are mapped into a Hilbert space H.</p><p>properties. They combine the representative power of graphs and the discrimination power of kernel-based methods. Hence, they constitute powerful tools for tackling the graph similarity and learning tasks at the same time.</p><p>From the previous Section, it is clear that the application of kernel methods consists of two steps. First, a kernel function is designed, and based on this function the kernel matrix is constructed. Second, a learning algorithm is employed to compute the optimal manifold in the feature space (e. g., a hyperplane in binary classification problems). Since several mature kernel-based classifiers are available in the literature, research on graph kernels has focused on the first step. Hence, the main effort has been devoted to developing expressive and efficient graph kernels capable of accurately measuring the similarity between input graphs. These kernels implicitly (or explicitly sometimes) project graphs into a feature space H as illustrated in <ref type="figure">Figure 2</ref>. As regards the second step, it is common to employ off-the-shelf algorithms such as the Support Vector Machines classifier <ref type="bibr" target="#b23">(Boser et al., 1992)</ref> or the kernel k-means algorithm <ref type="bibr" target="#b40">(Dhillon et al., 2004)</ref>, and thus, we will not enter into more details here. The interested reader is referred to <ref type="bibr" target="#b140">Sch?lkopf and Smola (2002)</ref> or to <ref type="bibr" target="#b143">Shawe-Taylor and Cristianini (2004)</ref>.</p><p>Concluding, the main challenge in applying kernel methods to graphs is to define appropriate positive semidefinte kernel functions on the set of input graphs which are able to reliably assess the similarity among them. We next present, for illustration purposes, two very simple kernels that compare node and edge labels of the two involved graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simple Kernels</head><p>The vertex histogram and edge histogram kernels are very simple instances of graph kernels which generate explicit graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Vertex Histogram Kernel</head><p>The vertex histogram kernel is a basic linear kernel on vertex label histograms. The kernel assumes node-labeled graphs. Let ? = {1, . . . , d} be a set of node labels. Clearly, there are d node labels in total, that is d = |?|. Then, the vertex label histogram of a graph</p><formula xml:id="formula_12">G = (V, E) is a vector f = (f 1 , f 2 , . . . , f d ) , such that f i = |{v ? V : (v) = i}| for each i ? ?.</formula><p>Let f, f be the vertex label histograms of two graphs G, G , respectively. The vertex histogram kernel is then defined as the linear kernel between f and f , that is</p><formula xml:id="formula_13">k(G, G ) = f, f<label>(8)</label></formula><p>The complexity of the vertex histogram kernel is linear in the number of vertices of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Edge Histogram Kernel</head><p>The edge histogram kernel is a basic linear kernel on edge label histograms. The kernel assumes edge-labeled graphs. Given a set of edge labels ? = {1, . . . , d} (d edge labels in total), the edge label histogram of a graph G =</p><formula xml:id="formula_14">(V, E) is a vector f = (f 1 , f 2 , . . . , f d ) , such that f i = |{(v, u) ? E : (v, u) = i}| for each i ? ?.</formula><p>Let f, f be the edge label histograms of two graphs G, G , respectively. The edge histogram kernel is then defined as the linear kernel between f and f , that is</p><formula xml:id="formula_15">k(G, G ) = f, f<label>(9)</label></formula><p>The complexity of the edge histogram kernel is linear in the number of edges of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expressiveness vs Efficiency</head><p>The two kernels defined above are indeed positive semidefinite, but they both correspond to rather naive concepts -as a distribution of values is. A question that may arise at this point is how expressive can graph kernels be in practice. Let us first define the class of kernels which are capable of distinguishing between all (non-isomorphic) graphs in the feature space. Such kernels are called complete. <ref type="bibr" target="#b54">G?rtner et al. (2003)</ref> showed that computing any complete graph kernel is at least as hard as deciding whether two graphs are isomorphic. The above result, in effect, prohibits the use of complete graph kernels in practical applications. Instead, by using kernels that are not complete, it is not further guaranteed that non-isomorphic graphs will not be mapped into the same point in the feature space. This is a negative result since it implies that to develop expressive kernels, it is necessary to sacrifice some of their efficiency. More recently, <ref type="bibr" target="#b88">Kriege et al. (2018)</ref> showed that several established graph kernels, such as the Weisfeiler-Lehman subtree kernel, cannot distinguish essential graph properties such as connectivity, planarity and bipartiteness. Considering that the Weisfeiler-Lehman subtree kernel achieves state-ofthe-art results on most benchmark datasets, this result blurs even more the already vague issue of choosing a graph kernel a practitioner is faced with when dealing with a particular application. In fact, devising a good trade-off between efficiency and effectiveness is an issue of vital importance when designing a graph kernel. </p><formula xml:id="formula_16">Definition 13 (Complete Graph Kernel). A graph kernel k(G i , G j ) = ?(G i ), ?(G j ) is complete if ? is injective.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Taxonomy of Graph Kernels</head><p>There exist many different criteria we can use to divide the various graph kernels into different categories. For instance, graph kernels are traditionally grouped into some major families, each focusing on a different structural aspect of graphs such as random walks, subtrees, cycles, paths, and small subgraphs. Alternatively, graph kernels can be divided into groups according to their ability to handle unlabeled graphs, node-labeled or nodeattributed graphs. Furthermore, graph kernels can be divided into approaches that employ explicit computation schemes and approaches that employ implicit computation schemes <ref type="bibr" target="#b85">(Kriege et al., 2014)</ref>. Graph kernels can also be grouped into categories based on the design paradigm that they follow (i. e., if they are R-convolution, assignment or intersection kernels). Note that groups emerging from different criteria may be related to each other. For instance, graph kernels that can handle node-attributed graphs usually employ implicit computation schemes. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the taxonomy of graph kernels. The devised taxonomy is based on some of the criteria mentioned above. However, in what follows, we do not adopt exclusively any of these criteria. We begin our treatment with approaches that were proposed in the early days of graph kernels, starting from the well-studied random , support for node-labeled and node-attributed graphs, type, and computational complexity. A dagger ( ?) implies that the kernel admits an explicit feature mapping for certain types of graphs. The complexity refers to the worst-case theoretical complexity for evaluating the kernel between two graphs. In practice, and for certain kinds of graphs, some graph kernels (e. g., the shortest-path kernel) can be evaluated much more efficiently.</p><formula xml:id="formula_17">Graph Kernel Exp. ? Node Node Type Complexity Labels Attributes Vertex Histogram R-convolution O(n) Edge Histogram R-convolution O(m) Random Walk ? R-convolution O(n 3 ) Subtree R-convolution O(n 2 4 deg * h) Cyclic Pattern intersection O((c + 2)n + 2m) Shortest Path ? R-convolution O(n 4 ) Graphlet R-convolution O(n k ) Weisfeiler-Lehman Subtree R-convolution O(hm) Neighborhood Hash intersection O(hm) Neighborhood Subgraph Pairwise Distance R-convolution O(n 2 m log(m)) Lov?sz ? R-convolution O(n(s + nm ) + s 2 ) SVM-? R-convolution O(n(s + n 2 ) + s 2 ) Ordered Decomposition DAGs R-convolution O(n log n) Pyramid Match assignment O(ndL) Weisfeiler-Lehman Optimal Assignment assignment O(hm) Subgraph Matching R-convolution O(kn k+1 ) GraphHopper R-convolution O(n 4 ) Graph Invariant Kernels R-convolution O(n 6 ) Propagation R-convolution O(hm) Multiscale Laplacian R-convolution O(n 5 h)</formula><p>The <ref type="table">Table uses</ref> notation that has not been introduced yet: k: size of largest subgraph considered, c: upper bound on the number of cycles, h: maximum distance between root of neighborhood subgraph/subtree pattern and its nodes, s: number of sampled subgraphs, : additive error associated with semidefinite programming solvers, d: dimensionality of node representations, L: number of levels.</p><p>walk kernel till the very popular Weisfeiler-Lehman subtree kernel. We next present some approaches that were inspired from the neighborhood aggregation schmeme of the Weisfeiler-Lehman subtree kernel, and then kernels that do not fall into either of the previous two categories. The subequent subsections are devoted to assignment kernels, and to kernels that can handle continuous node attributes. The final subsections deals with frameworks and approaches that can be applied on top of existing graph kernels. An overview of the graph kernels that are presented in this survey and their properties is given in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Early Days of Graph Kernels</head><p>While early studies on kernel functions and kernel methods focused almost exclusively on input data represented as vectors, it soon became clear that these methods could handle more complex structured objects such as strings, trees and graphs. One of the most popular methods for defining kernels between such objects is to decompose the objects into their "parts", and to compare all pairs of these "parts" by applying existing kernels on them. Kernels constructed using the above framework are called R-convolution kernels <ref type="bibr" target="#b69">(Haussler, 1999)</ref>. Most graph kernels in the literature are instances of the R-convolution framework. These kernels decompose graphs into their substructures and add up the pairwise similarities between these substructures. The most intuitive example of an R-convolution kernel is probably a kernel that decomposes each graph into the set of all of its subgraphs, and compares them pairwise. <ref type="bibr" target="#b54">G?rtner et al. (2003)</ref> showed that the problem of computing the kernel that compares all the subgraphs of two graphs is NP-hard. Based on this result, it becomes evident that we need to consider alternative, less powerful graph kernels that can be computed in polynomial time. However, as discussed above, it is necessary that these kernels provide an expressive measure of similarity on graphs. Over the years, several graph kernels have been proposed, each focusing on a different structural aspect of graphs. Such aspects involve comparing graphs based on random walks, subtrees, cycles, paths, and small subgraphs, to name a few. We next look at some kernels that date back to the early days of this field. Furthermore, we present kernels that were motivated by problems encountered by the above instances, and were proposed as more advanced alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Random Walk Kernel</head><p>The random walk kernels are perhaps one of the first successful efforts to design kernels between graphs that can be computed in polynomial time. The members of this wellstudied family of graph kernels quantify the similarity between a pair of graphs based on the number of common walks in the two graphs <ref type="bibr" target="#b78">(Kashima et al., 2003;</ref><ref type="bibr" target="#b54">G?rtner et al., 2003;</ref><ref type="bibr" target="#b100">Mah? et al., 2004;</ref><ref type="bibr" target="#b166">Vishwanathan et al., 2010;</ref><ref type="bibr" target="#b155">Sugiyama &amp; Borgwardt, 2015;</ref><ref type="bibr" target="#b182">Zhang et al., 2018b)</ref>. Kernels belonging to this family have concentrated mainly on counting matching walks in the two input graphs. There are several variations of random walk kernels. The k-step random walk kernel compares random walks up to length k in the two graphs. The most widely-used kernel from this family is the geometric random walk kernel  which compares walks up to infinity assigning a weight ? k (? &lt; 1) to walks of length k in order to ensure convergence of the corresponding geometric series. We next give the formal definition of the geometric random walk kernel. Given two node-labeled graphs G = (V, E) and G = (V , E ), their direct product G ? = (V ? , E ? ) is a graph with vertex set:</p><formula xml:id="formula_18">V ? = {(v, v ) : v ? V ? v ? V ? (v) = (v )}<label>(10)</label></formula><p>and edge set:</p><formula xml:id="formula_19">E ? = {{(v, v ), (u, u )} : (v, u) ? E ? (v , u ) ? E }<label>(11)</label></formula><p>An example of the product graph of two graphs is illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>. Performing a random walk on G ? is equivalent to performing a simultaneous random walk on G i and G j . The geometric random walk kernel counts common walks (of potentially infinite length) in two graphs and is defined as follows.</p><p>Definition 14 (Geometric Random Walk Kernel). Let G and G be two graphs, let A ? denote the adjacency matrix of their product graph G ? , and let V ? denote the vertex set of (2, 7)</p><p>(1, 7) the product graph G ? . Then, the geometric random walk kernel is defined as</p><formula xml:id="formula_20">K ? ? (G, G ) = |V ? | p,q=1 ? l=0 ? l A l ? pq = e (I ? ?A ? ) ?1 e<label>(12)</label></formula><p>where I is the identity matrix, e is the all-ones vector, and ? is a positive, real-valued weight. The geometric random walk kernel converges only if ? &lt; 1 ? ? where ? ? is the largest eigenvalue of A ? .</p><p>Direct computation of the geometric random walk kernel requires O(n 6 ) time. The computational complexity of the method severely limits its applicability to real-world applications. To account for this, <ref type="bibr" target="#b166">Vishwanathan et al. (2010)</ref> proposed four efficient methods to compute random walk graph kernels which generally reduce the computational complexity from O(n 6 ) to O(n 3 ). <ref type="bibr" target="#b100">Mah? et al. (2004)</ref> proposed some other extensions of random walk kernels. Specifically, they proposed a label enrichment approach which increases specificity and in most cases also reduces computational complexity. They also employed a second order Markov random walk to deal with the problem of "tottering". <ref type="bibr" target="#b155">Sugiyama and Borgwardt (2015)</ref> focused on a different problem of random walk kernels, a phenomenon referred to as "halting". More recently, <ref type="bibr" target="#b182">Zhang et al. (2018b)</ref> proposed a kernel that capitalizes on the isomorphism-invariance property of the return probabilities of random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Subtree Kernel</head><p>Due to problems with the expressiveness of the random walk kernels that they identified, <ref type="bibr" target="#b132">Ramon and G?rtner (2003)</ref> worked on designing new kernels. Their research efforts resulted in the development of the subtree kernel, an algorithm that counts the number of common subtree patterns in two graphs. The kernel is more expressive (in the sense that it can distinguish non-isomorphic graphs which walk-based kernels cannot), but also more computationally expensive than the random walk kernels.</p><p>The subtree patterns that the subtree kernel considers correspond to rooted subgraphs. Every subtree pattern has a tree-structured signature, and the kernel associates each possible subtree pattern signature to a feature. Given a graph, the value of each feature is the number of times that a subtree of the signature that corresponds to this feature occurs in the graph. Let k h (v, v ) be a kernel that counts the pairs of subtrees of the same signature of height less than or equal to h, where the first subtree is rooted at v and the second one</p><formula xml:id="formula_21">is rooted at v . The kernel k h (v, v ) is equal to: k h (v, v ) = ?( (v), (v )) if h = 1 ? v ? v R?M (v,v ) (u,u )?R k h?1 (u, u ) if h &gt; 1<label>(13)</label></formula><p>where ? v and ? v are positive values smaller than 1 to cause higher trees to have a smaller weight in the overall sum, and ? is the dirac kernel. Therefore, if h = 1 and the two nodes share the same label, then it holds that k 1 (v, v ) = 1. If h = 1 and the two nodes have different labels, we have k 1 (v, v ) = 0. For h ? 1, one can compute k h (v, v ) using a recursive scheme. Specifically, we define the set of all matchings from N (v) to N (v ) as follows</p><formula xml:id="formula_22">M (v, v ) = R ? N (v) ? N (v )| ?(u, u ), (w, w ) ? R : u = w ? u = w ? ?(u, u ) ? R : (u) = (u )<label>(14)</label></formula><p>Each element R of M (v, v ) is a set of pairs of nodes from the neighborhoods of v ? V and v ? V , such that nodes in each pair have identical labels and no node is contained in more than one pair. The subtree kernel compares all pairs of vertices from two graphs by iteratively comparing their neighborhoods.</p><p>Definition 15 (Subtree Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Then, the subtree kernel is defined as</p><formula xml:id="formula_23">k(G, G ) = v?V v ?V k h (v, v )<label>(15)</label></formula><p>The computational complexity of the subtree kernel for a pair of graphs is O(n 2 4 deg * h). Although in the worst-case scenario, the runtime complexity of the subtree kernel is very high, in practice, it can be quite low if the input graphs are sparse or if there is sufficient diversity in the labels of the vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Cyclic Pattern Kernel</head><p>The cyclic pattern kernel is also one of the earliest approaches developed in the area of graph kernels. This kernel decomposes a graph into cyclic and tree patterns, and counts the number of common patterns which occur in two graphs <ref type="bibr" target="#b72">(Horv?th et al., 2004)</ref>. More specifically, let G = (V, E) be a graph. Let also S(G) denote the set of cycles of G. Let C = (v 1 , v 2 , . . . , v k , v 1 ) be a sequence of vertices that forms a cycle in G, that is C ? S(G). The canonical representation of a cycle C is the lexicographically smallest string ?(C) among the strings obtained by concatenating the labels along the vertices of the cyclic permutations of C and its reverse. Formally, denoting by ?(s) the set of cyclic permutations of a sequence s and its reverse, the canonical representation of C is defined by</p><formula xml:id="formula_24">?(C) = min{w : w ? ? (v 1 ), (v 2 ), . . . , (v k ) }<label>(16)</label></formula><p>where is a function that assigns labels to the vertices of the graph. In case of edge-labeled graphs, edgle labels can also be taken into account. The set of cyclic patterns of G is then defined by</p><formula xml:id="formula_25">C(G) = {?(C) : C ? S(G)}<label>(17)</label></formula><p>The kernel then extracts from G all the edges that do not belong to any cycle (a.k.a bridges) by removing from G all the edges of all cycles. The set of bridges of G forms a set of trees (each tree is a connected component composed of bridges). Then, similarly to cycles, the kernel computes the canonical representation ?(T ) of each tree T . The set of tree patterns of G is then defined by</p><formula xml:id="formula_26">T (G) = {?(T ) : T is a tree}<label>(18)</label></formula><p>Then, given two graphs, the kernel computes the intersection of their sets of cyclic and tree patters.</p><p>Definition 16 (Cyclic Pattern Kernel). Let G, G be two graphs, and C(G), C(G ) and T (G), T (G ) be the sets of cyclic patterns and tree patters of the two graphs, respectively. Then, the cyclic pattern kernel is defined as</p><formula xml:id="formula_27">k(G, G ) = |C(G) ? C(G )| + |T (G) ? T (G )|<label>(19)</label></formula><p>Unfortunately, computing the cyclic pattern kernel is an NP-hard problem. The cardinality of the set of cyclic and tree patterns of a graph can be exponential in the number of vertices of the graph. However, the cyclic pattern kernel can prove useful for practical problem classes where the number of cycles in the input graphs is bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Shortest-Path Kernel</head><p>The high computational complexity of graph kernels based on walks, subtrees and cycles renders them impractical for most real-world scenarios.  worked on developing more efficient kernels based on paths. However, computing all the paths in a graph and computing the longest paths in a graph are both NP-hard problems. Instead, shortest paths can be computed in polynomial time, and they gave rise to the shortest-path kernel, one of the most popular kernels to this day.</p><p>The shortest-path kernel decomposes graphs into shortest paths and compares pairs of shortest paths according to their lengths and to the labels of their endpoints. The first step of the shortest-path kernel is to transform the input graphs into shortest-paths graphs. G i Triples:</p><p>?(G i ) = (2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0) ?(G j ) = (0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1) a b G j <ref type="figure">Figure 5</ref>: Example of explicit computation of the shortest path kernel. Each triple is a feature and corresponds to: (label of source vertex; label of sink vertex; shortest path length between the two vertices).</p><p>Given an input graph G = (V, E), the algorithm creates a new graph S = (V, E s ) (i. e., its shortest-path graph). The shortest-path graph S contains the same set of vertices as its source graph. The edge set of the former is a superset of that of the latter, since in the shortest-path graph S, there exists an edge between all vertices that are connected by a walk in the original graph G. To complete the transformation, the algorithm assigns labels to all the edges of the shortest-path graph S. The label of each edge is set equal to the shortest distance between its endpoints in the original graph G. Given the above procedure for transforming a graph into a shortest-path graph, the shortest-path kernel is defined as follows.</p><p>Definition 17 (Shortest-Path Kernel). Let G, G be two graphs, and S = (V, E), S = (V , E ) their corresponding shortest-path graphs. The shortest-path kernel is then defined as</p><formula xml:id="formula_28">k(G, G ) = e?E e ?E k (1) walk (e, e )<label>(20)</label></formula><p>where k</p><p>(1)</p><p>walk (e, e ) is a positive semidefinite kernel on edge walks of length 1.</p><p>In labeled graphs, the k</p><p>walk (e, e ) kernel is designed to compare both the lengths of the shortest paths corresponding to edges e and e , and the labels of their endpoint vertices. Let e = (v, u) and e = (v , u ). Then, k</p><p>walk (e, e ) is usually defined as</p><formula xml:id="formula_31">k (1) walk (e, e ) = k v (v), (v ) k e (e), (e ) k v (u), (u )<label>(21)</label></formula><p>where k v is a kernel comparing vertex labels, and k e a kernel comparing shortest path lengths. Vertex labels are usually compared via a dirac kernel, while shortest path lengths may also be compared via a dirac kernel or, more rarely, via a brownian bridge kernel . When k v and k e both are dirac kernels, an explicit computation scheme can be employed as shown in <ref type="figure">Figure 5</ref>. In terms of runtime complexity, the shortest-path kernel can be computed in O(n 4 ) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Graphlet Kernel</head><p>The graphlet kernel decomposes graphs into graphlets (i. e., small subgraphs with k vertices where k ? {3, 4, 5}) <ref type="bibr" target="#b130">(Pr?ulj, 2007)</ref> and counts matching graphlets in the input graphs. For example, the set of graphlets of size 4 is shown in <ref type="figure">Figure 6</ref>. This kernel was originally designed to address scalability issues experienced by earlier approaches. In fact, the graphlet kernel was one of the first kernels that could cope with very large graphs using a simple sampling scheme. However, apart from the scalability issue, the graphlet kernel was also motivated by the graph reconstruction conjecture <ref type="bibr" target="#b17">(Bondy &amp; Hemminger, 1977)</ref>, which states that any graph of size n can be reconstructed from the set of all its subgraphs of size n ? 1. This could possibly be interpreted as indicating that kernels that compare graphs based on their subgraphs should reflect graph similarity better than approaches that are defined based on random walks, subtrees, cyclic patterns or shortest paths. However, even if graphs that have similar distributions of graphlets are very likely to be similar themselves, there is no theoretical justification on why such a substructure (i. e., graphlets) is better than the others.</p><p>As mentioned above, the graphlet kernel computes the distribution of small subgraphs in a graph. Let G = {graphlet 1 , graphlet 2 , . . ., graphlet d } be the set of size-k graphlets. Let also f G ? N d be a vector such that its i-th entry is equal to the frequency of occurrence of graphlet i in G, f G,i = #(graphlet i G). Then, the graphlet kernel is defined as follows.</p><p>Definition 18 (Graphlet of size k Kernel). Let G, G be two graphs of size n ? k, and f G , f G vectors that count the occurrence of each graphlet of size k (not necessarily connected) in the two graphs. Then the graphlet kernel is defined as</p><formula xml:id="formula_32">k(G, G ) = f G f G (22)</formula><p>As is evident from the above definition, the graphlet kernel is computed by explicit feature maps. First, the representation of each graph in the feature space is computed. And then, the kernel value is computed as the dot product of the two feature vectors. The main problem of the graphlet kernel is that an exaustive enumeration of graphlets is very expensive. Since there are n k size-k subgraphs in a graph, computing the feature vector for a graph of size n requires O(n k ) time. To account for that, <ref type="bibr" target="#b145">Shervashidze et al. (2009)</ref> resorted to sampling. Following <ref type="bibr" target="#b171">Weissman et al. (2003)</ref>, they showed that by sampling a fixed number of graphlets the empirical distribution of graphlets will be sufficiently close to their actual distribution in the graph. An alternative proposed strategy that reduces the expressivity of the kernel is to enumerate only the connected graphlets of k vertices, and not all the possible graphlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.6">Weisfeiler-Lehman Subtree Kernel</head><p>The Weisfeiler-Lehman subtree kernel is a very popular algorithm, and is considered the state-of-the-art in graph classification. It belongs to the family of subtree kernels, and was motivated by the need for a fast subtree kernel that scales up to large, labeled graphs. The kernel is an instance of the Weisfeiler-Lehman framework. This framework operates on top of existing graph kernels and is inspired by the Weisfeiler-Lehman test of graph isomorphism <ref type="bibr" target="#b170">(Weisfeiler &amp; Lehman, 1968)</ref>. The key idea of the Weisfeiler-Lehman algorithm is to replace <ref type="figure">Figure 6</ref>: All graphlets of size 4. the label of each vertex with a multiset label consisting of the original label of the vertex and the sorted set of labels of its neighbors. The resultant multiset is then compressed into a new, short label. This relabeling procedure is then repeated for h iterations. Note that this procedure is performed simultaneously on all input graphs. Therefore, two vertices from different graphs will get identical new labels if and only if they have identical multiset labels.</p><formula xml:id="formula_33">G 1 G 2 G 3 G 4 G 5 G 6 G 7 G 8 G 9 G 10 G 11</formula><p>More formally, given a graph G = (V, E) endowed with a labeling function = 0 , the Weisfeiler-Lehman graph of G at height i is a graph G i = (V, E) endowed with a labeling function i which has emerged after i iterations of the relabeling procedure described above. The Weisfeiler-Lehman sequence up to height h of G consists of the Weisfeiler-Lehman graphs of G at heights from 0 to h, {G 0 , G 1 , . . . , G h }.</p><p>Definition 19 <ref type="bibr">(Weisfeiler-Lehman Framework)</ref>. Let k be any kernel for graphs, that we will call the base kernel. Then the Weisfeiler-Lehman kernel with h iterations with the base kernel k between two graphs G and G is defined as</p><formula xml:id="formula_34">k W L (G, G ) = k(G 0 , G 0 ) + k(G 1 , G 1 ) + . . . + k(G h , G h )<label>(23)</label></formula><p>where h is the number of Weisfeiler-Lehman iterations, and {G 0 , G 1 , . . . , G h } and {G 0 , G 1 , . . . , G h } are the Weisfeiler-Lehman sequences of G and G respectively.</p><p>From the above definition, it is clear that any graph kernel that takes into account discrete node labels can take advantage of the Weisfeiler-Lehman framework and compare graphs based on the whole Weisfeiler-Lehman sequence.</p><p>When the base kernel compares subtrees extracted from two graphs, the computation involves counting the common original and compressed labels in the two graphs. The emerging Weisfeiler-Lehman subtree kernel is a byproduct of the Weisfeiler-Lehman test of isomorphism.</p><p>Definition 20 (Weisfeiler-Lehman Subtree Kernel). Let G, G be two graphs. Define ? i ? ? as the set of letters that occur as node labels at least once in G or G at the end of the i-th iteration of the Weisfeiler-Lehman algorithm. Let ? 0 be the set of original node labels of G and G . Assume all ? i are pairwise disjoint. Without loss of generality, assume that every</p><formula xml:id="formula_35">? i = {? i1 , . . . , ? i|? i | } is ordered. Define a map c i : {G, G } ? ? i ? N such that c i (G, ? ij )</formula><p>is the number of occurrences of the letter ? ij in the graph G.  </p><formula xml:id="formula_36">G G ?(G) = ( ?(G ) (a) Input graphs (b) Updated labels (c) Label compression (d) Relabeled graphs (e) Feature</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vector representations of graphs</head><p>Original node labels Compressed node labels = ( 2, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, ) 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, ) <ref type="figure">Figure 7</ref>: Illustration of the computation of the Weisfeiler-Lehman subtree kernel with h = 1 for two graphs G and G . Here, 1, 2, . . . , 13 ? ? are letters that occur as node labels.</p><p>Compressed labels map to subtree patterns. For example, if a node has label 6, this means that there is a subtree pattern of height 1 rooted at this node, where the root has label 1 and its single neighbor has label 4.</p><p>The Weisfeiler-Lehman subtree kernel on two graphs G and G with h iterations is defined as</p><formula xml:id="formula_37">k(G, G ) = ?(G), ?(G ) (24) where ?(G) = (c 0 (G, ? 01 ), . . . , c 0 (G, ? 0|? 0 | ), . . . , c h (G, ? h1 ), . . . , c h (G, ? h|? h | ))<label>(25)</label></formula><p>and</p><formula xml:id="formula_38">?(G ) = (c 0 (G , ? 01 ), . . . , c 0 (G , ? 0|? 0 | ), . . . , c h (G , ? h1 ), . . . , c h (G , ? h|? h | ))<label>(26)</label></formula><p>An illustration of the Weisfeiler-Lehman subtree kernel is given in <ref type="figure">Figure 7</ref>. It can be shown that the above definition is equivalent to comparing the number of shared subtrees between the two input graphs <ref type="bibr" target="#b144">(Shervashidze et al., 2011)</ref>. In contrast to the subtree kernel that was proposed by Ramon and G?rtner and was presented above, the Weisfeiler-Lehman subtree kernel considers all subtrees up to height h, instead of subtrees of exactly height h. Furthermore, the Weisfeiler-Lehman subtree kernel checks whether the neighborhoods of two vertices match exactly, while the subtree kernel considers all pairs of matching subsets of the neighborhoods of two vertices. It is interesting to note that the Weisfeiler-Lehman subtree kernel exhibits a very attractive computational complexity since it can be computed in O(hm) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Neighborhood Aggregation Approaches</head><p>The Weisfeiler-Lehman subtree kernel triggered a lot of activity in the field of graph kernels. The relabeling procedure of the Weisfeiler-Lehman algorithm can be viewed as a neighborhood aggregation scheme. The main idea behind neighborhood aggregation algorithms (a.k.a. message-passing algorithms) is that each vertex receives messages from its neighbors and utilizes these messages to update its representation. Following the success of this kernel, several variations of it were proposed. All these variations employ a neighborhood aggregation scheme similar to that of the Weisfeiler-Lehman algorithm. The goal of most of these works is to speed-up the computation time of the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b71">(Hido &amp; Kashima, 2009;</ref><ref type="bibr" target="#b79">Kataoka &amp; Inokuchi, 2016)</ref>. However, other types of variations were also proposed such as a streaming version of the Weisfeiler-Lehman algorithm <ref type="bibr" target="#b93">(Li et al., 2012)</ref>, a kernel that uses the k-dimensional Weisfeiler-Lehman test of isomorphism <ref type="bibr" target="#b109">(Morris et al., 2017)</ref>, and a method that augments the subtree features with topological information . We next present the neighborhood hash kernel, a kernel that was born out of these research efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Neighborhood Hash Kernel</head><p>Similar to the Weisfeiler-Lehman subtree kernel, the neighborhood hash kernel also assumes node-labeled graphs <ref type="bibr" target="#b71">(Hido &amp; Kashima, 2009</ref>). It compares graphs by updating their node labels and counting the number of common labels. The kernel replaces the discrete node labels with binary arrays of fixed length, and it then employs logical operations to update the labels so that they contain information about the neighborhood structure of each vertex.</p><p>Let : V ? ? be a function that maps vertices to an alphabet ? which is the set of possible discrete node labels. Hence, given a vertex v, (v) ? ? is the label of vertex v. The algorithm first transforms each discrete node label to a bit label. A bit label is a binary array consisting of d bits as</p><formula xml:id="formula_39">s = (b 1 , b 2 , . . . , b d )<label>(27)</label></formula><p>where the constant d satisfies 2 d ? 1 |?| and b 1 , b 2 , . . . , b d ? {0, 1}. The most important step of the algorithm involves a procedure that updates the labels of the vertices. To achieve that, the kernel makes use of two very common bit operations:</p><p>(1) the exclusive or (XOR) operation, and (2) the bit rotation (ROT ) operation. Let XOR(s i , s j ) = s i ? s j denote the XOR operation between two bit labels s i and s j (i. e., the XOR operation is applied to all their components). The output of the operation is a new binary array whose components represent the XOR value between the corresponding components of the s i and s j arrays. The ROT o operation takes as input a bit array and shifts its last o bits to the left by o bits and moves the first o bits to the right end as shown</p><formula xml:id="formula_40">a(#1000) c(#1100) b(#1110) #1110 #1100 ? #0010 ? #0001 ROT1(#1000 ) = #0011</formula><p>Figure 8: Example of computation of the simple neighborhood hash for a vertex (in green). The vertex has two adjacent vertices (in red). The three vertices have different labels from each other. The algorithm uses XOR and ROT operations to compute the neighborhood hash of the vertex (#0011).</p><p>below</p><formula xml:id="formula_41">ROT o (s) = {b o+1 , b o+2 , . . . , b d , b 1 , . . . , b o }<label>(28)</label></formula><p>Below, we present in detail two procedures for updating the labels of the vertices: (1) the simple neighborhood hash, and (2) the count-sensitive neighborhood hash.</p><p>Simple Neighborhood Hash. Given a graph G = (V, E) with bit labels, the simple neighborhood hash update procedure computes a neighborhood hash for each vertex using the logical operations XOR and ROT . More specifically, given a vertex v ? V , let N (v) = {u 1 , . . . , u d } be the set of neighbors of v. Then, the kernel computes the neighborhood hash as</p><formula xml:id="formula_42">N H(v) = ROT 1 (v) ? (u 1 ) ? . . . ? (u d )<label>(29)</label></formula><p>The resulting hash N H(v) is still a bit array of length d, and we regard it as the new label of v. This new label represents the distribution of the node labels around v. Hence, if v i and v j are two vertices that have the same label (i. e., (v i ) = (v j )) and the label sets of their neighborhors are also identical, their hash values will be the same (i. e., N H(v i ) = N H(v j )). Otherwise, they will be different except for accidental hash collisions. The main idea behind this update procedure is that the hash value is independent of the order of the neighborhood values due to the properties of the XOR operation. Hence, one can check whether or not the distributions of neighborhood labels of two vertices are equivalent without sorting or matching these two label sets. <ref type="figure">Figure 8</ref> illustrates how the simple neighborhood hash is computed for a given vertex.</p><p>Count-sensitive Neighborhood Hash. The simple neighborhood hash update procedure described above suffers from some problematic hash collisions. Specifically, the neighborhood hash values for two independent nodes have a small probability of being the same even if there is no accidental hash collision. Such problematic hash collisions may affect the positive semidefiniteness of the kernel. To address that problem, the count-sensitive neighborhood hash update procedure counts the number of occurences of each label in the set. More specifically, it first uses a sorting algorithm (e. g., radix sort) to align the bit labels of the neighbors, and then, it extracts the unique labels (set { 1 , . . . , l } in the case of l unique labels) and for each label counts its number of occurences. Then, it updates each unique label based on its number of occurences as follows <ref type="figure">Figure 9</ref>: Example of computation of the count-sensitive neighborhood hash for a vertex (in green). The vertex has three adjacent vertices (in red). Two of these three vertices have identical labels. The algorithm uses XOR and ROT operations to compute the countsensitive neighborhood hash of the vertex (#0111).</p><formula xml:id="formula_43">i = ROT o i ? o (30) a(#1000) c(#1100) #0110 ? #0001 ROT1(#1000 ) = #0111 d(#0101) d(#0101) #0101 #1100 #0010) = #0001 = ? ? ROT2(#0111) = #1101 ROT1(#1101) = #1011 ROT2( ROT1( ?</formula><p>where i , i is the initial and updated label respectively, and o is the number of occurences of that label in the set of neighbors. The above operation makes the hash values unique by depending on the number of label occurrences. Then, the count-sensitive neighborhood hash is computed as <ref type="figure">Figure 9</ref> illustrates the operations of the count-sensitive neighborhood hash for a given vertex. Both the simple and the count-sensitive neighborhood hash can be seen as general approaches for enriching the labels of vertices based on the label distribution of their neighborhood vertices.</p><formula xml:id="formula_44">CSN H(v) = ROT 1 (v) ? 1 ? . . . ? l<label>(31)</label></formula><p>Kernel Calculation. The neighborhood hash update procedures presented above aggregate the information of the neighborhood vertices to each vertex. Then, given two graphs G and G , the updated labels of their vertices are compared using the following function</p><formula xml:id="formula_45">?(G, G ) = c |V | + |V | ? c<label>(32)</label></formula><p>where c is the number of labels the two graphs have in common. This function is equivalent to the Tanimoto coefficent which is commonly used as a similarity measure between sets of discrete values and which has been proven to be positive semidefinite <ref type="bibr" target="#b61">(Gower, 1971</ref>). The label-update procedures is not necessary to be applied once, but they can be applied iteratively. By updating the bit labels several times, the new labels can capture high-order relationships between vertices. For instance, if the procedure is performed h times in total, the updated label (v) of a vertex v represents the label distribution of its h-neighbors. Hence, two vertices v i , v j with identical labels and connections among their r-neighbors will be assigned the same label.</p><p>Definition 21 (Neighborhood Hash Kernel). Let G and G be two graphs, and let G 1 , . . . , G h and G 1 , . . . , G h denote their updated graphs where the node labels have been updated 1, . . . , h times based on one of the two procedures presented above, respectively. Then, the neighborhood hash kernel is defined as</p><formula xml:id="formula_46">k(G, G ) = 1 h h i=1 ?(G i , G i )<label>(33)</label></formula><p>The computational complexity of the neighborhood hash kernel is O(deg nhd) where n = |V | is the number of vertices of the graphs and deg is the average degree of their vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Other Approaches</head><p>Recently, several kernels were proposed that belong to the R-convolution framework, but do not perform neighborhood aggregation. There are, for instance, kernels specially designed for graphs with ordered neighborhoods <ref type="bibr" target="#b42">(Draief et al., 2018)</ref>, kernels that compare pairs of rooted subgraphs containing vertices up to a certain distance from the root <ref type="bibr" target="#b33">(Costa &amp; De Grave, 2010)</ref>, kernels that extract directed acyclic graphs from the input graphs <ref type="bibr" target="#b36">(Da San Martino et al., 2012)</ref>, and kernels that use the orthonormal representations of vertices introduced by Lov?sz <ref type="bibr" target="#b76">(Johansson et al., 2014)</ref>. We next present some of these kernels in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Neighborhood Subgraph Pairwise Distance Kernel</head><p>The neighborhood subgraph pairwise distance kernel extracts pairs of rooted subgraphs from each graph whose roots are located at a certain distance from each other, and which contain vertices up to a certain distance from the root. It then compares graphs based on these pairs of rooted subgraphs. To avoid isomorphism checking, graph invariants are employed to encode each rooted subgraph <ref type="bibr" target="#b33">(Costa &amp; De Grave, 2010)</ref>.</p><formula xml:id="formula_47">Let G = (V, E) be a graph. The distance between two vertices u, v ? V , denoted D(u, v)</formula><p>, is the length of the shortest path between them. The neighborhood of radius r of a vertex v is the set of vertices at a distance less than or equal to r from v, that is {u ? V : D(u, v) ? r}. Given a subset of vertices S ? V , let E(S) be the set of edges that have both end-points in S. Then, the subgraph with vertex set S and edge set E(S) is known as the subgraph induced by S. The neighborhood subgraph of radius r of vertex v is the subgraph induced by the neighborhood of radius r of v and is denoted by N r (v). Let also R r,d (A v , B u , G) be a relation between two rooted graphs A v , B u and a graph G = (V, E) that is true if and only if both</p><formula xml:id="formula_48">A v and B u are in {N r (v) : v ? V }, where we require A v , B u</formula><p>to be isomorphic to some N r (v) to verify the set inclusion, and that D(u, v) = d. We denote with R ?1 (G) the inverse relation that yields all the pairs of rooted graphs A v , B u satisfying the above constraints. Hence, R ?1 (G) selects all pairs of neighborhood graphs of radius r whose roots are at distance d in a given graph G.</p><p>Definition 22 (Neighborhood Subgraph Pairwise Distance Kernel). Let G, G be two graphs. The neighborhood subgraph pairwise distance kernel extracts from the two graphs pairs of rooted subgraphs of radius r whose roots are located at distance d from each other. It then utilizes the following kernel to compare them</p><formula xml:id="formula_49">k r,d (G, G ) = Av,Bv?R ?1 r,d (G) A v ,B v ?R ?1 r,d (G ) ?(A v , A v ) ?(B v , B v )<label>(34)</label></formula><p>where ? is 1 if its input subgraphs are isomorphic, and 0 otherwise. The above kernel counts the number of identical pairs of neighboring graphs of radius r at distance d between two graphs. Then, the neighborhood subgraph pairwise distance kernel is defined as</p><formula xml:id="formula_50">k(G, G ) = r * r=0 d * d=0k r,d (G, G ) (35)</formula><p>wherek r,d is a normalized version of k r,d , that i?</p><formula xml:id="formula_51">k r,d (G, G ) = k r,d (G, G ) k r,d (G, G)k r,d (G , G )<label>(36)</label></formula><p>The above version ensures that relations of all orders are equally weighted regardless of the size of the induced part sets. The neighborhood subgraph pairwise distance kernel includes an exact matching kernel over two graphs (i. e., the ? kernel) which is equivalent to solving the graph isomorphism problem. Solving the graph isomorphism problem is not feasible. Therefore, the kernel produces an approximate solution to it instead. Given a subgraph G S induced by the set of vertices S, the kernel computes a graph invariant encoding for the subgraph via a label function g : G ? ? * , where G is the set of rooted graphs and ? * is the set of strings over a finite alphabet ?. The function g makes use of two other label functions: (1) a function n for vertices, and (2) a function e for edges. The n function assigns to vertex v the concatenation of the lexicographically sorted list of triplets D(v, u), D(v, h), (u) for all u ? S, where h is the root of the subgraph and is a function that maps vertices/edges to their label symbol. Hence, the above function relabels each vertex with a string that encodes the initial label of the vertex, the vertex distance from all other labeled vertices, and the distance from the root vertex. The e (u, v) function assigns to edge (u, v) the label n (u), n (v), <ref type="bibr">(u, v)</ref> . The e (u, v) function thus annotates each edge based on the new labels of its endpoints, and its initial label, if any. Finally, the function g (G S ) assigns to the rooted graph induced by S the concatenation of the lexicographically sorted list of e (u, v) for all {u, v} ? E(S). The kernel then employs a hashing function from strings to natural numbers H : ? * ? N to obtain a unique identifier for each subgraph. Hence, instead of testing pairs of subgraphs for isomorphism, the kernel just checks if the subgraphs share the same identifier.</p><p>The computational complexity of the neighborhood subgraph pairwise distance kernel is O(n|S||E(S)| log |E(S)|) and is dominated by the repeated computation of the graph invariant for each vertex of the graph. Since this is a constant time procedure, for small values of d * and r * , the complexity of the kernel is in practice linear in the size of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Lov?sz ? Kernel</head><p>The Lov?sz number ?(G) of a graph G = (V, E) is a real number that is an upper bound on the Shannon capacity of the graph. It was introduced by L?szl? Lov?sz in 1979 <ref type="bibr" target="#b98">(Lov?sz, 1979)</ref>. The Lov?sz number is intimately connected with the notion of orthonormal representations of graphs. An orthonormal representation of a graph G consists of a set of unit vectors U G = {u i ? R d : ||u i || = 1} i?V where each vertex i is assigned a unit vector u i such that (i, j) ? E =? u i u j = 0. Specifically, the Lov?sz number of a graph G is defined as</p><formula xml:id="formula_52">?(G) = min c,U G max i?V 1 (c u i ) 2<label>(37)</label></formula><p>where c ? R d is a unit vector and U G is an orthonormal representation of G. Geometrically, ?(G) is defined by the smallest cone enclosing a valid orthonormal representation U G . The Lov?sz number ?(G) of a graph G can be computed to arbitrary precision in polynomial time by solving a semidefinite program. The Lov?sz ? kernel utilizes the orthonormal representations associated with the Lov?sz number to compare graphs <ref type="bibr" target="#b76">(Johansson et al., 2014)</ref>. The kernel is applicable only to unlabeled graphs. Given a collection of graphs, it first generates orthonormal representations for the vertices of each graph by computing the Lov?sz ? number. Hence, U G is a set that contains the orthonormal representations of G. Let S ? V be a subset of the vertex set of G. Then, the Lov?sz value of the set of vertices S is defined as</p><formula xml:id="formula_53">? S (G) = min c max i?S 1 (c u i ) 2<label>(38)</label></formula><p>where c ? R d is a unit vector and u i is the representation of vertex i obtained by computing the Lov?sz number ?(G) of G. The Lov?sz value of a set of vertices S represents the angle of the smallest cone enclosing the set of orthonormal representations of these vertices (i. e., subset of U G defined as</p><formula xml:id="formula_54">{u i : u i ? U G , i ? S}).</formula><p>Definition 23 (Lov?sz ? Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The Lov?sz ? kernel between the two graphs is defined as follows</p><formula xml:id="formula_55">k(G, G ) = S?V S ?V ?(|S|, |S |) 1 Z |S| k ? S (G), ? S (G )<label>(39)</label></formula><p>where Z |S| = |V | |S| |V | |S| , ?(|S|, |S |) is a delta kernel (equal to 1 if |S| = |S |, and 0 otherwise), and k is a positive semi-definite kernel between Lov?sz values (e. g., linear kernel, gaussian kernel).</p><p>The Lov?sz ? kernel consists of two main steps: (1) computing the Lov?sz number ? of each graph and obtaining the associated orthonormal representations, and (2) computing the Lov?sz value for all subgraphs (i. e., subsets of vertices S ? V ) of each graph. Exact computation of the Lov?sz ? kernel is in most real settings infeasible since it requires computing the minimum enclosing cones of 2 n sets of vertices.</p><p>When dealing with large graphs, it is thus necessary to resort to sampling. Given a graph G, instead of evaluating the Lov?sz value on all 2 n sets of vertices, the algorithm evaluates it in on a smaller number of subgraphs induced by sets of vertices contained in L ? 2 V . Then, the Lov?sz ? kernel is defined as follow?</p><formula xml:id="formula_56">k(G, G ) = S?L S ?L ?(|S|, |S |) 1 Z |S| k ? S (G), ? S (G )<label>(40)</label></formula><p>where? |S| = |L |S| ||L |S| | and L |S| denotes the subset of L consisting of all sets of cardinality |S|, that is</p><formula xml:id="formula_57">L |S| = {B ? L : |B| = |S|}. The time complexity of computingk(G, G ) is O(n 2 m ?1 + s 2 T (k) + sn) where T (k)</formula><p>is the complexity of computing the base kernel k, n = |V |, m = |E| and s = max(|L|, |L |). The first term represents the cost of solving the semi-definite program that computes the Lov?sz number ?. The second term corresponds to the worst-case complexity of computing the sum of the Lov?sz values. And finally, the third term is the cost of computing the Lov?sz values of the sampled subsets of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.3">SVM-? Kernel</head><p>The SVM-? kernel is closely related to the Lov?sz ? kernel <ref type="bibr" target="#b76">(Johansson et al., 2014)</ref>. The Lov?sz ? kernel suffers from high computational complexity, and the SVM-? kernel was developed as a more efficient alternative. Similar to the Lov?sz ? kernel, this kernel also assumes unlabeled graphs.</p><p>Given a graph G = (V, E) such that |V | = n, the Lov?sz number of G can be defined as</p><formula xml:id="formula_58">?(G) = min K?L ?(K)<label>(41)</label></formula><p>where ?(K) is the one-class SVM given by</p><formula xml:id="formula_59">?(K) = max ? i &gt;0 2 n i=1 ? i ? n i=1 n j=1 ? i ? j K ij<label>(42)</label></formula><p>and L is a set of positive semidefinite matrices defined as</p><formula xml:id="formula_60">L = {K ? S + n : K ii = 1, K ij = 0 ?(i, j) ? E}<label>(43)</label></formula><p>where S + n is the set of all n ? n positive semidefinite matrices. The SVM-? kernel first computes the matrix K LS which is equal to</p><formula xml:id="formula_61">K LS = A ? + I<label>(44)</label></formula><p>where A is the adjacency matrix of G, I is the n ? n identity matrix, and ? ? ?? n with ? n the minimum eigenvalue of A. The matrix K LS is positive semidefinite by construction and it has been shown in  that</p><formula xml:id="formula_62">?(K LS ) = n i=1 ? i<label>(45)</label></formula><p>where ? i are the maximizers of Equation <ref type="formula" target="#formula_3">(42)</ref>. Furthermore, it was shown that on certain families of graphs (e. g., Erd?s R?nyi random graphs), ?(K LS ) is with high probability a constant factor approximation to ?(G).</p><p>Definition 24 (SVM-? Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Then, the SVM-? kernel is defined as follows</p><formula xml:id="formula_63">k(G, G ) = S?V S ?V ?(|S|, |S |) 1 Z |S| k i?S ? i , j?S ? j<label>(46)</label></formula><p>where Z |S| = |V | |S| |V | |S| , ?(|S|, |S |) is a delta kernel (equal to 1 if |S| = |S |, and 0 otherwise), and k is a positive semi-definite kernel between real values (e. g., linear kernel, gaussian kernel).</p><p>The SVM-? kernel consists of three main steps: (1) constructing matrix K LS of G which takes O(n 3 ) time (2) solving the one-class SVM problem in O(n 2 ) time to obtain the ? i values, and (3) computing the sum of the ? i values for all subgraphs (i. e., subsets of vertices S ? V ) of each graph. Computing the above quantity for all 2 n sets of vertices is not feasible in real-world scenarios.</p><p>To address the above issue, the SVM-? kernel employs sampling schemes. Given a graph G, the kernel samples a specific number of subgraphs induced by sets of vertices contained in L ? 2 V . Then, the SVM-? kernel is defined as follow?</p><formula xml:id="formula_64">k(G, G ) = S?L S ?L ?(|S|, |S |) 1 Z |S| k i?S ? i , j?S ? j<label>(47)</label></formula><p>where? |S| = |L |S| ||L |S| | and L |S| denotes the subset of L consisting of all sets of cardinality</p><formula xml:id="formula_65">|S|, that is L |S| = {B ? L : |B| = |S|}. The time complexity of computingk(G, G ) is O(n 3 + s 2 T (k) + sn) where T (k)</formula><p>is the complexity of computing the base kernel k and s = max(|L|, |L |). The first term represents the cost of computing K LS (dominated by the eigenvalue decomposition). The second term corresponds to the worst-case complexity of comparing the sums of the ? i values. And finally, the third term is the cost of computing the sum of the ? i values for the sampled subsets of vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.4">Ordered Decomposition DAGs Kernel</head><p>In contrast to the above two kernels, the ordered decomposition DAGs kernel can handle node-labeled graphs. The kernel decomposes graphs into multisets of directed acyclic graphs (DAGs), and then uses existing tree kernels to compare these DAGs <ref type="bibr" target="#b36">(Da San Martino et al., 2012)</ref>.</p><p>Given a graph G = (V, E), the kernel generates one unordered rooted DAG, say DD v , for each vertex v ? V . To generate the DAG, the kernel keeps only those edges belonging to the shortest paths between v and any vertex u ? V \ {v}. Furthermore, a direction is given to each edge, while edges connecting vertices visited at level l to vertices visited at level l &lt; l are also removed. <ref type="figure">Figure 10</ref> gives an example of the decomposition of a graph into a set of DAGs.</p><p>Definition 25 (Ordered Decomposition DAGs Kernel). Let G = (V, E) and G = (V , E ) be two graphs. Let also DD(G) and DD(G ) be multisets defined as</p><formula xml:id="formula_66">{DD v : v ? V } and {DD v : v ? V }, respectively.</formula><p>Then, the ordered decomposition DAGs kernel is defined as</p><formula xml:id="formula_67">k(G, G ) = D?DD(G) D ?DD(G ) k DAG (D, D )<label>(48)</label></formula><p>where k DAG is a kernel between DAGs.</p><p>The kernel is thus defined as the sum of the computation of a local kernel for DAGs, over all pairs of DAGs in the multiset. Note that these DAGs are unordered. Moreover, there is a large literature on kernels for ordered trees, but only a few kernel functions for unordered trees. Hence, the ordered decomposition DAGs kernel transforms the unordered DAGs to ordered DAGs, and then applies a kernel for ordered trees. More specifically, the kernel defines a strict partial order among the vertices of each DAG. This partial order takes into account the labels of the vertices, the outdegrees of the vertices (in case of identical node labels), and the relation between the sequence of successors of each vertex (in case of identical node labels and equal outdegrees). Let ODD v denote the DAG of v ? V ordered according to the above relation. Let a tree visit be a function T (u) that, given a vertex u of a ODD v , returns the tree resulting from the visit of the DAG starting in u. <ref type="figure">Figure 11</ref> gives an example of tree visits. Then, the ordered decomposition DAGs kernel uses tree visits to project sub-DAGs to a tree space and applies tree kernels on the visits</p><formula xml:id="formula_68">k DAG (D, D ) = v?V D v ?V D k tree root(T (v)), root(T (v ))<label>(49)</label></formula><p>where V D , V D are the set of vertices of D and D , respectivevly, and k tree is a kernel between ordered trees. The time complexity of the ordered decomposition DAGs kernel depends on the employed tree kernel k tree . For instance, using the subtree and subset tree kernel leads to a time complexity of O(n 3 log n) and O(n 4 ), respectively. To reduce the time complexity, the kernel employs a strategy that allows it to compute k tree once for each unique pair of subtrees appearing in different DAGs. Furthermore, in case the subtree kernel is employed, some other strategies can be applied to speed up the computation such as for instance limiting the depth of the visits during the generation of the multiset of DAGs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Assignment Kernels</head><p>The majority of kernels presented so far belong to the family of R-convolution kernels. Besides this family of kernels, another family that has received a lot of attention recently is that of assignment kernels. In general, these kernels compute a matching between substructures of one object and substructures of a second object such that the overall similarity of the two objects is maximized <ref type="bibr" target="#b51">(Fr?hlich et al., 2005;</ref><ref type="bibr" target="#b138">Schiavinato et al., 2015;</ref><ref type="bibr" target="#b12">Bai et al., 2015b</ref><ref type="bibr" target="#b11">Bai et al., , 2015a</ref><ref type="bibr" target="#b86">Kriege et al., 2016;</ref><ref type="bibr" target="#b160">Togninalli et al., 2019)</ref>. Such a matching can reveal structural correspondences between the two objects. However, defining valid graph kernels that follow this design paradigm is not trivial. For example, an optimal assignment kernel that was proposed in the early days of graph kernels to compute a correpondence between the atoms of molecules <ref type="bibr" target="#b51">(Fr?hlich et al., 2005)</ref> was later proven not to always be positive semidefinite <ref type="bibr" target="#b165">(Vert, 2008)</ref>. Despite these design difficulties, there is a handful of valid assignment graph kernels. For instance, there is a method that capitalizes on the well-known pyramid match kernel to match the node embeddings of graphs , while another approach uses multi-graph matching techniques to obtain valid assignment kernels <ref type="bibr" target="#b138">(Schiavinato et al., 2015)</ref>. More importantly, it was recently shown that there exists a class of base kernels used to compare substructures that guarantees positive semidefinite optimal assignment kernels . We next present some of the above instances of assignment kernels in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Pyramid Match Graph Kernel</head><p>The pyramid match kernel is a very popular algorithm in Computer Vision, and has proven useful for many applications including object recognition and image retrieval <ref type="bibr" target="#b63">(Grauman &amp; Darrell, 2007;</ref><ref type="bibr" target="#b90">Lazebnik et al., 2006)</ref>. The pyramid match graph kernel extends its applicability to graph-structured data . The kernel can handle unlabeled graphs as well as graphs that contain discrete node labels. The pyramid match graph kernel first embeds the vertices of each graph into a lowdimensional vector space using the eigenvectors of the d largest in magnitude eigenvalues of the graph's adjacency matrix. Since the signs of these eigenvectors are arbitrary, it replaces all their components by their absolute values. Each vertex is thus a point in the d-dimensional unit hypercube. To find an approximate correspondence between the sets of vertices of two graphs, the kernel maps these points to multi-resolution histograms, and compares the emerging histograms with a weighted histogram intersection function.</p><p>Initially, the kernel partitions the feature space into regions of increasingly larger size and takes a weighted sum of the matches that occur at each level. Two points match with each other if they fall into the same region. Matches made within larger regions are weighted less than those found in smaller regions. The kernel repeatedly fits a grid with cells of increasing size to the d-dimensional unit hypercube. Each cell is related only to a specific dimension and its size along that dimension is doubled at each iteration, while its size along the other dimensions stays constant and equal to 1. Given a sequence of levels from 0 to L, then at level l, the d-dimensional unit hypercube has 2 l cells along each dimension and D = 2 l d cells in total. Given a pair of graphs G, G , let H l G and H l G denote the histograms of G and G at level l, and H l G (i), H l G (i), the number of vertices of G, G that lie in the i-th cell. The number of points in two sets which match at level l is then computed using the histogram intersection function</p><formula xml:id="formula_69">I(H l G , H l G ) = D i=1 min H l G (i), H l G (i)<label>(50)</label></formula><p>The matches that occur at level l also occur at levels 0, . . . , l ? 1. The algorithm takes into account only the new matches found at each level which is given by</p><formula xml:id="formula_70">I(H l G 1 , H l G 2 ) ? I(H l+1 G 1 , H l+1 G 2 ) for l = 0, . . . , L ? 1.</formula><p>Furthermore, the number of new matches found at each level in the pyramid is weighted according to the size of that level's cells. Matches found within smaller cells are weighted more than those that occur in larger cells. Specifically, the weight for level l is set equal to 1 /2 L?l . Hence, the weights are inversely proportional to the length of the side of the cells that varies in size as the levels increase.</p><p>Definition 26 (Pyramid Match Graph Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The pyramid match kernel is defined as follows</p><formula xml:id="formula_71">k(G, G ) = I(H L G , H L G ) + L?1 l=0 1 2 L?l I(H l G , H l G ) ? I(H l+1 G , H l+1 G )<label>(51)</label></formula><p>where L is the number of different levels.</p><p>The complexity of the pyramid match kernel is O(dnL) where n is the number of vertices of the graphs under comparison.</p><p>In the case of labeled graphs, the kernel restricts matchings to occur only between vertices that share same labels. It represents each graph as a set of sets of vectors, and matches pairs of sets of two graphs corresponding to the same label using the pyramid match kernel. The emerging kernel for labeled graphs corresponds to the sum of the separate kernels</p><formula xml:id="formula_72">k(G, G ) = |?| i=1 k i (G, G )<label>(52)</label></formula><p>where |?| is the number of distinct labels and k i (G, G ) is the pyramid match kernel between the sets of vertices of the two graphs which are assigned the label i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Weisfeiler-Lehman Optimal Assignment Kernel</head><p>The Weisfeiler-Lehman optimal assignment kernel is currently a state-of-the-art approach for learning on graphs . The kernel capitalizes on the theory of valid assignment kernels to improve the performance of the Weisfeiler-Lehman subtree kernel. Before we delve into the details of the kernel, it is necessary to introduce the theory of valid optimal assignment kernels. Let X be a set, and [X ] n denote the set of all n-element subsets of X . Let also X, X ? [X ] n for n ? N, and B(X, X ) denote the set of all bijections between X and X . The optimal assignment kernel on [X ] n is defined as  where k is a kernel between the elements of X and X . <ref type="bibr" target="#b86">Kriege et al. (2016)</ref> showed that the above function K B (X , X ) is a valid kernel only if the base kernel k is strong.</p><formula xml:id="formula_73">K k B (X, X ) = max B?B(X,X ) (x,x )?B k(x, x )<label>(53</label></formula><formula xml:id="formula_74">1 1 2 ?(a) = ( ) ?(b) = ( ) ?(c) = ( ) ? 1, ? 2, 0, ? 1, 0 ? 1, ? 2, 0, ? 1, 0 ? 1, 0, 0, 0, ? 1 r v a b c<label>(</label></formula><formula xml:id="formula_75">Definition 27 (Strong Kernel). A function k : X ? X ? R ?0 is called strong kernel if k(x, y) ? min{k(x, z), k(z, y)} for all x, y, z ? X .</formula><p>Strong kernels are equivalent to kernels obtained from a hierarchy defined on set X . More specifically, let T be a rooted tree such that the leaves of T are the elements of X . Let V (T ) be the set of vertices of T . Each inner vertex v ? T corresponds to a subset of X comprising all leaves of the subtree rooted at v. Let w : V (T ) ? R ?0 be a weight function such that w(v) ? w(p(v)) for all v in T where p(v) is the parent of vertex v. Then, the tuple (T, w) defines a hierarchy. Let LCA(u, v) be the lowest common ancestor of vertices u and v, that is, the unique vertex with maximum depth that is an ancestor of both u and v.</p><p>Definition 28 (Hierarchy-induced Kernel). Let H = (T, w) be a hierarchy on X , then the function defined as k(x, y) = w(LCA(x, y)) for all x, y in X is the kernel on X induced by H.</p><p>Interestingly, strong kernels are equivalent to kernels obtained from a hierarchical partition of the domain of the kernel. Hence, by constructing a hierarchy on X , we can derive a strong kernel k and ensure that the emerging assignment function is a valid kernel.</p><p>Based on the property that every strong kernel is induced by a hierarchy, we can derive explicit feature maps for strong kernels. Let ? : V (T ) ? R ?0 be an additive weight function defined as ?(v) = w(v) ? w(p(v)) and ?(r) = w(r) for the root r. Note that the property of a hierarchy assures that the values of the ? function are nonnegative. For v ? V (T ), let P (v) ? V (T ) denote the vertices on the path from v to the root r. The strong kernel k induced by the hierarchy H can be defined using the mapping ? : X ? R n , where n = |V (T )| and the components indexed by v ? V (T ) are <ref type="figure">Figure 12</ref> shows an example of a strong kernel, an associated hierarchy and the derived feature vectors. The set X contains three distinct vertices labeled a and the set Y two distinct vertices labeled b and c. Taking the multiplicities into account the histograms are obtained from the hierarchy of the base kernel k depicted in <ref type="figure">Figure 12</ref>. The optimal assignment yields a value of K k</p><formula xml:id="formula_76">?(v) = ?(u) if u ? P (v), 0 otherwise<label>(54)</label></formula><formula xml:id="formula_77">B (X, Y ) = n i=1 min H X (i), H Y (i) = min{5, 5} + min{8, 6} + min{3, 1} + min{2, 4} + min{1, 2} = 15.</formula><p>Let H = (T, w) be a hierarchy on X . As mentioned above, the hierarchy H induces a strong kernel k. Since k is strong, the function K k B defined in Equation 53 is a valid kernel. The kernel K k B can be computed in linear time in the number of vertices n of the tree T using the histogram intersection kernel <ref type="bibr" target="#b157">(Swain &amp; Ballard, 1991)</ref> as follows</p><formula xml:id="formula_78">K k B (X, X ) = n i=1 min H X (i), H X (i)<label>(55)</label></formula><p>which is known to be a valid kernel on R n <ref type="bibr" target="#b13">(Barla et al., 2003)</ref>. Hence, the complexity of the proposed kernel depends on the size of the tree T . <ref type="figure" target="#fig_0">Figure 13</ref> illustrates the relation between the optimal assignment kernel employing a strong base kernel and the histogram intersection kernel. We next present the Weisfeiler-Lehman optimal assignment kernel.</p><p>Definition 29 (Weisfeiler-Lehman Optimal Assignment Kernel). Let G = (V, E) and G = (V , E ) be two graphs. The Weisfeiler-Lehman optimal assignment kernel is defined as</p><formula xml:id="formula_79">k(G, G ) = K k B (V, V )<label>(56)</label></formula><p>where k is the following base kernel</p><formula xml:id="formula_80">k(v, v ) = h i=0 ?(? i (v), ? i (v ))<label>(57)</label></formula><p>where ? i (v) is the label of node v at the end of the i-th iteration of the Weisfeiler-Lehman relabeling procedure.</p><p>The base kernel value reflects to what extent two vertices v and v have a similar neighborhood. It can be shown that the colour refinement process of the Weisfeiler-Lehman algorithm defines a hierarchy on the set of all vertices of the input graphs. Specifically, the sequence (? i ) 0?i?h gives rise to a family of nested subsets, which can naturally be represented by a hierarchy (T, w). When assuming ?(v) = 1 for all vertices v ? V (T ), the hierarchy induces the kernel defined above. Such a hierarchy for a graph on six vertices is illustrated in <ref type="figure" target="#fig_1">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Kernels for Graphs with Continuous Attributes</head><p>Most existing graph kernels are designed to operate on both unlabeled and node-labeled graphs. However, many real-world graphs contain continuous real-valued node attributes. One example comes from the field of cybersecurity where the function call graphs extracted from the source code of application programs typically contain multi-dimensional node labels. Such types of graphs do not appear only in cybersecurity, but also in computer vision <ref type="bibr" target="#b68">(Harchaoui &amp; Bach, 2007)</ref> or even in bioinformatics , where labels may represent RGB values of colors or physical properties of protein secondary structure elements, respectively. Research in graph kernels has achieved a remarkable progress in recent years. However, it has focused mainly on unlabeled graphs and graphs with discrete node labels. For such kind of graphs, there are several highly scalable graph kernels available which can handle graphs with thousands of vertices (e. g., the Weisfeiler-Lehman subtree kernel). However, the same does not happen in the case of datasets where node labels correspond to vectors. Some of the existing graph kernels for node-labeled graphs such as the shortest-path kernel can be extended to handle continuous labels. Unfortunately, by taking into account these labels, their computational complexity becomes prohibitive. Designing graph kernels for graphs with continuous node labels is a much less well studied problem which started to gain some attention recently <ref type="bibr" target="#b84">(Kriege &amp; Mutzel, 2012;</ref><ref type="bibr" target="#b47">Feragen et al., 2013;</ref><ref type="bibr" target="#b129">Orsini et al., 2015;</ref><ref type="bibr" target="#b120">Neumann et al., 2016;</ref><ref type="bibr" target="#b110">Morris et al., 2016;</ref><ref type="bibr" target="#b153">Su et al., 2016;</ref><ref type="bibr" target="#b82">Kondor &amp; Pan, 2016)</ref>. There are mainly two categories of approaches for graphs with continuous node labels: (1) those that directly handle continuous node labels, and (2) those that first discretize the node labels and then employ existing kernels that operate on graphs with discrete node labels. We will next present some kernels belonging to the first category. With regards to the second category, worthy of mention is the work of <ref type="bibr" target="#b110">Morris et al. (2016)</ref> that proposes the hash graph kernel framework which iteratively transforms continuous attributes into discrete labels using randomized hash functions, thus allowing kernels that support discrete node labels to handle node-attributed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.1">Subgraph Matching Kernel</head><p>The subgraph matching kernel counts the number of matchings between subgraphs of bounded size in two graphs <ref type="bibr" target="#b84">(Kriege &amp; Mutzel, 2012)</ref>. The kernel is very general since it can be applied to graphs that contain node labels, edge labels, node attributes or edge attributes. Let G be a set of graphs. We assume that the graphs that are contained in the set are labeled or attributed. Specifically, let be a labeling function that assigns either discrete labels or continuous attributes to vertices and edges. A graph isomorphism between two labeled/attributed graphs G = (V, E) and</p><formula xml:id="formula_81">G = (V , E ) is a bijection ? : V ? V that preserves adjacencies, that is ?v, u ? V : (v, u) ? E ? (?(v), ?(u)) ? E , and labels, that is if ? ? V ? V ? V ? V is the mapping of vertex pairs implicated by the bijection ? such that ?((v, u)) = (?(v), ?(u)), then, the conditions ?v ? V : (v) ? (?(v)) and ?e ? E :</formula><p>(e) ? (?(e)) must hold, where ? denotes that two labels are considered equivalent.</p><p>Definition 30 (Subgraph Matching Kernel). Given two graphs G = (V, E) and G = (V , E ), let B(G, G ) denote the set of all bijections between sets S ? V and S ? V , and let ? : B(G, G ) ? R + be a weight function. The subgraph matching kernel is defined as</p><formula xml:id="formula_82">k(G, G ) = ??B(G,G ) ?(?) v?S ? V (v, ?(v)) e?S?S ? E (e, ?(e))<label>(58)</label></formula><p>where S = dom(?) and ? V , ? E are kernel functions defined on vertices and edges, respectively.</p><p>The instance of the subgraph matching kernel that is obtained if we set the ? V , ? E functions as follows</p><formula xml:id="formula_83">? V (v, v ) = 1, if (v) ? (v ), 0, otherwise and ? E (e, e ) = 1, if e ? E ? e ? E ? (e) ? (e ) or e ? E ? e ? E , 0, otherwise.<label>(59)</label></formula><p>is known as the common subgraph isomorphism kernel. This kernel counts the number of isomorphic subgraphs contained in two graphs.</p><p>To count the number of isomorphisms between subgraphs, the kernel capitalizes on a classical result of <ref type="bibr" target="#b92">Levi (1973)</ref> which makes a connection between common subgraphs of two graphs and cliques in their product graph. More specifically, each maximum clique in the product graph is associated with a maximum common subgraph of the factor graphs. This allows someone to compute the common subgraph isomorphism kernel by enumerating the cliques of the product graph.</p><p>The general subgraph matching kernel extends the theory of Levi and builds a weighted product graph to allow a more flexible scoring of bijections. Given two graphs G = (V, E), G = (V , E ), and vertex and edge kernels ? V and ? E , the weighted product graph G P = (V P , E P ) of G and G is defined as</p><formula xml:id="formula_84">V P = {(v, v ) ? V ? V : ? V (v, v ) &gt; 0} E P = {{(v, v ), (u, u )} ? V P ? V P : v = u ? v = u ? ? E ((v, u), (v , u )) &gt; 0} c(u) = ? V (v, v ) ?u = (v, v ) ? V P c(e) = ? E ((v, u), (v , u )) ?e ? E P , where e = ((v, u), (v , u ))<label>(60)</label></formula><p>After creating the weighted product graph, the kernel enumerates its cliques. The kernel starts from an empty clique and extends it stepwise by all vertices preserving the clique property. Let w be the weight of a clique C. Whenever the clique C is extended by a new vertex v, the weight of the clique is updated as follows: first it is multiplied by the weight of the vertex w = w c(v), and then, it is multiplied by all the edges connecting v to a vertex in C, that is w = u?C w c <ref type="bibr">((v, u)</ref>). The algorithm effectively avoids duplicates by removing a vertex from the candidate set after all cliques containing it have been exhaustively explored.</p><p>The runtime of the subgraph matching kernel depends on the number of cliques in the product graph. The worst-case runtime complexity of the kernel when considering subgraphs of size up to k is O(kn k+1 ), where n = |V | + |V | is the sum of the number of vertices of the two graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.2">GraphHopper Kernel</head><p>The GraphHopper kernel is closely related to the shortest path kernel. In the case of graphs with discrete node labels, the kernels k v and k e of the shortest-path kernel which compare vertex labels and path lengths correspond typically to dirac kernels. Hence, nodes and shortest path lengths are considered similar if they are completely identical. That specific instance of the shortest path kernel allows the use of an explicit computation scheme which is very efficient, even for larger datasets. However, for attributed graphs, such an explicit mapping is no longer possible. This has a large impact on the runtime of the algorithm which is generally O(n 4 ), and makes the kernel unfeasible for many real-world applications. GraphHopper is a kernel which also compares shortest paths between node pairs from the two graphs, but with a different path kernel <ref type="bibr" target="#b47">(Feragen et al., 2013)</ref>. The kernel takes into account both path lengths and the vertices encountered while "hopping" along shortest paths. The kernel is equivalent to a weighted sum of node kernels. Moreover, it can handle both labeled and attributed graphs, and is much more efficient than the shortest-path kernel.</p><p>Let G = (V, E) be a graph. The graph contains discrete node labels, continuous node attributes or both. Let be a labeling function that assigns either discrete labels or continuous attributes to vertices. The kernel compares node labels/attributes using a kernel k n (e. g., delta kernel in the case of node labels, and linear or gaussian kernel in the case of node attributes). Given two vertices v, u ? V , a path ? from v to u in G is defined as a sequence of vertices</p><formula xml:id="formula_85">? = [v 1 , v 2 , v 3 , . . . , v l ]<label>(61)</label></formula><p>where v 1 = v, v l = u and (v i , v i+1 ) ? E for all i = 1, . . . , l ? 1. Let ?(i) = v i denote the i-th vertex encountered when "hopping" along the path. Denote by l(?) the weighted length of ? and by |?| its discrete length, defined as the number of vertices in ?. The shortest path ? ij from v i to v j is defined in terms of weighted length. The diameter ?(G) of G is the maximal number of nodes in a shortest path in G, with respect to the weighted path length.</p><p>Definition 31 (GraphHopper Kernel). The GraphHopper kernel is defined as a sum of path kernels k p over the families P, P of shortest paths in G, G</p><formula xml:id="formula_86">k(G, G ) = ??P ? ?P k p (?, ? )<label>(62)</label></formula><p>The path kernel k p (?, ? ) is a sum of node kernels k n on vertices simultaneously encountered while simultaneously hopping along paths ? and ? of equal discrete length, that is</p><formula xml:id="formula_87">k p (?, ? ) = |?| j=1 k n (?(j), ? (j)), if |?| = |? |, 0, otherwise.<label>(63)</label></formula><p>The k(G, G ) kernel can be decomposed into a weighted sum of node kernels</p><formula xml:id="formula_88">k(G, G ) = v?V v ?V w(v, v )k n (v, v )<label>(64)</label></formula><p>where w(v, v ) counts the number of times v and v appear at the same hop, or coordinate, i of shortest paths ?, ? of equal discrete length |?| = |? |. We can decompose the weight w(v, v ) as</p><formula xml:id="formula_89">w(v, v ) = ? j=1 ? i=1 |{(?, ? ) : ?(i) = v, ? (i) = v , |?| = |? | = j}| = ? j=1 ? i=1 M v ij M v ij<label>(65)</label></formula><p>where M v is a ? ? ? matrix whose entry M v ij counts how many times v appears at the i-th coordinate of a shortest path in G of discrete length j, and ? = max(?(G), ?(G )). The components of these matrices can be computed efficiently using recursive message-passing algorithms. The total complexity of computing the GraphHopper kernel is O(n 2 (m + log n + d + ? 2 )) where n is the number of vertices, m is the number of edges and d is the dimensionality of the node attributes (d = 1 in the case of discrete node labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.3">Graph Invariant Kernels</head><p>Kernels for attributed graphs have received increased attention recently, and research efforts have focused not only on new kernels, but also on frameworks for building kernels that can handle such continuous node attributes. Graph invariant kernels are instances of such a framework <ref type="bibr" target="#b129">(Orsini et al., 2015)</ref>. These kernels decompose graphs into sets of vertices, and compare them to each other using a kernel that measures their similarity both in terms of their attributes and in terms of their structural roles.</p><p>Let G be a graph. Let R be a decomposition relation that specifies a decomposition of G into its parts. Then, we denote by R ?1 (G) the multiset of all patterns in G. An example of such a decomposition relation is the one that generates neighborhood subgraphs. Graph invariant kernels compare vertices of graphs based on their attributes, but also based on their structural role in subgraphs obtained using a decomposition relation.</p><p>Definition 32 (Graph Invariant Kernel). Given two attributed graphs G = (V, E) and G = (V , E ), the graph invariant kernels compare the attributes of all pairs of vertices of the two graphs using a kernel</p><formula xml:id="formula_90">k(G, G ) = v?V v ?V w(v, v ) k attr (v, v )<label>(66)</label></formula><p>where k attr is a kernel between vertex attributes, and w(v, v ) is a weight function defined as follows</p><formula xml:id="formula_91">w(v, v ) = g?R ?1 (G) g ?R ?1 (G ) k inv (v, v ) ? m (g, g ) |V g ||V g | 1{v ? V g ? v ? V g }<label>(67)</label></formula><p>where ? m is a dirac function that determines whether two patterns match, V g , V g are the set of vertices of patterns g, g , and 1 is an indicator function.</p><p>If g, g are subgraphs of G, G , ? m can be a dirac function that compares the canonical representations of the subgraphs obtained by applying a labeling function which produces efficient string encodings of the subgraphs along with a hash function from strings to natural numbers. The indicator function 1{v ? V g ? v ? V g } from all the subgraphs extracted from the two graphs selects only those in which vertices v and v are involved into. The kernel function k inv is used to measure the similarity between the colors produced by a vertex invariant L and encodes the extent to which the vertices play the same structural role in the two subgraphs. By employing different graph invariants L, different instances of graph invariant kernels emerge. Some common graph invariants include the Weisfeiler-Lehman relabeling procedure and coloring methods that capitalize on diffusion updates. For kernels that decompose graphs into sets of subgraphs, their complexity is O n 2 (d attr +d inv n 2 |V g | 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.4">Propagation Kernel</head><p>The propagation kernel is another instance of the neighborhood aggregation framework, and in contrast to most other instances, it can handle continuous node attributes . The kernel leverages quantization in order to transform continuous node attributes to discrete labels. Similarly to the Weisfeiler-Lehman subtree kernel, the propagation kernel applies an iterative procedure which updates the node attributes, places the nodes into bins based on their attributes, and counts nodes that fall into the same bins in two graphs.</p><p>Let G = (V, E) be a node-attributed graph. Let also P 0 be a matrix whose i-th row contains the intial attribute of vertex v i ? V . The propagation kernel first uses a hash function that maps the node attributes to integer-valued bins, such that vertices with similar attributes end up in the same bin. Hence, this function maps each row of matrix P 0 to an integer. Then, the kernel employs a propagation scheme to update the attributes of the vertices. Different schemes can be employed. A common scheme updates node attributes as follows</p><formula xml:id="formula_92">P t+1 = D ?1 AP t<label>(68)</label></formula><p>where D is a diagonal matrix with D ii = j A ij , and D ?1 A corresponds to the transition matrix, that is the row-normalized adjacency matrix. The above two steps (hashing and update of node attributes) are performed for T iterations.</p><p>Definition 33 (Propagation Kernel). Let G, G be two node-attributed graphs. Define n i as the number of integer bins occupied by nodes of G and G after applying the hashing function to the node attributes at the i-th iteration of the algorithm. Let also c t (G, i) be the number of nodes of G placed into bin i at the t-th iteration of the algorithm. Then, the propagation kernel on two graphs G and G with T iterations is defined as</p><formula xml:id="formula_93">k(G, G ) = ?(G), ?(G ) (69) where ?(G) = (c 0 (G, 1), . . . , c 0 (G, n 0 ), . . . , c T (G, 1), . . . , c T (G, n T ))<label>(70)</label></formula><p>and ?(G ) = (c 0 <ref type="figure">(G , 1)</ref>, . . . , c 0 (G , n 0 ), . . . , c T <ref type="figure">(G , 1)</ref>, . . . , c T (G , n T ))</p><p>An illustration of the propagation kernel is given in <ref type="figure">Figure 15</ref>. The total runtime complexity of the kernel is O (T ? 1)m + T n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.5">Multiscale Laplacian Graph Kernel</head><p>The multiscale Laplacian graph kernel can handle unlabeled graphs, graphs with discrete node labels, but also graphs with continuous node attributes <ref type="bibr" target="#b82">(Kondor &amp; Pan, 2016)</ref>. It takes into account structure in graphs at a range of different scales by building a hierarchy of nested subgraphs. These subgraphs are compared to each other using another graph kernel, called the feature space Laplacian graph kernel. This kernel is capable of lifting a base kernel defined on the vertices of two graphs to a kernel between the graphs themselves. Since exact computation of the multiscale Laplacian graph kernel is a very expensive operation, the kernel uses a randomized projection procedure similar to the popular Nystr?m approximation for kernel matrices <ref type="bibr" target="#b172">(Williams &amp; Seeger, 2001)</ref>.</p><p>Let G = (V, E) be an undirected graph such that n = |V | and let L be the Laplacian of G. Given two graphs G 1 and G 2 of n vertices, we can define the kernel between them to be a kernel between the corresponding normal distributions p 1 = N (0, L ?1 1 ) and p 2 = N (0, L ?1 2 ) where 0 is the n-dimensional all-zeros vector. Note that the Laplacian matrices of the two graphs have a zero eigenvalue eigenvector. Hence, in order to be able to invert them, the algorithm adds a small constant "regularizer" ?I to them. In the following, we denote the regularized Laplacians of G 1 and G 2 by L 1 and L 2 , respectively. More specifically, given two graphs G 1 and G 2 of n vertices with regularized Laplacians L 1 and L 2 respectively, the ) <ref type="figure">Figure 15</ref>: Propagation kernel computation. Distributions, bins, count features, and kernel contributions for two graphs G and G with binary node labels and one iteration of label propagation. Node-label distributions are decoded by color.</p><formula xml:id="formula_95">G G ?(G) = ( ?(G ) (a) Input</formula><p>Laplacian graph kernel with parameter ? between the two graphs is</p><formula xml:id="formula_96">k LG (G 1 , G 2 ) = |( 1 2 S ?1 1 + 1 2 S ?1 2 ) ?1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(72)</label></formula><p>where S 1 = L ?1 1 + ?I, S 2 = L ?1 2 + ?I and I is the n ? n identity matrix. The Laplacian graph kernel captures similarity between the overall shapes of the two graphs. However, it assumes that both graphs have the same size, and it is not invariant to permutations of the vertices.</p><p>To achieve permutation invariance, the multiscale Laplacian graph kernel represents each vertex as a d-dimensional vector whose components correspond to local and permutation invariant vertex features. Such features may include for instance the degree of the vertex or the number of triangles in which it participates. Then, it performs a linear transformation and represents each graph as a distribution of the considered features instead of a distribution of its vertices. Let U 1 , U 2 ? R d?n be the feature mapping matrices of the two graphs, that is the matrices whose columns contain the vector representations of the vertices of the two graphs. Then, the feature space Laplacian graph kernel is defined as</p><formula xml:id="formula_97">k F LG (G 1 , G 2 ) = |( 1 2 S ?1 1 + 1 2 S ?1 2 ) ?1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(73)</label></formula><p>where S 1 = U 1 L ?1 1 U 1 + ?I, S 2 = U 2 L ?1 2 U 2 + ?I and I is the d ? d identity matrix. Since the vertex features are local and invariant to vertex reordering, the feature space Laplacian graph kernel is permutation invariant. Furthermore, since the distributions now live in the space of features rather than the space of vertices, the feature space Laplacian graph kernel can be applied to graphs of different sizes.</p><p>Let ?(v) be the representation of vertex v constructed from local vertex features as described above. The base kernel ? between two vertices v 1 and v 2 corresponds to the dot product of their feature vectors</p><formula xml:id="formula_98">?(v 1 , v 2 ) = ?(v 1 ) ?(v 2 )<label>(74)</label></formula><p>Let G 1 and G 2 be two graphs with vertex sets V 1 = {v 1 , . . . , v n 1 } and V 2 = {u 1 , . . . , u n 2 } respectively, and letV = {v 1 , . . . ,v n 1 +n 2 } be the union of the two vertex sets. Let also K ? R (n 1 +n 2 )?(n 1 +n 2 ) be the kernel matrix defined as</p><formula xml:id="formula_99">K ij = ?(v i ,v j ) = ?(v i ) ?(v j )<label>(75)</label></formula><p>Let u 1 , . . . , u p be a maximal orthonormal set of the non-zero eigenvalue eigenvectors of K with corresponding eigenvalues ? 1 , . . . , ? p . Then the vectors</p><formula xml:id="formula_100">? i = 1 ? ? i n 1 +n 2 l=1 [u i ] l ?(v l )<label>(76)</label></formula><p>where [u i ] l is the l-th component of vector u i form an orthonormal basis for the subspace {?(v 1 ), . . . , ?(v n 1 +n 2 )}. Moreover, let Q = [? 1/2 1 u 1 , . . . , ? 1/2 p u p ] ? R p?p and Q 1 , Q 2 denote the first n 1 and last n 2 rows of matrix Q respectively. Then, the generalized feature space Laplacian graph kernel induced from the base kernel ? is defined as</p><formula xml:id="formula_101">k ? F LG (G 1 , G 2 ) = |( 1 2 S ?1 1 + 1 2 S ?1 2 ) ?1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(77)</label></formula><p>where S 1 = Q 1 L ?1 1 Q 1 + ?I and S 2 = Q 2 L ?1 2 Q 2 + ?I where I is the p ? p identity matrix. The multiscale Laplacian graph kernel builds a hierarchy of nested subgraphs, where each subgraph is centered around a vertex and computes the generalized feature space Laplacian graph kernel between every pair of these subgraphs. Let G be a graph with vertex set V , and ? a positive semi-definite kernel on V . Assume that for each v ? V , we have a nested sequence of L neighborhoods</p><formula xml:id="formula_102">v ? N 1 (v) ? N 2 (v) ? . . . ? N lmax (v)<label>(78)</label></formula><p>and for each N l (v), let G l (v) be the corresponding induced subgraph of G. The multiscale Laplacian subgraph kernels are defined as K 1 , . . . , K lmax : V ? V ? R as follows 1. K 1 is just the generalized feature space Laplacian graph kernel k ? F LG induced from the base kernel ? between the lowest level subgraphs (i. e., the vertices)</p><formula xml:id="formula_103">K 1 (v, u) = k ? F LG (v, u)<label>(79)</label></formula><p>2. For l = 2, 3, . . . , l max , K l is the the generalized feature space Laplacian graph kernel induced from K l?1 between G l (v) and G l (u)</p><formula xml:id="formula_104">K l (v, u) = k K l?1 F LG G l (v), G l (u)<label>(80)</label></formula><p>Definition 34 (Multiscale Laplacian Graph Kernel). Let G 1 , G 2 be two graphs. The multiscale Laplacian graph kernel between the two graphs is defined as follows</p><formula xml:id="formula_105">k(G 1 , G 2 ) = k K lmax F LG (G 1 , G 2 )<label>(81)</label></formula><p>The multiscale Laplacian graph kernel computes K 1 for all pairs of vertices, then computes K 2 for all pairs of vertices, and so on. Hence, it requires O(n 2 l max ) kernel evaluations. At the top levels of the hierarchy each subgraph centered around a vertex G l (v) may have as many as n vertices. Therefore, the cost of a single evaluation of the generalized feature space Laplacian graph kernel may take O(n 3 ) time. This means that in the worst case, the overall cost of computing k is O(n 5 l max ). Given a dataset of N graphs, computing the kernel matrix requires repeating this for all pairs of graphs, which takes O(N 2 n 5 l max ) time and is clearly problematic for real-world settings.</p><p>The solution to this issue is to compute for each level l = 1, 2, . . . , l max + 1 a single joint basis for all subgraphs at the given level across all graphs. Let G 1 , G 2 , . . . , G N be a collection of graphs, V 1 , V 2 , . . . , V N their vertex sets, and assume that V 1 , V 2 , . . . , V N ? V for some general vertex space V. The joint vertex feature space of the whole graph collection</p><formula xml:id="formula_106">is W = span N i=1 v?V i {?(v)} . Let c = N i=1</formula><p>|V i | be the total number of vertices and V = (v 1 , . . . ,v c ) be the concatenation of the vertex sets of all graphs. Let K be the corresponding joint kernel matrix and u 1 , . . . , u p be a maximal orthonormal set of non-zero eigenvalue eigenvectors of K with corresponding eigenvalues ? 1 , . . . , ? p and p = dim(W ). Then the vectors</p><formula xml:id="formula_107">? i = 1 ? ? i c l=1 [u i ] l ?(v l ) i = 1, . . . , p<label>(82)</label></formula><p>form an orthonormal basis for W . Moreover, let Q = [? 1/2 1 u 1 , . . . , ? 1/2 p u p ] ? R p?p and Q 1 denote the first n 1 rows of matrix Q, Q 2 denote the next n 2 rows of matrix Q and so on. For any pair of graphs G i , G j of the collection, the generalized feature space Laplacian graph kernel induced from ? can be expressed as</p><formula xml:id="formula_108">k ? F LG (G i , G j ) = |( 1 2S ?1 i + 1 2S ?1 j ) ?1 | 1/2 |S i | 1/4 |S j | 1/4<label>(83)</label></formula><p>whereS i = Q i L ?1 i Q i + ?I,S j = Q j L ?1 j Q j + ?I and I is the p ? p identity matrix. Computing the kernel matrix between all vertices of all graphs (c vertices in total) and storing it is a very costly procedure. Computing its eigendecomposition is even worse in terms of the required runtime. Morever, p is also very large. Hence, managing th? S 1 , . . . ,S N matrices (each of which is of size p ? p) becomes infeasible. Hence, the multiscale Laplacian graph kernel replaces W with a smaller, approximate joint features space. Let? = (? 1 , . . . ,?c) bec c vertices sampled from the joint vertex set. Then, the corresponding subsampled vertex feature space isW = span{?(v) : v ?? }. Letp = dim(W ). Similarly to before, the kernel constructs an orthonormal basis {? 1 , . . . , ?p} forW by forming the (now much smaller) kernel matrix K ij = ?(? i ,? j ), computing its eigenvalues and eigenvectors, and setting ?</p><formula xml:id="formula_109">i = 1 ? ? i c l=1 [u i ] l ?(? l ).</formula><p>The resulting approximate generalized feature space Laplacian graph kernel is</p><formula xml:id="formula_110">k ? F LG (G 1 , G 2 ) = |( 1 2S ?1 1 + 1 2S ?1 2 ) ?1 | 1/2 |S 1 | 1/4 |S 2 | 1/4<label>(84)</label></formula><p>whereS 1 =Q 1 L ?1 1Q 1 +?I,S 2 =Q 2 L ?1 2Q 2 +?I are the projections ofS 1 andS 2 toW and I is thep?p identity matrix. Finally, the kernel introduces a further layer of approximation by restrictingW to be the space spanned by the firstp &lt;p basis vectors (ordered by descending eigenvalue), effectively doing kernel PCA on {?(?)}? ?? . The combination of these two factors makes computing the entire stack of kernels feasible, reducing the complexity of computing the kernel matrix for a dataset of N graphs to O(Nc 2p3 l max + Nc 3 l max + N 2p3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">Frameworks</head><p>Besides designing kernels, research on graph kernels has also focused on frameworks and approaches that can be applied to existing graph kernels and increase their performance. The most popular of all frameworks is perhaps the Weisfeiler-Lehman framework which has been already presented <ref type="bibr" target="#b144">(Shervashidze et al., 2011)</ref>. Interestingly, any kernel that can handle discrete node labels can be plugged into that framework. Recently, two other frameworks were presented for deriving variants of popular R-convolution graph kernels <ref type="bibr" target="#b177">(Yanardag &amp; Vishwanathan, 2015b</ref>. Inspired by recent advances in NLP, these frameworks offer a way to take into account similarity between substructures. In addition, a method that combines several kernels using the multiple kernel learning framework was also recently proposed <ref type="bibr" target="#b0">(Aiolli et al., 2015)</ref>. Another recently proposed framework generates a hierarchy of subgraphs and compares the corresponding according to the hierarchy subgraphs using graph kernels . Moreover, a recent approach employs graph kernels and performs a series of successive embeddings in order to derive more expressive kernels . Some of these frameworks are described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.1">Frameworks Dealing with Diagonal Dominance</head><p>We next present two frameworks that are inspired by recent advances in natural language processing, namely the deep graph kernels framework <ref type="bibr" target="#b177">(Yanardag &amp; Vishwanathan, 2015b)</ref> and the structural smoothing framework <ref type="bibr" target="#b176">(Yanardag &amp; Vishwanathan, 2015a)</ref>. These two frameworks were developed to address the problem of diagonal dominance which is inherent to R-convolution kernels. The feature space of these kernels is usually large (i. e., grows exponentially) and we encounter the sparsity problem: only a few substructures will be common across graphs, and therefore each graph is similar to itself, but not to any other graph in the dataset. However, the substructures used to define a graph kernel are often related to each other, but commonly-used R-convolution kernels respect only exact matchings. For example, when the features correspond to large graphlets (e. g., k ? 5), two graphs may be composed of many similar graphlets, but not any identical. As a consequence, the kernel value between the two graphs (i. e., inner product of their feature representations) will be equal to 0 even though the two graphs are similar to each other.</p><p>Ideally, we would like the kernels to output large values for pairs of graphs that belong to the class, and lower values for pairs of graphs that belong to different classes. To deal with the aforementioned problem, the deep graph kernels framework computes the kernel between two graphs G and G as follows</p><formula xml:id="formula_111">k(G, G ) = ?(G) M ?(G )<label>(85)</label></formula><p>where M represents a positive semidefinite matrix that encodes the relationship between substructures and ?(G), ?(G ) are the representations of graphs G, G according to a graph kernel which contains counts of atomic substructures. Therefore, one can design an M matrix that respects the similarity of the substructure space. Clearly, the deep graph kernels framework can be applied only to graph kernels whose feature maps ? can be computed explicitly.</p><p>Matrix M can be generated by manually defining functions to compare substructures or alternatively, it can be learned using techniques inspired from the field of natural language processing. When substructures exhibit a clear mathematical relationship, one can define a function to measure the similarities between them (e. g., edit distance in the case of graphlets). However, the above approach requires manually designing the similarity functions. Furthermore, in many cases, it becomes prohibitively expensive to compare all pairs of substructures. On the other hand, learning the latent representations of substructures is more efficient and does not involve any manual intervention. Matrix M can then be computed based on the learned representations. To learn a latent representation for each substructure, the framework utilizes recent approaches for generating word embeddings such as the continuous bag-of-words (CBOW) and Skip-gram models <ref type="bibr" target="#b107">(Mikolov et al., 2013)</ref>. These models generate semantic representations from word co-occurrence statistics derived from large text corpora. However, unlike words in a traditional text corpora, substructures of graphs do not have a linear co-occurrence relationship. Hence, these co-occurrence relationships need to be manually defined. <ref type="bibr" target="#b177">Yanardag and Vishwanathan (2015b)</ref> proposed a methodology on how to generate corpora where co-occurrence relationship is meaningful on three popular kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel.</p><p>The structural smoothing framework is inspired by recent smoothing techniques in natural language processing. Similar to the deep graph kernels framework, this framework can also only be applied to graph kernels whose feature maps ? can be computed explicitly. The framework takes structural similarity into account by constructing a directed acyclic graph (DAG) that encodes the relationships between lower and higher order substructures. Each vertex of the DAG corresponds to a substructure (and also to a feature in the explicit graph representation). For each substructure s of size k, the framework determines all possible substructures of size k ? 1 into which s can be reduced. These substructures are the parents of s, and a weighted directed edge is drawn from each parent to its children vertices. Since all descendants of a given substructure at depth k ? 1 are at depth k, the emerging graph is indeed a DAG. <ref type="bibr" target="#b177">Yanardag and Vishwanathan (2015b)</ref> present how such a DAG can be constructed for three popular graph kernels, namely the Weisfeiler-Lehman subtree kernel, the graphlet kernel, and the shortest path kernel. Given the DAG, the structural smoothing for a substructure s at level k is defined as</p><formula xml:id="formula_112">P k SS (s) = max(c s ? d, 0) m + dm d m p?Ps P k?1 SS (p) w ps c?Cp w pc<label>(86)</label></formula><p>where c i denotes the number of times substructure i appears in the graph, m = i c i denotes the total number of substructures present in the graph, d &gt; 0 is a discount factor, m d = |{i : c i &gt; d}| is the number of substructures whose counts are larger than d, w ij denotes the weight of the edge connecting vertex i to vertex j, P s denotes the parents of vertex s, and C p the children of vertex p. The above equation subtracts a fixed discount factor d from every substructure that appears in the graph, and accumulates it to a total mass of dm d . Each substructure s receives some portion of this accumulated probability mass from its parents. The proportion of the mass that a parent p at level k ? 1 transmits to a given child a depends on the weight w ps between the parent and the child, and the probability mass P k?1 SS (p) that is assigned to the parent. It is thus clear that, even if a graph does not contain a substructure s (i. e., c s = 0), its value in the feature vector may become greater than 0 (i. e., P SS (s) &gt; 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.2">Core Framework</head><p>The core framework is another tool for improving the performance of graph kernels . This framework is not restricted to graph kernels, but can be applied to any graph comparison algorithm. It capitalizes on the k-core decomposition which is capable of uncovering topological and hierarchical properties of graphs. Specifically, the k-core decomposition is a powerful tool for network analysis and it is commonly used as a measure of importance and well connectedness of vertices in a broad spectrum of applications. The notion of k-core was first introduced by Seidman to study the cohesion of social networks <ref type="bibr" target="#b141">(Seidman, 1983)</ref>. In recent years, the k-core decomposition has been established as a standard tool in many application domains such as in network visualization <ref type="bibr" target="#b5">(Alvarez-Hamelin et al., 2006)</ref>. Core Decomposition. Let G = (V, E) be an undirected and unweighted graph. Given a subset of vertices S ? V , let E(S) be the set of edges that have both end-points in S. Then, G = (S, E(S)) is the subgraph induced by S. We use G ? G to denote that G is a subgraph of G. Let G be a graph and G a subgraph of G induced by a set of vertices S. Then, G is defined to be a k-core of G, denoted by C k , if it is a maximal subgraph of G in which all vertices have degree at least k. Hence, if G is a k-core of G, then ?v ? S, deg G (v) ? k. Each k-core is a unique subgraph of G, and it is not necessarily connected. The core number c(v) of a vertex v is equal to the highest-order core that v belongs to. In other words, v has core number c(v) = k, if it belongs to the k-core but not to the (k + 1)core. The degeneracy ? * (G) of a graph G is defined as the maximum k for which graph G contains a non-empty k-core subgraph, ? * (G) = max v?V c(v). Furthermore, assuming that C = {C 0 , C 1 , . . . , C ? * (G) } is the set of all k-cores, then C forms a nested chain</p><formula xml:id="formula_113">C ? * (G) ? . . . ? C 1 ? C 0 = G<label>(87)</label></formula><p>Therefore, the k-core decomposition is a very useful tool for discovering the hierarchical structure of graphs. The k-core decomposition of a graph can be computed in O(n + m) time <ref type="bibr" target="#b104">(Matula &amp; Beck, 1983;</ref><ref type="bibr" target="#b14">Batagelj &amp; Zaver?nik, 2011)</ref>. The underlying idea is that we can obtain the i-core of a graph if we recursively remove all vertices with degree less than i and their incident edges from the graph until no other vertex can be removed.</p><p>Core Kernels. The k-core decomposition builds a hierarchy of nested subgraphs, each having stronger connectedness properties compared to the previous ones. The core framework measures the similarity between the corresponding according to the hierarchy subgraphs and aggregates the results. Let G = (V, E) and G = (V , E ) be two graphs. Let also k be any kernel for graphs. Then, the core variant of the base kernel k is defined as</p><formula xml:id="formula_114">k c (G, G ) = k(C 0 , C 0 ) + k(C 1 , C 1 ) + . . . + k(C ? * min , C ? * min )<label>(88)</label></formula><p>where ? * min is the minimum of the degeneracies of the two graphs, and C 0 , C 1 , . . . , C ? * min and C 0 , C 1 , . . . , C ? * min are the 0-core, 1-core,. . ., ? * min -core subgraphs of G and G , respectively. By decomposing graphs into subgraphs of increasing importance, the algorithm is capable of more accurately capturing their underlying structure.</p><p>The computational complexity of the core framework depends on the complexity of the base kernel and the degeneracy of the graphs under comparison. Given a pair of graphs G, G and an algorithm A for comparing the two graphs, let O A be the time complexity of algorithm A. Let also ? * min = min ? * (G), ? * (G ) be the minimum of the degeneracies of the two graphs. Then, the complexity of computing the core variant of algorithm A is</p><formula xml:id="formula_115">O c = ? * min O A .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">Tree Kernels</head><p>Before delving into the connection between graph neural networks and graph kernels, it is important to stress that graph kernels are also very related to tree kernels which have been extensively studied mainly in the field of natural language processing <ref type="bibr" target="#b31">(Collins &amp; Duffy, 2001)</ref>, but also in other fields <ref type="bibr" target="#b164">(Vert, 2002)</ref>. In fact, tree kernels were introduced prior to graph kernels. A tree is an undirected graph in which any two vertices are connected by exactly one path, and thus a tree is a special case of a graph. Therefore, tree kernels can be thought of as instances of graph kernels specifically designed for trees. Note that any graph kernel can be applied to trees. However, the opposite does not hold. Tree kernels cannot be directly applied to general graphs. In should be mentioned that certain graph kernels such as the subtree kernel build on ideas from tree kernels. As already mentioned, tree kernels have found applications mainly in the field of natural language processing. Examples of applications include semantic role labeling <ref type="bibr" target="#b113">(Moschitti, 2004</ref><ref type="bibr" target="#b114">(Moschitti, , 2006a</ref><ref type="bibr" target="#b116">Moschitti et al., 2008;</ref><ref type="bibr" target="#b34">Croce et al., 2011)</ref>, relation extraction <ref type="bibr" target="#b179">(Zelenko et al., 2003;</ref><ref type="bibr" target="#b35">Culotta &amp; Sorensen, 2004;</ref><ref type="bibr" target="#b27">Bunescu &amp; Mooney, 2005)</ref>, syntactic parsing re-ranking <ref type="bibr" target="#b31">(Collins &amp; Duffy, 2001)</ref> and question classification <ref type="bibr" target="#b114">(Moschitti, 2006a;</ref><ref type="bibr" target="#b34">Croce et al., 2011)</ref>. In those tasks, an approach that has proven to be effective is to use a set of manually designed features that can capture the syntactic and semantic information encoded into the input data. However, this set of meaningful features is usually determined by some domain expert, while the whole process is in most cases very expensive and time-consuming. Instead of computing such handcrafted features, previous studies have capitalized on structured representations of text (e. g., dependency parse trees) that might take into account syntactic and semantic aspects of the input data, and have introduced kernels that operate on these representations (e. g., tree kernels). Therefore, tree kernels are very useful since they eliminate the need for the design of new features in the context of several natural language tasks. Most tree kernels represent trees in terms of their substructures. And then, they compute the number of common substructures between two trees. The most common substructures are the subtrees, the subset trees, and the partial trees which give rise to the subtree kernel <ref type="bibr" target="#b149">(Smola &amp; Vishwanathan, 2003)</ref>, the subset tree kernel <ref type="bibr" target="#b31">(Collins &amp; Duffy, 2001)</ref>, and the partial tree kernel <ref type="bibr" target="#b114">(Moschitti, 2006a)</ref>, respectively. A subtree is defined as a subgraph of the tree rooted at any non-leaf vertex along with all its descendants. A subset tree is a more flexible structure since its leaves may correspond to non-leaf vertices of the input tree. Subset trees satisfy the constraint that grammatical rules cannot be broken. On the other hand, partial trees relax the above constraint and can be generated by the application of partial production rules of the grammar.</p><p>It is interesting to mention that graph kernels and tree kernels suffer from common limitations. For instance, they both fix a set of features in advance, and they thus decouple data representation from learning. Furthermore, there is no justification on why certain tree kernels perform better than others in a given task <ref type="bibr" target="#b114">(Moschitti, 2006a</ref><ref type="bibr" target="#b115">(Moschitti, , 2006b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Link to Graph Neural Networks</head><p>In the past years, graph kernels have been largely overshadowed by a family of neural network architectures which operate on graphs, known as graph neural networks (GNNs). The field of graph neural networks has seen an explosion of interest in recent years, with dozens of models developed which have been applied to various tasks such as to drug design <ref type="bibr" target="#b80">(Kearnes et al., 2016)</ref> and to modeling physical systems <ref type="bibr" target="#b15">(Battaglia et al., 2016)</ref> The first instances of GNNs were proposed several years ago <ref type="bibr" target="#b150">(Sperduti &amp; Starita, 1997;</ref><ref type="bibr" target="#b105">Micheli, 2009;</ref><ref type="bibr" target="#b137">Scarselli et al., 2009)</ref>, however, these models have only recently received a great deal of attention, following the advent of deep learning. More specifically, GNNs were initially categorized into spectral and spatial approaches <ref type="bibr" target="#b26">(Bruna et al., 2014)</ref>. The first family of models operates on the spectral domain and draws on the properties of convolutions in the Fourier domain, while the second family of models operates on the spatial domain where the weights of the edges determine locality. Later, it became clear that all these models are special cases of a simple message passing framework (MPNNs) <ref type="bibr" target="#b58">(Gilmer et al., 2017)</ref>. Most of the recently proposed GNNs fit into this framework <ref type="bibr" target="#b26">(Bruna et al., 2014;</ref><ref type="bibr" target="#b45">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b96">Li et al., 2016c;</ref><ref type="bibr" target="#b39">Defferrard et al., 2016;</ref><ref type="bibr" target="#b181">Zhang et al., 2018a;</ref><ref type="bibr" target="#b175">Xu et al., 2019;</ref><ref type="bibr" target="#b117">Murphy et al., 2019)</ref>. Specifically, MPNNs employ a message passing procedure, where each vertex updates its feature vector by aggregating the feature vectors of its neighbors. After k iterations of the message passing procedure, each vertex obtains a feature vector which captures the structural information within its k-hop neighborhood. MPNNs then compute a feature vector for the entire graph using some permutation invariant readout function such as summing the feature vectors of all the vertices of the graph. In fact, the family of MPNNs is closely related to the Weisfeiler-Lehman test of isomorphism, and thus also to the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b144">(Shervashidze et al., 2011)</ref>. Specifically, these models generalize the relabeling procedure of the Weisfeiler-Lehman subtree kernel to the case where vertices are associated with continuous feature vectors. Standard MPNNs have been shown to be at most as powerful as the Weisfeiler-Lehman subtree kernel in distinguishing non-isomorphic graphs <ref type="bibr" target="#b112">Morris et al., 2019)</ref>.</p><p>It is interesting to mention that GNNs address some of the major limitations of graph kernels. More specifically, as already discussed, graph kernels typically fix a set of features in advance. This is one of the main limitations of graph kernels since data representation and learning are independent from each other. The input samples are first implicitly or explicitly transformed into feature vector representations using a user-defined kernel. Then, learning is perfomed based on these representations regardless of their quality. Thus, the feature generation scheme is fixed and it does not adapt to the given data distribution. Another limitation of graph kernels is that they cannot efficiently handle graphs whose vertices are annotated with continuous multi-dimensional attributes. Indeed, while for unlabeled and node-labeled graphs, there are now available very efficient kernels which can handle graphs containing up to thousands of nodes, unfortunately, the same does not hold for graphs with continuous node attributes. Such attributes play an important role in different fields such as in bioinformatics and chemoinformatics, and this limitation renders kernels infeasible for application to these domains. GNNs, on the other hand, have emerged as a machine learning framework addressing the above two challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Message Passing Models and the Weisfeiler-Lehman Test of Isomorphism</head><p>As mentioned above, the majority of existing GNNs belongs to the family of MPNNs. Suppose we have a MPNN model that contains T neighborhood aggregation layers. In the t-th neighborhood aggregation layer (t &gt; 0), the hidden state h</p><formula xml:id="formula_116">(t) v of a vertex v is updated as follows m (t) v = AGGREGATE (t) h (t?1) u : u ? N (v) h (t) v = COMBINE (t) h (t?1) v , m (t) v<label>(89)</label></formula><p>By defining different AGGREGATE (t) and COMBINE (t) functions, we obtain a different GNN variant. For the GNN to be end-to-end trainable, both functions need to be differentiable. Furthermore, since there is no natural ordering of the neighbors of a vertex, the AGGREGATE (t) function must be permutation invariant. Note that the neighborhood aggregation procedure is closely related to the Weisfeiler-Lehman test of isomorphism and the Weisfeiler-Lehman framework. More specifically, the number of neighborhood aggregation layers is analogous to the number of iterations of the Weisfeiler-Lehman framework. Furthermore, in the case of the Weisfeiler-Lehman framework, the employed AGGREGATE function computes the sorted set of labels of vertex v's neighbors, while the COMBINE function adds the label of the vertex v itself as the first element of the above set.</p><p>To compute a representation for the entire graph, GNNs apply a READOUT function to vertex representations generated by the final neighborhood aggregation layer to obtain a vector representation over the whole graph</p><formula xml:id="formula_117">h G = READOUT h (T ) v : v ? V<label>(90)</label></formula><p>The READOUT function needs also to be differentiable and permutation invariant. Common readout functions include the sum, mean and max aggregators. These aggregators are different than the one employed by the Weisfeiler-Lehman subtree kernel which produces the histogram of the labels encountered during the different iterations of the algorithm. Note that most standard GNNs are less powerful than the Weisfeiler-Lehman test of isomorphism in terms of distinguishing non-isomorphic graphs. In fact, it has been shown that if the AGGREGATE (t) , COMBINE (t) and READOUT functions are injective, then the emerging GNN model maps two graphs that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings . To achieve greater expressive power, some models have capitalized on high-order variants of the Weisfeiler-Lehman test of isomorphism <ref type="bibr" target="#b112">(Morris et al., 2019</ref>.</p><p>We next provide more details about four models which we employ in our experimental evaluation, namely Deep Graph Convolutional Neural Network (DGCNN) <ref type="bibr" target="#b181">(Zhang et al., 2018a)</ref>, GraphSAGE <ref type="bibr" target="#b67">(Hamilton et al., 2017)</ref>, Differentiable Graph Pooling (DiffPool) <ref type="bibr" target="#b178">(Ying et al., 2018)</ref>, and Graph Isomorphism Network (GIN) . Note that for clarity of presentation, in what follows, we omit biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Deep Graph Convolutional Neural Network</head><p>This model integrates the AGGREGATE (t) and COMBINE (t) functions into a single function as follows</p><formula xml:id="formula_118">h (t+1) v = f ? ? u?N (v)?{v} h (t) u 1 + deg(v) W (t) ? ?<label>(91)</label></formula><p>where f is a nonlinear activation function. Thus, the model aggregates vertex information in local neighborhoods to extract local substructure information. After T iterations, the model concatenates the outputs h (t) v , for t = 1, . . . , T horizontally to form a concatenated output</p><formula xml:id="formula_119">h v = h (1) v , h (2) v , . . . , h (T ) v<label>(92)</label></formula><p>To generate a representation for the entire graph, the model uses a SortPooling layer which imposes an order on the vertices of the graph. More specifically, vertices are sorted in a descending order based on the last component of their representations (i. e., h v for vertex v), while vertices that have the same value in the last component are compared based on the second to last component, and so on. Furthermore, to allow the model to handle graphs with different numbers of vertices, this layer unifies the sizes of the outputs for different graphs by truncating/extending the output tensor in the first dimension from n to k. Output is then passed on to a traditional convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">GraphSAGE</head><p>The GraphSAGE model can deal with very large graphs since it does not take into account all neighbors of a vertex, but uniformly samples a fixed-size set of neighbors. Let N k (v) be a uniformly drawn subset (of size k) from the set N (v) of a vertex v. The neighborhood aggregation scheme of GraphSAGE is defined as follows</p><formula xml:id="formula_120">m (t) v = AGGREGATE (t) h (t) u u ? N k (v) h (t+1) v = ? W (t) h (t) v , m (t) v h (t+1) v = h (t+1) v h (t+1) v 2<label>(93)</label></formula><p>where h</p><formula xml:id="formula_121">(t) v , m (t) v</formula><p>denotes the concatenation of the two input vectors. The model draws different uniform samples at each iteration, while it uses one of the following aggregation functions:</p><p>(1) Mean aggregator: the mean operator computes the elementwise mean of the representations of the neighbors and the vertex itself (the concatenation step shown above is skipped)</p><formula xml:id="formula_122">h (t+1) v = ? ? ? W (t) u?N k (v)?{v} h (t) u deg(v) + 1 ? ?<label>(94)</label></formula><p>(2) Long short-term memory aggregator: the representations of the neighbors are passed on to an long short-term memory (LSTM) architecture. However, LSTMs are not permutation invariant.</p><p>(3) Pooling aggregator: an elementwise max-pooling operation is applied to aggregate information across the neighbor set</p><formula xml:id="formula_123">AGGREGATE (t) = max ? W (t) pool h (t) u u ? N k (v)<label>(95)</label></formula><p>where max denotes the elementwise max operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Differentiable Graph Pooling</head><p>This model aggregates information in a hierarchical way to capture the structure of the entire graph. More specifically, for each layer, the model learns a soft assignment of the vertices of that layer to those of the next layer. This soft assignment considers both topological and feature information. Formally, a matrix S (t) ? R nt?n t+1 is associated with each layer of the model which corresponds to the learned cluster assignment matrix at layer t. Each row corresponds to one of the n t vertices (or clusters) at layer t and each column to one of the n t+1 clusters of the next layer t + 1. Matrix S (t) provides a soft assignment of each vertex at layer t to a cluster in the next coarsened layer t + 1. Each layer coarsens the input graph as follows</p><formula xml:id="formula_124">X (t+1) = S (t) Z (t) A (t+1) = S (t) A (t) S (t)<label>(96)</label></formula><p>where A (t+1) is the coarsened adjacency matrix, and X (t+1) is a matrix of embeddings for each vertex/cluster. To generate the assignment matrix S (t) and matrix Z (t) , the model utilizes two separate message passing neural networks. Both are applied to the input cluster vertex features X (t) and coarsened adjacency matrix A (t) as follows</p><formula xml:id="formula_125">Z (t) = GNN (t) embed A (t) , X (t) S (t) = softmax GNN (t) pool (A (t) , X (t) )<label>(97)</label></formula><p>where the softmax function is applied in a row-wise fashion. GNN pool generates a probabilistic assignment of the input vertices to n t+1 clusters. To generate a final embedding vector corresponding to the entire graph, the model sets the final assignment matrix equal to a vector of ones, that is all vertices at the final layer T are assigned to a single cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Graph Isomorphism Network</head><p>The neighborhood aggregation operation in MPNNs can be thought of as an aggregation function over the multiset that contains the representations of the neighbors of a given vertex. Specifically, a multiset is a generalized concept of a set that allows multiple instances for its elements. When node features are from a countable universe, both the representations of all vertices of a graph and the representations of the neighbors of a vertex can be thought of as multisets . Furthemore, the representations of vertices that emerge at deeper layers of a model are also from a countable universe . Importantly, an MPNN can map two graphs that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic to different embeddings if the AGGREGATE, COMBINE and READOUT functions of the model are all injective . It turns out that the sum aggregator is an injective multiset function. Based on the above result, the graph isomorphism network utilizes the sum aggregator to model injective multiset functions for the neighborhood and vertex aggregation, and has thus the same power as the Weisfeiler-Lehman test of isomorphism. Each neighborhood aggregation layer is defined as</p><formula xml:id="formula_126">h (t+1) v = MLP (t) 1 + (t) h (t) v + u?N (v) h (t) u<label>(98)</label></formula><p>where <ref type="bibr">(t)</ref> is an irrational number of layer t and MLP (t) is a multi-layer perceptron of layer t. The model also uses the sum aggregator as its readout function. Let h</p><formula xml:id="formula_127">(t) G = v?G h (t) v</formula><p>denote the sum of vertex representations at layer t. To produce a graph-level representation, the model utilizes the following readout function which concatenates information from all neighborhood aggregation layers</p><formula xml:id="formula_128">h G = h (0) G , h (1) G , . . . , h (T ) G<label>(99)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Other Models</head><p>While graph kernels focus on several different structural aspects of graphs (e. g., walks, subgraphs, cycles, etc.), the same does not hold for GNNs since most of these models are members of the family of MPNNs. However, there are exceptions to this "rule", and some of these models draw inspiration from graph kernels. For instance, <ref type="bibr" target="#b91">Lei et al. (2017)</ref> proposed a class of GNNs and characterized their associated kernel spaces which were found to be associated with either the random walk kernel or the Weisfeiler-Lehman subtree kernel. Specifically, the hidden states of these models live in the reproducing kernel Hilbert space of these kernels. In another study, <ref type="bibr" target="#b30">Chen et al. (2020)</ref> generated finite-dimensional vertex representations using the Nystr?m method to approximate a kernel that compares a set of local patterns centered at vertices. These representations can be learned without supervision by extracting a set of anchor points, or can be modeled as parameters of a neural network and be learned end-to-end. In the past years, several approaches have been proposed that combine graph kernels with neural networks. For instance, <ref type="bibr" target="#b119">Navarin et al. (2018)</ref> used graph kernels to pre-train GNNs, while  used graph kernels to extract features that are then fed to convolutional neural networks. <ref type="bibr" target="#b44">Du et al. (2019)</ref> followed the opposite direction and proposed a new graph kernel which corresponds to infinitely wide multi-layer GNNs trained by gradient descent, while <ref type="bibr" target="#b4">Al-Rfou et al. (2019)</ref> proposed an unsupervised method for learning graph representations by comparing the input graphs against a set of source graphs. Finally, <ref type="bibr" target="#b127">Nikolentzos and Vazirgiannis (2020)</ref> proposed a neural network model whose first layer consists of a number of latent graphs which are compared against the input graphs using a random walk kernel. The emerging kernel values are fed into a fully-connected neural network which acts as the classifier or regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Applications of Graph Kernels</head><p>In the past years, graph kernels have been applied successfully to a series of real-world problems. Most of these problems come from the fields of bioinformatics and chemoinformatics. However, graph kernels are not limited only to these two fields, but they have been applied to problems arising in other domains as well. We list below some examples of such fields of application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Chemoinformatics</head><p>Traditionally, chemistry is one of the richest sources of graph-structured data. A common problem in this field is to find chemical compounds with a specific property or activity. The experimental characterization of molecules is often an expensive and time-consuming process, and thus people usually resort to computational methods. Specifically, they model chemical compounds as graphs where vertices correspond to atoms and edges to bonds, and then they apply computational methods to identify a small set of potentially interesting molecules for a given property or activity, which are then tested experimentally. Graph kernels have been used extensively for predicting the mutagenicity, toxicity and anti-cancer activity of small molecules <ref type="bibr" target="#b131">Ralaivola et al., 2005;</ref><ref type="bibr" target="#b101">Mah? et al., 2005;</ref><ref type="bibr" target="#b28">Ceroni et al., 2007;</ref><ref type="bibr" target="#b102">Mah? &amp; Vert, 2009;</ref><ref type="bibr" target="#b147">Smalter et al., 2009</ref>) Furthermore, graph kernels have been applied to other problems such as the prediction of the atomization energies of organic molecules <ref type="bibr" target="#b48">(Ferr? et al., 2017)</ref>, the predicition of the boiling points of molecules <ref type="bibr" target="#b56">(Ga?z?re et al., 2011)</ref>, the prediction of the activity against HIV <ref type="bibr" target="#b56">(Ga?z?re et al., 2011)</ref>, and the prediction of properties of stereoisomers <ref type="bibr" target="#b24">(Brown et al., 2010;</ref><ref type="bibr" target="#b64">Grenier et al., 2017)</ref>. A review of the applications of graph kernels in chemoinformatics is provided by <ref type="bibr" target="#b135">Rupp and Schneider (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Bioinformatics</head><p>Bioinformatics is also one of the major application domains of graph representations and therefore, of graph kernels. Recent advances in technology have delivered a step change in our ability to sequence genomes, measure gene expression levels, and test large numbers of potential regulatory interactions between genes. Despite these advancements, some problems of high interest such as the experimental determination of the function of a protein still remain both expensive and time-consuming. Interestingly, the above-mentioned processes produce large volumes of data which can give rise to various types of graphs, such as protein structures, protein and gene co-expression networks, or protein-protein interaction networks. These graphs can then be processed by computational approaches such as graph kernels, and provide solutions to some of these challenging problems. Among others, in the field of bioinformatics, graph kernels have been applied to the prediction of the function of proteins with known sequence and structure <ref type="bibr" target="#b139">Schietgat et al., 2015)</ref>, to the identification of the interactions that are involved in disease outbreak and progression , to the analysis of functional non-coding RNA sequences <ref type="bibr" target="#b136">(Sato et al., 2008)</ref>, to the identification of temporally localized relationships among genes <ref type="bibr" target="#b8">(Antoniotti et al., 2010)</ref>, and to the prediction of domain-peptide interactions <ref type="bibr" target="#b89">(Kundu et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Computer Vision</head><p>Graph representations have been investigated a lot in the fields of image processing and computer vision. There exist many different approaches to represent images as graphs. For instance, vertices usually correspond to pixels or to segmented regions, while edges join neighboring pixels or neighboring regions with each other. Graph kernels have served as an effective tool for many computer vision tasks such as for classifying images <ref type="bibr" target="#b68">(Harchaoui &amp; Bach, 2007;</ref><ref type="bibr" target="#b99">Mahboubi et al., 2010;</ref><ref type="bibr" target="#b7">Antanas et al., 2012;</ref><ref type="bibr" target="#b180">Zhang et al., 2013)</ref>, for detecting objects represented as point clouds <ref type="bibr" target="#b10">(Bach, 2008;</ref><ref type="bibr" target="#b121">Neumann et al., 2013)</ref>, for achieving place recognition <ref type="bibr" target="#b152">(Stumm et al., 2016)</ref>, for achieving action recognition <ref type="bibr" target="#b169">(Wang &amp; Sahbi, 2013;</ref><ref type="bibr" target="#b173">Wu et al., 2014;</ref><ref type="bibr" target="#b94">Li et al., 2016a)</ref>, for scene modeling <ref type="bibr" target="#b50">(Fisher et al., 2011)</ref>, and for matching observations of persons across different cameras .</p><p>Besides the above applications, graphs are also used increasingly often in biomedical imaging. Different types of graphs such as connectivity graphs are usually extracted from functional magnetic resonance imaging (fMRI) data. Then, graph kernels capitalize on these graphs to address various tasks such as to distinguish between different brain states <ref type="bibr" target="#b142">(Shahnazian et al., 2012;</ref><ref type="bibr" target="#b108">Mokhtari &amp; Hossein-Zadeh, 2013;</ref><ref type="bibr" target="#b162">Vega-Pons &amp; Avesani, 2013;</ref><ref type="bibr" target="#b163">Vega-Pons et al., 2014)</ref>, to determine whether a subject is cocaine-addicted or not <ref type="bibr" target="#b59">(Gkirtzou &amp; Blaschko, 2016)</ref>, or to predict mild cognitive impairment, a prodromal stage of Alzheimer's disease <ref type="bibr" target="#b75">(Jie et al., 2014</ref><ref type="bibr" target="#b74">(Jie et al., , 2016</ref>. There have also been proposed kernels that can handle inter-subject variability in fMRI data <ref type="bibr" target="#b159">(Takerkart et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Cybersecurity and Software Verification</head><p>The number of malicious applications targeting desktop and mobile devices has increased daramatically in the past few years. Due to this unprecedented increase in the number of malicious applications, malware detection has recently become a very active area of research. It has been observed that most newly discovered malware samples are variations of existing malware. Furthermore, it has been shown that it is easier to detect these variations if highlevel code representations, such as function call graphs or control flow graphs, are employed. It should be mentioned that these graphs can prove useful not only for detecting malware, but also for retrieving similar application programs. Therefore, graph kernels can be applied to such graphs, and have served as a common tool for detecting malware <ref type="bibr" target="#b6">(Anderson et al., 2011;</ref><ref type="bibr" target="#b55">Gascon et al., 2013;</ref><ref type="bibr" target="#b118">Narayanan et al., 2016)</ref>, but also for analyzing execution traces obtained from dynamic analysis <ref type="bibr" target="#b167">(Wagner et al., 2009)</ref>, for measuring the similarity between programs <ref type="bibr" target="#b95">(Li et al., 2016b)</ref>, and for predicting metamorphic relations <ref type="bibr" target="#b77">(Kanewala et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Natural Language Processing</head><p>Although textual documents do not exhibit an underlying graph structure, in many cases, they are also modeled as graphs. A vertex corresponds to some meaningful linguistic unit such as a sentence, a word, or even a character, while an edge corresponds to some relationship between two vertices which can be statistical, syntactic, or semantic among others. A common representation is the word co-occurence network, where vertices correspond to terms and edges represent co-occurrences between the terms within a fixed-size sliding window. This representation addresses some of the limitations of the bag-of-words representation which treats terms as independent of one another. Graph kernels have proven useful for several text mining applications such as for recognizing identical real-world events modeled as event graphs <ref type="bibr" target="#b60">(Glava? &amp;?najder, 2013)</ref>, for classifying biomedical text documents represented as concept graphs <ref type="bibr" target="#b16">(Bleik et al., 2013)</ref>, for extracting protein-protein interactions from scientific literature <ref type="bibr" target="#b1">(Airola et al., 2008a</ref><ref type="bibr" target="#b2">(Airola et al., , 2008b</ref>, and for measuring the similarity between documents represented as word co-occurence networks . Besides the above applications, tree kernels (which can be considered as instances of graph kernels) have been heavily applied to different problems in the field of natural language processing (see subsection 4.11 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Other Applications</head><p>Graph kernels have been applied to many other practical problems involving graph representations such as for classifying Resource Description Framework (RDF) data <ref type="bibr" target="#b97">(L?sch et al., 2012;</ref><ref type="bibr" target="#b37">De Vries &amp; de Rooij, 2015)</ref>, for entity disambiguation in anonymized graphs <ref type="bibr" target="#b70">(Hermansson et al., 2013)</ref>, for classifying architectural designs into architectural styles <ref type="bibr" target="#b151">(Strobbe et al., 2016)</ref>, and for estimating the similarity of relational states in relational reinforcement learning <ref type="bibr" target="#b43">(Driessens et al., 2006;</ref><ref type="bibr" target="#b66">Halbritter &amp; Geibel, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Comparison</head><p>In this Section, we experimentally evaluate many of the graph kernels presented above and compare them to each other. Although there are approaches that measure the expressiveness of graph kernels by using recent results from the field of statistical learning theory <ref type="bibr" target="#b128">(Oneto et al., 2017)</ref>, empirically evaluating the graph kernels can provide insights into their utility in real-world scenarios. We first present the problem of graph classification, and we evaluate the kernels in this task. We describe the datasets that we used for our experiments, and give details about the experimental settings. We then report on the performance and running time of the different kernels. We finally experiment with a synthetic dataset, and compare the emerging kernel values against the similarities produced by an expressive but intractable graph similarity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Graph Classification</head><p>Classification is perhaps the most frequently encountered machine learning problem. In classification, the goal is to learn a mapping from input objects to their class labels, given a training set. When the input objects are graphs, the problem is called graph classification. More formally, in this setting, we are given a training set  graphs along with their class labels. The goal is to learn a function f : G ? Y, where G is the input space of graphs and Y the set of graph labels. This function can then be used to assign class labels to new previously unseen graphs, such as those contained in the test set.</p><formula xml:id="formula_129">D = {(G i , y i )} N i=1 consisting of N</formula><p>The problem of graph classification has become a popular area of research in recent years because it finds numerous applications in a wide variety of fields. Several of these application have already been discussed above. For example, graph classification arises in applications which range from predicting the mutagenicity of a chemical compound , and predicting the function of a protein given its amino acid sequence , to detecting if a software object is infected with malware <ref type="bibr" target="#b167">(Wagner et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Datasets</head><p>We next briefly describe the graph datasets used in our experiments. We have considered data from different domains, including chemoinformatics, bioinformatics and social networks. All graphs are undirected. Furtermore, the graphs contained in the chemoinformatics and bioinformatics datasets are node-labeled, node-attributed or both. All datasets are publicly available . <ref type="table" target="#tab_7">Table 3</ref> provides a summary of the employed datasets.</p><p>AIDS. It consists of molecular compounds represented as graphs. The compounds were obtained from the AIDS Antiviral Screen Database of Active Compounds. Vertices correspond to atoms and edges to covalent bonds. Vertices are labeled with the corresponding chemical symbol and edges with the valence of the linkage. The task is to predict whether or not each compound is active against HIV <ref type="bibr" target="#b134">(Riesen &amp; Bunke, 2008)</ref>.</p><p>BZR. It contains 405 chemical compounds (ligands for the benzodiazepine receptor) which are modeled as graphs. The task is to predict whether a compound is active or inactive <ref type="bibr" target="#b156">(Sutherland et al., 2003)</ref>.</p><p>COLLAB. This is a scientific collaboration dataset consisting of the ego-networks of several researchers from three subfields of Physics (High Energy Physics, Condensed Matter Physics and Astro Physics). The task is to determine the subfield of Physics to which the ego-network of each researcher belongs <ref type="bibr" target="#b177">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>D&amp;D. This dataset contains over a thousand protein structures. Each protein is a graph whose vertices correspond to amino acids and a pair of amino acids are connected by an edge if they are less than 6?ngstroms apart. The task is to predict if a protein is an enzyme or not <ref type="bibr" target="#b41">(Dobson &amp; Doig, 2003)</ref>.</p><p>ENZYMES. It comprises of 600 protein tertiary structures obtained from the BRENDA enzyme database. Each enzyme is a member of one of the Enzyme Commission top level enzyme classes (EC classes) and the task is to correctly assign the enzymes to their classes .</p><p>IMDB-BINARY and IMDB-MULTI. These datasets were created from IMDb (www. imdb.com), an online database of information related to movies and television programs. The graphs contained in the two datasets correspond to movie collaborations. The vertices of each graph represent actors/actresses and two vertices are connected by an edge if the corresponding actors/actresses appear in the same movie. Each graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network belongs to <ref type="bibr" target="#b177">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>MUTAG. This dataset consists of 188 mutagenic aromatic and heteroaromatic nitro compounds. The task is to predict whether or not each chemical compound has mutagenic effect on the Gram-negative bacterium Salmonella typhimurium <ref type="bibr" target="#b38">(Debnath et al., 1991)</ref>.</p><p>NCI1. This dataset contains a few thousand chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines <ref type="bibr" target="#b168">(Wale et al., 2008)</ref>.</p><p>PROTEINS, PROTEINS full. They contain proteins represented as graphs where vertices are secondary structure elements and there is an edge between two vertices if they are neighbors in the amino-acid sequence or in 3D space. The task is to classify proteins into enzymes and non-enzymes .</p><p>PTC-MR. This dataset contains 344 organic molecules represented as graphs. The task is to predict their carcinogenic effects on male rats <ref type="bibr" target="#b161">(Toivonen et al., 2003)</ref>.</p><p>REDDIT-BINARY, REDDIT-MULTI-5K, REDDIT-MULTI-12K. The graphs contained in these three datasets represent social interaction between users of Reddit (www. reddit.com), one of the most popular social media websites. Each graph represents an online discussion thread. Specifically, each vertex corresponds to a user, and two users are connected by an edge if one of them responded to at least one of the other's comments. The task is to classify graphs into either communities or subreddits <ref type="bibr" target="#b177">(Yanardag &amp; Vishwanathan, 2015b)</ref>.</p><p>SYNTHETICnew. It comprises of 300 synthetic graphs divided into two classes of equal size. Each graph is obtained by adding noise to a random graph with 100 vertices and 196 edges, whose vertices are endowed with normally distributed scalar attributes sampled from N (0, 1). The graphs of the first class were generated by rewiring 5 edges and permuting 10 node attributes, while the graphs of the second class were generated by rewiring 10 edges and permuting 5 node attributes. After the generation of all graphs, noise from N (0, 0.45 2 ) was also added to every node attribute in every graph <ref type="bibr" target="#b47">(Feragen et al., 2013)</ref>.</p><p>Synthie. This dataset consists of 400 synthetic graphs, subdivided into four classes, with 15 real-valued node attributes. Two types of graphs and two types of attributes were generated, and each combination of those gave rise to a class (four classes in total). All graphs were generated by randomly adding edges between 10 perturbed instances of two Erd?s R?nyi graphs. To generate graphs of the first type, perturbed instances of the first Erd?s R?nyi graph were choosen with probability 0.8, while perturbed instances of the second Erd?s R?nyi graph were choosen with probability 0.2. To generate graphs of the second type, the two probabilities were reversed. The vertices of each graph were then annotated by attributes drawn either from the first or from the second type of attributes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Experimental Setup</head><p>We evaluated the performance of the graph kernels on the datasets presented above. Specifically, we made use of the GraKeL library which contains implementations of a large number of graph kernels <ref type="bibr" target="#b146">(Siglidis et al., 2020)</ref>. We used the following 20 kernels in our experimental evaluation: (1) vertex histogram kernel (VH), (2) random walk kernel (RW), (3) shortest path kernel (SP), (4) graphlet kernel (GR), (5) Weisfeiler-Lehman subtree kernel (WL-VH), (6) Weisfeiler-Lehman shortest path kernel (WL-SP), (7) Weisfeiler-Lehman pyramid match kernel (WL-PM), (8) Weisfeiler-Lehman optimal assignment kernel (WL-OA), (9) neighborhood hash kernel (NH), (10) neighborhood subgraph pairwise distance kernel (NSPDK), (11) Lov?sz ? kernel (Lo-?), (12) SVM-? kernel (SVM-?), (13) ordered decompositional DAGs with subtree kernel (ODD-STh), (14) pyramid match kernel (PM), (15) GraphHopper kernel (GH), (16) subgraph matching kernel (SM), (17) propagation kernel (PK), (18) multiscale Laplacian kernel (ML), (19) core Weisfeiler-Lehman subtree kernel (CORE-WL-VH), and (20) core shortest path kernel (CORE-SP). Note that some of the kernels (e. g., WL-SP, CORE-SP) correspond to frameworks applied to graph kernels. Furthermore, since some kernels can handle different types of graphs than others, we conduct three distinct experiments. The three experiments are characterized by the types of graphs contained in the employed datasets: (1) datasets with unlabeled graphs, (2) datasets with node-labeled graphs, and (3) datasets with node-attributed graphs. It is important to mention that kernels that are designed for node-labeled graphs can also be applied to unlabaled graphs by initializing the node labels of all vertices of the unlabaled graphs to the same value. Hence, we evaluate these kernels on datasets that contain node-labeled graphs, but also on datasets that contain unlabeled graphs. Moreover, kernels that are designed for node-attributed graphs can be applied to unlabeled graphs and to graphs that contain discrete node labels. To achieve that, in the case of unlabeled graphs, all the vertices of all graphs are assigned the same attribute, while in the case of node-labeled graphs, node labels are transformed into feature vectors (e. g., using a "one-hot" encoding scheme). Hence, we evaluated these kernels on all three experimental scenarios.</p><p>We also compare the above graph kernels against the following 4 state-of-the-art GNNs: (1) DGCNN <ref type="bibr" target="#b181">(Zhang et al., 2018a)</ref>, (2) GraphSAGE <ref type="bibr" target="#b67">(Hamilton et al., 2017)</ref>, (3) DiffPool <ref type="bibr" target="#b178">(Ying et al., 2018)</ref>, and (4) GIN . Note that GNNs iteratively update the feature vectors of the vertices of each graph, and thus, they assume node-attributed graphs. Therefore, for unlabeled graphs and for graphs that contain discrete node labels, we follow the procedure described above. In the cased of unlabeled graphs, all the vertices of all graphs are assigned the same attribute, while in the case of node-labeled graphs, node labels are transformed into feature vectors. It should be mentioned that we only compare the performance of GNNs against that of kernels, and not their running time. The running time of a GNN depends on the values of some hyperparameters (e. g., number of epochs, batch size, etc.), while the different models run on a GPU instead of a CPU. Hence, the running time of a GNN is not directly comparable to that of a graph kernel, and thus we refrain from reporting those results.</p><p>In the case of graph kernels, to perform graph classification, we employed a Support Vector Machine (SVM) classifier and in particular, the LIB-SVM implementation <ref type="bibr" target="#b29">(Chang &amp; Lin, 2011)</ref>. To evaluate the performance of the different kernels and GNNs, we employ the framework proposed by <ref type="bibr" target="#b46">Errica et al. (2020)</ref>. Therefore, we perform 10-fold cross-validation to obtain an estimate of the generalization performance of each method. For the common datasets, we use the splits (and results) provided by <ref type="bibr" target="#b46">Errica et al. (2020)</ref>. For the remaining datasets, we use the code provided by <ref type="bibr" target="#b46">Errica et al. (2020)</ref> to evaluate the 4 GNNs. Within each fold, the parameter C of the SVM and the hyperparameters of the kernels (see below) and GNNs were chosen based on a validation experiment on a single 90% ? 10% split of the training data. We chose the value of parameter C from {10 ?7 , 10 ?5 , . . . , 10 5 , 10 7 }. Moreover, we normalized all kernel values as followsk</p><formula xml:id="formula_130">(G i , G j ) = k(G i ,G j ) / ? k(G i ,G i ) k(G j ,G j )</formula><p>for any graphs G i , G j . All experiments were performed on a cluster of 80 Intel ? Xeon ? CPU E7 ? 4860 @ 2.27GHz with 1TB RAM. Note that each kernel was computed on a single thread of the cluster. We set a time limit of 24 hours for each kernel to compute the kernel matrix. Hence, we denote by TIMEOUT kernel computations that did not finish within one day. We also set a memory limit of 64GB, and we denote by OUT-OF-MEM computations that exceeded this limit. As mentioned above, to choose the hyperparameters of the kernels, we performed a validation experiment on a single 90% ? 10% split of the training set. Hence, given a kernel, for each combination of hyperparameter values, we generated a seperate kernel matrix.</p><p>The hyperparameter values that result into the classifier with the best performance on the validation set are the ones selected for the final model learning. The values of the different hyperparameters of the kernels are shown in  that the number of hyperparameters ranges significantly across kernels. For instance, some kernels such as the vertex historgram kernel (VH) lack hyperparameters, while other kernels such as the multiscale Laplacian kernel (ML) contain a large number of hyperparameters. Hence, for the vertex historgram (VH) kernel, we compute only a single kernel matrix in each experiment, while for the multiscale Laplacian (ML) kernel, we compute 24 different kernel matrices in each experiment. Note also that instead of performing cross-validation to identify the best combination of hyperparameter values, we could have applied multiple kernel learning to the generated kernel matrices <ref type="bibr" target="#b103">(Massimo et al., 2016)</ref>. For each experiment, we report the average accuracy over the 10 runs of the crossvalidation procedure. Furthermore, we report running times averaged over the 10 independent runs. For each run, we compute running times as follows: for each fold of a 10-fold cross-validation experiment, the running time of the kernel corresponds to the running time for the computation of the kernel matrix that performed best on the validation experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Experimental Results</head><p>We next present our experimental results. As mentioned above, we evaluate the graph kernels by performing graph classification on unlabeled, node-labeled and node-attributed benchmark datasets.</p><p>Node-Labeled Graphs. Tables 5 and 6 illustrate average prediction accuracies and average running times of the compared kernels and the GNNs on the datasets that contain node-labeled graphs. We observe that the kernels that employ some neighborhood aggregation mechanism (e. g., the Weisfeiler-Lehman framework) yield very good performance.   <ref type="table">Table 6</ref>: Average CPU running time for kernel matrix computation on the 7 classification datasets containing node-labeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</p><p>Specifically, the WL-OA kernel outperforms all the other kernels on 3 out of the 7 datasets (ENZYMES, NCI1, and PTC-MR), while the CORE-WL-VH kernel is the best-performing approach on 2 out of the remaining 4 datasets (D&amp;D and PROTEINS). Moreover, the WL-VH, WL-PM and NH kernels also achive high accuracies on most datasets. Surprisingly, WL-SP, although equipped with a neighborhood aggregation scheme, performs much worse than the other kernels which employ the same framework and also much worse than the SP and CORE-SP kernels. The core framework leads to performance improvements on most datasets. For instance, in the case of the SP kernel, it leads to better accuracies on all but one dataset. It is worth mentioning that CORE-SP provides the highest accuracy on the PROTEINS dataset (along with CORE-WL-VH) and second best accuracy on the D&amp;D and AIDS datasets. As regards the kernels for graphs with continuous attributes (GH, SM, PK, and ML), most of them failed to produce results comparable to the best-performing kernels. The only exception is the ML kernel which yielded good results on most datasets. Moreover, the GH kernel reached the highest accuracy on the AIDS dataset. It is also interesting to mention that the VH and RW kernels achieved very low accuracy levels. The 4 GNN models also failed to yield performance competitive with that of the kernels that employ neighborhood aggregation mechanisms. Specifically, all GNNs were outperformed by some graph kernel on all 7 datasets. DGCNN performed better than the rest of the GNNs, but in most cases, all GNN models achieved similar accuracies to each other. On most datasets, the variability in the performance of the different kernels is low. The ENZYMES dataset is an exception to that, since the average accuracy of the bestperforming kernel is equal to 58.0%, while that of the worst-performing kernel is equal to 16.7%. Furthermore, the AIDS dataset is almost perfectly classified by several kernels, and this raises some concerns about the value of this dataset for graph kernel comparison.</p><p>In terms of running time, as expected, VH is the fastest kernel on all datasets. This kernel computes the dot product on vertex label histograms, hence, its complexity is linear to the number of vertices. The running time of WL-VH, CORE-WL-VH, SP, PK, and NH is also low compared to the other kernels on most datasets. Note also that while the worst-case complexity of SP is very high, by employing an explicit computation scheme, the running time of the kernel in real scenarios is very attractive. We also observe that the ML, RW, SM and WL-PM kernels are very expensive in terms of runtime. Specifically, the SM kernel failed to compute the kernel matrix on NCI1 within one day, while it exceeded the maximum available memory on two other datasets (D&amp;D and PROTEINS). It should be mentioned that the size of the graphs (i. e., number of vertices) and the size of the dataset (i. e., number of graphs) have a different impact on the running time of the kernels. For instance, the average running time of the PM kernel is relatively high on datasets that contain small graphs. However, this kernel is much more competitive on datasets which contain large graphs such as the D&amp;D dataset on which it was the third fastest kernel.</p><p>Overall, when dealing with tasks that involve node-labeled graphs, we suggest to use a kernel that utilizes some neighborhood aggregation mechanism. For instance, the WL-VH and NH kernels achieve high accuracies and are very efficient even when the size of the graphs and/or the dataset is large. The WL-OA kernel can potentially outperform the above two kernels, however, it is also more expensive to compute. From the above experimental evaluation, it is also clear that graph kernels are more effective than GNNs in classifying graphs whose vertices are annotated with discrete labels. Still, we need to stress that graph kernels do not scale to large datasets (e. g., datasets that contain hunderds of thousands of graphs), and this is a limitation inherent to kernel methods in general. In such scenarios, GNNs should be preferred over graph kernels. <ref type="table" target="#tab_14">Tables 7 and 8</ref> illustrate average prediction accuracies and average running times of the compared kernels and GNNs on the 6 datasets that contain unlabeled graphs. We observe that the GIN model is the best-performing method. It outperforms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unabeled Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATASETS</head><p>Avg <ref type="table" target="#tab_1">.  IMDB  IMDB  REDDIT  REDDIT  REDDIT  COLLAB  Rank  BINARY  MULTI  BINARY  MULTI-5K  MULTI-12K</ref> Kernels VH 50.0 (? 0.0) 33.3 (? 0.0) 50.0 (? 0.0) 20.0 (? 0.0) 21.7 (? 1.5) 52.0 (? 0.  <ref type="table">Table 7</ref>: Average classification accuracy (? standard deviation) on the 6 classification datasets containing unlabeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel/GNN. The lower the average rank, the better the overall performance of the kernel/GNN. all the other methods on 2 out of the 6 datasets (REDDIT-MULTI-5K and REDDIT-MULTI-12K). The CORE-WL-VH kernel achieves the second best performance. Indeed, the CORE-WL-VH kernel outperforms all the other approaches on 3 out of the 6 datasets (IMDB-BINARY, IMDB-MULTI and COLLAB). The WL-OA, NH and WL-VH kernels also yield high performance on most datasets. In fact, these kernels along with CORE-WL-VH outperform the remaining 3 GNN models, that is DGCNN, GraphSAGE and DiffPool. Furthermore, on the remaining datasets, it reached the second, the second and the eighth best accuracy levels among all methods considered. We should note that the core framework improved significantly the performance of the SP kernel on several datasets, while CORE-SP achieved the highest average accuracy on the REDDIT-BINARY dataset. The Lo-? kernel was the worst-performing kernel, followed by VH, WL-SP, VH, NSPDK and GH in that order. It is interesting to mention that most of the kernels that reached the highest accuracies can also handle graphs with dicrete node labels. For those kernels, the label of each vertex was set equal to its degree. The kernels that can handle only unlabeled graphs (GR, Lo-?, SVM-?) failed to achieve accuracies competitive to the best-performing kernels.</p><p>With regards to the kernels that can handle graphs with continuous attributes (GH, SM, PK, and ML), as mentioned above, ML yielded the best results. GH and PK achieved low accuracy levels, while SM failed to generate even a single kernel matrix due to running  time or memory issues. In this set of experiments, the 4 GNN models provided very good performance results. Before applying the GNNs to a dataset, the vertices of all graphs were annotated with a single feature that was set equal to the degree of the vertex. As already mentioned, GIN is the best-performing method. The remaining 3 GNNs yielded similar performance to each other, but were still outperformed by a few graph kernels.</p><p>On most datasets, the variability in the performance of the different kernels is low. The kernels achieve higher performance on binary classification tasks (IMDB-BINARY and REDDIT-BINARY) than on multi-class classification tasks. For instance, on IMDB-MULTI, REDDIT-MULTI-5K and REDDIT-MULTI-12K, the highest average accuracies obtained by the considered approaches are 51.7%, 56.1% and 48.3%, respectively. Hence, it is clear that these three datasets are very challenging even for state-of-the-art methods.</p><p>In terms of running time, similar to the labeled case, VH is again the fastest kernel on all datasets. The running time of PK, ODD-STh, and WL-VH is also low compared to the other kernels on most datasets. The SVM-?, NH, PM, SP and CORE-WL kernels were also competitive in terms of running time. Besides achieving low accuracy levels, the Lo-? kernel is also very computationally expensive. The WL-PM, RW, NSPDK, CORE-SP, WL-SP and GH are also very expensive in terms of running time. The above 7 kernels did not manage to calculate any kernel matrix on REDDIT-MULTI-5K and REDDIT-MULTI-12K within one day. It should be mentioned that these two datasets contain several thousands of graphs, while the size of the graphs is also large (i. e., several hundreds of vertices on average). The SM kernel failed to compute the kernel matrix on IMDB-BINARY, IMDB-MULTI and COLLAB within one day, while it exceeded the maximum available memory on the remaining three datasets.   <ref type="table" target="#tab_1">Table 10</ref>: Average CPU running time for kernel matrix computation on the 5 classification datasets containing node-attributed graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</p><p>When dealing with tasks that involve unlabeled graphs, we suggest to assign discrete node labels to the vertices of the graphs (e. g., set the label of each vertex equal to its degree), and then to again employ kernels that utilize some neighborhood aggregation mechanism. For instance, the CORE-WL-VH, WL-OA, NH, and WL-VH kernels achieve high accuracies, while their computational complexity is realtively low. Altenatively, a GNN model could be employed, especially in the case of large datasets. Note, however, that even though GIN was found to be the best-performing approach in this set of experiments, the highest performance on 4 out of the 6 datasets was achieved by some graph kernel and not by a GNN. Therefore, kernels still seem to be well-suited for such kind of datasets.</p><p>Node-Attributed Graphs As mentioned above, the majority of graph kernels can handle graphs that are either unlabeled or contain discrete node labels. On the other hand, the number of graph kernels that can handle graphs that contain continuous vertex attributes is limited. Moreover, most of these kernels do not scale even to relatively small datasets. Tables 9 and 10 illustrate average prediction accuracies and average runtimes of graph kernels and GNNs on datasets that contain node-attributed graphs. Note that although the graphs of some of these datasets contain discrete node labels, we did not take these discrete labels into account since our main aim was to evaluate the ability of the kernels to properly handle continuous node attributes. GIN is the best-performing approach also in this set of experiments, while GNNs generally outperform graph kernels. GH is the best-performing kernel since it outperforms all the other kernels on all datasets. Furthermore, GH outperforms 2 out of the 4 GNNs (DGCNN and GraphSAGE). GH is followed by ML and PK in terms of performance in that order. One of the most striking findings of this set of experiments is that the SP kernel did not manage to compute the kernel matrix even on a single dataset within one day, while the SM kernel finished its computations within one day only on the BZR dataset on which it was outperformed by GH and ML. The 4 GNNs yielded in most cases high levels of accuracy. However, some GNNs failed to produce competitive results on some datasets. For instance, DGCNN achieved an average accuracy of 53.7% on SYN-THETICnew, while GraphSAGE yielded an average accuracy of 51.3% on Synthie. This might be due to the neighborhood aggregation mechanisms or readout functions employed by these models.</p><p>In terms of running time, PK is the most efficient kernel since it handled all datasets in less than two minutes. GH and ML are much slower than PK on all datasets. For instance, the average computation time of ML and GH was greater than 2 hours and 3 hours on PROTEINS full, respectively. The SP and SM kernels, as already discussed, are very expensive in terms of running time, and hence, their usefulness in real-world problems is limited.</p><p>To sum up, it is clear that the running time of most kernels for node-attributed graphs is prohibitive, especially considering the relatively small size of the datasets. Although the running time of PK is attractive, it achieved low accuracies on almost all datasets. An open challenge in the field of graph kernels is thus to develop scalable kernels for graphs with continuous vertex attributes. On the other hand, GNNs can naturally handle continuous node features, while they have also outperformed graph kernels in the experimental evaluation. Therefore, when dealing with graphs whose vertices are annotated with continuous attributes, we recommend using a GNN model instead of a graph kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Expessiveness of Graph Kernels</head><p>Over the past years, the expessive power of graph kernels was assessed almost exclusively from empirical studies. So far, there are only a few theoretical findings related to the expressiveness of graph kernels. For instance, as already mentioned, it has been shown that the mapping induced by kernels that are computable in polynomial time is not injective (and thus these kernels cannot solve the graph isomorphism problem) . Recently, <ref type="bibr" target="#b88">Kriege et al. (2018)</ref> proposed a framework to measure the expressiveness of graph kernels based on ideas from property testing, and showed that some well-established graph kernels such as the shortest path kernel, the random walk kernel, and the Weisfeiler-Lehman subtree kernel cannot identify fundamental graph properties such as triangle-freeness and bipartitness. It is thus clear that there are several interesting questions about the expressiveness of graph kernels which are far from being answered. An example of such a question is whether a specific graph kernel captures graph similarity better than others for some specific application.</p><p>In what follows, we conduct an experiment to empirically answer the above question. Specifically, we build a dataset that contains instances of different families of graphs. Then, we compare the similarities (i. e., kernel values) produced by graph kernels against those generated by an intractable graph similarity function which we consider to be an oracle function that outputs the true similarity between graphs. Formally, for any two graphs G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ) on n vertices with respective n ? n adjacency matrices A 1 and A 2 , we define a function f : G ? G ? R where G is the space of graphs which quantifies the similarity of G 1 and G 2 . The function can be expressed as the following maximization problem:</p><formula xml:id="formula_131">f (G 1 , G 2 ) = max P ?? n i=1 n j=1 A 1 P A 2 P ij ||A 1 || F ||A 2 || F<label>(100)</label></formula><p>where ? denotes the set of n ? n permutation matrices, denotes the elementwise product, and || ? || F is the Froebenius matrix norm. For clarity of presentation we assume n to be fixed (i. e., both graphs consist of n vertices). In order to apply the function to graphs of different cardinality, one can append zero rows and columns to the adjacency matrix of the smaller graph to make its number of rows and columns equal to n. Therefore, the problem of graph comparison can be reformulated as the problem of maximizing the above function over the set of permutation matrices. A permutation matrix P gives rise to a bijection ? : V 1 ? V 2 . The function defined above seeks for a bijection such that the number of common edges |{(u, v) ? E 1 : ?(u), ?(v) ? E 2 }| is maximized. Then, the number of common edges is normalized such that it takes values between 0 and 1. Observe that the above definition is symmetric in G 1 and G 2 . The two graphs are isomorphic to each other if and only if there exists a permutation matrix P for which the above function is equal to 1. Therefore, a value equal to 0 denotes maximal dissimilarity, while a value equal to 1 denotes that the two graphs are isomorphic to each other. Note that if the compared graphs are not empty (i. e., they contain at least one edge), the function will take some value greater than 0. Solving the above optimization problem for large graphs is clearly intractable since there are n! permutation matrices of size n. In fact, the above function is related to the well-studied Frobenius distance between graphs which is known to be an NP-complete problem <ref type="bibr" target="#b65">(Grohe et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Dataset</head><p>Since the function defined in Equation <ref type="formula" target="#formula_18">(100)</ref> is intractable for large graphs, we generated graphs consisting of at most 9 vertices. Furthermore, each graph is connected and contains at least 1 edge. We generated 191 pairwise non-isomorphic graphs. The dataset consists of different types of synthetic graphs. These include simple structures such as cycle graphs, path graphs, grid graphs, complete graphs and star graphs, but also randomly-generated graphs such as Erd?s-R?nyi graphs, Barab?si-Albert graphs and Watts-Strogatz graphs. <ref type="table" target="#tab_1">Table 11</ref> shows statistics of the synthetic dataset that we used in our experiments. <ref type="figure" target="#fig_9">Figure 16</ref> illustrates the distribution of the similarities of the generated graphs as computed by the proposed measure. There are 191 * 192 /2 = 18, 336 pairs of graphs in total (including pairs consisting of a graph and itself). Interestingly, most of the similarities take values between 0.5 and 0.8.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Experimental Settings</head><p>The set of kernels for this experiment contains the same 20 kernels that were evaluated in the context of the graph classification experiments. Once again, we use the implementations of the kernels contained in the GraKeL library <ref type="bibr" target="#b146">(Siglidis et al., 2020)</ref>. Note that the synthetic graphs are unlabeled. Therefore, for kernels that assume node-labeled graphs, all vertices of all graphs are annotated with a single label, while for kernels that assume node-attributed graphs, all the vertices of all graphs are assigned the same attribute. As discussed above, the function defined in Equation <ref type="formula" target="#formula_18">(100)</ref> gives an output in the range [0, 1]. We normalize the obtained kernel values as follows such that they also take values in the range [0, 1]:k(G i , G j ) = k(G i ,G j ) / ? k(G i ,G i ) k(G j ,G j ) for any graphs G i , G j . We should stress that the normalized kernel value can take a value equal to 1 even if the compared graphs are mapped to different representations. Indeed, if the angle between the vector representations of two graphs is 0 ? , then their normalized kernel value is equal to 1. To avoid such a scenario, we could define a distance function between graphs and accordingly compute the Euclidean distance between the graph representations generated by the different approaches. However, it turns out that most widely-used learning algorithms compute the inner products between the input objects or some transformations of these objects. In fact, when learning with kernels, we usually normalize the kernel matrices using the equation defined above before feeding to a kernel method such as the SVM classifier. Therefore, we believe that evaluating the "similarity" of the obtained representations is more natural than evaluating their "distance". With regards to the values of the hyperparameters of the 20 kernels, we experiment with the same values as in the case of graph classification. Specifically, we choose the hyperparameter values that result into the highest correlation between the kernel values generated by a given kernel and the similarities produced by the function of Equation (100).</p><p>To assess how well the different approaches approximate the similarity function, we employed two evaluation metrics: the Pearson correlation coefficient and the mean squared error (MSE). In our setting, a high value of correlation would mean that the approach under consideration captures the relationships between the similarities (e. g., whether the similarity of a pair of graphs is greater or lower than that of another pair). On the other hand, a very small value of MSE denotes that the derived similarities are very close to those produced by the function defined in Equation <ref type="formula" target="#formula_18">(100)</ref> learning/similarity approach would yield both a high correlation and a small MSE. The former would ensure that similar/dissimilar graphs are indeed deemed similar/dissimilar by the considered approach, while the latter would verify that the similarity values are on par with those produced by the similarity function of Equation <ref type="formula" target="#formula_18">(100)</ref>. <ref type="figure" target="#fig_10">Figure 17</ref> illustrates the correlation and MSE between the kernel values produced by the 20 kernels and the similarities produced by the function of Equation <ref type="formula" target="#formula_18">(100)</ref>. In terms of correlation, WL-PM and SM are the best-performing approaches followed by CORE-SP, WL-SP, WL-OA and GR. The correlation between the first two kernels and the function of Equation (100) is greater than 0.7, while the rest of the above kernels achieve a correlation slightly lower than 0.7. On the other hand, VH, SVM-? and PK yield very low levels of correlation (smaller than 0.1) followed by Lo-?, ODD-STh and ML. Note that for unlabeled graphs, the normalized kernel values of the VH kernel are always equal to 1, and thus correlation is not defined since the values produced by the constant function have a variance equal to zero. Overall, the majority of correlations is greater than 0.5 which demonstrates  <ref type="figure">Figure 18</ref>: Projection of the representations of the 20 kernels in R 2 (using PCA). Each kernel is represented as a vector of kernel values between the graphs of the synthetic dataset. that most kernels indeed capture some notion of similarity between graphs. In terms of MSE, the PM, WL-VH, WL-SP and CORE-SP kernels are the best-performing approaches. Notably, these graph kernels achieve very low values of MSE which indicates that the produced kernel values are very close to the similarities that emerge from the function of Equation 100. It is interesting to mention that most kernels achieve an MSE smaller than 0.1. The Lo-? kernel yields an MSE value much greater than those of the other kernels, while the NH, ML, VH, PK and SVM-? kernels also fail to achieve low levels of MSE. As already mentioned, most graph kernels are generally motivated by runtime considerations. They are computable in polynomial time, which usually has an impact on their expressive power. Our results indicate that even though kernels do not provide any guarantees on how well they can approximate the above function, empirically, they seem to capture several aspects of graph similarity to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Results</head><p>We next study whether there exist groups of kernels that are more similar to each other than to other kernels. To discover such groups of kernels, we utilize the kernel values produced by the different kernels. When a kernel is applied to the synthetic dataset that was introduced above, 191 * 192 /2 = 18, 336 kernel values are computed in total. We thus represent each kernel as a vector in a common space (of dimension 18, 336) based on the emerging kernel values. We then project the representations of the kernels to the 2-dimensional space using PCA. The results are shown in <ref type="figure">Figure 18</ref>. The position of each dot represents a projection of the kernel values generated by a single kernel. We observe that VH, RW, PK and SVM-? form a cluster, while NH and Lo-? are isolated and are thus far from the other kernels. All the remaining kernels are close to each other in the low dimensional space, but they do not form well-defined clusters. Interestingly, the SP and GH kernels which both extract shortest paths are very close to each other, while all the Weisfeiler-Lehman kernels (i. e., WL-VH, WL-SP, CORE-WL-VH, WL-OA and WL-PM) are also relatively close to each other in the 2-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Recent years have witnessed a tremendous increase in the availability of graph-structured data. Graphs arise in many different contexts where it is necessary to represent relationships between entities. Specifically, graphs are the commonly employed structure for representing data in various domains including bioinformatics, chemoinformatics, social networks and information networks. The abundance of graph-structured data and the need to perform machine learning tasks on this kind of data led to the development of several sophisticated approaches such as graph kernels. In this survey, we provided a detailed overview of graph kernels. Furthermore, we empirically evaluated the effectiveness of several graph kernels, and measured their running time. We hope that this survey will provide a better understanding of the current progress on graph kernels and graph classification, and offer some guidelines on how to apply these approaches in order to solve real-world problems.</p><p>Although graph kernels have achieved remarkable results in many tasks, there are still some challenges to be addressed, while there is also still some room for improvement. For example, the majority of the kernels that can handle graphs with continuous attributes are either very expensive in terms of computational complexity or fail to produce competitive results. Hence, we believe that an important direction of research is the development of scalable graph kernels for graphs annotated with continuous attributes which will also provide improvements over the state-of-the-art approaches. Another useful direction of research is to capitalize on the framework for designing valid assignment kernels presented above, and to develop new kernels which compute an optimal assignment between substructures extracted from graphs. In general, the complexity of the assignment kernels is more attractive than that of kernels that belong to the R-convolution framework, and hence, it is our belief that this framework can pave the way for the development of more efficient graph kernels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Taxonomy of graph kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Two graphs (top left and right) and their direct product (bottom). Each vertex of the direct product graph is labeled with a pair of vertices; an edge exists in the direct product if and only if the corresponding vertices are adjacent in both original graphs. For instance, nodes 1 ? 4 and 3 ? 5 are adjacent because there is an edge between vertices 1 and 3 in the first, and 4 and 5 in the second graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a; 2), (a; b; 1), (a; b; 2), (a; c; 2), (b; a; 1), (b; a; 2), (b; b; 1), (b; c; 2), (c; a; 2), (c; b; 1), (c; b; 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Example of decomposition of a graph into its four DAGs (one for each vertex). Two DAGs (left) and their associated tree visits T (u) starting from each vertex u (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c) Feature vectors (b) Hierarchy (a) Kernel matrixFigure 12: The matrix of a strong kernel on objects a, b and c (a) induced by the hierarchy (b) and the derived feature vectors (c). A vertex v in (b) is annotated by its weights w(v); ?(v).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 13: An assignment instance (a) for X, Y ? [X ] 5 and the derived histograms (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 14</head><label>14</label><figDesc>A graph whose vertices have been (b) Associated hierarchy relabeled three times (from left to right) : A graph G with uniform initial labels ? 0 and refined labels ? i for i ? {1, . . . , 3} (a), and the associated hierarchy (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Distribution of similarities between the synthetic graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :</head><label>17</label><figDesc>Correlation and mean squared error between the kernel values produced by the 20 kernels and the similarities produced by the function defined in Equation(100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Commonly used symbols and notations</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of selected graph kernels regarding computation by explicit feature mapping (Exp. ?)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Summary of the 17 datasets used in our experiments. The "Max Class Imbalance" column indicates the ratio of the size of the smallest class of the dataset to the size of its largest class.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>It is interesting to mention</figDesc><table><row><cell>Kernels</cell><cell></cell><cell cols="2">Fixed</cell><cell cols="2">Hyperparameters Chosen based on validation set performance</cell></row><row><cell>VH</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>RW</cell><cell>? = 10</cell><cell>log 10 (</cell><cell>1 deg 2 max</cell><cell>)</cell><cell>k ? {2, . . . 10, ?}</cell></row><row><cell>SP</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>GR</cell><cell></cell><cell cols="2">k = 5</cell><cell></cell><cell>n samples = {200, 500, 1000, 2000, 5000}</cell></row><row><cell>WL</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>h ? {4, . . . , 8}</cell></row><row><cell>WL-OA</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>h ? {4, . . . , 8}</cell></row><row><cell>NH</cell><cell cols="4">Count-sensitive neighborhood hash</cell><cell>h ? {1, . . . , 6}</cell></row><row><cell>NSPDK</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>r  *  ? {1, . . . , 6}, d  *  ? {3, . . . , 7}</cell></row><row><cell>Lo-?</cell><cell cols="3">2 ? |S| ? 8</cell><cell></cell><cell>n samples = {100, 200, 500, 1000}</cell></row><row><cell>SVM-?</cell><cell cols="3">2 ? |S| ? 8</cell><cell></cell><cell>n samples = {100, 200, 500, 1000}</cell></row><row><cell>ODD-STh</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>h ? {1, . . . , 11}</cell></row><row><cell>PM</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>L ? {2, 4, 6}, d ? {4, 6, 8, 10}</cell></row><row><cell>GH</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>linear kernel/gaussian kernel</cell></row><row><cell>SM</cell><cell></cell><cell cols="2">k = 3</cell><cell></cell><cell>-</cell></row><row><cell>PK</cell><cell cols="3">w = 10 ?5</cell><cell></cell><cell>T ? {1, . . . , 6}</cell></row><row><cell>ML</cell><cell cols="4">? = 0.01, ? = 0.01,p = 10</cell><cell>lmax ? {0, . . . , 5},c ? {50, 100, 200, 300}</cell></row><row><cell>CORE</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Values of the hyperparameters of the graphs kernels and frameworks included in our experimental comparison. Note that for some kernels, only a subset of the hyperparameters was optimized, while the rest of the hyperparameters were kept fixed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Average classification accuracy (? standard deviation) on the 7 classification datasets containing node-labeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel/GNN. The lower the average rank, the better the overall performance of the kernel/GNN.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DATASETS</cell><cell></cell></row><row><cell>Kernels</cell><cell>MUTAG</cell><cell>ENZYMES</cell><cell>NCI1</cell><cell>PTC-MR</cell></row><row><cell>VH</cell><cell>0.01s</cell><cell>0.04s</cell><cell>0.84s</cell><cell>0.02s</cell></row><row><cell>RW</cell><cell>1m 31.24s</cell><cell>3h 36m 7.01s</cell><cell>TIMEOUT</cell><cell>9m 9.27s</cell></row><row><cell>SP</cell><cell>0.92s</cell><cell>11.03s</cell><cell>1m 9.69s</cell><cell>1.52s</cell></row><row><cell>WL-VH</cell><cell>0.2s</cell><cell>3.54s</cell><cell>7m 5.11s</cell><cell>0.42s</cell></row><row><cell>WL-SP</cell><cell>6.3s</cell><cell>1m 15.06s</cell><cell>10m 55.37s</cell><cell>10.78s</cell></row><row><cell>WL-PM</cell><cell cols="4">2m 0.87s 1h 10m 28.36s 13h 28m 58.53s 11m 53.67s</cell></row><row><cell>WL-OA</cell><cell>0.65s</cell><cell>21.89s</cell><cell>2h 27m 25.05s</cell><cell>3.29s</cell></row><row><cell>NH</cell><cell>0.98s</cell><cell>12.65s</cell><cell>13m 56.45s</cell><cell>4.35s</cell></row><row><cell>NSPDK</cell><cell>3.94s</cell><cell>25.77s</cell><cell>4m 29.99s</cell><cell>7.81s</cell></row><row><cell>ODD-STh</cell><cell>1.49s</cell><cell>1m 2.86s</cell><cell>49m 2.76s</cell><cell>4.2s</cell></row><row><cell>PM</cell><cell>3.17s</cell><cell>30.86s</cell><cell>41m 51.78s</cell><cell>13.14s</cell></row><row><cell>GH</cell><cell>24.79s</cell><cell>15m 35.62s</cell><cell>3h 43m 7.2s</cell><cell>1m 33.9s</cell></row><row><cell>SM</cell><cell cols="2">1m 57.25s 3h 25m 43.59s</cell><cell>TIMEOUT</cell><cell>4m 19.8s</cell></row><row><cell>PK</cell><cell>0.53s</cell><cell>11.71s</cell><cell>10m 30.02s</cell><cell>1.79s</cell></row><row><cell>ML</cell><cell>8m 16.84s</cell><cell>58m 40.97s</cell><cell>7h 18m 35.72s</cell><cell>22m 9.56s</cell></row><row><cell>CORE-WL-VH</cell><cell>0.63s</cell><cell>7.95s</cell><cell>12m 36.28s</cell><cell>1.01s</cell></row><row><cell>CORE-SP</cell><cell>2.69s</cell><cell>48.02s</cell><cell>3m 16.54s</cell><cell>3.97s</cell></row><row><cell>Kernels</cell><cell>D&amp;D</cell><cell>DATASETS PROTEINS</cell><cell>AIDS</cell><cell>Avg. Rank</cell></row><row><cell>VH</cell><cell>0.24s</cell><cell>0.1s</cell><cell>0.25s</cell><cell>1.0</cell></row><row><cell>RW</cell><cell>OUT-OF-MEM</cell><cell cols="2">51m 10.11s 1h 51m 56.47s</cell><cell>15.1</cell></row><row><cell>SP</cell><cell>55m 58.79s</cell><cell>1m 18.91s</cell><cell>13.93s</cell><cell>4.7</cell></row><row><cell>WL-VH</cell><cell>4m 42.13s</cell><cell>25.34s</cell><cell>28.89s</cell><cell>2.7</cell></row><row><cell>WL-SP</cell><cell>6h 42m 57.36s</cell><cell>6m 55.52s</cell><cell>1m 22.62s</cell><cell>10.7</cell></row><row><cell>WL-PM</cell><cell cols="3">OUT-OF-MEM 6h 20m 51.01s 6h 44m 21.01s</cell><cell>15.8</cell></row><row><cell>WL-OA</cell><cell>1h 22m 26.27s</cell><cell>3m 48.99s</cell><cell>4m 36.18s</cell><cell>8.6</cell></row><row><cell>NH</cell><cell>11m 31.88s</cell><cell>1m 4.52s</cell><cell>1m 16.5s</cell><cell>6.9</cell></row><row><cell>NSPDK</cell><cell>5h 15m 23.52s</cell><cell>6m 35.72s</cell><cell>56.01s</cell><cell>8.7</cell></row><row><cell>ODD-STh</cell><cell>30m 39.54s</cell><cell>2m 6.26s</cell><cell>1m 57.88s</cell><cell>9.0</cell></row><row><cell>PM</cell><cell>4m 55.53s</cell><cell>1m 15.8s</cell><cell>5m 33.7s</cell><cell>9.0</cell></row><row><cell>GH</cell><cell cols="2">TIMEOUT 3h 44m 19.99s</cell><cell>38m 48.57s</cell><cell>13.8</cell></row><row><cell>SM</cell><cell>OUT-OF-MEM</cell><cell cols="2">OUT-OF-MEM 4h 26m 46.71s</cell><cell>15.7</cell></row><row><cell>PK</cell><cell>7m 29.57s</cell><cell>45.6s</cell><cell>1m 46.21s</cell><cell>5.1</cell></row><row><cell>ML</cell><cell>1h 26m 59.75s</cell><cell>1h 35m 36.4s</cell><cell>33m 16.63s</cell><cell>14.1</cell></row><row><cell>CORE-WL-VH</cell><cell>1m 53.21s</cell><cell>1m 11.44s</cell><cell>1m 15.03s</cell><cell>4.4</cell></row><row><cell>CORE-SP</cell><cell>5h 2m 39.71s</cell><cell>3m 31.97s</cell><cell>40.11s</cell><cell>7.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Average CPU running time for kernel matrix computation on the 6 classification datasets containing unlabeled graphs. The "Avg. Rank" column illustrates the average rank of each kernel. The lower the average rank, the lower the overall running time of the kernel.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Average classification accuracy (? standard deviation) on the 5 classification datasets containing node-attributed graphs. The "Avg. Rank" column illustrates the average rank of each kernel/GNN. The lower the average rank, the better the overall performance of the kernel/GNN.</figDesc><table><row><cell>Kernels</cell><cell>ENZYMES</cell><cell cols="2">DATASETS PROTEINS full SYNTHETICnew</cell><cell>Synthie</cell><cell>BZR</cell><cell>Avg. Rank</cell></row><row><cell>SP</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>-</cell></row><row><cell>GH</cell><cell>16m 36.48s</cell><cell>3h 15m 5.36s</cell><cell>12m 37.05s</cell><cell>17m 44.28s</cell><cell>4m 11.74s</cell><cell>2.2</cell></row><row><cell>SM</cell><cell>TIMEOUT</cell><cell>OUT-OF-MEM</cell><cell>TIMEOUT</cell><cell>TIMEOUT</cell><cell>6h 15m 59.36s</cell><cell>4.0</cell></row><row><cell>PK</cell><cell>14.93s</cell><cell>1m 10.25s</cell><cell>14.43s</cell><cell>11.37s</cell><cell>7.24s</cell><cell>1.0</cell></row><row><cell>ML</cell><cell>1h 4m 52.54s</cell><cell>2h 48m 24.59s</cell><cell cols="2">2h 49m 35.19s 1h 44m 10.48s</cell><cell>47m 44.62s</cell><cell>2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Summary of the synthetic dataset that we used in our experiment.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple Graph-Kernel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Symposium Series on Computational Intelligence</title>
		<meeting>the 2015 Symposium Series on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Graph Kernel for Protein-Protein Interaction Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bj?rne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing</title>
		<meeting>the Workshop on Current Trends in Biomedical Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bj?rne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning</title>
		<idno>S2</idno>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DDGK: Learning Graph Representations for Deep Divergence Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference</title>
		<meeting>the 2019 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale networks fingerprinting and visualization using the k-core decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alvarez-Hamelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dall&amp;apos;asta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-based malware detection using dynamic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Storlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal in Computer Virology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="258" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Relational Kernel-Based Framework for Hierarchical Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</title>
		<meeting>the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An application of kernel methods to gene cluster temporal meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antoniotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farinaccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zoppis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1361" to="1368" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph kernels between point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Aligned Subtree Kernel for Weighted Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Graph Kernel Based on the Jensen-Shannon Representation Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence</title>
		<meeting>the 24th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3322" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histogram intersection kernel for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Odone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 International Conference on Image Processing</title>
		<meeting>the 2003 International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="513" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast algorithms for determining (generalized) core groups in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaver?nik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Data Analysis and Classification</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interaction Networks for Learning about Objects,Relations and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text Categorization of Biomedical Data Sets using Graph Kernels and a Controlled Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bleik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1211" to="1217" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph Reconstruction-A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hemminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Theory</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="268" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph Kernels: State-of-the-Art and Future Challenges. Foundations and Trends? in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>O&amp;apos;bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Ludwig-Maximilian University of Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining</title>
		<meeting>the 5th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph kernels for disease outcome prediction from protein-protein interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Training Algorithm for Optimal Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Workshop on Computational Learning Theory</title>
		<meeting>the 5th Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compound analysis via graph kernels incorporating chirality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Urata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">supp01</biblScope>
			<biblScope unit="page" from="63" to="81" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">People re-identification by Graph Kernels Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Shortest Path Dependency Kernel for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification of small molecules by two-and three-dimensional decomposition kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ceroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2038" to="2045" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LIBSVM: A Library for Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional Kernel Networks for Graph-Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolution Kernels for Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Thirty years of graph matching in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="265" to="298" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast Neighborhood Subgraph Pairwise Distance Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>De Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured Lexical Similarity via Convolution Kernels on Dependency Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1034" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dependency Tree Kernels for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Tree-Based Kernel for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM International Conference on Data Mining</title>
		<meeting>the 2012 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="975" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Substructure counting graph kernels for machine learning from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K D</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Rooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="71" to="84" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structure-Activity Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. Correlation with Molecular Orbital Energies and Hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kernel k-means, Spectral Clustering and Normalized Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distinguishing Enzyme Structures from Non-enzymes Without Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">KONG: Kernels for orderedneighborhood graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Draief</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vojnovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph kernels and gaussian processes for relational reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Driessens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="91" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Fair Comparison of Graph Neural Networks for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning molecular energies using localized graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">114107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-Sentence Compression: Finding Shortest Paths in Word Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Characterizing Structural Relationships in Scenes Using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optimal Assignment Kernels For Attributed Molecular Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fr?hlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sieker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Computers and intractability: a guide to NPcompleteness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>WH Freeman and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Survey of Kernels for Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On Graph Kernels: Hardness Results and Efficient Alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structural Detection of Android Malwareusing Embedded Call Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 2013 ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two New Graph Kernels and Applications to Chemoinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<meeting>the 8th International Workshop on Graph-Based Representations in Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The journey of graph kernels through two decades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="88" to="111" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The pyramid quantized Weisfeiler-Lehman graph representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gkirtzou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="1495" to="1507" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recognizing Identical Events with Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>&amp;amp;?najder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="797" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A general coefficient of similarity and some of its properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="857" to="871" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Classification on Pairwise Proximity Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bollmann-Sdorra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="438" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Pyramid Match Kernel: Efficient Learning with Sets of Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="725" to="760" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chemoinformatics and stereoisomerism: A stereo graph kernel together with three new extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Grenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph Similarity and Approximate Isomorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Woeginger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Mathematical Foundations of Computer Science</title>
		<meeting>the 43rd International Symposium on Mathematical Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning Models of Relational MDPs Using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Halbritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Mexican International Conference on Artificial Intelligence</title>
		<meeting>the 6th Mexican International Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Image Classification with Segmentation Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2007 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of California in Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Entity Disambiguation in Anonymized Graphs using Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hermansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1037" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A Linear-time Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IEEE International Conference on Data Mining</title>
		<meeting>the 9th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cyclic Pattern Kernels for Predictive Graph Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Lov?sz ? function, SVMs and Finding Dense Subgraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3495" to="3536" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Sub-network Based Kernels for Brain Network Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="622" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Topological Graph Kernel on Multiple Thresholded Functional Connectivity Networks for Mild Cognitive Impairment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2876" to="2897" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Global graph kernels using geometric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jethava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="694" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Predicting metamorphic relations for testing scientific software: a machine learning approach using graph kernels. Software Testing, Verification and Reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kanewala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bieman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="245" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Marginalized Kernels Between Labeled Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Conference in Machine Learning</title>
		<meeting>the 20th Conference in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hadamard Code Graph Kernels for Classifying Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the 5th International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The Multiscale Laplacian Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Diffusion Kernels on Graphs and Other Discrete Input Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Machine Learning</title>
		<meeting>the 19th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Subgraph Matching Kernels for Attributed Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Explicit versus Implicit Graph Feature Maps:A Computational Phase Transition for Walk Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE International Conference on Data Mining</title>
		<meeting>the 2014 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On Valid Optimal Assignment Kernels and Applications to Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A Property Testing Framework for the Theoretical Expressivity of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2348" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A graph kernel approach for alignment-free domain-peptide interaction prediction with an application to human sh3 domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Backofen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Deriving Neural Architectures from Sequence and Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A note on the derivation of maximal common subgraphs of two directed or undirected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calcolo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">341</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Nested Subtree Hash Kernels for Large-scale Graph Classification over Streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining</title>
		<meeting>the 12th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">3d human motion retrieval using graph kernels based on adaptive graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="104" to="112" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sch?f</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Software Reuse</title>
		<meeting>the 15th International Conference on Software Reuse</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="315" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Graph Kernels for RDF Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>L?sch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Extended Semantic Web Conference</title>
		<meeting>the 9th Extended Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="134" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">On the shannon capacity of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lov?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Object Classification Based On Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahboubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-X</forename><surname>Dup?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 International Conference on High Performance Computing &amp; Simulation</title>
		<meeting>the 2010 International Conference on High Performance Computing &amp; Simulation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Extensions of marginalized graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mah?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Graph Kernels for Molecular Structure-Activity Relationship Analysis with SupportVector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mah?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Perret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="951" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Graph kernels based on tree patterns for molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mah?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="3" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Hyper-Parameter Tuning for Graph Kernels via Multiple Kernel Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Massimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Neural Information Processing</title>
		<meeting>the 23rd International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Smallest-last Ordering and Clustering and Graph Coloring Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="427" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Neural Network for Graphs: A Contextual Constructive Approachs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Decoding brain states using backward edge elimination and graph kernels in fMRI connectivity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Hossein-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Conference on Data Mining</title>
		<meeting>the 2017 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Faster Kernels for Graphs with Continuous Attributes via Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE International Conference on Data Mining</title>
		<meeting>the 16th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21824" to="21840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A Study on Convolution Kernels for Shallow Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="335" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th European Conference on Machine Learning</title>
		<meeting>the 17th European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Making Tree Kernels practical for Natural Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter</title>
		<meeting>the 11th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Tree Kernels for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="224" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Relational Pooling for Graph Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Joint Conference on Neural Networks</title>
		<meeting>the 2016 International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4701" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06930</idno>
		<title level="m">Pre-training Graph Neural Networks with Kernels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Graph Kernels for Object Category Predictionin Task-Dependent Robot Grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Mining and Learning with Graphs</title>
		<meeting>the 11th Workshop on Mining and Learning with Graphs</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A Degeneracy Framework for Graph Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2595" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Shortest-path Graph Kernels for Document Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stavrakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Kernel Graph Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-P</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Artificial Neural Networks</title>
		<meeting>the 27th International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Matching Node Embeddings for Graph Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Enhancing Graph Kernels via Successive Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1583" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Random Walk Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16211" to="16222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Measuring the expressivity of graph kernels through statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Graph Invariant Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pr?ulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Graph Kernels for Chemical Informatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1110" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Expressivity versus Efficiency of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Mining Graphs, Trees and Sequences</title>
		<meeting>the 1st International Workshop on Mining Graphs, Trees and Sequences</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A Persistent Weisfeiler-Lehman Procedure for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</title>
		<meeting>the Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition and Structural and Syntactic Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Graph Kernels for Molecular Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="266" to="273" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Directed acyclic graph kernels for structural RNA analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mituyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakakibara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">318</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Transitive Assignment Kernels for Structural Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schiavinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasparetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Similarity-Based Pattern Recognition</title>
		<meeting>the 3rd International Workshop on Similarity-Based Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Predicting Protein Function and Protein-Ligand Interaction with the 3D Neighborhood Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Discovery Science</title>
		<meeting>the 18th International Conference on Discovery Science</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Structure and Minimum Degree. Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A Method Based on the Granger Causality and Graph Kernels for Discriminating Resting State from Attentional Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shahnazian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Hossein-Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Biomedical Engineering</title>
		<meeting>the 2012 International Conference on Biomedical Engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Efficient Graphlet Kernels for Large Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Grakel: A graph kernel library in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">54</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Graph wavelet alignment kernels for drug virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lushington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="473" to="497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Kernels and Regularization on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th Conference on Learning Theory</title>
		<meeting>16th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Fast kernels for string and tree matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Supervised Neural Networks for the Classification of Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Automatic architectural style detection using one-class support vector machines and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Automation in Construction</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Robust Visual Place Recognition with Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4535" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A Fast Kernel for Attributed Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Harang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Evolution, Detection and Analysis of Malware forSmart Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Suarez-Tangil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Tapiador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peris-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribagorda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="961" to="987" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Halting in Random Walk Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1639" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Spline-Fitting with a Genetic Algorithm: A Method for Developing ClassificationS tructure-Activity Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>O&amp;apos;brien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1906" to="1915" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Color Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Graph-Based Inter-Subject Pattern Analysis of fMRI Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takerkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Auzias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">104586</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Wasserstein Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6439" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the Predictive Toxicology Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Brain Decoding via Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Workshop on Pattern Recognition in Neuroimaging</title>
		<meeting>the 2013 International Workshop on Pattern Recognition in Neuroimaging</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="136" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Classification of inter-subject fMRI data based on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vega-Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Workshop on Pattern Recognition in Neuroimaging</title>
		<meeting>the 2014 International Workshop on Pattern Recognition in Neuroimaging</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">A tree kernel to analyse phylogenetic profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="276" to="284" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">The optimal assignment kernel is not positive definite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno>abs/0801.4061</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Malware analysis with graph kernels and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wagener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Malicious and Unwanted Software</title>
		<meeting>the 4th International Conference on Malicious and Unwanted Software</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Directed Acyclic Graph Kernels for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Inequalities for the l 1 deviation of the empirical distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Using the Nystr?m Method to Speed Up Kernel Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Human Action Recognition Based on Context-Dependent Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2609" to="2616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">A Structural Smoothing Framework For Robust Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4801" to="4811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Kernel Methods for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Fast multi-view segment graph kernel for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">RetGK: Graph Kernels based on Return Probabilities of Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3968" to="3978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
