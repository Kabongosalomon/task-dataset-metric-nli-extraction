<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ITERATIVE HIERARCHICAL ATTENTION FOR ANSWER- ING COMPLEX QUESTIONS OVER LONG DOCUMENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
							<email>haitians@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ITERATIVE HIERARCHICAL ATTENTION FOR ANSWER- ING COMPLEX QUESTIONS OVER LONG DOCUMENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new model, DOCHOPPER, that iteratively attends to different parts of long, heirarchically structured documents to answer complex questions. Similar to multi-hop question-answering (QA) systems, at each step, DOCHOPPER uses a query q to attend to information from a document, combines this "retrieved" information with q to produce the next query. However, in contrast to most previous multi-hop QA systems, DOCHOPPER is able to "retrieve" either short passages or long sections of the document, thus emulating a multi-step process of "navigating" through a long document to answer a question. To enable this novel behavior, DOCHOPPER does not combine document information with q by concatenating text to the text of q, but by combining a compact neural representation of q with a compact neural representation of a hierarchical part of the document, which can potentially be quite large. We experiment with DOCHOPPER on four different QA tasks that require reading long and complex documents to answer multi-hop questions, and show that DOCHOPPER achieves state-of-the-art results on three of the datasets. Additionally, DOCHOPPER is efficient at inference time, being 3-10 times faster than the baselines. 1 1  We will open-source our code and data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In this work we focus on the problem of answering complex questions over long structured documents. A long document typically contains coherent information on a certain topic, and the contents are grouped by sections or other structures. To answer complex, multi-hop questions over long documents often requires navigating through different parts of the documents to find different pieces of information relevant to a question. This navigation, in turn, requires understanding high-level information about the structure of the document. For example, to answer the question "What modules in DOCHOPPER will be finetuned in all the experiments?", one might first turn to the section title "Model" to identify the different modules in DOCHOPPER, and then read the "Experiments" with these modules in mind, potentially further attending to specific subsections (such as the ones titled "Implementation Details"). As in academic papers <ref type="bibr" target="#b4">(Dasigi et al., 2021)</ref>, similar tasks are common for questions about government policies <ref type="bibr" target="#b21">(Saeidi et al., 2018)</ref> or legal documents. This type of QA tests not only the ability to understand short passages of text, but also the ability to understand the goals of the question and the structure of documents in a domain.</p><p>A common approach of solving multi-hop questions is to iteratively find evidence for one hop, and then use that evidence to update the query used in the next hop of the QA process. The update can be performed by either explicitly predicting the intermediate answers <ref type="bibr" target="#b25">(Talmor &amp; Berant, 2018;</ref> or directly appending previous evidences to the questions <ref type="bibr">(Zhao et al., 2021;</ref><ref type="bibr" target="#b18">Qi et al., 2021;</ref><ref type="bibr">Xiong et al., 2021)</ref>. While appending retrieved evidence to a query works well on many factual QA tasks, where it is possible to answer questions with evidences that are short pieces of text, this approach is expensive if one wishes to retrieve larger pieces of text as evidences (e.g., the "experiments" section of a paper). Another disadvantage is that appending together many small fragments of text intuitively fails to capture the relationships between them, and the structure of the document from which they were extracted.</p><p>To capture high-level structural information in a document as well as detailed information from short passages, hierarchical attention mechanisms have been proposed, which learn neural representations at different levels that are then mixed to make final predictions for simple questions <ref type="bibr" target="#b27">(Wang et al., 2018;</ref>. Hierarchical attention has also been adopted in pretrained language models, e.g. ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref>, which introduced a global-local attention mechanism where embeddings of special global tokens are used to encode high-level information. Our DOCHOPPER system incorporates ETC as a document encoder. However, while ETC has previously performed well on multi-hop QA tasks like HotpotQA and <ref type="bibr">WikiHop (Yang et al., 2018;</ref><ref type="bibr" target="#b28">Welbl et al., 2018)</ref> which require combining information from a small number of short passages, it has not been previously evaluated on tasks of the sort considered here. Our experiments show that DOCHOPPER outperforms past approaches to using ETC for multi-hop questions.</p><p>DOCHOPPER extends the existing hierarchical attention methods with a novel approach to updating queries in a multi-hop setting. DOCHOPPER iteratively attends to different parts of the document, either at fine-grained level or at higher level. This process can be viewed as either retrieving a short passage, or navigating to a part of a document. In each iteration, the query vector is updated in the embedding space rather than by re-encoding a sequence of concatenated tokens. This updating step is end-to-end differentiable and efficient. In our experiments, we also show it is effective on four different benchmarks involving complex queries over long structured documents.</p><p>In particular, we evaluate DOCHOPPER on four different tasks: conversational QA for discourse entailment reasoning, using the ShARC <ref type="bibr" target="#b21">(Saeidi et al., 2018)</ref> benchmark 2 ; factual QA with table and text, using HybridQA ; information seeking QA on academic papers, using QASPER <ref type="bibr" target="#b4">(Dasigi et al., 2021)</ref>; and multi-hop factual <ref type="bibr">QA, using HotpotQA (Yang et al., 2018)</ref>. Since the outputs of the four tasks are different, additional layers or simple downstream models are appended to DOCHOPPER to get the final answers. DOCHOPPER achieves state-of-the-art results on three of datasets, outperforming the baseline models by 3%-5%. Additionally, DOCHOPPER runs 3-10 faster than the baseline models, because the neural representations of documents is pre-computed, which significantly reduces computation cost at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Graph-based models have been widely used for answering multi-hop questions in factual QA <ref type="bibr" target="#b22">Sun et al., 2018;</ref><ref type="bibr" target="#b19">Qiu et al., 2019;</ref><ref type="bibr" target="#b8">Fang et al., 2019)</ref>. However, most of the graphbased models are grounded to entities, i.e., evidences (from knowledge bases or text corpus) are connected by entities in the graph. The graph construction step also heavily relies on many discrete features such as hyperlinks or entities predicted with external entity linkers. It's not clear how to apply these models to more general tasks if context is not entity-centric, such as questions about academic papers or government documents. Similar problems also exist in memory-augmented language models that achieved the state-of-the-art on many factual QA tasks <ref type="bibr" target="#b11">(Guu et al., 2020;</ref><ref type="bibr" target="#b14">Lewis et al., 2021;</ref><ref type="bibr" target="#b26">Verga et al., 2020;</ref><ref type="bibr" target="#b6">Dhingra et al., 2020;</ref><ref type="bibr" target="#b24">Sun et al., 2021)</ref>.</p><p>Alternatively, one can adopt the "retrieve and read" pipeline to answer multi-hop questions over long documents. Recent works proposed to extend the dense retrieval methods <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> to multi-hop questions <ref type="bibr">(Zhao et al., 2021;</ref><ref type="bibr" target="#b18">Qi et al., 2021;</ref>. However, such models retrieve one small piece of evidence at a time, lacking the ability of navigating between different parts of the documents to find relevant information at both higher and lower levels of the documentstructure hierarchy. Another disadvantage of these iterative models is that they are not end-to-end differentiable. Updating the questions for the next hop requires re-encoding the concatenated tokens from the questions and previously retrieved evidences. It also makes the model inefficient because re-encoding tokens with large Transformer models is very expensive.</p><p>Besides question answering tasks, hierarchical attention has been successfully used in tasks such as document classification <ref type="bibr">(Yang et al., 2016;</ref>, summarization <ref type="bibr" target="#b10">(Gidiotis &amp; Tsoumakas, 2020;</ref><ref type="bibr">Xiao &amp; Carenini, 2019;</ref><ref type="bibr" target="#b19">Zhang et al., 2019)</ref>, sentiment analysis <ref type="bibr" target="#b20">(Ruder et al., 2016)</ref>, text segmentation <ref type="bibr" target="#b13">(Koshorek et al., 2018)</ref>, etc. It is worth mentioning that ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> was also used on a key-phrase extraction task on web pages using the structured DOM Figure 1: DOCHOPPER Overview for a structured document consisting of sentences and paragraphs. The query encoder first computes embeddings q 0 , . . . , q n for a sequence of sub-questions. During the iterative attention process, DOCHOPPER attends to a paragraph or a sentence from the context embedding table that contains both paragraph embeddings and sentence embeddings. Information in the attended sentence or paragraph will be mixed with the query vector to compute query update. If the query attends to a paragraph, e.g. the first query q 0 in the figure, the query vector will be broadcast to the associated sentences. If the query attends to a sentence, e.g. the second query q 1 , only the sentence s 0 1 will be used. DOCHOPPER then updates the query vector q 1 for the next round of attention. The selected sentences will be used to make final predictions. tree. However, none of these models can be easily adapted to answering complex questions over long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>In this section, we first introduce how to compute neural representations for questions and context at different levels. Then, we present the iterative process that attends to information in the document and updates the query vector. Additional layers or simple downstream models required for different downstream tasks will be explained in the next section ( ?4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BACKGROUND</head><p>Input A long document usually contains multiple levels of hierarchy, e.g. sections, sub-sections, paragraphs, sentences, etc. For simplicity, we only consider two levels of hierarchy in this paper: paragraph-level and sentence-level. A sentence is the lowest granularity that can be retrieved, while a paragraph is an abstraction of a collection of sentences, which can be used to represent sections or other levels in the hierarchy, depending on the application. Formally, let d = {p 0 , . . . , p |d| } ? D be a document in the corpus D that contains a sequence of paragraphs p i , and let a paragraph p i = {s i 0 , . . . , s i |pi| } contain a sequence of sentences s i 0 . A sentence s i j will be encoded into a fixed length vector s i j ? R d . Pretrained ETC We use ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> as our query and context encoder as it is pretrained to produce both token-level and sentence-level embeddings. ETC is pretrained as a Mask Language Model (MLM). Different from BERT , it employs an additional global-local attention mechanism. ETC assigns to each sentence a special global token that only attends to local tokens in the sentence, and its embedding is trained to summarize the information of local tokens in the sentence. A global token also attends the global tokens of other sentences in the input. ETC additionally adopts Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b17">(Oord et al., 2018)</ref> to train the embedding of global tokens to make them aware of other sentences in the context. We use the embeddings of global tokens in ETC as encodings for sentences.</p><p>Specifically, ETC takes a list of sentences p i = {s i 0 , . . . , s i |pi| } as input, and returns a list of vectors s i 0 , . . . , s i |pi| , where each s i j ? R d represents the embedding of a sentence s i j .</p><formula xml:id="formula_0">s i 0 , . . . , s i |pi| = ETC({s i 0 , . . . , s i |pi| }) ? R |pi|?d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">QUERY EMBEDDINGS</head><p>We discuss the strategy of computing query vectors for two types of input, conversational QA and multi-hop QA. We use ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref> as our query encoder for the reasons discussed above. Other pretrained language models for similar purposes should work as well.</p><p>Conversational QA A conversational question contains multiple rounds of interaction between the machine and human. As the conversation goes on, the topic of the conversation will shift, but questions asked in the next turn are related to the history of the conversation. We consider this multi-turn process as a multi-hop QA task. In our specific task, the conversation starts with an initial question q 0 . The model will answer the question if there's enough information. Otherwise, the model will ask a followup question f 1 and expect the user's answer a 1 . The model will keep asking followup questions for a few more iterations. The task is to predict the answer if there is enough information after reading the full conversation history, or mark the question as not answerable.</p><p>We denote a i as the answer to the followup question f i received from the users at the i'th iteration. We consider the full conversation history as the question p q = {q 0 , (f 1 , a 1 ), . . . , (f n , a n )} with the initial question denoted q 0 and the i-th followup question-answer pair denoted (f i , a i ). The answer a i to the followup question f i is concatenated to the end of the questions and represented as a single sentence. We call p q a question paragraph as it contains a sequence of question-answer pairs. We use ETC to compute the query embeddings q i , one for each sentence in the query paragraph p q . The query embeddings q i will be used to perform iterative attention over the document.</p><formula xml:id="formula_1">q 0 , . . . , q n = ETC q ({q 0 , (f 1 , a 1 ), . . . , (f n , a n )})<label>(1)</label></formula><p>Multi-hop QA A multi-hop question, e.g. "Which gulf is north of the Somalian city with 550,000 residents", requires the model to first find the answer of the first "hop" of the question, and then perform a second round to find the final answer. Assume for now there are exactly two hops. Different from conversational QA, questions in multi-hop QA do not have a clear split between the two hops of questions, making it impossible to explicitly split a question into two sub-questions. Instead, we add a dummy question q null to the question paragraph q p = {q 0 , q null }. The global-to-local attention mask of ETC is modified to allow the global token of the dummy question to attend to tokens in the question q 0 . With this modification, the query embeddings q 0 and q 1 for q 0 and q null can to attend any part of the question, but each can also attend to different parts of the question. One could append additional dummy questions to the question paragraph p q if the number of hops is larger. Thus we let</p><formula xml:id="formula_2">q 0 , q 1 = ETC q ({q 0 , q null })<label>(2)</label></formula><p>Note that we assume the true number of hops is known in multi-hop QA tasks, which is two or four for the experiments in this paper. However, one can train models to decide when to stop if questions have various numbers of hops. We leave this as a topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NEURAL REPRESENTATIONS OF PARAGRAPHS</head><p>We compute embeddings s i j for the sentence s i j with the context encoder. We use ETC as our context encoder, but similar pretrained language models will also work. Recall that a paragraph p i = {s i 0 , . . . , s i |pi| } contains a sequence of sentences s i j . The sentence embeddings s i j in the paragraph p i are simply computed by applying ETC on the paragraph input.</p><formula xml:id="formula_3">s i 0 , . . . , s i |pi| = ETC c ({s i 0 , . . . , s i |pi| })</formula><p>The paragraph embeddings are directly derived from sentence embeddings s i j and dependent on the queries q t , the embedding of the t'th hop of the question. The paragraph embedding p i is a weighted sum of sentence embeddings s i j in the paragraph p i , where ? j is the attention weights of the query vector q t to the sentence embedding s i j . The paragraph embeddings p i are thus dependent on the query, but do not require jointly encoding tokens from queries and context, as in many BERT-style reading comprehension models. Computing paragraph embeddings with Eq.3 is hence very efficient.</p><formula xml:id="formula_4">p i = j ? j s i j , ? j = softmax(q T t s i j )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ITERATIVE ATTENTION</head><p>We put the sentence embeddings and paragraph embeddings of document d into a combined context embedding table, so the model has the flexibility to decide which sentence or paragraph to attend to. Different update rules will be applied according to whether sentences or paragraphs are attended to.</p><p>To construct the embedding table, we iterate through all paragraphs in a document and apply the ETC encoder on each paragraph to compute the paragraph and sentences embeddings. Sentence and paragraph embeddings from all paragraphs are then merged together to form the combined embedding table. We denote the combined embedding table for document d as</p><formula xml:id="formula_5">C d = {p 0 , s 0 0 , . . . , s 0 |p0| , p 1 , s 1 0 , . . . , s 1 |p1| , . . . }.</formula><p>Let c m be the embedding of the m'th entry from C d ; we emphasize that c m can represent either a sentence or a paragraph embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention</head><p>Step At each iteration, DOCHOPPER computes the inner product scores between the query vector q t and embeddings c m in C d , and returns the entry? with the largest score, which is usually referred as hard attention. (As we will see? is not directly used for computation, but it is helpful in explaining the attention method).</p><formula xml:id="formula_6">c = argmax cm (q T t c m )</formula><p>Mixing</p><p>Step DOCHOPPER then mixes the embedding of the attended entry? with the query vector q t to find the missing information. Since the combined embedding table C d contains both sentence and paragraph embeddings, the selected entry? can represent either a sentence or a paragraph. The two cases are separately considered. If? is a sentence, i.e.? = s i j , DOCHOPPER computes the mixed embeddings asq</p><formula xml:id="formula_7">t = W T q [q t ; s i j ]<label>(4)</label></formula><p>where [q t ; s i j ] is the concatenation of two vectors q t and s i j . Assume? is a paragraph, i.e.,? = p i , DOCHOPPER first looks up sentences in p i , i.e. the list {s i 0 , . . . , s i |pi| }. The following process is then used to compute the mixed embeddingq t : (1) DOCHOPPER computes the attention weights of the query vector q t to the embeddings of associated sentences {s i 0 , . . . , s i |pi| } that measures the relevance scores between the query vector and the sentences. This attention weight is the same as the weight ? j in Eq.3 that is used to compute the paragraph embeddings, so we reuse ? j in this equation. In the implementation, we also re-use the value of ? j if it has been computed for the query-dependent paragraph embeddings. (2) The query vector q t is compared with every sentences in paragraph to extract missing information. q t is broadcast to the sentence embedding s i j with the weight ? j , i.e. the query vector is more important if more relevant. The comparison is performed as a linear projection of the concatenated query and sentence embeddings, q t and s i j .</p><formula xml:id="formula_8">k j = W T q [? j q t ; s i j ]<label>(5)</label></formula><p>Then <ref type="formula" target="#formula_4">(3)</ref> the concatenated vectors k j are summed with the weight ? j , where ? j is the attention weight of a learned vector v to the concatenated vector k j . The learned vector v coordinates the importance of sentences from the attended paragraph after comparing them with the query vector and decides what information to pass to the next step of retrieval.</p><formula xml:id="formula_9">q t = j ? j k j , ? j = softmax(v T k j )<label>(6)</label></formula><p>It is not hard to see that computing the mixed embedding in Eq. 6 for the case that a paragraph is retrieved is essentially the same as in Eq. 4 if the retrieved paragraph p i only contains one sentence, i.e. ? j = 1 and ? j = 1 if |p i | = 1; hence the same logic can be used regardless of whether? is a sentence or a paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update</head><p>Step The mixed embeddingq t is then used to update the query vector q t+1 for the next step. Intuitively,q t is the residual from the previous step. Adding the residual embedding encourages the model to attend to information that is not fully satisfied from previous steps.</p><formula xml:id="formula_10">q t+1 ? q t+1 +q t<label>(7)</label></formula><p>Loss Function Attention is supervised if (distantly) supervised labels are available in the dataset. q T t c m is the inner product score between the query vector q t and a context embedding c m . I cm is an indicator function that equals to 1 iff the label of c m is positive.</p><formula xml:id="formula_11">l t = cross entropy(softmax(q T t c m ), I cm )</formula><p>The loss function is computed at the final step, and possibly at intermediate steps if labels are available. Supervision labels are sometimes distantly constructed. For example, in the extractive QA task, a positive candidate is the sentence or paragraph that contains the answer span (see ?4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">RUNTIME EFFICIENCY</head><p>DOCHOPPER is very efficient at runtime thanks to the precomputed context embeddings ( ?3.3) at inference time. Different from previous reading comprehension models that jointly encode questions and context <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b0">Ainslie et al., 2020)</ref>, DOCHOPPER encode question embeddings and context embeddings independently. At inference time, DOCHOPPER lets questions directly attend to the precomputed context embeddings, significantly reducing the computation cost compared to cross-attention models that jointly encode questions and context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate DOCHOPPER on four different datasets: ShARC <ref type="bibr" target="#b21">(Saeidi et al., 2018)</ref>, HybridQA , QASPER <ref type="bibr" target="#b4">(Dasigi et al., 2021)</ref>, and HotpotQA <ref type="bibr">(Yang et al., 2018)</ref>. <ref type="bibr">3</ref> Since the downstream tasks of the four datasets are different, we apply an additional layer for ShARC, and a Transformer-based extractive QA model for HybridQA, QASPER and HotpotQA to extract the final answers. We will discuss the setup for each case separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXTRACTIVE QA: HYBRIDQA AND QASPER</head><p>Dataset HybridQA is a dataset that requires jointly using information from tables and hyperlinked text from cells to find the answers. Please see the Appendix for more information. This dataset requires the model to first locate the correct row in the table, and then find the answer from cells (or their hyperlinked text). To apply DOCHOPPER on the HybridQA dataset, we first convert a table with hyperlinked text into a long document. Each row in the table is considered a paragraph by concatenating the column header, cell text, and hyperlinked text if any. The average length of the documents is 9345.5 tokens.</p><p>Dataset QASPER <ref type="bibr" target="#b4">(Dasigi et al., 2021</ref>) is a QA dataset constructed from NLP papers. The dataset contains a mixture of extractive, abstractive, and yes/no questions. We experiment with the subset of extractive questions (51.8% of the datasets) in this paper. Some questions in the dataset are answerable with a single-hop of retrieval. However, as described in the original paper, 55.5% of the questions have multi-paragraph evidence, and thus aggregating multiple pieces of information should improve the accuracy. Answers in the QASPER dataset have an average of 14.4 tokens. We treat each subsection as a paragraph and prepend the section title and subsection title to the beginning of the subsection.</p><p>Implementation Details Question embeddings q 0 , q 1 are initialized from Eq. 2 and updated as in Eq. 7. For the two datasets, we perform 2-hop attention: the first hop is trained to attend to a paragraph, and the second hop to attend to a sentence. Note that we do not require that sentences attended to in the second hop must come from the previously attended paragraphs. The attention scores of paragraphs and sentences are linearly combined to find the best sentence that contains the answer, which will then be read by a BERT-based reader to extract the final answer. The final attention score of a sentence s i j ? p i is  where q T 1 s i j and q T 0 p i are the attention scores at sentence and paragraph levels, and sparse(q 0 , p i ) is the similarity score using sparse features. 4 ? 1 and ? 2 are hyper-parameters tuned on dev data. We set ? 1 = 1.5 and ? 2 = 3.0 for HybridQA, and ? 1 = 0.5 and ? 2 = 0.0 for QASPER.</p><formula xml:id="formula_12">score(s i j ) = q T 1 s i j + ? 1 ? q T 0 p i + ? 2 ? sparse(q 0 , p i )<label>(8)</label></formula><p>Baselines We compare DOCHOPPER with the previous state-of-the-art models, MATE (Eisenschlos et al., 2021) and LED <ref type="bibr" target="#b4">(Dasigi et al., 2021)</ref>, and several other competitive baselines to show the efficacy of DOCHOPPER. <ref type="bibr">MATE (Eisenschlos et al., 2021)</ref> is a pretrained Transformer model with sparse attention between cells that is specifically designed for tabular data. HYBRIDER ) is a pipeline system that (1) links cells, (2) reranks linked cells, (3) hops from one cell to another, and (4) reads text to find answers. All four stages are trained separately. LED is an encoderdecoder model that builds on Longformer <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>. We also experiment with directly reading the documents with a Transformer-based reader ETC <ref type="bibr" target="#b0">(Ainslie et al., 2020)</ref>: though it can't fit the entire document into its input, it is still one of the best models for reading long sequences (up to 4096 tokens). To handle longer documents, we adopt the sequential reading strategy: the model reads the document paragraph by paragraph, and picks the most confident prediction as the answer. We also report the numbers of a "retrieve and read" pipeline with a BM25 retriever and a finetuned ETC reader. The numbers are shown in <ref type="table" target="#tab_1">Table 1</ref>. Runtime is measured as examples per second with a batch size of 1.</p><p>Results and Analysis On QASPER, DOCHOPPER outperforms the previous state-of-the-art models by 3-5%, and runs more than 10 times faster. DOCHOPPER performs worse than MATE (Eisenschlos et al., 2021) on HybridQA. This is due to three reasons. (1) MATE is specifically designed and pretrained to understand tabular data, while DOCHOPPER is applied to general documents. (2) To convert tables to DOCHOPPER's input, DOCHOPPER serializes the tables by rows and thus loses information from other table structures, e.g. cells and columns, that are commonly used to understand tabular data. In an ablated experiment with the assumption that sentences are grouped by cells, we let DOCHOPPER return cells that contain the selected sentences and pass the cells (instead of single sentences) to the underlying extractive QA model (labeled w/ cell). This improves the performance by 5.4 points. (3) MATE restricts the length of text in cells to 5 sentences and enforces it by retrieving top-k sentences from the hyperlinked text. It also restricts the total length of tables to 2048 tokens. DOCHOPPER does not put any restrictions on the length of cell or the length of tables, and can thus be easily applied to more general tasks.</p><p>We additionally performed more ablated experiments with DOCHOPPER. The query update (see the row w/o query update) in Eq. 7 is also important, causing 3.5% and 1.8% difference in performance on both datasets. We also ablated the model by using one step of attention to select the most relevant sentence from the document (single-hop), and note again that performance drops noticeably. Adding one more step of attention, while only attending to sentences (sentence-only in the   some improvement, but is still worse than attending at both paragraph and sentence levels. The performance of sentence selection (with ablated numbers) is presented in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">HOTPOTQA-LONG</head><p>Dataset HotpotQA requires multi-hop reasoning to answer two types of questions, bridge and comparison. The original dataset provides the first paragraphs of 10 Wikipedia pages as its context. To test the model's ability to extract information from a longer context, we use first 2048 tokens of the 10 Wikipedia pages, and concatenate the 10 pages into a single document. This increases the average length of context from 897 to 9970 tokens. We call this variant of the dataset HotpotQA-Long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The experiment setup for HotpotQA-Long is similar to HybridQA and QASPER. Attention for this dataset is supervised, using the information about which evidences support each answer provided in the dataset. Here, we refer to a Wikipedia page (with a maximum of 2048 tokens) as a paragraph. The iterative attention is performed similarly to ?4.1, but repeated for 4 hops: paragraph, sentence, paragraph, and sentence. The first two hops are supervised by the first supporting evidence, and the last two hops are supervised by the second supporting evidence. The final score for each evidence is computed as in Eq. 8 with ? 1 = 0.7 for the first evidence and ? 1 = 0.0 for the second, and ? 2 = 0 for both. As suggested by <ref type="bibr">Xiong et al. (2021)</ref>, reranking can improve the overall performance, so we take the top 4 sentences for each step and rerank the 16 combinations. 5</p><p>Baselines We compare DOCHOPPER with <ref type="bibr">IRRR (Xiong et al., 2021)</ref> which is an multi-hop retrieval model that iteratively retrieves a small piece of evidence and re-encodes query vectors at each step by concatenating tokens from the retrieved passages at the previous step. <ref type="bibr">6</ref> We compare to IRRR both with or without the reranking step. Since questions in HotpotQA require combining evidences from multiple paragraphs, it's not clear how to run the sequential reading baseline on HotpotQA-Long, but as a reference, we show the performance of ETC and HGN in the distractor setting (with the original context, which is 10x smaller) in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Results DOCHOPPER outperforms the baseline models by 4-6 points on the accuracy of both answer and supporting evidence. More surprisingly, the performance of DOCHOPPER in the long document setting is only ?4 points lower than the state-of-the-art on the distractor setting, even though the context is more than 10 times longer. Please see the Appendix for ablated experiments and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CONVERSATIONAL QA: SHARC-LONG</head><p>Dataset ShARC <ref type="bibr" target="#b21">Saeidi et al. (2018)</ref> is a conversational QA dataset for discourse entailment reasoning. Each example consists of a document that describes a government policy and a conversation history about the document between the machine and the user. The conversation starts with an initial questions asked by the user and follows by a sequence of clarification questions and answers collected from the interaction between the machine and the user. The model takes the conversation history as input and predicts the answer to the initial question. An answer has one of the four labels: "Yes", "No", "Irrelevant", or "Inquire". "Irrelevant" means the question is not related to the provided context and "Inquire" means there's not enough information to answer the question. Similar to HotpotQA-Long, we expand the original context to test the long document setting. The expanded context contains 737.1 tokens on average, 13.5 times longer than the original context. Please see the Appendix for more details and examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The query embeddings q 0 , . . . , q n are initialized from Eq. 1 where n is the number of followup questions in the conversation. The query vector q t at the step t will be updated with the residuals following the update rule in Eq.7. The iterative attention process is distantly supervised at intermediate steps. See the Appendix for information on distant supervision.</p><p>We add a simple classification layer on DOCHOPPER to make the final prediction. As input to this layer, we reuse the concatenated vector k j in Eq. 5. Let k (t) j be the concatenated vector of the sentence attended at the t'th retrieval step, either a sentence directly attended or associated from a attended paragraph. Concatenated embeddings at all attention steps {k (0) 0 , . . . , k (0) * , . . . , k (n) 0 , . . . , k (n) * } are linearly combined into one vectork that will be used to make the final prediction, using weights ? (t) j computed across the attended sentences from all steps t. m ? R 4 holds the logits of the 4 classes that is used to compute the softmax cross entropy with the one-hot encoding of the positive class labels.</p><formula xml:id="formula_13">k = t j ? (t) j k (t) j , ? (t) j = softmax(u T k (t) j ), m = W T ck ? R 4</formula><p>Baselines We compare our model with ETC and a "retrieve and read" baseline. In using ETC, we concatenate the full context and the conversation into a single input, and prepend a global <ref type="bibr">[CLS]</ref> token. The embedding of the global [CLS] token will be used to make the final prediction. For the "retrieve and read" pipeline, we adopt the previous state-of-the-art model DISCERN as the reader, and pair it with a learned retriever. <ref type="bibr">7</ref> We also run a sequential reading baseline with DISCERN where documents are chunked every 128 tokens with a stride of 32 tokens and then read with DISCERN, predicting the class with the highest probability among all chunked inputs. 8 Runtime is measured as examples per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The evaluation is performed in two settings: Easy and Strict. The Easy setting only evaluates the accuracy of classification. In the Strict setting, we additionally require that all evidences (provided in the original dataset) are retrieved. We report the (micro) accuracy as the evaluation metric. DOCHOPPER outperforms all baseline models by more than 7 points in both easy and strict settings, while being more than 3 times faster than all baseline models. We performed the previous ablated experiments and observed similar changes in performance. Please see the Appendix for more numbers on evidence selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We consider on the problem of answering complex questions over long structured documents. Like multi-hop open QA tasks, this problem requires not only conventional "machine reading" abilities, but the ability to retrieve relevant information and refine queries based on retrieved information. Additionally, it requires the ability to navigate through a document, by understanding the relationship between sections of the document and parts of the question. In our framework, navigation is modeling similarly to retrieval in multi-hop models: the model attends to a document section, and uses a compact neural encoding of the section to update the query. Unlike most prior multihop QA models, however, queries are updated in embedding space, rather than by appending to a discrete representation of question text. This approach is end-to-end differentiable and very fast. Experiments also demonstrate that this use of hierarchical attention can significantly improve the performance on QA tasks: in fact, the DOCHOPPER model achieves the start-of-the-art results on four challenging QA datasets, outperforming the baseline models by 3-5%, while also being 3-10 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ETHICS STATEMENT</head><p>Pretrained language models (LM) are effective in many tasks but are criticized due to their high training cost and environmental concerns. Recently, more attention has been brought into development efficient models. The DOCHOPPER model proposed in this paper follows research in this direction. DOCHOPPER exploits the existing language models released by other research institutes. Without any additional pretraining, DOCHOPPER achieved competitive or state-of-the-art results in several challenging QA datasets. All experiments in this paper were conducted with a single GPU.</p><p>Besides being efficient in training, DOCHOPPER is also extremely efficient during inference. Experiments show that DOCHOPPER runs 3-10 times faster compared to existing QA models and thus could be deployed using limited computation resources.  <ref type="table">Table 4</ref>: Hits@1 accuracy on selecting sentences that actually contains the answer (on dev set).</p><p>initial question, e.g. "Can I get standard deduction for my federal tax return?", with a user scenario, e.g. "I lived in the US for 5 years with a student visa", and a few followup questions and answers through the interaction between the machine and users, e.g. "Bot: Are you a resident alien for tax purpose? User: No". The model reviews the conversation and predicts one of the three labels: "Yes", "No", or "Irrelevant". If the model think there's not enough information to make the prediction, it should predict a fourth label "Inquire".</p><p>Besides the conversation, each example in the ShARC dataset provides a snippet that the conversation is originated from. A snippet is a short paragraph that the conversation is created from, e.g. "Certain taxpayers aren't entitled to the standard deduction: (1) A married individual filing as married... (2) An individual ...". Since the snippets are usually short, with an average of 54.7 tokens, previous models, e.g. <ref type="bibr">DISCERN Gao et al. (2020)</ref>, concatenate the snippet and the conversation, and jointly encode them with Transformer-based models, e.g. BERT or RoBERTa. Here we consider instead a more challeging long-document setting, in which the snippet is not known, and the model must also locate the snippet from the document. We crawl the web pages with the provided URL. The pages contain 737.1 tokens on average, 13.5 times longer than the original snippets, and the longest page contains 3927 tokens. We name this new variant ShARC-Long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 IMPLEMENTATION DETAILS FOR SHARC-LONG</head><p>Changes to Context Representations Instead of computing the paragraph embeddings as a weighted sum of sentence embeddings, we directly obtain the paragraph embeddings from ETC output for this dataset. Recall that a paragraph p i = {s i 0 , . . . , s i |pi| } contains a sequence of sentences s i j . We prepend a dummy sentence s null to the beginning of the paragraph, and again, we modify the global-to-local attention mask to allow the global token of the dummy sentence to attend to all tokens in the paragraph p i . Let p i ? R d be the embedding of paragraph p i . The embeddings for a paragraph and its contained sentences are: p i , s i 0 , . . . , s i |pi| = ETC({s null , s i 0 , . . . , s i |pi| })</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distant Supervision</head><p>The iterative attention process is distantly supervised with supervision at intermediate steps. At each step, the model is trained to attend to both the correct paragraph and the correct sentences if they exists. Since the embedding table C d consists of both paragraph and sentence embeddings, we only need to compute the attention scores once at each step, but consider both the correct paragraph and the correct sentence as positive. The positive paragraph is one of the paragraphs from the crawled web page with the highest BLEU score. <ref type="bibr">10</ref> We notice that some web pages at the provided URLs have been changed significantly, so the snippets provided in the datasets may not exist any more, hence we discard the associated data if the highest BLEU scores of the paragraphs is less than 0.7. We follow the heuristics used by baseline models <ref type="bibr" target="#b9">(Gao et al., 2020)</ref> to get positive sentence candidates by finding the sentence with the minimum edit distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ADDITIONAL RESULTS</head><p>We report the performance of eventually selecting the correct evidences in <ref type="table">Table 4</ref>, 5, and 6.</p><p>Comments on HotpotQA-Long We also observe that ablated experiment on evidence selection (w/o query update) is only 7.8 points lower than the full model. To understand the underlying   <ref type="table">Table 6</ref>: Accuracy of selecting all required evidences on ShARC-Long.</p><p>reason, we train the model to perform a one-step attention only for supporting facts of the second hop (for bridge questions). The accuracy is 71.7, only 3.6 points lower than the accuracy of the full multi-hop process. This is likely due to the high surface form overlap between the questions and their context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>EM/F1 performance on HybridQA and QASPER. Runtime is measured by reruning their opensourced codes (failed to rerun MATE). Numbers for QASPER are reported on the subset of extractive questions.</figDesc><table /><note>* See Results and Analysis for discussion on MATE's performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table )</head><label>)</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">HotpotQA-Long</cell></row><row><cell></cell><cell>Answer</cell><cell>Support</cell></row><row><cell>IRRR (direct)</cell><cell cols="2">53.6 / 64.8 44.7 / 73.2</cell></row><row><cell>IRRR (rerank)</cell><cell cols="2">62.1 / 75.6 54.5 / 81.3</cell></row><row><cell>DOCHOPPER (direct)</cell><cell cols="2">57.4 / 69.5 45.1 / 73.8</cell></row><row><cell cols="3">DOCHOPPER (rerank) 66.5 / 79.7 61.4 / 85.9</cell></row><row><cell>ETC (distractor)</cell><cell>-/ 81.7</cell><cell>-/ 89.4</cell></row><row><cell>HGN (distractor)</cell><cell>-/ 83.4</cell><cell>-/ 89.2</cell></row><row><cell></cell><cell></cell><cell>, leads to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">ShARC-Long</cell><cell></cell></row><row><cell></cell><cell cols="3">Easy Strict runtime</cell></row><row><cell>ETC</cell><cell>61.1</cell><cell>-</cell><cell>11.2/s</cell></row><row><cell>Retrieval + DISCERN</cell><cell>65.2</cell><cell>50.7</cell><cell>42.0/s</cell></row><row><cell cols="2">Sequential (DISCERN) 63.7</cell><cell>54.2</cell><cell>8.8/s</cell></row><row><cell>DOCHOPPER</cell><cell>72.3</cell><cell>60.2</cell><cell>164.3/s</cell></row><row><cell>(w/o query update)</cell><cell>72.4</cell><cell>59.9</cell><cell>-</cell></row><row><cell>(sentence only)</cell><cell>62.2</cell><cell>43.2</cell><cell>-</cell></row><row><cell>(single-hop)</cell><cell>68.0</cell><cell>52.7</cell><cell>-</cell></row><row><cell>: EM/F1 performance on answer and sup-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>porting evidence on HotpotQA (dev set) with full</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wikipedia pages as context. HGN Fang et al. (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>and ETC Fang et al. (2019) are evaluated on the dis-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tractor setting.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy on ShARC-Long dataset. The Easy setting only checks the predicted labels, while the Strict setting additionally checks if all required evidences are retrieved. DISCERN is run with their open-sourced codes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Wen Xiao and Giuseppe Carenini. Extractive summarization of long documents by combining global and local context. arXiv preprint arXiv:1909.08089, 2019. Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Wen tau Yih, Sebastian Riedel, Douwe Kiela, and Barlas Oguz. Answering complex open-domain questions with multi-hop dense retrieval, 2021. Chen et al., 2020) is a dataset that requires jointly using information from tables and text hyperlinked from table cells to find the answers of multi-hop questions. A row in the table describes attributes of an instance, for example, a person or an event. Attributes are organized by columns. For example, the table of Medalist of Sweden in 1932, 9 contains a row "[Medal:] Gold;[Name:] Rudolf Svensson; [Sport:] Wrestling (Greco-Roman); [Event:] Men's Heavyweight". Text in the square brackets are the headers of the table. The medal winner "Rudolf Svensson" and the event "Wrestling (Greco-Roman)" are hyperlinked to the first paragraph of their Wikipedia pages. A question asks "What was the nickname of the gold medal winner in the men 's heavyweight grecoroman wrestling event of the 1932 Summer Olympics?" requires the model to first locate the correct row in the table, and find the answer from other cells in the row or their hyperlinked text.To apply our model on the HybridQA dataset, we first convert a table with hyperlinked text into a long document. Each row in the table is considered a paragraph by concatenating the column header, cell text, and hyperlinked text if any. The column name and cell text are each treated as one sentence. Hyperlinked text is also split into sentences. In the example above, the row becomes "Medal. Gold. Name. Rudolf Svensson. Johan Rudolf Svensson (27 March 1899 -4 December 1978) was a Swedish wrestler. He competed ...". The average length of the documents is 9345.5.QASPER<ref type="bibr" target="#b4">(Dasigi et al., 2021</ref>) is a QA dataset constructed from NLP papers. They hired graduate students to read the papers and ask questions. A different group of students are hired to answer the questions. For example, a question asks "What are the baseline models used in this paper?". The answers are {"BERT", "RoBERTa"}. The dataset contains a mixture of extractive, abstractive, and yes/no questions. We focus on the subset of extractive questions (51.8% of the datasets) in this paper. Some questions in the dataset are answerable with a single-hop. However, as suggested in the original paper, 55.5% of the questions have multi-paragraph evidence, and thus aggregating multiple pieces of information should improve the accuracy. Answers in the QASPER dataset are longer, with an average of 14.4 tokens. We treat each subsection as a paragraph and prepend the section title and subsection title to the beginning of the subsection.ShARC<ref type="bibr" target="#b21">Saeidi et al. (2018)</ref> is a conversational QA dataset for discourse entailment reasoning. Questions in ShARC are about government policy crawled from government websites. Users engage with a machine to check if they qualify for some benefits. A question in the dataset starts with a</figDesc><table><row><cell>7 REPRODUCIBILITY STATEMENT Experiments in this paper are conducted on publicly available datasets. Codes and processed data will be open-sourced upon the acceptance of this paper. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classification. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, pp. 1480-1489, 2016. Xingxing Zhang, Furu Wei, and Ming Zhou. Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization, 2019. Chen Zhao, Chenyan Xiong, Jordan Boyd-Graber, and Hal Daum? III au2. Multi-step reasoning over unstructured text with beam dense retrieval, 2021. A APPENDIX A.1 DATASET DETAILS QASPER (Extractive) DOCHOPPER 56.5 39.1 (w/o sparse) 53.3 (39.1) (w/o query update) 51.8 37.2 (sentence-only) 46.4 36.1 HybridQA (HybridQA (single-hop) 34.2 36.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of correctly predicting supporting facts for both hops on HotpotQA-Long (without reranking).</figDesc><table><row><cell></cell><cell>ShARC-Long</cell></row><row><cell>DOCHOPPER</cell><cell>82.2</cell></row><row><cell>(w/o query update)</cell><cell>81.8</cell></row><row><cell>(sentence-only)</cell><cell>63.0</cell></row><row><cell>(single-hop)</cell><cell>72.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We modify the original dataset and replace the oracle snippet with the entire web page as input. Please see section ?4.1 for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We modify the original ShARC and HotpotQA datasets to evaluate them in the long-document setting. Please see the dataset descriptions for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Similar to the baseline model, we use paragraph-level sparse features to improve accuracy. In HybridQA, sparse(q0, pi) computes the length of longest common substrings in the question q0 and the paragraph pi. Sparse features are only used at the end of retrieval, not at any intermediate steps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Please refer to Xiong et al. (2021) for more details on the reranking step. 6 IRRR is the best model (ranked #9 on leaderboard) on HotpotQA (fullwiki) with open-sourced codes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The retriever is a ablated DOCHOPPER model that attends at the paragraph level only.8  The model is often extremely confident on the "Irrelevant" class because most chunked inputs are obviously irrelevant. We tuned a hyper-parameter = 0.99 so the model predicts "Irrelevant" if the probabilities of all chunked inputs are larger than .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://en.wikipedia.org/wiki/Sweden_at_the_1932_Summer_Olympics</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We drop the brevity penalty term in BLEU score.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language model pre-training for hierarchical document representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09128</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A dataset of information-seeking questions and answers anchored in research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03011</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10640</idno>
		<title level="m">Differentiable reasoning over a virtual knowledge base</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mate: Multi-view attention for table transformer efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03631</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01838</idno>
		<title level="m">Discern: Discourse-aware entailment reasoning network for conversational machine reading</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A divide-and-conquer approach to the summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexios</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3029" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<title level="m">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Text segmentation as a supervised learning task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Koshorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adir</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09337</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hopretriever: Retrieve hops over wikipedia to answer complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge guided text retrieval and reading for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Retrieve, read, rerank, then iterate: Answering open-domain questions of varying reasoning steps from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haejun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Oghenetegiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Tg&amp;quot; Sido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6140" to="6150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A hierarchical model of reviews for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02745</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Interpretation of natural language rules in conversational machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Sheldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01494</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00782</idno>
		<title level="m">Open domain question answering using early fusion of knowledge bases and text</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reasoning over virtual knowledge bases with open predicate relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07043</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00849</idno>
		<title level="m">Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11934</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
