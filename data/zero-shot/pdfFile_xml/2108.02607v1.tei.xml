<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniCon: Unified Context Network for Robust Active Speaker Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
							<email>zhangyuanhang15@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Liang</surname></persName>
							<email>liangsusan18@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<email>shuang.yang@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<email>sgshan@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><forename type="middle">Chen</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CAS)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Tomorrow Advancing Life</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Tomorrow Advancing Life</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Acad-emy of Sciences</orgName>
								<orgName type="institution">University of Chinese</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UniCon: Unified Context Network for Robust Active Speaker Detection</title>
					</analytic>
					<monogr>
						<title level="m">MM &apos;21</title>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475275</idno>
					<note>2021. UniCon: Unified Context Network for Robust Active Speaker Detection. In Proceedings of the 29th ACM Inter-national Conference on Multimedia (MM &apos;21), October 20-24, 2021, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new efficient framework, the Unified Context Network (UniCon), for robust active speaker detection (ASD). Traditional methods for ASD usually operate on each candidate's precropped face track separately and do not sufficiently consider the relationships among the candidates. This potentially limits performance, especially in challenging scenarios with low-resolution faces, multiple candidates, etc. Our solution is a novel, unified framework that focuses on jointly modeling multiple types of contextual information: spatial context to indicate the position and scale of each candidate's face, relational context to capture the visual relationships among the candidates and contrast audio-visual affinities with each other, and temporal context to aggregate longterm information and smooth out local uncertainties. Based on such information, our model optimizes all candidates in a unified process for robust and reliable ASD. A thorough ablation study is performed * Both authors contributed equally to this research. on several challenging ASD benchmarks under different settings. In particular, our method outperforms the state-of-the-art by a large margin of about 15% mean Average Precision (mAP) absolute on two challenging subsets: one with three candidate speakers, and the other with faces smaller than 64 pixels. Together, our UniCon achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for the first time on this challenging dataset at the time of submission. Project website: https://unicon-asd.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Activity recognition and understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KEYWORDS</head><p>active speaker detection, audio-visual, speech, computer vision ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Active Speaker Detection (ASD) refers to the task of identifying when each visible person is speaking in a video, typically through careful joint analysis of face motion and voices. It has a wide range of modern practical applications, such as video re-targeting <ref type="bibr" target="#b13">[14]</ref>,  <ref type="figure">Figure 1</ref>: What marks an active speaker out from others? Admittedly, face motion and its synchrony with the audio are the most obvious clues; however, as shown in the figure above, the underlying signals can be highly ambiguous, especially in hard scenarios with poorly lit, low-resolution faces and noisy acoustics, etc. We observe that three additional types of contextual clues can help identify the active speaker: (a) higher correspondence between his/her face track and the audio relative to other candidates, (b) a form of contrast with the remaining candidates in terms of visual saliency, the amount of attention he/she receives from others, etc. and (c) temporal continuity. In this paper, we propose a novel Unified Context Network (UniCon) which models such context information to jointly optimize all candidates for robust active speaker detection. speech diarization <ref type="bibr" target="#b8">[9]</ref>, automatic video indexing, human-robot interaction <ref type="bibr" target="#b5">[6]</ref>, speech enhancement, and the automatic curation of large-scale audio-visual speech datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Although research in this direction dates back almost two decades <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>, most early work only experiment with simple, short sequences depicting frontal faces, which is a major simplification of practical settings. Recent developments in deep learning for audiovisual tasks have led to renewed interest in the ASD problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. In 2020, Roth et al. <ref type="bibr" target="#b34">[35]</ref> introduced AVA-ActiveSpeaker, the first large-scale, challenging ASD benchmark extracted from diverse YouTube movies, which significantly boosted subsequent research. Several effective methods have been proposed, building upon 2D and 3D convolutional neural networks (CNNs) and recurrent neural networks (RNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. Among them, most closely related to our work is the Active Speaker Context (ASC) model proposed in 2020 <ref type="bibr" target="#b1">[2]</ref>, which combines cropped face tracks from all candidates with the audio using a self-attention module <ref type="bibr" target="#b39">[40]</ref> to model relationships among the candidates. However, relationships learned in this manner are weak, and lack clear interpretation. In addition, like most existing methods, the ASC model also focuses on optimizing each candidate separately. As a consequence, in complex scenarios with multiple candidates, low-resolution faces, occlusion, noisy acoustics etc., current ASD models often lack robustness and cannot yield satisfactory results.</p><p>In this paper, we present a fundamentally different approach where we jointly model multiple sources of contextual information and simultaneously optimize all candidates in the scene in a unified process for robust ASD. As shown in <ref type="figure">Fig. 1</ref>, we observe that when face motion and audio signals are ambiguous, three other types of contextual clues often help mark the true active speaker out from others: (a) higher correspondence between his/her face track and the audio relative to others, e.g. hearing a male's voice helps eliminate female candidates; (b) a certain form of contrast with the remaining candidates in terms of his/her visual saliency, the amount of attention he/she received from others, etc., for instance, one person speaks while the rest look at him/her; and (c) temporal continuity, i.e. by observing each candidate's behavior in a temporal neighborhood, we can smooth out erroneous predictions that arise from instantaneous frame-level decisions.</p><p>In light of these observations, we propose a novel framework that seeks to resolve local ambiguities and achieve robust ASD by jointly modeling different kinds of contextual evidence. Specifically, we introduce face position and scale information as global spatial context to implicitly reflect the visual saliency and the visual focus of attention (VFoA) of each candidate in the scene. Building upon this information, we then construct a powerful and efficient multispeaker relational context where each candidate is contrasted with others from both visual and audio-visual perspectives. Finally, we integrate temporal context into the previous relational context to aggregate long-term information and remove noises in frame-level predictions. By incorporating spatial, relational, and temporal context into a unified framework, our method can jointly optimize all candidates in the scene end-to-end. In the experiments, we show that our method significantly improves ASD performance, especially in challenging scenarios.</p><p>In summary, the main contributions of our work are:</p><p>? We propose a well-motivated, compact framework, the Unified Context Network (UniCon) for robust ASD, which unifies spatial, relational, and temporal context to jointly optimize all candidates in the scene. ? We highlight the use of face position and scale in constructing each candidate's global spatial context, and explore the modeling of the relationships among the candidates. ? We conduct thorough ablation studies that prove the effectiveness of our method, particularly under challenging multi-speaker and low-resolution settings. Moreover, we outperform the current state-of-the-art by a large margin on challenging ASD benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Active speaker detection (ASD). Current approaches for ASD can be summarized into two paradigms: unsupervised and supervised. Unsupervised methods usually operate under one of two assumptions: (a) the active speaker's face is the one with the highest correspondence between the audio and its visual information, or (b) the active speaker's face is the one that co-occurs most frequently with the person's idiosyncratic voice. Under assumption (a), one typical practice is to learn an audio-visual two-stream network using a contrastive loss between the two modalities <ref type="bibr" target="#b10">[11]</ref>. During training, pairs of temporally aligned audio and visual signals are pulled close, while misaligned ones are pushed apart. During inference, the active speaker is naturally the one with lowest distance between audio and visual features. Recently, the method has been improved by better training objective designs, such as multi-way classification <ref type="bibr" target="#b11">[12]</ref> and multinomial loss <ref type="bibr" target="#b16">[17]</ref>. Methods based on assumption (b) require strong face and voice embeddings for robust clustering and matching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. For example, Hoover et al. <ref type="bibr" target="#b25">[26]</ref> first detect speech segments and face tracks, and cluster them independently using pre-trained speech embeddings and FaceNet <ref type="bibr" target="#b35">[36]</ref>. They then apply bipartite matching to assign each speech cluster to the face track cluster with maximum temporal overlap. However, unsupervised methods are not robust enough when the assumptions are not met or clustering results are noisy. Supervised methods usually formulate ASD as a binary classification problem, and directly learn a frame-level binary classifier on fused audio-visual features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref>. Some works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref> apply multi-task learning to boost performance, by introducing an additional contrastive loss on audio and visual features. An interesting recent work <ref type="bibr" target="#b1">[2]</ref> constructs an Active Speaker Context ensemble to improve local predictions by querying adjacent time intervals and other speakers in the scene. Our work is a new and fundamentally different attempt in this direction, which proposes a methodical process to better incorporate different types of meaningful context information. We leverage such information in a unified framework to optimize all candidates jointly rather than independently for more robust ASD.</p><p>Context-aware action recognition and detection. There has been a large body of work that focuses on how to integrate contextual information in the field of action recognition and spatio-temporal action detection. For example, <ref type="bibr" target="#b22">[23]</ref> uses a modified Transformer <ref type="bibr" target="#b39">[40]</ref> architecture to aggregate context from other people to recognize the target person's actions. <ref type="bibr" target="#b33">[34]</ref> proposes a high-order relation reasoning operator that enhances action recognition by modeling Actor-Context-Actor relations. <ref type="bibr" target="#b40">[41]</ref> improves spatio-temporal action detection by expanding actor bounding boxes to include more background context. We draw inspiration from these successful works and explore the use of contextual information in ASD.</p><p>Gaze and social interaction. Evidence from psychology suggests that gaze direction plays an important role in social interactions <ref type="bibr" target="#b29">[30]</ref>, such as conversations. Since the VFoA of conversation participants typically converge on the active speaker, some previous work on speaker turn detection <ref type="bibr" target="#b28">[29]</ref> and audio-visual diarization of meetings <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> have reported good results by explicitly modeling head pose or gaze information with ad-hoc modules. In this work, we implicitly utilize such information through our spatial and relational contexts, which model the relationships among the candidates in an efficient and unified process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE UNICON MODEL</head><p>In this section, we describe our UniCon model, which integrates spatial, relational, and temporal context in a unified framework. First, for each candidate, the scale and position of all candidates' faces are introduced as global spatial context to complement facial information and help learn the relationships among the speakers. Each candidate is then contrasted with others from both a visual and audio-visual perspective in the relational context modeling component. To further improve the robustness of the model's predictions, temporal context is integrated. Finally, based on the aggregated contextual features, we generate speaker activity predictions for all speakers simultaneously with a shared prediction layer. <ref type="figure" target="#fig_0">Fig. 2</ref> provides an overview of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoders</head><p>Given an input video clip, we first crop face tracks for each candidate speaker and transform them into low-dimensional speaker descriptors for further analysis. Likewise, we encode each audio frame into a low-dimensional audio descriptor.</p><p>Face Track Encoder: To encode short-term temporal dynamics, at each time step = 1, 2, . . . , , where is the total number of time steps (i.e. frames), the -th candidate's input ( </p><formula xml:id="formula_0">= FC x 1 , FC x 2 , . . . , FC x ? R ? ? .</formula><p>Audio Encoder: At each time step ( = 1, 2, . . . , ), we obtain audio representations from a 400ms window preceding . A ResNet-18 encoder takes 13-dimensional MFCCs of the window as input and outputs a 512-dimensional average-pooled feature. These features are also dimension-reduced to 128 using a single fully-connected layer. We denote the final audio features by = ( 1 , 2 , . . . , ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Context</head><p>The purpose of modeling spatial context is twofold. First, active speakers usually occupy a central position in the scene and depict higher visual saliency, especially in movies (commonly known as "the language of the lens"). Hence knowing the scale, position, and trajectory of a face in the video can help eliminate unlikely candidates. Second, people tend to look at the active speaker as they listen. Therefore, we wish to reflect such gaze-related information to some degree, by providing the model with both facial features and the relative positions of the candidates in the scene.</p><p>Specifically, we encode head positions of all candidates in the scene using 64 ? 64 coordinate-normalized maps of 2D Gaussians, which is motivated by <ref type="bibr" target="#b32">[33]</ref>. The position and radius of each Gaussian represent the relative position and size of each candidate's face. Next, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, to further indicate the candidates' relationships, for each candidate ( = 1, 2, . . . , ) we construct his/her person-specific head map by generating a color-coded version of the initial Gaussian map in the following principle: yellow denotes candidate , and blue denotes other candidates in the scene. To further facilitate the subsequent modeling of relationships between candidate and every other candidate ( = 1, 2, . . . , , ? ), we tweak the color coding scheme to construct paired variants as follows: red denotes candidate , green denotes candidate and blue denotes the rest. Alternatively, can be thought of an RGB image: the red channel shows the "subject", i.e. candidate ; the green channel shows the "object", i.e. candidate ; and the blue channel shows other "context" candidates, i.e. those other than and ( is identified with ). Then a VGG-M-inspired network with four 2D convolutional layers <ref type="bibr" target="#b32">[33]</ref> is used to embed these colored head maps and into 64-dimensional vectors per frame, ? and ? for each candidate and candidate pair ( , ) with ? . We term the resulting embeddings ? and ? candidate 's spatial context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relational Context</head><p>After obtaining face track and audio descriptors as well as the spatial context embeddings, we jointly model all candidates in the scene and refine each candidate's representations for robust ASD. Our motivation for modeling relational context is the inevitable presence of local ambiguities which make directly matching face motion (e.g. lip movements) and audio non-trivial. Therefore, we integrate different types of contextual clues and consider all candidates in the scene holistically to facilitate the ASD task. For example, a person who is speaking tends to receive more attention from others and is usually portrayed with higher visual saliency (especially in movies).</p><p>Specifically, our relational context component is designed to complete two natural sub-tasks for ASD: learning a contextual visual representation for visual voice activity detection, and a contextual audio-visual representation for audio-visual affinity modeling. The two resulting representations will then be fused for joint analysis to produce the final prediction. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our relational context contains two parts: visual ( V ) and audio-visual relational context ( AV ). This special decoupled design strengthens the model's robustness to synchronization errors between audio and video. We now elaborate on the design of V and AV .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Relational Context:</head><p>Our key idea is to represent each candidate's visual activity by aggregating his/her locally perceived activity and all pairwise interactions with other candidates in the scene. Specifically, we design a permutation-equivariant layer to process all candidates simultaneously, while preserving the order in which they are provided to the network. Denote the input visual feature stack by</p><formula xml:id="formula_1">v = ( 1 , 2 , . . . , ) ? R ? ? ? . For each candidate = 1, 2, . . . , , the output in the -th position of V (v) is V, = ( ) = 1 ( , ? ) + ?? ? ( , , ? ) ,<label>(1)</label></formula><p>where (?, ?) and (?, ?, ?) are two networks that respectively model visual activity and pairwise visual interactions, denotes candidate 's visual features, ? is the head map embedding for candidate , and ? is the head map embedding for the candidate pair ( , ).</p><p>Here, the parameters of are shared across all candidates, and the parameters of are shared across all candidate pairs, so Eq. (1) is directly applicable to any number of co-occurring candidates . This in turn means that our network can analyze a theoretically unlimited number of candidates at test time. On the surface, Eq. (1) appears to suggest that obtaining V (v) requires 2 + evaluations in total. In our implementation, we reduce this number by half to ( +1)/2 by parameterizing (?, ?, ?) as a skew-symmetric function:</p><formula xml:id="formula_2">= ( , , ? ) = ? ( , , ? ) = ? .<label>(2)</label></formula><p>Our intuition is that should learn to represent the amount of attention that candidate directs at candidate , and rank the relative "activeness" and visual saliency of candidate against candidate . In this case, it suffices to express the direction of such relationships using a single positive or negative sign for each component.</p><p>Audio-Visual Relational Context: We model the affinity between the audio and each speaker's face track in terms of both synchrony and cross-modal biometrics. We fuse per-frame face track features with the audio features via concatenation, and then pass them through a shared network to compute local A-V affinity features ( , ). Considering that local A-V affinity estimation is vulnerable to signal ambiguity (e.g. low-resolution faces, profile faces, noisy audio etc.), we aggregate evidence from all candidates to make more reliable predictions: if one or some candidates display strong A-V agreement, then the network can accordingly lower its affinity predictions for other candidates to mitigate false positives. We therefore design a learnable module to adaptively suppress only the non-active candidates whose A-V affinities are of significantly lower magnitudes, while leaving the active candidates' features "as-is". To this end, we introduce an element-wise max-pooling operation over all A-V affinity features to obtain a global representation. Specifically, the pooled feature is computed as global = max 1? ? ( , ), and then concatenated to each candidate's initial features ( , ). The concatenated features are further processed with two fully connected layers to generate each candidate's final contextual audiovisual representation AV, . We name this operation non-active speaker suppression after its effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal Context</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we incorporate temporal context in the networks , , and inside V and AV . This benefits ASD in two ways: first, it improves the consistency of the relational context modeling process and smoothes out local, instantaneous noises; second, it helps to alleviate synchronization errors between the audio and the video stream, an issue that is ubiquitous with in-the-wild videos.</p><p>We choose two basic architectures as our temporal modeling back-end: temporal convolutions (1D CNNs), and bi-directional Gated Recurrent Units (Bi-GRUs). The former is favorable since it has a fixed temporal receptive field, and can be easily adapted for online settings. The latter is more reliable since it has access to both past and future information. Hence, for the rest of the paper, by mentioning temporal context we refer to the usage of a one-layer Bi-GRU backend with 256 cells. Otherwise, we apply a stack of two 1D convolutional layers of kernel size 3 interleaved with Batch Normalization <ref type="bibr" target="#b27">[28]</ref> and ReLU activation by default.</p><p>Finally, the refined contextual representations V and AV are concatenated and fed to a fully-connected layer that is shared across candidates. The outputs are passed through sigmoid activation, resulting in real values between 0 and 1 that indicate each candidate's probability of being the active speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Losses</head><p>The model is trained end-to-end with a multi-task loss formulation. Since the goal is to predict a binary speaking/not speaking label, for each loss term we apply the standard binary cross-entropy (BCE) loss, averaged over all time steps. The BCE loss is defined as</p><formula xml:id="formula_3">L BCE ( ,?) = ??log ? (1 ??) log(1 ? ),<label>(3)</label></formula><p>where and?are the predictions and ground truth labels, respectively. For a scene with frames and candidates, let aux and aux denote the audio and visual auxiliary prediction layer, pred the final audio-visual prediction layer, and?1 ,2,..., v, and?1 ,2,..., av, the corresponding visual and audio-visual ground truths of the -th candidate. We add two auxiliary losses L a and L v to learn discriminative features for both visual and audio streams. The audio prediction loss is</p><formula xml:id="formula_4">L a = 1 ?? =1 L BCE ( aux ( )),?a ,<label>(4)</label></formula><p>where?a = max 1? ??a v, is the audio ground truth indicating whether at least one of the candidates is speaking, and (?) is the sigmoid function. When training on a single candidate (i.e. without employing relational context), the visual prediction loss and main audio-visual prediction loss are given by</p><formula xml:id="formula_5">L v = 1 ?? =1 L BCE ( aux ( )),?v , , L av = 1 ?? =1 L BCE pred ? ,?a v, ,<label>(5)</label></formula><p>where ? denotes feature concatenation. When training with relational context on multiple candidates, the losses are slightly different. To compute the visual and audio-visual losses, we replace the inputs to the auxiliary prediction layers with (concatenated) contextual representations, and aggregate losses from all candidates:</p><formula xml:id="formula_6">L v = 1 ?? =1 1 ?? =1 L BCE aux V, ,?v , , L av = 1 ?? =1 1 ?? =1 L BCE pred V, ? AV, ,?a v, .<label>(6)</label></formula><p>Finally, the total loss is defined as follows:</p><formula xml:id="formula_7">L = L a + L v + L av .<label>(7)</label></formula><p>Note that in practice, may vary over time as candidates enter and leave the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To validate the effectiveness of our model, we thoroughly evaluate and analyze the performance of UniCon from two aspects. First, we perform comprehensive ablation studies and comparison on a popular and challenging ASD benchmark derived from diverse, in-the-wild movies, AVA-ActiveSpeaker. Second, we perform crossdataset testing on three additional datasets, Columbia, RealVAD, and AVDIAR. Example frames from each dataset are shown in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets: The AVA-ActiveSpeaker dataset <ref type="bibr" target="#b34">[35]</ref> consists of 262 YouTube movies from film industries around the world. Each video is annotated from minutes 15 to 30. The dataset provides face bounding boxes that are linked into face tracks and labeled for both speech  <ref type="figure">Figure 4</ref>: Example frames from the datasets used in this paper. Green boxes denote the active speakers. activity and whether the speech is audible. As shown in <ref type="figure">Fig. 4a</ref>, the dataset is highly challenging as it contains occlusions, lowresolution faces, low-quality audio, and varied lighting conditions. It has become a mainstream benchmark for the ASD task. The Columbia dataset <ref type="bibr" target="#b6">[7]</ref> annotates 35 minutes of a one-hour panel discussion video featuring 6 unique panelists (Bell, Bollinger, Lieberman, Long, Sick, Abbas). The panelists directly face the camera, but occasionally look elsewhere, or perform spontaneous activities (e.g. drinking water). Upper body bounding boxes and voice activity ground truth labels are provided, as shown in <ref type="figure">Fig. 4b</ref>.</p><p>The RealVAD dataset <ref type="bibr" target="#b3">[4]</ref>  <ref type="figure">(Fig. 4c)</ref> is also constructed from a panel discussion lasting approximately 83 minutes. The video is recorded using a static, mounted camera, capturing the 9 panelists in a full shot. The panelists sit in two rows, and at any given time can be looking anywhere. In addition, the panelists perform natural, spontaneous actions that may result in partial occlusion of the face and mouth (e.g. touching faces, cupping chins). Similar to Columbia, upper body bounding boxes and voice activity labels are provided. These two datasets are often adopted by works on visual voice activity detection (V-VAD) and ASD <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>The AVDIAR dataset <ref type="bibr" target="#b21">[22]</ref>  <ref type="figure">(Fig. 4d</ref>) provides 27 recordings of informal conversations involving one to four participants. The dataset is extremely challenging due to varying levels of speaker movement and speech overlap, occlusions, profile faces, and even back-facing speakers. The dataset was originally used to evaluate speaker diarization and tracking systems. We re-purpose the dataset to evaluate our ASD model by aggregating per-frame ASD results into diarization results.</p><p>Data Preprocessing: We preprocess each dataset by cropping the face tracks of each visible candidate. For the AVA-ActiveSpeaker dataset, to group face tracks into "scenes" for training and testing, each video is first segmented into shots with ffmpeg <ref type="bibr" target="#b30">[31]</ref>. Within each shot, disconnected tracks belonging to the same candidate are then merged based on bounding box overlap with an IoU threshold of 0.8. All videos are resampled to 25fps, and ground truth labels are computed according to frame timestamps via nearest-neighbor interpolation. The Columbia and RealVAD datasets do not provide face bounding boxes, so face detection is performed with an off-theshelf RetinaFace detector <ref type="bibr" target="#b14">[15]</ref>. The resulting bounding boxes are expanded by 1.3x to mimic AVA-style detections. For the AVDIAR dataset, the provided bounding boxes are used directly.</p><p>Train/test Split and Evaluation Metric: For AVA-ActiveSpeaker, the official train/validation/test split is adopted. The test set is held out for the ActivityNet challenge and unavailable, so we perform our analysis on the validation set instead, as several previous methods do <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref>. The Columbia and AVDIAR datasets do not provide training sets, so we only perform zero-shot testing. For Columbia, following previous practice <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>, we report results on all but one speaker (Abbas) which is usually held out for validation. For RealVAD, in addition to zero-shot results, we also report leave-one-out cross-validation results for each of the 9 panelists to compare with previous work. We only train on scenes in which the test speaker does not co-occur with the training speakers.</p><p>Finally, we report the metric that was commonly adopted by previous work for each dataset: mean Average Precision (mAP) and Area Under Receiver Operating Characteristic Curve (AUROC) for AVA-ActiveSpeaker, frame-wise -1 score for Columbia and RealVAD, and Diarization Error Rate (DER) for AVDIAR, which is defined as the sum of false alarm rate, missed speech rate, and speaker confusion rate.</p><p>Implementation Details: We implement our model with PyTorch and the pytorch-lightning package. All models are trained using the AdamW optimizer <ref type="bibr" target="#b31">[32]</ref>. The network parameters are initialized using He initialization <ref type="bibr" target="#b23">[24]</ref>. During training, we augment the data via random horizontal flipping and uniform corner cropping along the input face tracks, followed by random adjustments to brightness, contrast, and saturation. All cropped face tracks are resized to 144 ? 144, and randomly cropped to 128 ? 128 for training. We use a central 128 ? 128 patch for testing. We only run one trial with a fixed random seed for all experiments to ensure the results are comparable, but find the results to be stable under different network initializations and data augmentations. During inference, occasional long segments that do not entirely fit into GPU memory are split into shorter, fixed-size chunks and the predictions are re-combined later.</p><p>To facilitate training, we apply curriculum learning <ref type="bibr" target="#b2">[3]</ref> by first training a single-candidate model without relational context according to Eq. <ref type="bibr" target="#b6">(7)</ref>, and then continuing to train on up to 3 candidates, i.e. = 3 in Eq. (1), but without the L a term in Eq. <ref type="bibr" target="#b6">(7)</ref>. This is theoretically and empirically enough to learn the relevant parameters since V captures pairwise interactions, and we observe diminishing returns by sampling 4 or more candidates (note that we can still test on &gt; 3 candidates due to parameter sharing). In addition, we follow the sampling strategy in <ref type="bibr" target="#b1">[2]</ref> during training, and randomly sample a 1.12s segment (28 frames at 25fps) 1 from every training example in the dataset for each epoch. Therefore, our epoch size correlates with the number of identities or scenes rather than face detections, which prevents over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>In this subsection, we provide a thorough ablation study on the modeling components on the challenging AVA-ActiveSpeaker dataset.</p><p>Spatial Context: We compare two different ways to incorporate spatial context. The first approach applies early fusion and concatenates speaker-centric spatial embeddings ? with the -th candidate's face track features before dimensionality reduction. In contrast, the second approach applies late fusion and concatenates ? with the dimension-reduced face track features . As shown in <ref type="table">Table 1</ref>, both improve upon the simplest baseline that only uses convolutional temporal context (see Sec. 3.4) on audio and face track features, but early fusion performs 0.8% worse than late fusion, likely because information is lost too early as features pass the 128-dim bottleneck. Thus, we adopt late fusion hereafter. As shown in <ref type="table" target="#tab_3">Table 2</ref>, simply concatenating the spatial context embeddings to the face track features (denoted by +S) already yields stronger visual representations that improve the baseline (+1.7% mAP). <ref type="table">Table 1</ref>: Comparison w.r.t. the stage at which spatial context is introduced. We observe that applying late fusion yields better results since it helps to retain more spatial information for subsequent modeling. Relational Context: We assess the efficacy of our relational context component. As shown in <ref type="table" target="#tab_3">Table 2</ref>, incorporating the relational context module yields noticeable improvements of 1.7% absolute over the 1D CNN baseline when used alone (+R), and works in synergy with spatial context (+2.6% mAP for +S+R). We observe that this improvement is especially pronounced in multi-speaker and small face scenarios. This is because UniCon can model the relationships among the candidates in the scene, which leads to a better holistic understanding under these challenging settings. Moreover, we introduce weight sharing strategies within a permutationequivariant formulation, which enables our model to process a theoretically unlimited number of speakers at test time.</p><p>Temporal Context: Finally, incorporating temporal context with Bi-GRUs yields the most substantial improvement, which is reasonable because Bi-GRUs have access to each candidate's complete track history, and can thus make more reliable predictions. By leveraging long-term temporal context, our model ultimately achieves 92.0% mAP, improving upon our initial baseline by as much as 8.0%.</p><p>Non-active Speaker Suppression: In this part, we discuss the role of non-active speaker suppression in ASD. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we compare two ways of integrating global information against a baseline that does not apply non-active speaker suppression. One uses mean-pooling to obtain global , and the other uses max-pooling. Both improve over the model that only introduces visual relational context, and max-pooling performs slightly better, yielding fewer  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art</head><p>As shown in <ref type="table">Table 4</ref>, our full UniCon outperforms previous audiovisual ASD methods including the state-of-the-art by a large margin.</p><p>Remarkably, we achieve 92.0% mAP without any pre-training, surpassing 90% for the first time on the AVA-ActiveSpeaker validation set at the time of submission. This overall performance strongly supports the effectiveness and superiority of our UniCon. It is also worth noting that the +S+T model in <ref type="table" target="#tab_3">Table 2</ref> provides a very strong baseline (90.3% mAP) that already exceeds all methods available for comparison here, which further confirms the benefits of our model. As a side note, applying ImageNet pre-training to the encoders further boosts performance by 0.2% to 92.2%. <ref type="table">Table 4</ref>: Comparison with previous work on the AVA-ActiveSpeaker validation set. For each method, we copy the results from its original paper. mAP is calculated using the official evaluation tool, after interpolating our predictions to the timestamps in the original annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross-dataset Evaluation</head><p>Apart from evaluating on the large-scale AVA-ActiveSpeaker dataset, we also conduct cross-dataset evaluation on three other datasets: Columbia, RealVAD and AVDIAR. These datasets contain challenges that are previously rare, if not unseen in AVA-ActiveSpeaker, such as overlapped speech, reverberation, extreme poses, heavy face occlusion, and different spatial distributions of candidates. Our cross-dataset evaluation protocols include zero-shot testing on all three datasets using the model trained on AVA-ActiveSpeaker, and leave-one-out testing after fine-tuning on RealVAD. We report both visual-only and audio-visual performance. The former is obtained using only the auxiliary visual prediction layer aux in Sec. 3.5. Zero-shot Testing: Overall, our UniCon model achieves good zeroshot generalization performance on all three datasets. In particular, it achieves state-of-the-art performance on both Columbia (90.9% average -1 score) and RealVAD (80.30% average -1 score). On AVDIAR, we are the first to report diarization performance using single-channel audio and a single camera viewpoint.</p><p>On the Columbia dataset, we outperform all previous stateof-the-art under both visual-only and audio-visual settings. It is worth noting that the listed visual-only methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> use the entire upper body, while we only use cropped face tracks. We also outperform previous audio-visual models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> that are pre-trained on datasets a magnitude larger (224 and 606 hours, respectively). Similarly, on RealVAD, our zero-shot visual-only and audio-visual results outperform the only existing method available for equal comparison by a large margin (87.22% V, 80.30% AV vs. 53.04%). An interesting observation is that on RealVAD and Columbia, our visual-only model outperforms the audio-visual model in most cases. This is because labels provided with the two datasets are not frame-accurate; our audio-visual model successfully detects short inter-sentence pauses that are not annotated as "not speaking" by the dataset curators. Due to the page limit, please refer to the supplementary video for details.</p><p>On AVDIAR, our model provides the first baseline that only uses mono microphone input, and a single camera viewpointa highly challenging setup. Nevertheless, our audio-visual model still achieves 55.16% DER on average, which is a very promising zero-shot result. Moreover, our full UniCon model reduces DER by 4 to 5% over the +S+T model that does not model relationships among the candidates, which once again supports the efficacy of our relational context in multi-speaker scenarios.</p><p>Fine-tuned Results: We also evaluate fine-tuned leave-one-out performance on RealVAD (Columbia and AVDIAR do not provide training splits). After fine-tuning on homogeneous data, our model quickly adapts to the unseen scenario (panel discussion) and noises (e.g. spontaneous actions near the face). The average audio-visual -1 score is improved by 5.34% from 80.30% to 85.64%, and the average visual -1 score is improved by 1.47% from 87.22% to 88.69%. More detailed results can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Breakdown</head><p>In this part, we provide a more in-depth analysis of three known challenging scenarios on AVA-ActiveSpeaker. We provide a comparison with the existing state-of-the-art and highlight some advantages and appealing properties of the UniCon model.</p><p>Low-resolution Faces. We summarize performance for different face sizes in <ref type="figure" target="#fig_4">Fig. 5a</ref>. Following previous evaluation procedures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>, we partition the dataset into three bins by the widths of the detected faces: small (width ? 64 pixels), medium (width between 64 and 128 pixels), and large (width &gt; 128 pixels). While the +S+T model provides a strong baseline with 90.3%mAP that already outperforms the previous state-of-the-art <ref type="bibr" target="#b1">[2]</ref>, our full model which exploits relational context yields further performance gains of around 2% mAP absolute on all subsets. In particular, on the "small" subset, we outperform the previous state-of-the-art (56.2% mAP) by a large margin of 15.1%.</p><p>Multiple Candidates. <ref type="figure" target="#fig_4">Fig. 5b</ref> shows the model performance according to the number of detected faces in a frame. We consistently improve over the previous state-of-the-art for different numbers of detected faces, with the most significant gain being in the subset with 3 faces (+14.7% mAP). Moreover, we surpass 90% on the two-faces subset for the first time. The improvements prove the effectiveness of spatial and relational context modeling, which equip the model with the ability to discern background actors from main  actors, as well as an enhanced understanding of the relationships between the visible speakers. We refer interested readers to the supplemental video for more interesting qualitative results.</p><p>Out-of-sync Audio and Video. Many real-world videos suffer from poor synchronization between audio and video, caused by transmission or re-encoding. To evaluate our model's performance on out-of-sync data, we assume perfect synchronization in the source videos and artificially shift the audio stream by up to 10 frames to mimic out-of-sync videos. We then assess the performance of our model on manually de-synchronized videos. As shown in <ref type="figure">Fig. 6</ref>, our model is fairly resilient to A-V sync errors. Remarkably, even on videos that are shifted by 10 frames (0.4sec), our full UniCon model only loses 0.67% mAP, and still outperforms a simple Bi-GRU baseline by 1.64% absolute, which does not model spatial and relational context (+T in <ref type="table" target="#tab_3">Table 2</ref>), and receives synchronized inputs. This shows that our model is robust to synchronization error.  <ref type="figure">Figure 6</ref>: Results on out-of-sync videos. Our model is fairly resilient to A-V synchronization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have proposed a novel model named UniCon for active speaker detection. Key to our method is a unified modeling framework which efficiently aggregates different types of contextual evidence to make robust decisions, by considering the relationships between each candidate and others. We demonstrate via experiments on the large-scale AVA-ActiveSpeaker dataset and various other realworld datasets that our model successfully tackles challenging cases with multiple candidates and low-resolution faces, and outperforms state-of-the-art methods by a large margin.</p><p>A AVA-ACTIVESPEAKER DATASET STATISTICS <ref type="table" target="#tab_6">Table 5</ref> provides some statistics about the AVA-ActiveSpeaker dataset. Note that the number of entities is a rough estimate of the upper bound, which is the number of merged face tracks that are obtained through the procedure described in Sec. 4.1 of the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CROSS-DATASET EVALUATION RESULTS</head><p>In this section, we provide detailed results for the cross-dataset evaluations in Sec. 4.4 of the main paper. Results for Columbia, RealVAD and AVDIAR are presented in <ref type="table">Table 6</ref>, 7, and 8, respectively. For an analysis of these results, please refer to Sec. 4.4 of the main paper. Note that we find if the sampling rate of the videos are changed to 25fps (the value used during training), the results will be different. In these tables, we report the results at the original sampling rate. For fine-tuned results on RealVAD, we report the numbers from the best models on the validation set.  <ref type="table">Table 6</ref>: Zero-shot results on the Columbia dataset. We report -1 scores (%) for each speaker, and the overall average. The methods marked with an asterisk ( * ) employ large-scale self-supervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Speaker Bell Boll Lieb Long Sick Avg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-only</head><p>Cross-modal <ref type="bibr" target="#b6">[7]</ref>   <ref type="table">Table 7</ref>: Zero-shot and leave-one-out fine-tuning results on the RealVAD dataset. We report -1 scores (%) for each panelist, the overall average, and the overall standard deviation. <ref type="table" target="#tab_3">P1  P2  P3  P4  P5  P6  P7  P8  P9</ref> Avg. Std.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Speaker</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Model overview. Given a video, we first extract face track features and audio features. Face scale and position information are encoded as 2D Gaussians and embedded with CNN layers, which we refer to as spatial context. Next, we construct contextual audio-visual representations for each candidate, through intermediate visual relational context and audio-visual relational context modules. For the visual relational context, we introduce a permutation-equivariant layer to refine each speaker's visual representation by incorporating pairwise relationships and long-term temporal context. For the audio-visual relational context, we model audio-visual affinity over time for each candidate and then suppress non-active speakers by contrasting their affinity features with others. The final refined visual and audio-visual representations are concatenated and passed through a shared prediction layer to estimate a confidence score for each visible candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Head map construction. For each candidate , we construct his/her speaker-centric head map and pairwise head maps { | ? } for each candidate pair ( , ). Note that the lower-triangular elements with &gt; are not used due to the skew-symmetric implementation (see Sec. 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>mAP breakdown by number of onscreen faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Performance breakdown. We evaluate the performance of our model on faces of different sizes and on frames with one, two, and three detected faces (see main text). The performance of the previous state-of-the-art<ref type="bibr" target="#b1">[2]</ref> (ASC) and our best model are annotated for better comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>91.96 91.96 91.90 91.74 91.55 91.29 89.65 89.61 89.54 89.34 89.13 88</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2108.02607v1 [cs.CV] 5 Aug 2021</figDesc><table><row><cell>Video</cell><cell></cell><cell></cell><cell></cell><cell>Speaking</cell></row><row><cell></cell><cell>background actor</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Not Speaking</cell></row><row><cell>Audio</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>(female voice)</cell><cell>(male voice)</cell><cell>(male voice)</cell><cell>(female voice with music)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation results on the AVA-ActiveSpeaker validation set. S: Spatial Context; R: Relational Context; T: Temporal Context. Predictions from models without temporal context are smoothed over 11-frame windows via Wiener filtering.</figDesc><table><row><cell>Method</cell><cell cols="3">#Params mAP (%) AUROC</cell></row><row><cell>Baseline</cell><cell>23.2M</cell><cell>84.0</cell><cell>0.93</cell></row><row><cell>+S</cell><cell>23.4M</cell><cell>85.7</cell><cell>0.94</cell></row><row><cell>+R</cell><cell>23.8M</cell><cell>85.7</cell><cell>0.94</cell></row><row><cell>+S+R</cell><cell>24.0M</cell><cell>86.6</cell><cell>0.94</cell></row><row><cell>+T (Bi-GRU)</cell><cell>23.0M</cell><cell>89.6</cell><cell>0.95</cell></row><row><cell>+S+T</cell><cell>23.2M</cell><cell>90.3</cell><cell>0.96</cell></row><row><cell>+R+T</cell><cell>23.5M</cell><cell>90.4</cell><cell>0.96</cell></row><row><cell>+S+R+T (UniCon)</cell><cell>23.8M</cell><cell>92.0</cell><cell>0.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of implementing non-active speaker suppression with different pooling methods.</figDesc><table><row><cell>Pooling</cell><cell cols="2">mAP (%) AUROC</cell></row><row><cell>None (no suppression)</cell><cell>90.8</cell><cell>0.96</cell></row><row><cell>Mean-pooling</cell><cell>91.6</cell><cell>0.96</cell></row><row><cell>Max-pooling</cell><cell>92.0</cell><cell>0.97</cell></row><row><cell cols="3">false negatives. One possibility is that max-pooling back-propagates</cell></row><row><cell cols="3">more gradients to the most salient speaker, while mean-pooling</cell></row><row><cell cols="3">results in weaker gradients for the true active speaker, making the</cell></row><row><cell>model less confident.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>AVA-ActiveSpeaker dataset statistics. The number of entities and scenes are obtained after merging annotated face tracks as described in Sec. 4.1, and the last column indicates the total duration of all scenes in each partition. * Test set labels are held out for a separate ActivityNet challenge.Partition #Videos #Faces #Entities #Scenes #Hours</figDesc><table><row><cell>Train</cell><cell>120</cell><cell>2, 676k</cell><cell>28, 274</cell><cell>15, 499</cell><cell>19.82</cell></row><row><cell>Validation</cell><cell>33</cell><cell>768k</cell><cell>7549</cell><cell>4310</cell><cell>5.82</cell></row><row><cell>Test  *</cell><cell>109</cell><cell>2, 054k</cell><cell>21, 247</cell><cell>13, 529</cell><cell>16.16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The mean speech segment duration is 1.11s on the AVA-ActiveSpeaker dataset<ref type="bibr" target="#b34">[35]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised Learning of Audio-Visual Objects from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="volume">12363</biblScope>
			<biblScope unit="page" from="208" to="224" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVIII</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active Speakers in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Juan Le?n Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12462" to="12471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (ACM International Conference Proceeding Series</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RealVAD: A Real-world Dataset and A Method for Voice Activity Detection by Body Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.3007350</idno>
		<ptr target="https://doi.org/10.1109/TMM.2020.3007350" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving Speaker Diarization of TV Series using Talking-Face Detection and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="157" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active-speaker detection and localization with microphones and cameras embedded into a robotic head</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez-Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<idno type="DOI">10.1109/HUMANOIDS.2013.7029977</idno>
		<ptr target="https://doi.org/10.1109/HUMANOIDS.2013.7029977" />
	</analytic>
	<monogr>
		<title level="m">13th IEEE-RAS International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-Modal Supervision for Learning Active Speaker Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5) (Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9909</biblScope>
			<biblScope unit="page" from="285" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Naver at ActivityNet Challenge 2019 -Task B Active Speaker Detection (AVA). CoRR abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spot the Conversation: Speaker Diarisation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="299" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Out of Time: Automated Lip Sync in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10117</biblScope>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perfect Match: Improved Cross-modal Embeddings for Audio-visual Synchronisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Whan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Goo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3965" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look Who&apos;s Talking: Speaker Detection using Video and Audio Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (III)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal Active Speaker Detection and Virtual Cinematography for Video Conferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4527" to="4531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5202" to="5211" />
		</imprint>
	</monogr>
	<note>Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lionel Pibre, and Isabelle Ferran?. 2021. Audio-Video detection of the active speaker in meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge Francisco Madrigal</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint/>
	</monogr>
	<note>ICPR 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Audio-Visual Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahuan</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4367" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CN-Celeb: A Challenging Chinese Speaker Recognition Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7604" to="7608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Investigating the use of visual focus of attention for audio-visual speaker diarisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sileye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio-visual synchronisation for speaker diarisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Dielmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2654" to="2657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sileye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1086" to="1099" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computer Vision Foundation / IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
	<note>Video Action Transformer Network</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Using audio-visual information to understand speaker activity: Tracking active speakers on and off screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Sturdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6558" to="6562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved Active Speaker Detection based on Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops. IEEE</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4084" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Turn-alignment using eye-gaze and speech in conversational interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristiina</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH. ISCA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some functions of gaze-direction in social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta psychologica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="22" to="63" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reliable Transition Detection in Videos: A Survey and Practitioner&apos;s Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Graph</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="469" to="486" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LAEO-Net: Revisiting People Looking at Each Other in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Medina-Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Computer Vision Foundation / IEEE</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3477" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2006.07976</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4492" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voice Activity Detection by Upper Body Motion Analysis and Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cigdem</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="1260" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">S-VVAD: Visual Voice Activity Detection by Motion Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cigdem</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2332" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="814" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context-Aware RCNN: A Baseline for Action Detection in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (25) (Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="440" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Multi-Task Learning for Audio-Visual Active Speaker Detection. The ActivityNet Large-Scale Activity Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<idno>zero-shot) 94.32 73.98 89.93 76.73 80.57 93.61 98.78 83.51 93.54 87.22</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<idno>96.47 81.05 86.93 84.36 89.94 85.60 94.92 88.07 90.90 88.69 4.66</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<idno>zero-shot) 86.72 78.11 70.48 73.14 68.93 84.90 93.01 80.40 86.99 80.30 7.82</idno>
	</analytic>
	<monogr>
		<title level="j">Ours</title>
		<imprint>
			<publisher>AV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ours (AV, fine-tuned) 86</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Zero-shot results on the AVDIAR dataset. The average is computed over all test sequences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>Diarization Error Rate (DER</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
