<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VILLAR-CORRALES ET AL.: MSPRED: VIDEO PREDICTION AT MULTIPLE SCALES MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Villar-Corrales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Intelligent Systems</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Endenicher Allee 19 A</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Karapetyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Intelligent Systems</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Endenicher Allee 19 A</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Boltres</surname></persName>
							<email>andreas.boltres@partner.kit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">SAP SE -Walldorf Dietmar-Hopp-Allee</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>69190</postCode>
									<settlement>Walldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Autonomous Learning Robots Karlsruhe Institute of Technology (KIT)</orgName>
								<address>
									<addrLine>Adenauerring 4</addrLine>
									<postCode>76131</postCode>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
							<email>behnke@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="department">Autonomous Intelligent Systems</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>Endenicher Allee 19 A</addrLine>
									<postCode>53115</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VILLAR-CORRALES ET AL.: MSPRED: VIDEO PREDICTION AT MULTIPLE SCALES MSPred: Video Prediction at Multiple Spatio-Temporal Scales with Hierarchical Recurrent Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autonomous systems not only need to understand their current environment, but should also be able to predict future actions conditioned on past states, for instance based on captured camera frames. However, existing models mainly focus on forecasting future video frames for short time-horizons, hence being of limited use for long-term action planning. We propose Multi-Scale Hierarchical Prediction (MSPred), a novel video prediction model able to simultaneously forecast future possible outcomes of different levels of granularity at different spatio-temporal scales. By combining spatial and temporal downsampling, MSPred efficiently predicts abstract representations such as human poses or locations over long time horizons, while still maintaining a competitive performance for video frame prediction. In our experiments, we demonstrate that MSPred accurately predicts future video frames as well as high-level representations (e.g. keypoints or semantics) on bin-picking and action recognition datasets, while consistently outperforming popular approaches for future frame prediction. Furthermore, we ablate different modules and design choices in MSPred, experimentally validating that combining features of different spatial and temporal granularity leads to a superior performance. Code and models to reproduce our experiments can be found in https://github.com/AIS-Bonn/MSPred.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For effective human-robot collaboration, autonomous systems, such as domestic robots, need not only to perceive and understand their surroundings, but should also be able to estimate the intentions of nearby agents and make predictions about their actions and behavior. Depending on the desired prediction time-horizon, the level of abstraction of the predicted representations might differ. On the one hand, when forecasting the immediate future, predictions of high level of detail, such as subsequent video frames, are desirable. On the other hand, for longer time horizons it is no longer possible to foresee exact details, hence it can be advantageous to predict more abstract representations like human poses or scene semantics.</p><p>Recently, several deep-learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref> have been proposed for video prediction. These methods predict future frames in an autoregressive manner, conditioned on observed or generated images, often achieving realistic predictions for short time horizons or deterministic datasets.</p><p>Despite these successes, existing models are explicitly designed to predict future video frames, either in a self-supervised manner or in a supervised setting, thus lacking the flexibility to simultaneously make predictions of different abstraction level. Furthermore, these methods operate in an autoregressive manner in the image space, thus suffering from the increasing difficulty of predicting image-level details for longer time-horizons.</p><p>To overcome these issues, we propose Multi-Scale Hierarchical Prediction (MSPred), a convolutional and recurrent neural network designed to simultaneously predict future possible outcomes of different levels of abstraction with different spatial and temporal granularity.</p><p>To better model the scene dynamics and allow for better temporal modeling, MSPred utilizes a hierarchical predictor module, which applies both spatial and temporal downsampling in order to allow MSPred to extract features of different levels of abstraction that change at different temporal resolutions. The hierarchical predictor module is composed of multiple long short-term memory <ref type="bibr" target="#b12">[13]</ref> (LSTM) cells operating at different periods (T ), i.e., processing every T -th input frame. LSTMs operating at a higher frequency specialize on modeling lowlevel fast-changing features, whereas LSTMs with a lower frequency capture more abstract representations that change at slower rates. Using this coarser temporal resolution allows us to predict abstract features far into the future, while requiring only few recurrent iterations.</p><p>In summary, our contributions are: <ref type="bibr" target="#b0">(1)</ref> We propose MSPred, a hierarchical video prediction model able to simultaneously predict future possible outcomes of distinct levels of abstraction at different spatio-temporal granularity, conditioned on past video frames.</p><p>(2) MSPred outperforms popular video prediction models on perceptual metrics on three diverse datasets, while achieving more realistic predictions. (3) MSPred predicts plausible high-level representations (e.g. semantics or poses) long into the future using a coarse temporal resolution, while maintaining a competitive performance for video frame prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Future Frame Video Prediction</head><p>Video prediction is the task of forecasting future video frames conditioned on past frames. This task gained popularity due to its relevance for generating anticipative behavior. For a comprehensive review of deep-learning-based video prediction, we refer to Oprea et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>Several approaches have been proposed to perform video prediction, including modeling geometric transformations between consecutive frames <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>, learning transformations in the frequency domain <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, or using optical flow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. The most popular approach to video prediction, also followed by our proposed method, is the use of recurrent networks in combination with convolutional autoencoders in order to extract features from the seed frames and projecting them into the future <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr">42]</ref>. These approaches were later extended by integrating variational inference into the models, thus allowing the neural networks to model the underlying uncertainty of the data. Stochastic prediction models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref> use an inference network to approximate the true posterior distribution over a set of latent variables, which model the stochastic properties of the data. Some methods, including MSPred, organize the network into a hierarchical structure <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">44]</ref>, allowing the models to use features of different abstraction levels in order to predict future video frames.</p><p>The work that is conceptually most similar to ours is CW-VAE <ref type="bibr" target="#b31">[32]</ref>, in which -similarly to our MSPred model -a hierarchy of recurrent modules ticking at different clock rates is used to predict higher-level features using coarse time resolutions. Despite the similarities, the problems addressed by CW-VAE and MSPred are inherently different: CW-VAE combines features of distinct temporal resolutions to predict detailed frames far into the future. In contrast, we do not aim to forecast realistic frames long into the future, but instead to simultaneously predict detailed frames for short time horizons, as well as higher-level representations longer into the future using coarser temporal resolutions, which suffices for coarse-to-fine behavior planning in the now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">High-Level Feature Prediction</head><p>Another line of work performs video prediction using an intermediate high-level representation, instead of directly predicting future frames in the pixel space. These models first extract some high-level representation from the seed frames, either using pretrained models or human annotations, and project them into the future. Then, the model combines the predicted structured representations and the seed frames in order to forecast the future video frames. This approach simplifies the task of prediction, often leading to long-term accurate predictions, and has been proven successful for different high-level representations, including human poses <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>, semantic segmentation maps <ref type="bibr" target="#b23">[24]</ref>, or instance segmentations <ref type="bibr" target="#b20">[21]</ref>.</p><p>Similarly, MSPred also predicts high-level representations. However, instead of simply using them as intermediate features for future frame prediction, MSPred exploits the highlevel of abstraction of these representations in order to forecast them long into the future using RNNs operating with coarse temporal resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Scale Recurrent Networks</head><p>Since the introduction of recurrent neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> (RNNs), several approaches have been proposed to extend recurrent models into temporal hierarchies. Hihi and Bengio <ref type="bibr" target="#b11">[12]</ref> propose different architectures utilizing several RNNs operating at different time scales in order to learn long-term dependencies on simple sequential tasks. Clockwork RNNs <ref type="bibr" target="#b17">[18]</ref> split a recurrent network into parallel recurrent sub-modules that process their inputs at a different temporal granularity, hence allowing the model to learn complex dependencies between temporally distant inputs. Similarly, HM-RNN <ref type="bibr" target="#b4">[5]</ref> proposes a multi-scale recurrent model with different modules operating at distinct time periods. However, the specific values for these rates are not fixed, but learned via an adaptive mechanism.</p><p>Like the previous methods, MSPred uses recurrent models operating at distinct timescales to capture representations at different temporal resolutions. However, whereas previous methods processed low-dimensional sequences, we employ convolutional LSTMs <ref type="bibr" target="#b33">[34]</ref> in combination with convolutional autoencoders to forecast high-dimensional video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Video prediction is defined as the task of predicting subsequent video frames? =? 1 ,? 2 , ...,? N conditioned on C past context frames C = C 1 , C 2 , ..., C C . In this work, we extend the task of Slower LSTMs model abstract representations, whereas the fastest LSTM processes lowlevel details. Right: Predicted feature maps are fed through a decoder, which processes and fuses information from different LSTM modules. Finally, three distinct decoder heads predict outputs of different abstraction level. Note that we do not show the stochastic modules, and only display high-level decoder heads at the last time-step to unclutter the visualization. video prediction to predict not only the future frames, but also higher-level representations (? 1 ,? 2 ), such as human poses or object locations, conditioned on the same seed frames.</p><p>In this section we present MSPred, our proposed model for simultaneous prediction of representations of different levels of abstraction at multiple spatio-temporal scales, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Its key component is a hierarchical predictor module (Sec. 3.2), which forecasts features of different granularity. These features are extracted from seed frames using a convolutional encoder (Sec. 3.1), and decoded into future frames or higher-level representations using convolutional decoders (Sec. 3.3). Additionally, we discuss the stochastic components of MSPred (Sec. 3.4), as well as training and implementation details (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>MSPred uses a 2D convolutional encoder to process the seed frames. This module consists of four convolutional blocks, which extract increasingly abstract features of coarser spatial resolution. After the second and third blocks, skip connections bridge from the encoder to the decoder through the two lowest-level RNNs respectively, providing features of different levels of abstraction to the decoder. Supplying low-level representations of high-spatial resolution prevents the loss of information in the bottleneck layers, whereas higher-level features provide the decoder with abstract semantic information. The combination of these features allows our model to achieve feasible future predictions for increasingly complex structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Scale Prediction</head><p>Information in images is often processed in a hierarchical manner using features of different spatial resolution and level of abstraction. Similarly, the flow of information in videos can be represented in a temporal hierarchy. Higher-level features model the slowly changing information that is shared across many frames, whereas lower-level representations model faster-changing information. To account for this temporal hierarchy, MSPred uses a predictor module composed of three recurrent neural networks operating at different temporal resolu-tions, i.e., processing input frames with a period of one, T 1 and T 2 respectively (1 &lt; T 1 &lt; T 2 ). As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, the recurrent module at a particular level receives feature maps from the respective stage of the encoder. The lowest-level RNN, which processes all inputs, receives low-level feature maps of high spatial resolution, which correspond to fine-grained representations that quickly change between consecutive frames. The second RNN receives feature maps of coarser spatial resolution, containing more slowly-changing representations, thus operating at a slower clock-rate of T 1 . Finally, the highest-level RNN receives abstract feature maps of even coarser spatial resolution, which contain high-level features that are shared across many video frames, hence processing just one input every T 2 time steps.</p><p>The use of this hierarchy of RNNs allows our model to disentangle the temporal information into three different flows, each modeling features varying at distinct time-scales. Furthermore, the temporal abstraction in higher levels allows MSPred to forecast high-level features far into the future using just a small number of iterations, hence mitigating the error accumulation characteristic of autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>The decoder architecture corresponds to a mirrored version of the convolutional encoder, and is composed of four decoding stages. After the first and second one, features from the skip connections are fused with the decoded feature maps via channel-wise concatenation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, MSPred uses three separate decoder heads in correspondence to its three levels of processing in order to predict representations of different level of abstraction. Forecasting detailed video frames requires high-level knowledge (i.e. semantics or dynamics) as well as low-level information (i.e. texture and color). Therefore, the lowest-level decoder uses the most recent predicted features from all levels of the hierarchy to predict the subsequent video frames. As higher levels operate at coarser time scales, predicted feature maps from higher levels are re-used until a new feature map is generated. The mid-level decoder produces more abstract representations (e.g. poses or semantic segmentation) every T 1 time steps by processing the most recent feature maps of its own level and the level above. Finally, the highest-level decoder generates abstract representations, such as person positions, every T 2 time steps using only the predicted features from the highest level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stochastic Components</head><p>Inspired by Denton and Fergus <ref type="bibr" target="#b5">[6]</ref>, we enhance our MSPred model with a stochastic component to account for the uncertainty of real-world data and to achieve more diverse predictions. During training, MSPred uses the current target frame I t , as well as all previous frames I 1:t?1 to compute a posterior distribution q ? (z t |I 1:t ) and sample a latent variable z t , which encodes the dynamics of the sequence. By constraining the posterior to be close to a prior distribution p ? (z t |I 1:t?1 ), we enforce the model to encode the dynamics of the sequence, instead of simply condensing information from the target frame. At inference time, MSPred combines previous frames I 1:t?1 and latent vectors z 1:t to predict future frames and representations.</p><p>In practice, we use RNNs to estimate the mean and covariance of Gaussian prior and posterior distributions, and use the reparameterization trick <ref type="bibr" target="#b16">[17]</ref> to sample latent vectors. Due to the hierarchical structure of MSPred, we employ separate recurrent prior p ? and posterior q ? modules for each level. The latent vectors from each level are concatenated with the corresponding encoded features, and then fed to the respective predictor module <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. For a more detailed description of stochastic components in video prediction, we refer to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Implementation Details</head><p>Given a sequence of seed images, MSPred encodes these frames and feeds the embedded features to the corresponding recurrent modules <ref type="figure" target="#fig_0">(Figure 1 left)</ref>. During the prediction stage <ref type="figure" target="#fig_0">(Figure 1</ref> right), the model forecasts future representations in an autoregressive manner in the feature space, i.e., the output of a recurrent module is used as input in the subsequent time step. The forecasted features are fed to the corresponding decoder stage in order to decode future frames and high-level representations. Images are predicted at every time-step, whereas higher-level representations are predicted at the same clock-rate as their corresponding higher-level recurrent module, i.e., once every T 1 and T 2 time-steps, respectively.</p><p>For a fair comparison with baseline methods, for the Moving MNIST dataset our encoder and decoder follow DCGAN-like <ref type="bibr" target="#b28">[29]</ref> discriminator and generator architectures, respectively; whereas for other datasets we use VGG16-like <ref type="bibr" target="#b35">[36]</ref> modules. Each level of our hierarchical predictor uses four ConvLSTM <ref type="bibr" target="#b33">[34]</ref> cells. The stochastic components are implemented using, for each level in the hierarchy, a single ConvLSTM cell operating at the same frequency as the corresponding predictor module. We provide further implementation details in the supplementary material.</p><p>Similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>, we include skip connections from the last observed context frame to the decoder for all prediction time-steps. The features from the skip connections are added to the outputs of the corresponding ConvLSTM. The role of these skip connections is to directly provide the decoder with features of the background and static objects, hence allowing the predictor to focus on modeling pixel-level dynamics that change throughout the sequence.</p><p>Given C seed frames, we train MSPred to make five predictions (N = 5) on each level, hence predicting the five subsequent video frames, and forecasting higher-level representations for five prediction steps with a temporal resolution of T 1 and T 2 , respectively. Our model is trained using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> and the following loss function:</p><formula xml:id="formula_0">L = 1 N N ? i=1 ||I i ?? i || 2 + ? 1 L H 1 (H 1 T 1 i ,? 1 T 1 i ) + ? 2 L H 2 (H 2 T 2 i ,? 2 T 2 i ) + ? D KL (q ? , p ? ) ,</formula><p>where || ? || is the 2 norm, I t , H 1 t , H 2 t correspond to the ground truth frames and higherlevel targets at time-step t,? t ,? 1 t ,? 2 t are the corresponding predictions of each level at time-step t, and ? 1 , ? 2 and ? are coefficients to weight the different loss terms. L H 1 and L H 2 correspond the loss functions used on the higher-level decoder heads. We employ the mean-squared error when predicting poses or positions, and pixel-wise cross-entropy when forecasting semantic maps. Finally, D KL corresponds to the KL-divergence error between the posterior q ? and prior p ? distributions estimated by the stochastic component, averaged over the three levels in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform an ablation study investigating several MSPred modules and design choices in Section 4.2. In Section 4.3 we compare MSPred with existing video prediction models on three diverse datasets. Finally, in Section 4.4 we present results of multi-scale prediction. Animations and further qualitative results are available in the project website 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate MSPred for different prediction tasks on three video datasets of different levels of complexity, namely Moving MNIST <ref type="bibr" target="#b37">[38]</ref>, KTH-Actions <ref type="bibr" target="#b32">[33]</ref>, and SynpickVP. In all cases, we average the results across five predicted frames or high-level representations. Further details about datasets and evaluation metrics are provided in the supplementary material.</p><p>Moving MNIST is a standard video prediction dataset containing sequences of two random digits from the MNIST dataset <ref type="bibr" target="#b18">[19]</ref> moving with constant speed in a 64 ? 64 grid, and bouncing off the image boundaries. We train our models on random sequences generated on the fly, and evaluate on a fix test set containing 10,000 sequences.</p><p>KTH-Actions is a dataset consisting of real videos of humans performing one out of six possible actions, e.g. jogging or waving. The dataset includes 600 videos of 25 different humans performing the actions in various indoor and outdoor environments.</p><p>SynpickVP is a video prediction dataset containing sequences of bin-picking scenarios, in which a suction-cap gripper robot moves objects in a cluttered box. We generate the dataset by selecting sequences from the recently proposed SynPick <ref type="bibr" target="#b26">[27]</ref> dataset. This is a challenging benchmark, in which the model needs to predict the motion of the robotic gripper, as well as the displaced objects, while representing a complex and cluttered background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We train modified variants of MSPred to investigate how different components and design choices affect the video prediction performance of our model. Namely, we evaluate the relevance of temporal and spatial hierarchy, the type of RNN used, and the effect of multilevel supervision. For our ablation study, we focus on the Moving MNIST dataset. The results are reported in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>First, the hierarchical MSPred model (Rows 1 and 2) outperforms all other variants, whereas removing the spatial (Row 4), temporal (Row 5), or all (Row 6) hierarchical structure from the predictor leads to loss of performance, thus demonstrating that providing features of different spatio-temporal granularity improves the prediction performance of the model. Second, MSPred (Row 1) performs comparably to an MSPred variant trained only for image-level prediction (Row 2), thus indicating that adding hierarchical supervision to the loss function is not a key factor for the success of our model for future frame prediction. Finally, when replacing the ConvLSTM predictor with a linear LSTM (Row 3), the performance is significantly decreased, leading to the worst results among all compared models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to Existing Methods</head><p>We compare MSPred for the task of future frame prediction with several existing video prediction methods, including ConvLSTM <ref type="bibr" target="#b33">[34]</ref>, TrajectoryGRU <ref type="bibr" target="#b34">[35]</ref>, two variants of SVG <ref type="bibr" target="#b5">[6]</ref>, PredRNN++ [42], and PhyDNet <ref type="bibr" target="#b10">[11]</ref>. Additionally, we include an MSPred variant, denoted as MSPred NoSup, trained only for future frame prediction, i.e., without any additional supervision. For a fair comparison, all models use the same encoder and decoder architectures. The evaluation results are reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>We observe that both MSPred variants consistently perform among the best models on all datasets. Furthermore, MSPred NoSup shows comparable video prediction performance to MSPred trained to simultaneously make predictions of different levels of granularity. This indicates that adding hierarchical supervision to the loss function (e.g. poses or segmentation) allows MSPred to predict higher-level representations over long time-horizons, but it is not a key factor for the success of our model for video prediction.</p><p>Moving MNIST: In general, due to the simplicity of the dataset, all models achieve overall good prediction scores and accurate future frame predictions. However, both MSPred variants achieve exceptionally sharp and precise reconstructions, outperforming all other models by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KTH-Actions:</head><p>MSPred produces low PSNR and SSIM scores, indicating higher pixel differences with respect to the target frames. However, MSPred achieves the best LPIPS result, demonstrating a high perceptual similarity to the target frames. <ref type="figure">Figure 6</ref> contains a qualitative comparison on three KTH-Actions sequences. Baseline methods with the highest PSNR results, i.e. ConvLSTM and TrajGRU, blur the predictions around the arms and legs of the person, whereas our MSPred model achieves sharper predictions.</p><p>SynpickVP: MSPred outperforms existing video prediction models on this challenging dataset, achieving the highest PSNR result, and the best LPIPS perceptual score, indicating more realistic predictions than the baselines. <ref type="figure">Figure 7</ref> depicts a qualitative comparison on the SynpickVP dataset. Baseline video prediction methods tend to blur the suction cap gripper as well as the objects moved by it, whereas MSPred achieves more realistic predictions.  We display three seed frames and five predictions for two test-set sequences. Whereas all baseline methods blur the robot gripper in the predicted frames, MSPred achieves sharper predictions,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-Scale Prediction</head><p>Unlike existing video prediction models, MSPred predicts higher-level representations with coarse temporal resolutions in addition to video frames. For KTH-Actions, MSPred predicts human keypoints on its intermediate level, and a center-point on its highest level. Keypoint annotations are obtained using OpenPose <ref type="bibr" target="#b2">[3]</ref>. On SynpickVP, MSPred predicts semantic segmentation and the location of the gripper on the mid-and high-levels, respectively. We average the segmentation results in three different groups: gripper, static, and background.</p><p>For benchmarking the performance of MSPred, we train a baseline model SVG', based on a modified SVG-LP <ref type="bibr" target="#b5">[6]</ref> model, which predicts high-level representations conditioned on input frames. Further details about SVG' are provided in the supplementary material. Furthermore, we include a CopyLast baseline that copies the ground-truth representations of the last seed frame. <ref type="table" target="#tab_2">Table 3</ref> reports a quantitative comparison for pose and semantic segmentation forecasting, respectively. MSPred outperforms the two baselines for human pose forecasting on the KTH-Actions dataset. On the SynpickVP dataset, MSPred precisely predicts the segmentation of the robot gripper. However, due to the complex and mostly static scenes, the CopyLast baseline outperforms SVG' and MSPred when forecasting the segmentation of static objects and background. This is due to the fact that CopyLast has access to the last ground-truth representations, thus having access to the perfect segmentation of static objects and background; whereas MSPred and SVG' predict the high-level representations condition on seed images, without access to ground truth segmentation maps. <ref type="figure" target="#fig_3">Figure 4</ref> depicts examples of multi-scale prediction on the KTH-Actions and SynpickVP datasets. The lowest-level decoder achieves detailed subsequent frame predictions for a short time horizon, whereas higher level decoders accurately predict abstract representations (e.g. poses or locations) up to 40 frames into the future using coarser temporal resolutions. We display three seed frames and five targets and predictions for each decoder head. MSPred forecasts frames on short time horizons while also predicting higher-level representations longer into the future using a coarse temporal resolution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed MSPred, a novel video prediction model that extends the effective prediction horizon of related approaches by leveraging hierarchies of recurrent neural networks operating at different spatio-temporal resolutions in order to predict outcomes of varying levels of abstraction with different granularity. At its lowest prediction level, MSPred forecasts subsequent video frames, whereas at higher levels it predicts more abstract representations longer into the future using coarser spatio-temporal resolutions. In our experiments, we show how MSPred outperforms several existing video prediction methods for the task of future frame prediction. Furthermore, we show how the higher level decoder heads can be used to forecast more abstract representations, such as human poses or semantic maps, over longer time horizons. We firmly believe that the hierarchical features from MSPred could be used as representations to improve perception and reasoning capabilities in autonomous agents, and serve as basis for planning anticipative behavior.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We evaluate MSPred for different prediction tasks on three video datasets of different levels of complexity, namely Moving MNIST <ref type="bibr" target="#b37">[38]</ref>, KTH-Actions <ref type="bibr" target="#b32">[33]</ref>, and SynpickVP. <ref type="table" target="#tab_4">Table 4</ref> summarizes the three datasets used in our work.</p><p>Moving MNIST is a standard video prediction dataset containing sequences of two random digits from the MNIST dataset <ref type="bibr" target="#b18">[19]</ref> moving with constant speed in a 64 ? 64 grid, and bouncing off the image boundaries. In our experiments, we treat Moving MNIST frames as RGB images, i.e., repeating the MNIST digits across the RGB channels. For the high-level representations, we use Gaussian blobs centered at the digit locations. Despite its simplicity, this dataset is commonly used as a benchmark for video prediction. For training, we randomly generate sequences of 49 frames by sampling two random MNIST digits, a starting position and speed; whereas for testing we use a fixed set containing 10,000 sequences.</p><p>KTH-Actions is a dataset consisting of real videos of humans performing one out of six possible actions, namely boxing, hand-clapping, hand-waving, walking, running and jogging. The dataset includes 600 videos of 25 different human actors performing the actions in various indoor and outdoor environments. In our experiments, we downsample the images to a resolution of 64 ? 64. We use 1436 training sequences of length 49 from 16 different actors, whereas for testing we use 824 sequences from the remaining nine actors. In addition to video frames, we use nine human keypoints as intermediate level representations, and a center-point of the person a high-level representation. We generate the ground-truth keypoints using a pretrained OpenPose <ref type="bibr" target="#b2">[3]</ref> model for human pose estimation.</p><p>SynpickVP is a new synthetic video prediction dataset, consisting of videos of various bin-picking scenarios in which a suction-cap gripper robot moves in arbitrary directions in a box containing different objects. We generate the dataset by selecting sequences from the recently proposed SynPick <ref type="bibr" target="#b26">[27]</ref> dataset. We use 1975 training and 200 evaluation sequences containing 29 RGB video frames of size 64 ? 112. This is a challenging video prediction benchmark, since the model needs to capture the motion of the robotic gripper, as well as predict the future arrangement of displaced objects, while still representing a complex and cluttered background. In our experiments on SynpickVP, we train our model to predict image frames at the lowest level in the hierarchy, semantic segmentation maps from the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Metrics</head><p>We employ several evaluation metrics designed for different tasks in order to evaluate the predictions from the different MSPred decoder heads. For future frame prediction, we compute several popular metrics which measure the visual similarity between the predicted and ground-truth video frames. Furthermore, we employ different metrics to evaluate the ability of our model to make high-level structured predictions, such as human poses or semantic segmentation maps. For all metrics, we average the results across all predicted frames or high-level structured representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Similarity Metrics:</head><p>We evaluate our models for future frame prediction using four popular metrics, namely MSE, PSNR, SSIM [43], and LPIPS <ref type="bibr">[45]</ref>. MSE, PSNR and SSIM measure pixel or statistical differences between predicted and target images. However, they have been proven to correlate poorly with human perception, favoring blurred predictions over more detailed, though imperfect, generations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">45]</ref>. Therefore, we favor LPIPS in our experiments, which measures the distance between CNN feature maps, and has been shown to better correlate with human judgment.</p><p>Pose and Keypoint Prediction Metrics: MSPred forecasts future human poses at its intermediate level on the KTH-Actions dataset. Given a predicted heatmap representing the location of a body joint, we extract the position coordinates by taking the location with maximum value of the heatmap, provided that the maximum value exceeds a certain threshold. Through empirical validation, we set the threshold value to 0.05. In order to assess our model's performance for human pose forecasting, we employ three popular metrics. Mean Per Joint Position Error (MPJPE) calculates the average 2-norm across predicted and target joints. Percentage of Detected Joints (PDJ) measures the fraction of the correctly estimated joints among the joints present in the ground-truth pose. A predicted keypoint is marked as a correct detection if its distance from the respective target keypoint does not exceed a certain threshold. We select this threshold as 20% of the groundtruth person's height <ref type="bibr" target="#b29">[30]</ref>. Similarly, Percentage of Correct Keypoints (PCK) measures the fraction of correctly detected joints among the overall predicted joints. Additionally, we also  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation Metrics:</head><p>We predict semantic segmentation maps as the intermediate-level representation on the SynpickVP dataset. We evaluate our predicted segmentation maps using two popular evaluation metrics. Pixel accuracy (Acc) measures the fraction of correctly classified pixels in the image, whereas Intersection over Union (IoU) is computed by dividing the corresponding number of correctly estimated pixels, i.e. the area of overlap between predicted and ground-truth segments, by the area of union of the very segments. We compute the average Acc and IoU metrics for three subsets of the classes. More precisely, we average the metrics separately for three object categories: robot gripper, static objects placed on the box, and the red box itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>In this section we provide further implementation details of MSPred (Section C.1), and the hyper-parameter values used in our experiments (Section C.2). Additionally we discuss the implementation of the SVG' baseline (Section C.3). Our codebase is implemented using the PyTorch <ref type="bibr" target="#b24">[25]</ref> deep learning framework. We run our experiments on an NVIDIA A6000 GPU with 48 GiB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 MSPred Architecture Details</head><p>Encoder: In order to ensure a fair comparison with baseline methods, the encoder is implemented following the SVG <ref type="bibr" target="#b5">[6]</ref> architecture. For the Moving MNIST dataset, the encoder follows the DCGAN discriminator <ref type="bibr" target="#b28">[29]</ref> architecture, whereas for KTH-Actions and SynpickVP we employ VGG16-like <ref type="bibr" target="#b35">[36]</ref> modules. The architectures of both encoders are depicted in <ref type="table" target="#tab_5">Tables 5 and 6</ref>, respectively. All convolutional layers use padding 'SAME', include a bias weight, and are followed by batch normalization <ref type="bibr" target="#b13">[14]</ref>.</p><p>Decoder: The decoder in MSPred is implemented as a mirrored version of the corresponding encoder. In the DCGAN-like decoder, feature maps are upsampled via transposed convolutions, whereas in the VGG-like decoder upsampling is achieved via nearest neighbor interpolation. The higher-level decoder heads are each composed of two convolutional blocks with the same structure as the decoder. Predictor: Our predictor module uses four ConvLSTM <ref type="bibr" target="#b33">[34]</ref> cells for each of the three levels in the hierarchy, each with 128 kernels of size 3 ? 3. The lowest-level LSTM processes all inputs, whereas the higher-level LSTMs process one out of every T 1 and T 2 inputs respectively.</p><p>Stochastic Component: The prior (LST M ? ) and posterior (LST M ? ) modules are implemented as a single-cell ConvLSTM with 64 kernels of size 3?3, followed by a convolutional layer mapping the feature maps into the desired latent dimensionality. Inspired by SVG <ref type="bibr" target="#b39">[40]</ref>, we sample latent tensors with 10, 24 and 32 channels for the Moving MNIST, KTH-Actions, and SypickVP datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyper-Parameters</head><p>The hyper-parameters used in our experiments are reported in <ref type="table" target="#tab_7">Table 7</ref>. We report the specific values for the experiments on each of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SVG'</head><p>As described in Section 4.4 of the paper, we train a specialized baseline SVG', based on a modified SVG-LP [6] model, which predicts high-level representations (e.g. human poses or semantic segmentation) conditioned on input video frames. SVG' follows the same architecture as SVG-LP, but we apply some modifications to adapt the model for the tasks of pose and semantic segmentation forecasting, and for a fair comparison with MSPred. First, the linear LSTM recurrent blocks are replaced by ConvLSTMs operating with a period of T 1 , i.e., processing every T 1 -th input. Second, the number of output channels is changed from three to nine for KTH-Actions, and to 22 for SynpickVP. Finally, since there are no predicted image frames to be fed back into the model, we design SVG' to be autoregressive in the feature space, i.e., the output of the predictor module becomes its input at the subsequent time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Results</head><p>In <ref type="figure">Figures 5-8</ref>, we qualitatively compare several video prediction models for the task of future frame prediction on the Moving MNIST, KTH-Actions and SynpickVP datasets, re- <ref type="figure">Figure 5</ref>: Qualitative results on the Moving MNIST dataset. Top row corresponds to ground truth frames. We display four seed frames and five predictions for three test-set sequences.</p><p>In general, all compared methods achieve good frame predictions. However, only MSPred accurately resolves challenging cases in which digits overlap. Colors are inverted to improve the visualization. <ref type="figure">Figure 6</ref>: Qualitative results on the KTH-Actions dataset. Top row corresponds to ground truth frames. We display four seed frames and five predictions for three test-set sequences. MSPred achieves the sharpest and more accurate predictions among the compared methods. <ref type="figure">Figure 7</ref>: Qualitative results on the SynpickVP dataset. Top row corresponds to ground truth frames. We display three seed frames and five predictions for two test-set sequences. MSPred qualitatively outperforms the compared methods, achieving sharp reconstructions, whereas the baseline methods tend to blur the predictions. spectively. <ref type="figure" target="#fig_0">Figures 9 and 10</ref> depict additional examples of multi-scale prediction on the KTH and SynpickVP datasets, respectively. Further images and animations can be found in the project website 2 . <ref type="figure">Figure 8</ref>: Qualitative results on the SynpickVP dataset. Top row corresponds to ground truth frames. We display three seed frames and five predictions for two test-set sequences. MSPred qualitatively outperforms the compared methods, achieving sharp reconstructions, whereas the baseline methods tend to blur the predictions. <ref type="figure">Figure 9</ref>: Predictions of different level of abstraction on the KTH-Actions dataset. We display three seed frames and five targets and predictions for each decoder head. MSPred forecasts frames on short time horizons, while also predicting human poses and person locations longer into the future using coarser temporal resolutions. <ref type="figure" target="#fig_0">Figure 10</ref>: Predictions of different levels of abstraction on the SynpickVP dataset. We display three seed frames, and five targets and predictions for each decoder head. MSPred forecasts frames on short time horizons, while also predicting the semantic segmentation of the scene and the gripper location long into the future using coarser temporal resolutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>MSPred model. Left: Seed video frames are encoded with a convolutional encoder. LSTM modules ticking at different periods forecast features at future time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative results on the KTH-Actions dataset. Top row shows ground truth frames. We display four seed frames and five predictions for three test-set sequences. MSPred achieves the sharpest and most accurate predictions among the compared methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on the SynpickVP dataset. Top row shows ground truth frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Multi-scale predictions on the KTH-Action (left) and SynpickVP (right) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study investigating several MSPred modules and design choices, i.e. different RNNs, temporal and spatial hierarchy, and the effect of supervision on the higher levels of the hierarchy. Best result is highlighted in boldface, second best is underlined.</figDesc><table><row><cell></cell><cell cols="2">MSPred Modules</cell><cell></cell><cell cols="4">Video Prediction Results</cell></row><row><cell cols="2">RNN Spatial</cell><cell cols="3">Temporal Hierarch. Supervision MSE?</cell><cell cols="3">PSNR? SSIM? LPIPS?</cell></row><row><cell>1 Conv.</cell><cell></cell><cell></cell><cell></cell><cell>41.52</cell><cell>25.99</cell><cell>0.970</cell><cell>0.030</cell></row><row><cell>2 Conv.</cell><cell></cell><cell></cell><cell>-</cell><cell>42.47</cell><cell>25.94</cell><cell>0.970</cell><cell>0.030</cell></row><row><cell>3 Linear</cell><cell></cell><cell></cell><cell></cell><cell cols="2">208.71 17.95</cell><cell>0.827</cell><cell>0.202</cell></row><row><cell>4 Conv.</cell><cell>-</cell><cell></cell><cell></cell><cell>73.47</cell><cell>22.81</cell><cell>0.950</cell><cell>0.057</cell></row><row><cell>5 Conv.</cell><cell></cell><cell>-</cell><cell></cell><cell>92.45</cell><cell>20.81</cell><cell>0.921</cell><cell>0.093</cell></row><row><cell>6 Conv.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">112.18 20.97</cell><cell>0.912</cell><cell>0.097</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison between video prediction models. MSPred outperforms all other methods on Moving MNIST, and achieves the best perceptual results (LPIPS) on KTH-Actions and SynpickVP. Best result is highlighted in boldface, second best is underlined.</figDesc><table><row><cell></cell><cell>Moving MNIST</cell><cell>KTH-Actions</cell><cell>SynpickVP</cell></row><row><cell></cell><cell>PSNR? SSIM? LPIPS?</cell><cell>PSNR? SSIM? LPIPS?</cell><cell>PSNR? SSIM? LPIPS?</cell></row><row><cell cols="2">ConvLSTM [34] 17.22 0.833 0.144</cell><cell>29.93 0.957 0.048</cell><cell>27.98 0.907 0.059</cell></row><row><cell>TrajGRU [35]</cell><cell>20.02 0.895 0.075</cell><cell>30.02 0.958 0.039</cell><cell>28.10 0.908 0.041</cell></row><row><cell>SVG-Det [6]</cell><cell>20.31 0.900 0.114</cell><cell>26.64 0.927 0.068</cell><cell>26.92 0.879 0.068</cell></row><row><cell>SVG-LP [6]</cell><cell>20.36 0.907 0.115</cell><cell>27.60 0.932 0.063</cell><cell>27.38 0.886 0.066</cell></row><row><cell cols="2">PredRNN++ [42] 20.20 0.911 0.055</cell><cell>29.51 0.941 0.068</cell><cell>27.50 0.894 0.053</cell></row><row><cell>PhyDNet [11]</cell><cell>20.43 0.915 0.054</cell><cell>28.01 0.913 0.125</cell><cell>26.84 0.877 0.053</cell></row><row><cell cols="2">MSPred NoSup 25.94 0.970 0.030</cell><cell>28.65 0.929 0.034</cell><cell>28.92 0.902 0.031</cell></row><row><cell>MSPred (ours)</cell><cell>25.99 0.970 0.030</cell><cell>28.93 0.930 0.032</cell><cell>28.61 0.903 0.030</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between MSPred and two baselines on pose prediction and segmentation forecasting. Best result is highlighted in boldface, second best is underlined.</figDesc><table><row><cell></cell><cell></cell><cell>KTH-Actions</cell><cell></cell><cell>SynpickVP</cell></row><row><cell></cell><cell></cell><cell>Pose Forecasting</cell><cell>Gripper Seg.</cell><cell>Static Seg.</cell><cell>Backgr. Seg.</cell></row><row><cell></cell><cell cols="2">MPJPE? PCK? PDJ? AP?</cell><cell>Acc? IoU?</cell><cell>Acc? IoU? Acc? IoU?</cell></row><row><cell>CopyLast</cell><cell>10.50</cell><cell>0.550 0.558 0.620</cell><cell cols="2">0.397 0.324 0.557 0.455 0.957 0.919</cell></row><row><cell>SVG' [6]</cell><cell>4.32</cell><cell>0.810 0.888 0.825</cell><cell cols="2">0.606 0.443 0.255 0.197 0.956 0.898</cell></row><row><cell>MSPred</cell><cell>3.41</cell><cell>0.812 0.867 0.833</cell><cell cols="2">0.651 0.471 0.202 0.151 0.952 0.901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>42] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, Philip Yu, and Mingsheng Long. PredRNN: A recurrent neural network for spatiotemporal predictive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), pages 1-1, 2022.[43] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity.</figDesc><table><row><cell>IEEE Transactions on Image</cell></row><row><cell>Processing, 13(4):600-612, 2004.</cell></row><row><cell>[44] Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, and Chelsea Finn. Greedy</cell></row><row><cell>hierarchical variational autoencoders for large-scale video prediction. In IEEE/CVF</cell></row><row><cell>Conference on Computer Vision and Pattern Recognition (CVPR), pages 2318-2328,</cell></row><row><cell>2021.</cell></row><row><cell>[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The</cell></row><row><cell>unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conference</cell></row><row><cell>on Computer Vision and Pattern Recognition (CVPR), pages 586-595, 2018.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Summary of the datasets used in our experiments, including the size of the dataset splits and the type of high-level representations used for each dataset. at the intermediate level, and a single-keypoint heatmap for the robotic gripper position at the highest level. Due to the synthetic nature of the dataset, semantic segmentation and object localization annotations are readily available. When evaluating semantic segmentation forecasting, we average the class-wise results into three different categories: gripper corresponds to the robot gripper, static includes the different objects contained in the box, and background corresponds to the red box where objects are placed.</figDesc><table><row><cell>Dataset Name</cell><cell cols="2">Img. Size # Train</cell><cell># Test</cell><cell>Mid-Level Rep.</cell><cell>High-Level Rep.</cell></row><row><cell cols="2">Moving MNIST [38] (3, 64, 64)</cell><cell>-</cell><cell>10.000</cell><cell>Digit Blob</cell><cell>Digit Position</cell></row><row><cell>KTH-Actions [33]</cell><cell cols="2">(3, 64, 64) 1.436</cell><cell>824</cell><cell>Human Pose</cell><cell>Person Position</cell></row><row><cell>SynpickVP [27]</cell><cell cols="2">(3, 64, 112) 1.975</cell><cell>200</cell><cell cols="2">Segmentation Maps Gripper Position</cell></row><row><cell>22 different classes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>DCGAN Discriminator Encoder</figDesc><table><row><cell>Layer</cell><cell cols="2">Size Activation Comment</cell></row><row><cell>Conv 4x4</cell><cell cols="2">64 LeakyReLU Stride 2</cell></row><row><cell>Conv 4x4</cell><cell cols="2">128 LeakyReLU Stride 2</cell></row><row><cell>Conv 4x4</cell><cell cols="2">256 LeakyReLU Stride 2</cell></row><row><cell>Conv 4x4</cell><cell>512 LeakyReLU</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>VGG16-Like Encoder</figDesc><table><row><cell>Layer</cell><cell>Size</cell><cell>Activation</cell></row><row><cell>2x Conv 3x3</cell><cell>64</cell><cell>LeakyReLU</cell></row><row><cell>MaxPool 2x2</cell><cell>-</cell><cell>-</cell></row><row><cell>2x Conv 3x3</cell><cell cols="2">128 LeakyReLU</cell></row><row><cell>MaxPool 2x2</cell><cell>-</cell><cell>-</cell></row><row><cell>2x Conv 3x3</cell><cell cols="2">256 LeakyReLU</cell></row><row><cell>MaxPool 2x2</cell><cell>-</cell><cell>-</cell></row><row><cell>2x Conv 3x3</cell><cell cols="2">512 LeakyReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter values used for each dataset in our experiments</figDesc><table><row><cell>Hyper-Parameter</cell><cell>Moving MNIST</cell><cell>KTH-Actions</cell><cell>SynpickVP</cell></row><row><cell>C</cell><cell>9</cell><cell>9</cell><cell>9</cell></row><row><cell>T 1</cell><cell>4</cell><cell>4</cell><cell>2</cell></row><row><cell>T 2</cell><cell>8</cell><cell>8</cell><cell>4</cell></row><row><cell>Learning rate</cell><cell>10 ?4</cell><cell>3 ? 10 ?4</cell><cell>5 ? 10 ?4</cell></row><row><cell>Batch size</cell><cell>16</cell><cell>12</cell><cell>12</cell></row><row><cell>Num. Epochs</cell><cell>350</cell><cell>800</cell><cell>200</cell></row><row><cell>? 1</cell><cell>2.5</cell><cell>1.4</cell><cell>2.0</cell></row><row><cell>? 2</cell><cell>2.5</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>?</cell><cell>10 ?4</cell><cell>5 ? 10 ?5</cell><cell>10 ?4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2022. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.? denotes equal contribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/view/mspred/home</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sites.google.com/view/mspred/home</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by grant BE 2556/16-2 (Research Unit FOR 2535 Anticipating Human Behavior) of the German Research Foundation (DFG).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLAMP: Stochastic latent appearance and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Adil Kaan Akan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved conditional VRNNs for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7608" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Hierarchical multiscale recurrent neural networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion segmentation using frequency domain transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hafez</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local frequency domain transformer networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hafez</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Nogga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term human video generation of multiple futures using poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Fushishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Tejero-De Pablos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="596" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disentangling physical dynamics from unknown factors for unsupervised video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vincent Le Guen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A clockwork RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flow-grounded spatial-temporal video prediction from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="600" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="584" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling deep temporal dependencies with recurrent grammar cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Konda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A review on deep learning techniques for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Alejandro</forename><surname>Castro-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2806" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video generation from single semantic label map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Viorica P?tr?ucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SynPick: A dataset for dynamic bin picking scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Selvam Periyasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 17th International Conference on Automation Science and Engineering (CASE)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="488" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabeau</forename><surname>Pr?mont-Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tele</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinu</forename><surname>Boney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment through FSIM, SSIM, MSE and PSNR-a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umme</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morium</forename><surname>Akter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shorif Uddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clockwork variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>S?ren Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High fidelity video prediction with large stochastic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkanath</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harini</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pre-dRNN: Recurrent neural networks for predictive learning using spatiotemporal LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
