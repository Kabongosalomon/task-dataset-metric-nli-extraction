<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Image Reconstruction using Deep Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
							<email>razvan@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Moyer</surname></persName>
							<email>dmoyer@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Golland</surname></persName>
							<email>polina@csail.mit.edu</email>
							<affiliation key="aff2">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Image Reconstruction using Deep Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning models are commonly trained end-to-end and in a supervised setting, using paired (input, output)  data. Examples include recent super-resolution methods that train on pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches require re-training every time there is a distribution shift in the inputs (e.g., night images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for building powerful image priors, which enable application of Bayes' theorem for many downstream reconstruction tasks. Our method, Bayesian Reconstruction through Generative Models (BRGM), uses a single pre-trained generator model to solve different image restoration tasks, i.e., super-resolution and in-painting, by combining it with different forward corruption models. We keep the weights of the generator model fixed, and reconstruct the image by estimating the Bayesian maximum a-posteriori (MAP) estimate over the input latent vector that generated the reconstructed image. We further use Variational Inference to approximate the posterior distribution over the latent vectors, from which we sample multiple solutions. We demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from the Flick Faces High Quality dataset [1] (ii) 240,000 chest X-rays from MIMIC III [2] and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans <ref type="bibr" target="#b2">[3]</ref>. Across all three datasets and without any dataset-specific hyperparameter tuning, our simple approach yields performance competitive with current task-specific state-of-the-art methods on super-resolution and in-painting, while being more generalisable and without requiring any training. Our source code and pre-trained models are available online: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While end-to-end supervised learning is currently the most popular paradigm in the research community, it suffers from several problems. First, distribution shifts in the inputs often require re-training, as well as the effort of collecting an updated dataset. In some settings, such shifts can occur often (hospital scanners are upgraded) and even continuously (population is slowly aging due to improved healthcare). Secondly, current state-of-the-art machine learning (ML) models often require prohibitive computational resources, which are only available in a select number of companies and research <ref type="figure">Figure 1</ref>: (a) Classical deep-learning methods for image reconstruction learn to invert specific corruption models such as downsampling with a specific kernel or in-painting with rectangular masks. (b) We use a generative approach that can handle arbitrary corruption processes, such as downsampling or in-painting with an arbitrary mask, by optimizing for it on-the-fly at inference time. Given a latent vector w, we use generator G to generate clean images G(w), followed by a corruption model f to generate a corrupted image f ? G(w). Given an input image I, we find the latent w * that generated the input image using the Bayesian MAP estimate w * = arg max w p(w)p(I|f ? G(w)), and we use Variational Inference to sample from the posterior p(w|I). This can be repeated for other corruption processes (f 2 , f 3 ) such as masking, motion, to-grayscale, as well as for other parametrisations of the process (e.g., super-resolution with different kernels or factors).</p><p>centers. Therefore, the ability to leverage pre-trained models for solving downstream prediction or reconstruction tasks becomes crucial.</p><p>Deep generative models have recently obtained state-of-the-art results in simulating high-quality images from a variety of computer vision datasets. Generative Adversarial Networks (GANs) such as StyleGAN2 <ref type="bibr" target="#b3">[4]</ref> and StyleGAN-ADA <ref type="bibr" target="#b4">[5]</ref> have been demonstrated for unconditional image generation, while BigGAN has shown impressive performance in class-conditional image generation <ref type="bibr" target="#b5">[6]</ref>. Similarly, Variational Autoencoder-based methods such as VQ-VAE <ref type="bibr" target="#b6">[7]</ref> and ?-VAE <ref type="bibr" target="#b7">[8]</ref> have also been competitive in several image generation tasks. Other lines of research in deep generative models are auto-regressive models such as PixelCNN <ref type="bibr" target="#b8">[9]</ref> and PixelRNN <ref type="bibr" target="#b9">[10]</ref>, as well as invertible flow models such as NeuralODE <ref type="bibr" target="#b10">[11]</ref>, Glow <ref type="bibr" target="#b11">[12]</ref> and RealNVP <ref type="bibr" target="#b12">[13]</ref>. While these models generate high-quality images that are similar to the training distribution, they are not directly applicable for solving more complex tasks such as image reconstruction.</p><p>A particularly important application domain for generative models are inverse problems, which aim to reconstruct an image that has undergone a corruption process such as blurring. Previous work has focused on regularizing the inversion process using smoothness <ref type="bibr" target="#b13">[14]</ref> or sparsity <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> priors. However, such priors often result in blurry images, and do not enable hallucination of features, which is essential for such ill-posed problems. More recent deep-learning approaches <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> address this challenge using training data made of pairs of (low-resolution, high-resolution) images. However, one fundamental limitation is that they compute the pixelwise or perceptual loss in the high-resolution/inpainted space, which leads to the so-called averaging effect <ref type="bibr" target="#b21">[22]</ref>: since multiple high-resolution images map to the same low-resolution image I, the loss function minimizes the average of all such solutions, resulting in a blurry image. Some methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> address this through adversarial losses, which force the model to output a solution that lies on the image manifold. However, even with adversarial losses, it is not clear which solution image is retrieved, and how to sample multiple solutions from the posterior distribution.</p><p>To overcome the averaging of all possible solutions to ill-posed problems, one can build methods that estimate by design all potential solutions, or a distribution of solutions, for which a Bayesian framework is a natural choice. Bayesian solutions for image reconstruction problems include: Markov Random Field (MRF) priors for denoising and in-painting <ref type="bibr" target="#b23">[24]</ref>, generative models of photosensor responses from surfaces and illuminants <ref type="bibr" target="#b24">[25]</ref>, MRF models that leverage global statistics for inpainting <ref type="bibr" target="#b25">[26]</ref>, Bayesian quantification of the distribution of scene parameters and light direction for inference of shape, surface properties and motion <ref type="bibr" target="#b26">[27]</ref>, and sparse derivative priors for super-resolution and image demosaicing <ref type="bibr" target="#b27">[28]</ref>. The seminal work of <ref type="bibr" target="#b28">[29]</ref> formulated the image reconstruction task as the Bayesian optimization over an energy function in a lattice-like physical system.</p><p>Starting with <ref type="bibr" target="#b29">[30]</ref>, several works proposed solving inverse problems using deep generative priors <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>, which constrain the solution to belong to the learned image manifold of a deep generative model. While all these methods focus on deriving a single point estimate, some recent methods further derive a distribution of potential solutions <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> through Langevin dynamics. However, Langevin dynamics has slow mixing, and additionally, their loss function is only loosely based on a Bayesian MAP estimate. A fully-Bayesian model allows optimizing distributional losses instead of single point-estimate losses, and allows the derivation of more complex uncertainty measures, such as ?-level confidence intervals. While <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">38]</ref> solve the inverse problem in a Bayesian setting, they only demonstrate it for invertible flow models. It is thus still unclear how to use models other than flows, such as GANs or VAEs, to perform this reconstruction in a Bayesian framework, and how to use GANs to capture a distribution of solutions for a given corrupted image.</p><p>In this work, we propose a Bayesian method to perform reconstruction through deep generative priors built using state-of-the-art GAN models. Given a pre-trained generator G (here StyleGAN2), a known corruption model f , and a corrupted image I to be restored, we minimize at inference time the Bayesian MAP estimate w * = arg max w p(w)p(I|f ? G(w)), where w is latent vector given as input to G. We further adapt previous Variational Inference (VI) methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref> to approximate the posterior p(w|I) with a Gaussian distribution, thus enabling sampling of multiple solutions. Our key theoretical contributions are (i) the formulation of image reconstruction in a principled Bayesian framework using deep generative priors and (ii) the adaptation of previous deep-learning based VI methods to sample multiple reconstructions, while on the application side, we demonstrate our method on three datasets, including two challenging medical datasets, and show its competitive performance against four state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Deep Generative Prior (DGP) methods: A related work to ours is PULSE <ref type="bibr" target="#b41">[41]</ref>, which uses pretrained StyleGAN models for face super-resolution. Other DGP methods <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> are close to our work, especially those that derive a distribution of potential solutions either through (i) direct sampling from conditionalized flow models <ref type="bibr" target="#b34">[35]</ref>, (ii) Langevin dynamics <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> or (iii) Variational Inference (VI) <ref type="bibr" target="#b38">[38]</ref>, each having several advantages and disadvantages: (i) conditionalized flow models generate independent samples from the exact posterior, but use more restricted transformations due to the need to have computable Jacobian determinants, (ii) Langevin dynamics also sample the exact posterior, but generate highly correlated samples and can get stuck in distributional modes while (iii) VI methods can generate a large number of independent samples from an approximate posterior, but require fitting the variational parameters. Our sampling method that uses VI is closest to <ref type="bibr" target="#b38">[38]</ref>, but we demonstrate it for GAN models such as StyleGAN2 instead of normalizing flows as in <ref type="bibr" target="#b38">[38]</ref>.</p><p>Encoder methods: Other approaches attempt to invert generative models by estimating encoders that map the input images directly into the generator's latent space <ref type="bibr" target="#b44">[44]</ref> in an end-to-end framework. Encoder methods are complimentary to our work, as they can be used to obtain a fast initial estimate of the latent w, followed by a slightly longer optimisation process such as ours that can give more accurate reconstructions.</p><p>Other inverse problems methods: Approaches similar to ours have also been discussed in inverse problems research. Deep Bayesian Inversion <ref type="bibr" target="#b45">[45]</ref> performs image reconstruction using a supervised learning approximation. AmbientGAN <ref type="bibr" target="#b46">[46]</ref> builds a GAN model of clean images given noisy observations only, for a specified corruption model. Deep Image Prior (DIP) <ref type="bibr" target="#b47">[47]</ref> has shown that the structure of deep convolutional networks captures texture-level statistics that can be used for zero-shot image reconstruction. MimicGAN <ref type="bibr" target="#b48">[48]</ref> has shown how to optimise the parameters of the unknown corruption model. Image2StyleGAN <ref type="bibr" target="#b42">[42]</ref> finds a projection of a given clean image in the latent space of StyleGAN, while the more updated Image2StyleGAN++ <ref type="bibr" target="#b43">[43]</ref> uses the estimated latent projections to demonstrate image manipulations, as well as in-painting. Recent neural radiance fields (NeRF) <ref type="bibr" target="#b49">[49]</ref> achieved state-of-the-art results for synthesizing novel views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>An overview of our method is shown in <ref type="figure">Fig. 1</ref>. We assume a given generator G can model the distribution of clean images in a given dataset (e.g., human faces), then use a pre-defined forward model f that corrupts the clean image. Given a corrupted input image I, we reconstruct it as G(w * ),  where w * is the Bayesian MAP estimate over the latent vector w of G. The graphical model is given in Supp. <ref type="figure" target="#fig_3">Fig. 7</ref>.</p><p>Given an input corrupted image I, we aim to reconstruct the clean image I CLN . In practice, there could be a distribution p(I CLN |I) of such clean images given a particular input image I, which is estimated using Bayes' theorem as p(I CLN |I) ? p(I CLN )p(I|I CLN ). The prior term p(I CLN ) describes the manifold of clean images, restricting the possible reconstructions I CLN to realistic images. In our context, the likelihood term p(I|I CLN ) describes the corruption process f , which takes a clean image and produces a corrupted image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The image prior term</head><p>The prior model p(I CLN ) has been trained a-priori, before the corruption task is known, hence satisfying the principle of independent mechanisms from causal modelling <ref type="bibr" target="#b50">[50]</ref>. In our experiments,</p><formula xml:id="formula_0">I CLN = G(w), where w = [w 1 , . . . , w 18 ] ? R 512?18</formula><p>is the latent vector of StyleGAN2 (18 vectors for each resolution level), G : R 512?18 ? R n G ?n G is the deterministic function given by the StyleGAN2 synthesis network, and n G ? n G is the output resolution of StyleGAN2, in our case 1024x1024 (FFHQ, X-Rays) or 256x256 (brains). Our framework is not specific to StyleGAN2: other generator models such as invertible flows <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> or VAEs <ref type="bibr" target="#b39">[39]</ref> can be used, as long as one can flow gradients through the model.</p><p>We use the change of variables to express the probability density function over clean images:</p><formula xml:id="formula_1">p(I CLN ) := p(G(w)) = p(w) ?G(w) ?w ?1<label>(1)</label></formula><p>While the traditional change of variables formula assumes that the function G is invertible, it can be extended to non-invertible 1 mappings <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>. In addition, we assume that the Jacobian determinant ?G(w) ?w is constant for all w, which is a reasonable assumption in StyleGAN2 due to its path length regularization (see Eq. 4 in <ref type="bibr" target="#b3">[4]</ref>).</p><p>We now seek to instantiate p(w). Since the latent space of StyleGAN2 consists of many vectors w = [w 1 , . . . , w L ], where L = 18 (one for each layer), we need to set meaningful priors for them. While StyleGAN2 assumed that all vectors w i are equal, we slightly relax that assumption but set two priors: (i) a cosine similarity prior similar to PULSE [41] that ensures every pair w i and w j are roughly colinear, and (ii) another prior N (w i |?, ? 2 ) that ensures the w vectors lie in the same region as the vectors used during training. We use the following distribution for p(w):</p><formula xml:id="formula_2">p(w) = i N w i ?, ? 2 i,j M cos ?1 w i w T j |w i ||w j | 0, ?<label>(2)</label></formula><p>where cos ?1 wiw T j |wi||wj | is the angle between vectors w i and w j , and M(.|0, ?) is the von Mises distribution with mean zero and scale parameter ? which ensures that vectors w i are aligned. This distribution is analogous to a Gaussian distribution over angles in [0, 2?]. We compute ? and ? as the mean and standard deviation of 10,000 latent variables passed through the mapping network, like the original StyleGAN2 inversion <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The image likelihood term</head><p>We instantiate the likelihood term p(I|I CLN ) with a potentially probabilistic forward corruption process f (I CLN ; ?), parameterized 2 by ?. We study two types of corruption processes f as follows:</p><p>? Super-resolution: f SR is defined as the forward operator that performs downsampling parameterized by a given kernel k. The likelihood model becomes:</p><formula xml:id="formula_3">p(I|I CLN ) = p(I|G(w)) = p(I|f ? G(w))|J f (G(w)) | ?1 (3) where J f (G(w)) = ?f ?G(w) ?G(w)</formula><p>is the Jacobian matrix of f evaluated at G(w), and is again assumed constant. For the noise model in p(I|f ? G(w)), we consider two types of noise distributions: pixelwise independent Gaussian noise, as well as "perceptual noise", i.e. independent Gaussian noise in the perceptual VGG embedding space. This yields the following model <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_4">p(I|f ? G(w)) = N (I|f ? G(w), ? 2 pixel I n 2 f ) N (?(I)|? ? f ? G(w), ? 2 percept I n 2 ? )<label>(4)</label></formula><p>where ? : R n f ?n f ? R n ? ?n ? is the VGG network, ? 2 pixel I n 2 f and ? 2 percept I n 2 ? are diagonal covariance matrices, n f ? n f and n ? ? n ? are the resolutions of the corrupted images f ? G(w) as well as perceptual embeddings ? ? f ? G(w). Images I, ?(I), f ? G(w) and ? ? f ? G(w) are flattened to 1D vectors, while covariance matrices I n 2 f and I n 2 ? are of dimensions n 2 f ? n 2 f and n 2 ? ? n 2 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image restoration as Bayesian MAP estimate</head><p>The restoration of the optimal clean image I * CLN given a noisy input image I can be performed through the Bayesian maximum a-posteriori (MAP) estimate:</p><formula xml:id="formula_5">I * CLN = arg max I CLN p(I CLN |I) = arg max I CLN p(I CLN )p(I|I CLN )<label>(5)</label></formula><p>We now instantiate the prior p(I CLN ) and the likelihood p(I|I CLN ) with formulas from Eq. 2 and Eq. 4, and recast the problem as an optimisation over w: w * = arg max w p(w)p(I|w). This can be simplified to the following loss function (see Supplementary section A for full derivation):</p><formula xml:id="formula_6">w * = arg min w i w i ? ? ? i 2 Lw ?2? i,j w i w T j |w i ||w j | Lcolin +? ?2 pixel I ? f ? G(w) 2 2 Lpixel +? ?2 percept I ? ? ? f ? G(w) 2 2</formula><p>Lpercept (6) which can be succinctly written as a weighted sum of four loss terms: w * = arg min w L w + ? colin L colin + ? pixel L pixel + ? percept L percept , where L w is the prior loss over w, L colin is the colinearity loss on w, L pixel is the pixelwise loss on the corrupted images, and L percept is the perceptual loss, ? colin = ?2?, ? pixel = ? ?2 pixel and ? percept = ? ?2 percept . Given the Bayesian MAP solution w * , the clean image is returned as I * CLN = G(w * ) <ref type="bibr" target="#b1">2</ref> Since in our experiments ? is fixed, we drop the notation of ? in subsequent derivations. <ref type="bibr" target="#b2">3</ref> Model is equivalent to </p><formula xml:id="formula_7">p(I|f ? G(w)) = N I ?(I) f ? G(w) ? ? f ? G(w) , ? 2 pixel I n 2 f 0 0 ? 2 percept I n 2 ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sampling multiple reconstructions using Variational Inference</head><p>To sample multiple image reconstructions from the posterior distribution p(w|I), we use Variational Inference. We use an approach similar to the Variational Auto-encoder <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b53">53]</ref>, where for each data-point we estimate a Gaussian distribution of latent vectors, with the main difference that we do not use an encoder network, but instead optimise the mean and covariance directly. Thus, our approach is also similar to Bayes-by-Backprop (BBB) <ref type="bibr" target="#b40">[40]</ref>, but we estimate a Gaussian distribution over the latent vector, instead of the network weights as in their case.</p><p>Variational inference (Hinton and Van Camp 1993, Graves 2011) aims to find a parametric approximation q(w|?), where ? are parameters to be learned, to the true posterior p(w|I) over the latent inputs w to the generator network. We seek to minimize:</p><formula xml:id="formula_8">? * = arg min ? KL [q(w|?)||p(w|I)] = arg min ? q(w|?) log q(w|?) p(w)p(I|w) dw</formula><p>Using the same approach as in Bayes-by-Backprop <ref type="bibr" target="#b40">[40]</ref>, we approximate the expected value over q(w|?) using Monte Carlo samples w (i) taken from q(w|?):</p><formula xml:id="formula_9">? * = arg min ? n i=1 log q(w (i) |?) ? log p(w (i) ) ? log p(I|w (i) )<label>(7)</label></formula><p>We parameterize q(w|?) as a Gaussian distribution, although in practice we can choose any parametric form for q (e.g. mixture of Gaussians) due to the Monte Carlo approximation. We sample the Gaussian by first sampling unit Gaussian noise , and then shifting it by the variational mean ? v and variational standard deviation ? v . To ensure ? v is always positive, we re-parameterize it as</p><formula xml:id="formula_10">? v = log(1 + exp(? v )). The variational posterior parameters are ? = [? v , ? v ].</formula><p>The prior and likelihood models, p(w (i) ) and p(I|w (i) ) are as defined in Eq. 2 and Eq. 4.</p><p>While the role of the entropy term n i=1 q(w (i) |?) is to regularize the variance of q and ensure there is no mode-collapse, we found it useful to also add a prior over the variational parameter ? v , to give the samples more variability. We therefore optimise the following:</p><formula xml:id="formula_11">? * = arg min ? ? log p(?) + n i=1 log q(w (i) |?) ? log p(w (i) ) ? log p(I|w (i) )<label>(8)</label></formula><p>where</p><formula xml:id="formula_12">p(?) = f (? v ; ?, ?)</formula><p>is an inverse gamma distribution on the variational parameter ? v , with concentration ? and rate ?. This prior, although optional, encourages larger standard deviations, which ensure that as much of the posterior as possible is covered. Note that, even with this prior, we  <ref type="figure">Figure 4</ref>: Qualitative evaluation on medical datasets at different resolutions. The left column shows input images, while the right column shows the true high-quality images. BRGM shows improved quality of reconstructions across all resolution levels and datasets. We used the exact same setup as in FFHQ in <ref type="figure">Fig. 3</ref>, without any dataset-specific parameter tuning.</p><p>don't optimise ? v directly, rather we optimise ? v . We compute the gradients using the same approach as in Bayes-by-Backprop <ref type="bibr" target="#b40">[40]</ref>.</p><p>To sample the posterior p(w|I), we also tried Stochastic Gradient Langevin Dynamics (SGLD) <ref type="bibr" target="#b54">[54]</ref> and Variational Adam <ref type="bibr" target="#b55">[55]</ref>, which is equivalent to Variational Online Gauss-Newton (VOGN) <ref type="bibr" target="#b55">[55]</ref> in our case when the batch size is 1. However, we could not get these methods to work in our setup: SGLD was adding noise of too-high magnitude and the optimisation quickly diverged, while Variational Adam produced little variability between the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model Optimisation</head><p>We optimise the loss in Eq. 6 using Adam <ref type="bibr" target="#b56">[56]</ref> with learning rate of 0.001, while fixing ? colin , ? pixel and ? percept , ? and ? a-priori. On our datasets, we found the following values to give good results: ? colin = 0.03, ? pixel = 10 ?5 , ? percept = 0.01, ? = 0.1 and ? = 0.95. In <ref type="figure" target="#fig_0">Fig 2,</ref> we show image super-resolution and in-painting starting from the original StyleGAN2 inversion, and gradually modify the loss function and optimisation until we arrive at our proposed solution. The original StyleGAN2 inversion results in line artifacts for super-resolution, while for in-painting it cannot reconstruct well. After removing the optimisation of noise layers from the original StyleGAN2 inversion <ref type="bibr" target="#b3">[4]</ref> and switching to the extended latent space W + , where each resolution-specific vector w 1 , ... , w 18 is independent, the image quality improves for super-resolution, while for in-painting the existing image is recovered well, but the reconstructed part gets even worse. More improvements are observed by adding the pixelwise L 2 loss, mostly because the perceptual loss only operates at 256x256 resolution. Adding the prior on w and the cosine loss produces smoother reconstructions with less artifacts, especially for in-painting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Model training and evaluation</head><p>We train our model on data from three datasets: (i) 70,000 images from FFHQ <ref type="bibr" target="#b0">[1]</ref> at 1024 2 resolution, 240,000 frontal-view chest X-ray image from MIMIC III <ref type="bibr" target="#b1">[2]</ref> at 1024 2 resolution, as well as 7,329 middle coronal 2D slices from a collection of 5 brain datasets: ADNI <ref type="bibr" target="#b57">[57]</ref>, OASIS <ref type="bibr" target="#b58">[58]</ref>, PPMI <ref type="bibr" target="#b59">[59]</ref>, AIBL <ref type="bibr" target="#b60">[60]</ref> and ABIDE <ref type="bibr" target="#b61">[61]</ref>. We obtained ethical approval for all data used. All brain images were pre-registered rigidly. For all experiments, we trained the generator, in our case StyleGAN2, on 90% of the data, and left the remaining 10% for testing. We did not use the pre-trained StyleGAN2 on FFHQ as it was trained on the full FFHQ. Training was performed on 4 Titan-Xp GPUs using StyleGAN2 config-e, and was performed for 20,000,000 images shown to the discriminator (20,000 kimg), which took almost 2 weeks on our hardware. For a description of the generator training on all We compare our method with the state-of-the-art methods in super-resolution (ESRGAN <ref type="bibr" target="#b17">[18]</ref> and SRFBN <ref type="bibr" target="#b18">[19]</ref>) as well as in-painting (SN-PatchGAN <ref type="bibr" target="#b20">[21]</ref>). We additionally compared with PULSE <ref type="bibr" target="#b41">[41]</ref> due to its closeness to our method, as well as the use of the state-of-the-art StyleGAN model. For these methods, we downloaded the pre-trained models. We could not compare with NeRF <ref type="bibr" target="#b49">[49]</ref> as it requires multiple views of the same object. Since DIP <ref type="bibr" target="#b47">[47]</ref> uses statistics in the input image only and cannot handle large masks or large super-resolution factors, we did not include it in the performance evaluation, although we show results with DIP in the supplementary material. For PULSE <ref type="bibr" target="#b41">[41]</ref>, we only tested it on FFHQ, as for the medical datasets it required re-training of the StyleGAN2 generator in their own PyTorch implementation, that was different from the official StyleGAN2 implementation.</p><p>We release our code with the CC-BY license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We applied BRGM and the other models on super-resolution at different resolution levels ( <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4</ref>). On all three datasets, our method performs considerably better than other models, in particular at lower input resolutions: ESRGAN yields jittery artifacts, SRFBN gives smoothed-out results, while PULSE generates very high-resolution images that don't match the true image, likely due to the hard projection of their optimized latent to S d?1 , the unit sphere in d-dimensions, as opposed to a soft prior term such as L w in our case. Moreover, as opposed to ESRGAN and SRFBN, both our model as well as PULSE can perform more than x4 super-resolution, going up to 1024x1024. Without changing any hyper-parameters, we observe similar trends on the other two medical datasets. <ref type="figure">Fig. 5</ref> illustrates our method's performance on in-painting with arbitrary as well as rectangular masks, as compared to to the leading in-painting model SN-PatchGAN <ref type="bibr" target="#b20">[21]</ref>. Our method produces considerably better results than SN-PatchGAN. In particular, SN-PatchGAN lacks high-level semantics in the reconstruction, and cannot handle large masks. For example, in the first figure, when the mother is cropped out, SN-PatchGAN is unable to reconstruct the ear. Our method on the other hand is able to reconstruct the ear and the jawline. One reason for the lower performance of SN-PatchGAN could be that it was trained on CelebA, which has lower variation than FFHQ. In <ref type="figure">Supplementary Figs. 9, 10</ref> and 11, we show further in-painting examples with our method as well as SN-PatchGAN <ref type="bibr" target="#b20">[21]</ref>, on all three datasets, and for different types of arbitrary masks.</p><p>In <ref type="figure" target="#fig_2">Fig. 6</ref>, we show samples from the variational posterior q(w|?), for both super-resolution and in-painting. For super-resolution, we show an extreme downsampling example (x256) going from 1024x1024 to 4x4, in order to clearly see the potential variability in the reconstructions. The Variational Inference method gives samples of reasonably high variability and fidelity, although in harder cases (Supp. <ref type="figure">Fig. 13 and 14</ref>) it overfits the posterior.</p><p>In supplementary section E, we show in an ablation study over the hyper-parameters ? pixel , ? percept and ? colin that our method is not sensitive to the choice of these parameters, as there are multiple levels of magnitudes giving good results. In addition, in supplementary section F, we show that BRGM-reconstructed images can be used for downstream processing, through an example of edema severity prediction on the Chest X-Ray images.   <ref type="bibr" target="#b18">[19]</ref> achieves the lowest LPIPS <ref type="bibr" target="#b62">[62]</ref> and root mean squared error (RMSE), albeit the qualitative results from this method showed that the reconstructions are overly smooth and lack detail. The performance degradation of our model is likely because the StyleGAN2 generator G cannot easily generate these unseen images at high resolutions, although this is expected to change in the near future given the fast-paced improvements in such generator models. However, compared to those methods, our method is more generalisable as it is not specific to a particular type of corruption, and can increase the resolution by a factor higher than 4x. In <ref type="table" target="#tab_8">Supplementary Table 2</ref>, we additionally provide PSNR, SSIM and MAE scores, which show a similar behavior to LPIPS and RMSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative evaluation</head><p>For quantitative evaluation on in-painting, we generated 7 masks similar to the setup of <ref type="bibr" target="#b43">[43]</ref>, and applied them in cyclical order to 100 unseen images from the test sets of each dataset. In <ref type="table" target="#tab_4">Table 1</ref> (top-right), we show that our method consistently outperforms SN-PatchGAN <ref type="bibr" target="#b20">[21]</ref> with respect to all performance measures.</p><p>To account for human perceptual quality, we performed a forced-choice pairwise comparison test, which has been shown to be most sensitive and simple for users to perform <ref type="bibr" target="#b63">[63]</ref>. Twenty raters were each shown 100 test pairs of the true image and the four reconstructed images by each algorithm, and raters were asked to choose the best reconstruction (see supplementary section D for more information on the design). We opted for this paired test instead of the mean opinion score (MOS) because it also accounts for fidelity of the reconstruction to the true image. This is important in our setup, because a method such as PULSE can reconstruct high-resolution faces that are nonetheless of a different person (see <ref type="figure">Fig. 3</ref>). In <ref type="table" target="#tab_4">Table 1</ref> (bottom-right), the results confirm that out method is the best at low 16 2 resolution and second-best at 32 2 resolution, with lower performance at 64 2 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method limitations and potential negative societal impact</head><p>In Supp. <ref type="figure" target="#fig_3">Fig. 17</ref>, we show failure cases on the super-resolution task. The reason for the failures is likely due to the limited generalisation abilities of the StyleGAN2 generator to such unseen images. We particularly note that, as opposed to the simple inversion of Image2StyleGAN <ref type="bibr" target="#b42">[42]</ref>, which relies on latent variables at high resolution to recover the fine details, we cannot optimize these highresolution latent variables, thus having to rely on the proper ability of StyleGAN2 to extrapolate from lower-level latent variables. Another limitation of our method is the inconsistency between the downsampled input image and the given input image, which we exemplify in Supp. <ref type="figure" target="#fig_2">Figs 15 and 16</ref>.</p><p>We attribute this again to the limited generalisation of the generator to these unseen images. The same inconsistency also applies to in-painting, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>By leveraging models pre-trained on FFHQ, our methodology can be potentially biased towards images from people that are over-represented in the dataset. On the medical datasets, we also have biases in disease labels. For example, the MIMIC dataset contains both healthy and pneumonia  lung images, but many other lung conditions are not covered, while for the brain dataset it contains healthy brains as well as Alzheimer's and Parkinson's, but does not cover rarer brain diseases. Before deployment in the real-world, further work is required to make the method robust to diverse inputs, in order to avoid negative impact on the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a simple Bayesian framework for performing different reconstruction tasks using deep generative models such as StyleGAN2. We estimate the optimal reconstruction as the Bayesian MAP estimate, and use Variational Inference to sample from an approximate posterior of all possible solutions. We demonstrated our method on two reconstruction tasks, and on three distinct datasets, including two challenging medical datasets, obtaining competitive results in comparison with state-ofthe-art models. Future work can focus on jointly optimizing the parameters of the corruption models, as well as extending to more complex corruption models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of loss function for the Bayesian MAP estimate</head><p>We assume w = [w 1 , . . . , w 18 ] ? R 512?18 is the StyleGAN2 latent vector, I ? R n?n is the corrupted input image, G : R 512?18 ? R n G ?n G is the StyleGAN2 generator network function, f : R n G ?n G ? R n f ?n f is the corruption function, and ? : R n f ?n f ? R n ? ?n ? is a function describing the perceptual network. n G ? n G , n f ? n f and n ? ? n ? are the resolutions of the clean image G(w), corrupted image f ? G(w) and of the perceptual embedding ? ? f ? G(w). The full Bayesian posterior p(w|I) of our model is proportional to:</p><formula xml:id="formula_13">p(w|I) ? p(w)p(I|w) = i N (w i |?, ? 2 ) i,j M(cos ?1 w i w T j |w i ||w j | |0, ?) N (I|f ? G(w), ? 2 pixel I n 2 f ) N (?(I)|? ? f ? G(w), ? 2 percept I n 2 ? )<label>(9)</label></formula><p>where ? ? R, ? ? R are means and standard deviations of the prior on w i , M(.|0, ?) is the von Mises distribution 4 with mean zero and scale parameter ?, and ? 2 pixel I n 2 f and ? 2 percept I n 2 ? are identity matrices scaled by variance terms.</p><p>The Bayesian MAP estimate is the vector w * that maximizes Eq. 9, and provides the most likely vector w that could have generated input image I:</p><formula xml:id="formula_14">w * = arg max w p(w)p(I|w) = arg max w i N (w i |?, ? 2 ) i,j M(cos ?1 w i w T j |w i ||w j | |0, ?) N (I|f ? G(w), ? 2 pixel I n 2 f )N (?(I)|? ? f ? G(w), ? 2 percept I n 2 ? )<label>(10)</label></formula><p>Since logarithm is a strictly increasing function that won't change the output of the arg max w operator, we take the logarithm to simplify Eq. 10 to:</p><formula xml:id="formula_15">w * = arg max w i log N (w i |?, ? 2 ) + i,j log M(cos ?1 w i w T j |w i ||w j | |0, ?)+ log N (I|f ? G(w), ? 2 pixel I n 2 f ) + log N (?(I)|? ? f ? G(w), ? 2 percept I n 2 ? )<label>(11)</label></formula><p>We expand the probability density functions of each distribution to get:</p><formula xml:id="formula_16">w * = arg max w i C 1 ? (w i ? ?) 2 2? 2 i + i,j C 2 + ?cos(cos ?1 w i w T j |w i ||w j | ) + C 1 ? 1 2 (I ? f ? G(w)) T (? ?2 pixel I n 2 f )(I ? f ? G(w)) + C 2 ? 1 2 (I ? ? ? f ? G(w)) T (? ?2 percept I n 2 ? )(I ? ? ? f ? G(w))<label>(12)</label></formula><p>where C 1 = log (2?? 2 i ) ? 1 2 , C 2 = log (?2?I 0 (?)), C 3 = log ((2?) n 2 |? pixel I n 2 f |) ? 1 2 and C 4 = log ((2?) m 2 |? percept I n 2 ? |) ? 1 2 are constants with respect to w, so we can ignore them. We remove the constants, multiply by (-2), which requires switching to the arg min operator, to get:</p><formula xml:id="formula_17">I COR = f ? G(w) I I CLN = G(w) w ? ? ? ? v ? v ? v ?pixel ?perc</formula><formula xml:id="formula_18">w * = arg min w i w i ? ? ? i 2 ? 2? i,j w i w T j |w i ||w j | + ? ?2 pixel I ? f ? G(w) 2 2 + ? ?2 percept I ? ? ? f ? G(w) 2 2<label>(13)</label></formula><p>This is equivalent to Eq. 6, which finishes our proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training StyleGAN2</head><p>In <ref type="figure">Fig. 8</ref>, we show uncurated images generated by the cross-validated StyleGAN2 trained on our medical datasets, along with a few real examples. For the high-resolution X-rays, we notice that the image quality is very good, although some artifacts are still present: some text tags are not properly generated, some bones and rib contours are wiggly, and the shoulder bones show less contrast. For the brain dataset, we do not notice any clear artifacts, although we did not assess distributional preservation of regional volumes as in <ref type="bibr" target="#b64">[64]</ref>. For the cross-validated FFHQ model, we obtained an FID of 4.01, around 0.7 points higher than the best result of 3.31 reported for config-e <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Inference times of our method</head><p>The inference time of our method is as follows: for MAP inference, it takes between 32-34 seconds for 500 iterations on a 1024x1024 image, while for fitting the variational posterior parameters (?, ?) it takes approximately 2.5 minutes for 500 iterations. Once the variational posterior is fit, the model can generate any arbitrary number of samples instantaneously.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Evaluation Results</head><p>To evaluate human perceptual quality, we performed a forced-choice pairwise comparison test as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. Each rater is shown a true, high-quality image on the left, and four potential reconstructions they have to choose from. For each input resolution level (16 2 , 32 2 , . . . ), we ran the human evaluation on 20 raters using 100 pairs of 5 images each (total of 500 images per experiment shown to each rater). We launched all human evaluations on Amazon Mechanical Turk. We paid $346 for the crowdsourcing effort, which gave workers an hourly wage of approximately $9. We obtained IRB approval for this study from our institution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Sensitivity to hyper-parameters</head><p>To understand how sensitive our model is to the hyper-parameters ? pixel , ? percept and ? colin , we performed in <ref type="table" target="#tab_11">Table 4</ref> an ablation analysis and computed the perceptual distance (LPIPS) and root mean squared error (RMSE) between our reconstructions and the true images. We performed this ablation on 64x super-resolution on 5 FFHQ test images, where we varied only one parameter at a time, keeping the others fixed to the following values: ? pixel = 10 ?5 , ? percept = 10 5 and ? colin = 10 ?2 . The results show that our method is not overly-sensitive to the choice of the lambda hyper-parameters, as there are multiple values over which results are satisfactory: ? pixel ? 10 ?6 , 10 5 ? ? percept ? 10 6 , and ? colin ? 10 ?2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Downstream processing of BRGM-restored images</head><p>In order to show that the BRGM-reconstructed images are useful for downstream tasks, we ran a ResNet-16 that predicts lung-edema severity on BRGM-restored Chest X-Ray images. Edema severity was predicted as either (1) no edema, (2) mild -vascular congestion, (3) moderate -interstitial edema and (4) severe -alveolar edema. The confusion matrix is shown in <ref type="table">Table 5 (top table)</ref>. We ran the model on 100 normal images (denoted as True), as well as the same 100 images but with the left-half masked and then reconstructed by our BRGM method (denoted as Reconstructed). We obtained a precision of 0.58 and a recall of 0.58. Most (73/100) of the severity scores are either preserved in the reconstructed images (diagonal), or change to an adjacent score (just above/below the diagonal). We note that, in general, the classification between adjacent severity levels (e.g. No edema vs Mild, Mild vs Moderate, etc ...) is a very hard problem, where the ResNet itself has an score of 45.03% (random guessing would give 25% on a balanced dataset).</p><p>We also note that these results depend on how much of the image is reconstructed. If we re-run the analysis with a smaller mask (only a quadrant masked and reconstructed instead of half of the imagesee <ref type="table">Table 5</ref> bottom), we obtain significantly better results in the confusion matrix, and a precision of 0.75 and recall of 0.75. In particular, we note that the true severe cases improved their diagnosis.   <ref type="table">Table 5</ref>: Results of downstream processing on BRGM-reconstructed images vs true images. <ref type="table">(Top  table)</ref> We show the confusion matrix of a ResNet-16 at edema prediction on BRGM-inpainted images vs True images. For the inpainted images, the mask covered 50% of the pixels (left-half). <ref type="table">(Bottom  table)</ref> As above, but only 25% of image is masked and then reconstructed.  <ref type="figure">Figure 15</ref>: Inconsistency of our method on FFHQ, across different resolution levels, using uncurated example pictures. Left columns shows input images, and the middle columns show downsampled reconstructions (i.e. f ? G(w * )) where images were 4x super-resolved, then downsampled by 4x to match again the input. Right columns show difference between input and the downsampled reconstructions. For higher resolution inputs (128x128), the method cannot accurately reconstruct the input image, likely because the generator has limited generalisability to such unseen faces from FFHQ (our method was trained not on the entire FFHQ, but on a training subset). The difference maps, representing x3 scaled mean absolute errors, show that certain regions in particular are not well reconstructed, such as the hair of the girl on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Method Inconsistency</head><p>One caveat of our method is that it can create reconstructions that are inconsistent with the input data. We highlight this in <ref type="figure" target="#fig_2">Figs. 15 and 16</ref>. This is because our method relies on the ability of a pre-trained generator to generate any potential realistic image as input. In addition to that, in Eq. (6), our method optimizes the pixelwise and perceptual loss terms between the input image and the downsampled reconstruction. As we show in <ref type="figure" target="#fig_2">Figs. 15 and 16</ref>, while there are little differences between the input and the reconstruction at low 16x16 resolutions, at higher 128x128 resolutions, these differences become larger and more noticeable. Another aspect that contributes to this issue is the extra prior term L cosine , which is however required to ensure better reconstructions (see <ref type="figure" target="#fig_0">Fig 2)</ref>. Nevertheless, we believe that the inconsistency is fundamentally caused by limitations of the generator G, that will be solved in the near future with better generator models that offer improved generalisability to unseen images. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Reconstructions as the loss function evolves from the original StyleGAN2 inversion to our proposed method. Top row shows super resolution, while bottom row shows in-painting. We start from (a) the original StyleGAN2 inversion, and (b) remove noise optimisation, (c) extend optimisation to full W + space, (d) add pixelwise L 2 term, (e) add prior on w latent variables and (f) add colinear loss term for w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For a high-resolution image I CLN , this produces a low-resolution (corrupted) image I COR = (I CLN k) ? s , where denotes convolution and ? s denotes downsampling operator by a factor s. The parameters are ? = {k, s} ? In-painting with arbitrary mask: f IN is implemented as an operator that performs pixelwise multiplication with a mask M . For a given clean image I CLN and a 2D binary mask M , it produces a cropped-out (corrupted) image I COR = I CLN M , where is the Hadamard product. The parameters of this corruption process are ? = {M } where M ? {0, 1} H?W , where H and W are the height and width of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Sampling using Variational Inference. Given the input image (left column), we show the estimated mean image G(? v ) (third column), alongside samples around the mean G(? v + ? v ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Graphical model of our method. In gray shade are known observations or parameters: the input corrupted image I, the parameters ?, ? and ? defining the prior on latent vector w, and ? pixel , ? percept , the parameters defining the noise model over I. In the red box are the variational parameters ? v , ? v and ? v defining an approximated Gaussian posterior over w (section 2.4). Unknown latent variables (in white), to be estimated, are w, the latent vectors of StyleGAN, I CLN , the clean image, and I COR , the corrupted image simulated through the pipeline. Transformation G is modelled by the StyleGAN2 generator, while f by a known corruption model (e.g. downsampling with a known kernel).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Uncurated in-painting examples by BRGM on the FFHQ dataset, compared against SN-PatchGAN [21] and Deep Image Prior [47]. Original Masked SN-PatchGAN [21] Deep Image Prior [47] BRGM Uncurated in-painting examples by BRGM on the Chest X-ray dataset, compared against SN-PatchGAN [21] and Deep Image Prior [47]. Original Masked SN-PatchGAN [21] Deep Image Prior [47] BRGM Uncurated in-painting examples by BRGM on the brain dataset, compared against SN-PatchGAN [21] and Deep Image Prior<ref type="bibr" target="#b47">[47]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Sampling of multiple reconstructions using Variational Inference on super-resolution tasks with varying factors. From left, we show the true image, the estimated variational mean, alongside five random samples around that mean. For each high-resolution (HR) image, we show the corresponding low-resolution (LR) image below. While for some of the images, the reconstructions don't match the true image, the downsampled low-resolution images do match with the true image. We chose such extreme super-resolution in order to obtain a wide posterior distribution. Sampling of multiple reconstructions using Variational Inference on in-painting tasks. From left, we show the true image, the estimated variational mean, alongside five random samples around that mean. For the mean and for each sample, we show both the clean image, as well as the true image with the in-painted area from the sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Inconsistency of our method on the medical datasets, using uncurated examples. Same setup as inFig. 15. Failure cases of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Qualitative evaluation on FFHQ at different input resolutions. Left column shows low resolution inputs, while right column shows true high-quality images. ESRGAN and SRFBN show clear distortion and blurriness, while PULSE does not recover the true image due to strong priors. BRGM shows significant improvements, especially at low resolutions.</figDesc><table><row><cell>Low-Res</cell><cell cols="4">Bicubic ESRGAN [18] SRFBN [19] PULSE [41]</cell><cell>BRGM</cell><cell>BRGM</cell><cell>True</cell></row><row><cell></cell><cell>(x4)</cell><cell>(x4)</cell><cell>(x4)</cell><cell>1024x1024</cell><cell>(x4)</cell><cell>1024x1024 1024x1024</cell></row><row><cell>16x16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>32x32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>64x64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparison between our method and SN-PatchGAN<ref type="bibr" target="#b20">[21]</ref> on in-painting. SN-PatchGAN fails on large masks, while our method can still recover the high-level structure.</figDesc><table><row><cell>Original</cell><cell>Masked</cell><cell>SN-PatchGAN</cell><cell>BRGM</cell><cell>Original</cell><cell>Masked</cell><cell>SN-PatchGAN</cell><cell>BRGM</cell></row><row><cell></cell><cell></cell><cell>[21]</cell><cell></cell><cell></cell><cell></cell><cell>[21]</cell><cell></cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>three datasets, see Supp. Section B. For a description of the inference times of BRGM MAP and VI, see Supp. Section C.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 (</head><label>1</label><figDesc></figDesc><table /><note>left) reports performance metrics of super-resolution on 100 unseen images at different resolution levels. At low 16 2 input resolutions, our method outperforms all other super-resolution methods consistently on all three datasets. However, at resolutions of 32 2 and higher, SRFBN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Super-resolution (LPIPS? / RMSE?) Dataset BRGM PULSE [41] ESRGAN [18] SRFBN [19] FFHQ 16 2 0.24/25.66 0.29/27.14 0.35/29.32 0.33/22.07 FFHQ 32 2 0.30/18.93 0.48/42.97 0.29/23.02 0.23/12.73 FFHQ 64 2 0.36/16.07 0.53/41.31 0.26/18.37 0.23/9.40 Dataset LPIPS RMSE PSNR SSIM LPIPS RMSE PSNR SSIM FFHQ 0.19 24.28 21.33 0.84 0.24 30.75 19.67 0.82 X-ray 0.13 13.55 27.47 0.91 0.20 27.80 22.02 0.86 Brains 0.09 8.65 30.94 0.88 0.22 24.74 21.47 0.75</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>In-painting</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>BRGM</cell><cell></cell><cell cols="2">SN-PatchGAN [21]</cell></row><row><cell>X-ray 16 2 0.18/11.61</cell><cell>-</cell><cell>0.32/14.67</cell><cell>0.37/12.28</cell><cell cols="4">Human evaluation (proportion of votes for best image)</cell></row><row><cell>X-ray 32 2 0.23/10.47 X-ray 64 2 0.31/10.58 Brains 16 2 0.12/12.42</cell><cell>---</cell><cell>0.32/12.56 0.30/8.67 0.34/22.81</cell><cell>0.21/6.84 0.22/5.32 0.33/12.57</cell><cell cols="4">Dataset FFHQ 16 2 42% BRGM PULSE [41] ESRGAN [18] SRFBN [19] 32% 11% 15% FFHQ 32 2 39% 2% 12% 47%</cell></row><row><cell>Brains 32 2 0.17/11.08</cell><cell>-</cell><cell>0.31/14.16</cell><cell>0.18/6.80</cell><cell>FFHQ 64 2 14%</cell><cell>8%</cell><cell>32%</cell><cell>45%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: (left) Evaluation on (x4) super-resolution at different input resolution levels. Reported are</cell></row><row><cell>LPIPS/RMSE scores. (top-right) Evaluation of BRGM and SN-PatchGAN on in-painting. (bottom-</cell></row><row><cell>right) Human evaluation showing the proportion of votes for the best super-resolution re-construction</cell></row><row><cell>in the forced-choice pairwise comparison test. Bold numbers show best performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>PSNR? SSIM? MAE? PSNR? SSIM? MAE? PSNR? SSIM? MAE? PSNR? SSIM? MAE? FFHQ 16 2 20.13 0.74 17.46 19.51 0.68 19.20 18.91 0.69 20.01 21.43 0.76 15.01 FFHQ 32 2 22.74 0.74 12.52 15.37 0.35 33.13 21.10 0.72 14.53 26.28 0.89 7.46 FFHQ 64 2 24.16 0.70 10.63 15.74 0.37 31.54 23.14 0.72 10.94 28.96 0.90 5.21 X-ray 16 2 27.14 0.</figDesc><table><row><cell>Dataset</cell><cell>BRGM</cell><cell></cell><cell>PULSE [41]</cell><cell></cell><cell>ESRGAN [18]</cell><cell>SRFBN [19]</cell></row><row><cell></cell><cell>91 7.45</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.17 0.87 10.14 26.88 0.92 7.63</cell></row><row><cell cols="2">X-ray 32 2 27.84 0.84 6.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.44 0.81 8.36 31.80 0.95 3.71</cell></row><row><cell cols="2">X-ray 64 2 27.62 0.79 6.63</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.47 0.87 5.33 33.91 0.95 2.47</cell></row><row><cell cols="2">Brains 16 2 26.33 0.84 7.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.06 0.60 14.27 26.21 0.77 8.62</cell></row><row><cell cols="2">Brains 32 2 27.30 0.81 6.54</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>25.23 0.78 8.35 31.60 0.93 3.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Additional performance metrics (PSNR, SSIM and MAE) for the super-resolution evaluation. 0.24 ? 0.07 25.66 ? 6.13 0.29 ? 0.07 27.14 ? 4.08 0.35 ? 0.07 29.32 ? 6.52 0.33 ? 0.05 22.07 ? 4.47 FFHQ 32 2 0.30 ? 0.07 18.93 ? 3.95 0.48 ? 0.08 42.97 ? 3.78 0.29 ? 0.05 23.02 ? 5.48 0.23 ? 0.04 12.73 ? 3.08 FFHQ 64 2 0.36 ? 0.06 16.07 ? 3.21 0.53 ? 0.07 41.31 ? 3.57 0.26 ? 0.05 18.37 ? 5.06 0.23 ? 0.04 9.40 ? 2.48 FFHQ 128 2 0.34 ? 0.05 15.84 ? 3.23 0.57 ? 0.06 34.89 ? 2.21 0.15 ? 0.05 15.84 ? 4.83 0.09 ? 0.02 7.55 ? 2.30 X-ray 16 2 0.18 ? 0.05 11.61 ? 3.22 --0.32 ? 0.07 14.67 ? 4.48 0.37 ? 0.04 12.28 ? 4.39 X-ray 32 2 0.23 ? 0.05 10.47 ?</figDesc><table><row><cell>Real</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generated (FID: 9.2)</cell><cell></cell></row><row><cell>Real</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generated (FID: 7.3)</cell><cell></cell></row><row><cell cols="9">Figure 8: Uncurated images generated by our StyleGAN2 generator trained on the chest X-ray dataset</cell></row><row><cell cols="9">(MIMIC III) (top) and the brain dataset (bottom). Left images are random examples of real images</cell></row><row><cell cols="9">from the actual datasets, while the right-side images are generated. The image quality is relatively</cell></row><row><cell cols="9">good, albeit some anatomical artifacts are still observed, such as incomplete labels, wiggly bones or</cell></row><row><cell cols="2">discontinuous wires.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">BRGM</cell><cell cols="2">PULSE [41]</cell><cell cols="2">ESRGAN [18]</cell><cell cols="2">SRFBN [19]</cell></row><row><cell cols="2">LPIPS?</cell><cell>RMSE?</cell><cell>LPIPS?</cell><cell>RMSE?</cell><cell>LPIPS?</cell><cell>RMSE?</cell><cell>LPIPS?</cell><cell>RMSE?</cell></row><row><cell cols="3">FFHQ 16 2 2.04</cell><cell>-</cell><cell>-</cell><cell cols="4">0.32 ? 0.05 12.56 ? 3.34 0.21 ? 0.03 6.84 ? 2.01</cell></row><row><cell cols="3">X-ray 64 2 0.31 ? 0.04 10.58 ? 1.81</cell><cell>-</cell><cell>-</cell><cell cols="4">0.30 ? 0.03 8.67 ? 1.86 0.22 ? 0.02 5.32 ? 1.44</cell></row><row><cell cols="3">X-ray 128 2 0.27 ? 0.03 10.53 ? 1.91</cell><cell>-</cell><cell>-</cell><cell cols="4">0.20 ? 0.02 7.19 ? 1.34 0.07 ? 0.01 4.33 ? 1.30</cell></row><row><cell cols="3">Brains 16 2 0.12 ? 0.03 12.42 ? 1.71</cell><cell>-</cell><cell>-</cell><cell cols="4">0.34 ? 0.04 22.81 ? 3.26 0.33 ? 0.03 12.57 ? 1.51</cell></row><row><cell cols="3">Brains 32 2 0.17 ? 0.03 11.08 ? 1.29</cell><cell>-</cell><cell>-</cell><cell cols="4">0.31 ? 0.03 14.16 ? 2.36 0.18 ? 0.03 6.80 ? 1.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Performance metrics for super-resolution as inTable 1(left), but additionally including the standard deviation of scores across the 100 test images.</figDesc><table><row><cell>Original</cell><cell>Masked</cell><cell>SN-PatchGAN [21] Deep Image Prior [47]</cell><cell>BRGM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>LPIPS/RMSE 0.56/31.57 0.57/34.38 0.58/35.35 0.64/44.83 0.68/63.95</figDesc><table><row><cell>? pixel</cell><cell>10 ?7</cell><cell>10 ?6</cell><cell>10 ?5</cell><cell>10 ?4</cell><cell>10 ?3</cell></row><row><cell cols="6">LPIPS/RMSE 0.65/49.48 0.57/36.33 0.59/36.73 0.58/35.87 0.58/36.77</cell></row><row><cell>? percept</cell><cell>10 3</cell><cell>10 4</cell><cell>10 5</cell><cell>10 6</cell><cell>10 7</cell></row><row><cell cols="6">LPIPS/RMSE 0.60/36.42 0.59/35.19 0.57/34.67 0.56/33.56 0.60/38.86</cell></row><row><cell>? colin</cell><cell>10 ?4</cell><cell>10 ?3</cell><cell>10 ?2</cell><cell>10 ?1</cell><cell>10 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Results of ablation study over hyper-parameters ? pixel , ? percept and ? colin . Reported are perceptual distance (LPIPS) and pixelwise root mean squared errors (RMSE) between the true images and the reconstructed images.Figure 12: Setup of our human study, using a forced-choice pairwise comparison design. Each rater is shown a true, high-quality image on the left, and four potential reconstructions (A-D) by different algorithms. They have to select which reconstruction best resembled the HQ image.</figDesc><table><row><cell cols="4">Mask 50% of image (left half of image)</cell><cell></cell></row><row><cell cols="5">True Reconstructed No edema Mild Moderate Severe</cell></row><row><cell>No edema</cell><cell>53</cell><cell>2</cell><cell>11</cell><cell>1</cell></row><row><cell>Mild</cell><cell>5</cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>Moderate</cell><cell>9</cell><cell>3</cell><cell>5</cell><cell>0</cell></row><row><cell>Severe</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>0</cell></row><row><cell cols="4">Mask 25% of image (top-right quadrant only)</cell><cell></cell></row><row><cell cols="5">True Reconstructed No edema Mild Moderate Severe</cell></row><row><cell>No edema</cell><cell>66</cell><cell>2</cell><cell>8</cell><cell>1</cell></row><row><cell>Mild</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>0</cell></row><row><cell>Moderate</cell><cell>5</cell><cell>0</cell><cell>4</cell><cell>1</cell></row><row><cell>Severe</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The supp. section of<ref type="bibr" target="#b51">[51]</ref> presents an excellent introduction to the generalized change of variable theorem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The von-Mises distribution is the analogous of the Gaussian distribution over angles [0 ? 2?]. M(.|?, ?) is analogous to N (.|?, ?), where ? ?1 = ? 2</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anatomical priors in convolutional networks for unsupervised biomedical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert R</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9290" to="9299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the solution of ill-posed problems and the method of regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Nikolaevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tikhonov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doklady Akademii Nauk</title>
		<imprint>
			<date type="published" when="1963" />
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="501" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A bound optimization approach to wavelet-based image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>M?rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert D</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">782</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. K-Svd</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwanggil</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian color constancy. JOSA A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1393" to="1411" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning how to inpaint from global image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The generic viewpoint assumption in a framework for visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">6471</biblScope>
			<biblScope unit="page" from="542" to="545" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploiting the sparse derivative prior for super-resolution and image demosaicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marshall F Tappen Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<title level="m">Compressed sensing using generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting deep generative prior for versatile image restoration and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Invertible generative models for inverse problems: mitigating representation error and dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Asim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Phase retrieval under a generative prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Voroninski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04261</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Prior image-constrained reconstruction using style-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Kelkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anastasio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12525</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Srflow: Learning the superresolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Robust compressed sensing mri with deep generative priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Arvinte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">I</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01368</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Instance-optimal compressed sensing via posterior sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushrut</forename><surname>Karmalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexandros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11438</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Composing normalizing flows for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11158" to="11169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PULSE: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2437" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image2stylegan++: How to edit the embedded images?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8296" to="8305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>?ktem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05910</idno>
		<title level="m">Deep bayesian inversion</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ambientgan: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">ICLR</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mimicgan: Robust projection onto image manifolds with corruption mimicking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushil</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer-Timo</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Elements of causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Minimal achievable sufficient statistic learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nther</forename><surname>Koliander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1465" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Geometric integration theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold R Parks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast and scalable bayesian deep learning by weight-perturbation in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Voot</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2611" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford R Jack</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Borowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><forename type="middle">J</forename><surname>Britson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Whitwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chadwick</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Fotenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">L</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2677" to="2684" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Parkinson&apos;s progression markers initiative (PPMI)-establishing a PD biomarker cohort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohini</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Siderowf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Lasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Caspell-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Simuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><forename type="middle">M</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Q</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of clinical and translational neurology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1460" to="1477" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The australian imaging, biomarkers and lifestyle (AIBL) study of aging: methodology and baseline characteristics of 1112 individuals recruited for a longitudinal study of Alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kathryn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashley</forename><forename type="middle">I</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><forename type="middle">De</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">T</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Lautenschlager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International psychogeriatrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="672" to="687" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Identification of autism spectrum disorder using deep learning and the ABIDE dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anibal</forename><surname>S?lon Heinsfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">Rosa</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Craddock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Buchweitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Meneguzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="16" to="23" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Comparison of four subjective methods for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rados?aw</forename><surname>Tomaszewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2478" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petru-Daniel</forename><surname>Tudosiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parashkev</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05692</idno>
		<title level="m">Neuromorphologicaly-preserving volumetric data encoding using VQ-VAE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
