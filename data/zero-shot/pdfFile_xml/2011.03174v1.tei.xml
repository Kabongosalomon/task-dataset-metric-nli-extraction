<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huai</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
						</author>
						<title level="a" type="main">ULSD: Unified Line Segment Detection across Pinhole, Fisheye, and Spherical Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Line segment detection is essential for high-level tasks in computer vision and robotics. Currently, most stateof-the-art (SOTA) methods are dedicated to detecting straight line segments in undistorted pinhole images, thus distortions on fisheye or spherical images may largely degenerate their performance. Targeting at the unified line segment detection (ULSD) for both distorted and undistorted images, we propose to represent line segments with the Bezier curve model. Then the line segment detection is tackled by the Bezier curve regression with an end-to-end network, which is model-free and without any undistortion preprocessing. Experimental results on the pinhole, fisheye, and spherical image datasets validate the superiority of the proposed ULSD to the SOTA methods both in accuracy and efficiency (40.6fps for pinhole images). The source code is available at https://github.com/lh9171338/Unified-Line-Segment-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Line segment detection is one of the most fundamental problems in computer vision and robotics, which can facilitate many high-level vision tasks such as image matching <ref type="bibr" target="#b0">[1]</ref>, camera calibration <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, structure from motion <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and visual SLAM <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, most current line segment detection methods model line segments as straight lines, thus cannot be directly applied to the distorted images from fisheye or spherical cameras, which are widely used in indoor camera localization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, room layout estimation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and other tasks.</p><p>The existing methods for distorted line segment detection are almost model-based, i.e., dependent on camera distortion parameters. One category of methods among them requires rectification of lens distortion before applying straight line segments detector. The other methods such as extended Hough transform <ref type="bibr" target="#b16">[17]</ref> and RANSAC-based methods <ref type="bibr" target="#b17">[18]</ref>, utilize camera distortion parameters to model the distorted line segments and can be directly applied to the distorted images. However, the performance of model-based methods is largely dependent on the accuracy of camera distortion parameters, which might be even unavailable in hand. Therefore, the model-free approach for distorted line segment detection, i.e., independent of camera distortion parameters, is applaudable in practice. As a kind of modelfree representation, two endpoints model has been commonly used in straight line segments detectors <ref type="bibr">[?]</ref>, <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b32">[30]</ref>, but is not enough to represent curved line segments in distorted *Equal contribution. <ref type="bibr" target="#b0">1</ref>   images. Considering the strong fitting ability of the Bezier curve which has been successfully applied to arbitrarilyshaped text detection <ref type="bibr" target="#b18">[19]</ref>, we adopt the Bezier curve as a unified representation for line segments in both distorted and undistorted images. Similar to the two endpoints model, the Bezier curve can be represented as a vector parameterized by its equipartition points. Analogically, the line segment detection based on the Bezier curve model can be efficiently tackled with the coordinate regression for equipartition points, and the line classification which can be implemented by the center point detection of the target line segment <ref type="bibr" target="#b20">[20]</ref>.</p><p>Exploiting the Bezier curve model and detection method based on the center point, we design an end-to-end network for unified line segment detection (ULSD). The proposed ULSD is a model-free approach that can take both distorted and undistorted images from the pinhole, fisheye or spherical cameras as input, and directly output vectorized line segments, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The performance of ULSD is evaluated on various datasets which demonstrates the superiority to state-of-the-art (SOTA). As far as we know, the proposed ULSD is the first deep learning-based method to unify the line segment detection for both distorted and undistorted images. The main contribution of this work is three folded:</p><p>? We propose a model-free line segment representation for unified line segment detection based on the Bezier curve model, which is independent of camera distortion parameters. <ref type="bibr">?</ref> We design a unified end-to-end line segment detection network, which can be directly applied to both distorted and undistorted images from the pinhole, fisheye, and spherical cameras. <ref type="bibr">?</ref> We construct fisheye and spherical image datasets for line segment detection tasks in distorted images. With these datasets, the performance of the proposed ULSD is evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Line segment detection is an attractive research topic during the last two decades in computer vision and robotics. Most of the related methods work well for undistorted pinhole images. For distorted images from fisheye or spherical cameras, the common practice is to undistort images and then deploy the straight-line models. Thus we will mainly review the straight line detection and distorted line detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Straight Line Segment Detection</head><p>For pinhole images, a lot of line segment detection methods have been proposed. Among them, traditional approaches such as <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref> detect lines based on the edge or gradient. The main drawback is that they are sensitive to noise and the detected lines are often fragmented. Recently, deep learning-based works such as <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b30">[28]</ref> significantly improve the performance by leveraging the deep features. Compared to the local edge or gradient features, the learningbased features are more robust to noise. Huang et al. <ref type="bibr" target="#b21">[21]</ref> propose the wireframe representation and provide the first high-quality wireframe dataset. Compared with traditional line segment representation, wireframe parsing leverages the constraint of endpoint junctions, thus the output line segments are of higher quality in terms of line completeness and robustness to noise. Their method detects wireframe by two deep neural networks combined with a heuristic wireframe fusion algorithm. Zhou et al. propose the first end-to-end trainable neural network named L-CNN <ref type="bibr" target="#b31">[29]</ref> for wireframe parsing. Based on L-CNN, Xue et al. introduce a holistic attraction field map (HAFM) <ref type="bibr" target="#b32">[30]</ref> to represent line segments and achieve SOTA performance in accuracy and efficiency. Although HAFM brings significant improvement of performance, it needs big efforts to expand from the straight line to the distorted line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distorted Line Segment Detection</head><p>As far as we know, there is no specially-designed method to detect distorted line segments for fisheye or spherical images. But there are some domains closely related. In the straight line-based fisheye image rectification field, various distorted lines detection methods have been used. These approaches can be divided into geometric-based methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and deep learning-based methods <ref type="bibr" target="#b1">[2]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, an extended Hough transform is utilized to detect the straight lines. The work <ref type="bibr" target="#b17">[18]</ref> proposes a 5-points RANSAC method for robust line extraction. By leveraging the strong capability of networks, LaRecNet <ref type="bibr" target="#b1">[2]</ref> can obtain more accurate results of distorted lines extraction. As for spherical images, there are some related works about line-based spherical camera localization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the authors first utilize Canny edge detection in the 2D equirectangular image, and then deploy spherical Hough transform to extract lines. In general, most of the above works are based on the traditional Hough transform or RANSAC algorithm. These methods are generally sensitive to noise and their detection accuracies are far from satisfactory. Additionally, there is no existing learning-based method modeling the distorted line segments in fisheye or spherical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bezier Curve Representation</head><p>Since the most general line representation with the straight connection of two endpoints cannot fit lines in arbitrarily distorted images, we introduce the Bezier curve as a unified parameterized representation. The Bezier curve uses the Bernstein Polynomials as its basis to represent a parametric curve. Its definition is shown in Eq. 1.</p><formula xml:id="formula_0">B(t) = n i=0 b i B i,n (t), 0 ? t ? 1<label>(1)</label></formula><p>where t is the proportional coefficient of a point on the curve, n represents the order of the Bezier curve, b i represents the i-th control point, and B i,n (t) = C i n t i (1 ? t) n?i represents the Bernstein basis Polynomial.</p><p>According to Eq. 1, the interpolation formula for a Bezier curve can be obtained, as shown in Eq. 2:</p><formula xml:id="formula_1">? ? ? ? ? B 0,n (t 0 ) ? ? ? B n,n (t 0 ) B 0,n (t 1 ) ? ? ? B n,n (t 1 ) . . . . . . . . . B 0,n (t m?1 ) ? ? ? B n,n (t m?1 ) ? ? ? ? ? ? ? ? ? ? b 0 b 1 . . . b n ? ? ? ? ? = ? ? ? ? ? p 0 p 1 . . . p m?1 ? ? ? ? ?</formula><p>(2) where m is the number of the interpolation points, p i represents the i-th interpolation point.</p><p>An n-th order Bezier curve can be determined by its n + 1 control points. However, the control points lack geometric meaning and may be located outside the image, so it's difficult to directly learn the positions of the control points.</p><p>Therefore, our network tries to predict the positions of the equipartition points of the Bezier curve instead, and then use Eq. 2 to calculate the control points through the least square method. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, a 3rd Bezier curve can be determined by 4 control points b i , but the prediction of b 1 , b 2 is non-trivial. Therefore, we represent the Bezier curve by using its n + 1 equipartition points p 0 , p 1 , p 2 , p 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bezier Curve</head><p>Control Polygon </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overall Network Architecture</head><p>Based on the Bezier curve representation, our network is designed to detect line segments in arbitrarily distorted images (from the pinhole, fisheye, and spherical camera). <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the network architecture. It mainly contains three modules: 1) a feature extraction backbone that takes a single image as input and outputs a shared feature map for the successive modules; 2) a Line Proposal Network (LPN) which outputs the candidate line segments; 3) an LoI (Line of Interest) head module which classifies the candidate line segments using the line features obtained through the BezierAlign module. Our pipeline is similar to the Faster R-CNN <ref type="bibr" target="#b33">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Backbone</head><p>We choose the stacked hourglass network <ref type="bibr" target="#b34">[32]</ref> as the backbone for its efficiency and effectiveness. Taking an image with size H ? W ? 3 as input, the stacked hourglass network first downsamples the input image via convolution layers, then extracts features through multiple hourglass modules, and finally outputs the feature map with size</p><formula xml:id="formula_2">H b ? W b ? C.</formula><p>The feature map is shared by the subsequent LPN and LoI head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Line Proposal Network</head><p>The Line Proposal Network contains four sub-modules: junction prediction module, line prediction module, line and jucntion matching module, and line sample module.</p><p>1) Junction Prediction Module: Junction prediction is addressed as a classification and regression problem. The input image with spatial size H ?W is divided into W b ?H b bins, same as the spatial size of the feature map. For each bin b, the network predicts whether there exists a junction inside it. If a junction p is inside bin b, it will also predict the offset vector from p to the center b of the bin. Therefore, the network outputs a junction confidence map J and a junction offset map O. The ground truth of the two maps can be obtained by Eq. 3 and Eq. 4.</p><formula xml:id="formula_3">J(b) = 1 ? p ? V : p inside b 0 otherwise (3) O(b) = (b ? p) ? p ? V : p inside b 0 otherwise.<label>(4)</label></formula><p>where V is the set of junctions. J and O are predicted by two decoder head consisting of two convolution layers separately. In the training phase, the binary cross-entropy loss and smooth l 1 loss are used to predict J and O respectively. The total loss of junction prediction is the weighted sum of the two losses.</p><formula xml:id="formula_4">L junc = ? j conf L j conf + ? j of f set L j of f set<label>(5)</label></formula><p>Furthermore, the non-maximum suppression (NMS) is applied in J to remove duplicates, and only the top-K junctions with the highest confidence are kept for the line and junction matching module.</p><p>2) Line Prediction Module: For an arbitrarily distorted line segment represented by an n-th order Bezier curve, the line prediction module tries to predict the location of the center point of the line segment and the offset vectors from the equipartition points to the center point. Center point prediction is the same as junction prediction. If n is even, the center point is one of the n + 1 equipartition points, whose offset vector is 0, so only n offset vectors need to predict. The smooth l 1 loss is used to predict the offset vectors, and the total loss is shown in Eq. 6.</p><formula xml:id="formula_5">L line = ? center L center + ? of f set m i=1 L of f set<label>(6)</label></formula><p>where m = n if n is even, otherwise m = n + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Line and Junction Matching Module:</head><p>To improve the quality of the line segment proposals, a line and junction matching module is adopted. The matching strategy is similar to HAWP <ref type="bibr" target="#b32">[30]</ref>. A line segment proposal is kept if and only if its two endpoints can be matched with two junction proposals based on the Euclidean distance, and then the two endpoints of the line proposals are replaced by the two matched junction proposals. If there are multiple line proposals matched with the same pair of junction proposals, only the one with the shortest distance is kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Line Sample Module:</head><p>The Line sample module is used to sample positive and negative line proposals for training the classifier of the LoI head. A line segment proposal is assigned with a positive label if there is a ground truth line segment and their distance calculated by Eq. 7 is less than a predefined threshold ?. Otherwise, it's assigned with a negative label. Then, we can obtain a positive and a negative sample set. Finally, a certain number of positive and negative line segment proposals are randomly sampled from the two sets respectively. Apart from the above positive samples, we also sample some positive line segments from the ground truth to increase the number of positive samples and help cold-start <ref type="bibr" target="#b31">[29]</ref> the training at the beginning. d(l, l ) = min n i=0 p i ? p i 2 , n i=0 p i ? p n?i 2 , l = (p 0 , ? ? ? , p n ), l = (p 0 , ? ? ? , p n ) (7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. LoI Head Module</head><p>The LoI (Line of Interest) head module takes a list of candidate line segments together with the feature map F as input and predicts whether or not each candidate line segment is true. To extract the fixed-length line feature vector from the feature map, previous methods <ref type="bibr" target="#b31">[29]</ref>, <ref type="bibr" target="#b32">[30]</ref> adopt LoI Pooling layer <ref type="bibr" target="#b31">[29]</ref> which is based on the linear interpolation of straight line segments. However, LoI Pooling does not work for the distorted line segments. Therefore, we introduce the BezierAlign layer. Based on Eq. 2, it can uniformly sample N p points from a distorted line segment. The feature for each sampled point is computed from F using bi-linear interpolation. After a 1D max-pooling operator, all the features from the N p points are concatenated as the line feature vector. After the BezierAlign operation, we feed all feature vectors into a classifier consisting of multiple fullyconnected layers followed by a sigmoid layer, and finally get the confidence of each candidate line segment. The binarycross entropy loss is used in the LoI head module. To balance the loss of positive and negative samples, the two losses are calculated and weighted separately. The total loss of the LoI head module is shown in Eq. 8.</p><formula xml:id="formula_6">L cls = ? pos L pos + ? neg L neg<label>(8)</label></formula><p>The loss of the whole network is the sum of all above losses. L = L junc + L line + L cls</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we will discuss the datasets, network implementation details, and the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Pinhole image. The common Wireframe dataset <ref type="bibr" target="#b21">[21]</ref> and YorkUrban dataset <ref type="bibr" target="#b23">[22]</ref> are used. The former contains 5000 training images and 462 testing images. The latter contains 102 images. Fisheye image. Currently, there is no available public fisheye line segment detection dataset, thus we build the F-Wireframe dataset and F-YorkUrban dataset by distorting images from the Wireframe dataset and YorkUrban dataset with the fisheye distortion model. The synthesized dataset is the same size as the original dataset. Spherical image. Since no public spherical line segment dataset is available, we build a dataset by manually annotating images from the SUN360 dataset <ref type="bibr" target="#b24">[23]</ref> and then augmenting by flip (horizontal, vertical, and horizontal-vertical) and periodical shifting (horizontal). Finally, the dataset contains 5200 training images and 68 testing images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For the pinhole and fisheye networks, the input image is resized to (H, W ) = (512, 512) and the feature map spatial size is (H b , W b ) = (128, 128). For the spherical network, the input image size and the feature map spatial size are (512, 1024) and (128, 256) respectively. The network's hyper-parameter settings are the same as L-CNN and HAWP for a fair comparison. Our network is trained using the Adam optimizer <ref type="bibr" target="#b35">[33]</ref>. The learning rate, weight decay, and training batch size are set to 4?10 ?4 , 1?10 ?4 , and 6 respectively. We use step decay as the learning rate scheduler. All experiments are conducted on a single NVIDIA GTX 2080Ti GPU.</p><p>As for the order setting of the Bezier curve, the 1st order is enough for pinhole images. While for fisheye and spherical images, the order is determined by experimental evaluations. We measure the fitting errors of Bezier curves with the order ranging from the 1 to 6. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the 2nd order Bezier curve is enough to make the fitting error less than 1 pixel for line segments in both spherical and fisheye images. Even though, the fitting error can be further decreased by increasing the order, this will largely increase the computational complexity. To balance the accuracy of line representation and efficiency of the network, we leave the setting for the order of the Bezier curve model in the following section by evaluating the performance of ULSD with the order of 2 to 4 for fisheye and spherical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Comparisons</head><p>To evaluate the performance of the proposed ULSD, comparisons are made to the conventional methods LSD <ref type="bibr" target="#b27">[26]</ref> and spherical Hough transform (SHT), deep learning-based methods DWP <ref type="bibr" target="#b21">[21]</ref>, AFM <ref type="bibr" target="#b28">[27]</ref>, L-CNN <ref type="bibr" target="#b31">[29]</ref>, and HAWP <ref type="bibr" target="#b32">[30]</ref>. For pinhole images targeting to detect straight line segments, the USLD with 1st order Bezier curve (ULSD 1 ) is trained on the Wireframe dataset and tested on both two pinhole image datasets. For fisheye and spherical images with distorted line segments, the ULSD with the 2nd, 3rd, and 4th order Bezier curve (ULSD 2 , ULSD 3 , ULSD 4 ) are respectively evaluated by training on the F-Wireframe dataset and SUN360 dataset.  Quantitative Results. Quantitative performance in the accuracy of line segment detection is evaluated based on metrics including structural average precision (sAP) of line segments under the threshold of 5, 10, 15 pixels, mean structural average precision (msAP) over the threshold of 5, 10, and 15 pixels and vectorized junction mean AP (mAP J ). And the efficiency of the aforementioned algorithms is further evaluated based on the frames per second (FPS). The quantitative results and comparisons for pinhole images are shown in <ref type="table">Table.</ref> I and <ref type="figure" target="#fig_4">Fig. 5</ref>. The proposed ULSD 1 obtains much higher detection accuracy compared with traditional LSD. Furthermore, compared to the deep learningbased methods, the proposed ULSD achieves the accuracy comparable to the SOTA, but with remarkable improvement in efficiency by at least 31%, which comes from the high efficiency of the ULSD's line segment prediction module. Then by deploying higher-order Bezier curve representation, the quantitative performance for different algorithms is evaluated on fisheye and spherical datasets. For the fair comparison, the performance of SOTA straight line segment detection, HAWP, is further evaluated with an additional prepossessing by rectifying the fisheye images with randomly corrupted camera distortion parameters (10% random noise). And we denote it as HAWP + . The quantitative results are  shown in <ref type="table">Table.</ref> II, <ref type="table">Table.</ref> III, and the corresponding PR curves in <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref>. For both fisheye and spherical images, the proposed ULSD outperforms the SOTA in both accuracy and efficiency. It is worth noting that although the rectification brings some performance improvement for HAWP + comparing to its origin, the proposed ULSD still outperforms HAWP + . Even more, ULSD is a model-free method that is independent of camera distortion parameters.</p><p>Comparing the results of ULSD on different order Bezier representations, we can observe that ULSD 2 is slightly better than higher-order ULSD 3 and ULSD 4 for both accuracy and efficiency. Although higher-order Bezier representation can improve the line fitting precision <ref type="figure" target="#fig_3">(Fig. 4)</ref>, less than 1 pixel fitting error does not make big differences for the precision evaluation. Thus the higher-order Bezier curve representation does not effectively improve the line detection accuracy, instead makes the model more complex and difficult to learn, which results in lower accuracy. Thus, we evaluate the qualitative results with ULSD 2 for distorted images. Qualitative Results. The qualitative results are shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. Since LSD and SHT are based on the edge or gradient, they detect some noise edges without geometric meaning. Besides, they also produce a lot of fragmented line segments due to not exploiting the constraint of junctions. By leveraging the learning-based features as well as the constraints of junctions, deep models L-CNN and HAWP can produce high-quality straight line segments detection results for pinhole images. However, restricted by the twoendpoint representation, they are incapable of detecting the distorted line segments for fisheye and spherical images. Even though, the performance of SOTA methods for straight line segments detection can be further improved by rectification preprocessing, our proposed ULSD armed with the parameterized Bezier curve model for line segments has the best performance both in accuracy and efficiency. Furthermore, ULSD can directly extract line segments in distorted or undistorted images V. CONCLUSION</p><p>In this paper, we present a unified line segment detection method (ULSD) to detect arbitrarily distorted lines in pinhole, fisheye and, spherical images. With the novel Bezier curve representation, our network can formulate arbitrarily distorted line models in the three core modules: backbone, Line Proposal Network, and LoI head. Compared with the SOTA methods L-CNN <ref type="bibr" target="#b31">[29]</ref> and HAWP <ref type="bibr" target="#b32">[30]</ref>, our ULSD achieves competitive results for pinhole images. More importantly, it obtains much higher line detection accuracy and efficiency for the fisheye and spherical images. With the realtime line detection speed using a single GPU, the proposed ULSD has great potential for online visual tasks such as SLAM and 3D reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Demonstration of the line segment detection on a pinhole (top-left), fisheye (top-right), and spherical image (bottom) with the proposed ULSD method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of our network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>A 3rd order Bezier curve. bi represents the control points, and pi represents the equipartition points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of the fitting errors of Bezier curves with different orders for fisheye and spherical images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Precision-Recall (PR) curves of sAP 10 on the Wireframe dataset (the left plot) and YorkUrban dataset (the right plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 : 9 PR Curve for sAP 10 [Fig. 7 :</head><label>69107</label><figDesc>PR curves of sAP 10 on the F-Wireframe dataset (the left plot) and F-YorkUrban dataset (the right plot). sAP 10 = 1.7] SHT [sAP 10 =42.5] L-CNN [sAP 10 =44.7] HAWP [sAP 10 =67.6] ULSD 2 (ours) [sAP 10 =66.7] ULSD 3 (ours) [sAP 10 =66.1] ULSD 4 (ours) PR curves of sAP 10 on the SUN360 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results and comparisons. Rows: (a)-(b) pinhole case, (c)-(d) fisheye case, (e)-(g) spherical case</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hao Li, Huai Yu, Wen Yang and Lei Yu are with the Electronic Information School, Wuhan University, Wuhan 430072, China {lihao2015, yuhuai, yangwen, ly.wd}@whu.edu.cn 2 Sebastian Scherer is with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA basti@andrew.cmu.edu</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results and comparisons on the Wireframe dataset and YorkUrban dataset. CNN [29] 62.9 62.1 59.3 26.4 26.1 30.4 13.7 HAWP [30] 66.5 65.7 60.2 28.5 28.1 31.7 30.9 ULSD 1 (ours) 66.4 65.6 61.4 27.4 27.0 31.0 40.6</figDesc><table><row><cell>Method</cell><cell cols="6">Wireframe Dataset sAP 10 msAP mAP J sAP 10 msAP mAP J YorkUrban Dataset FPS</cell></row><row><cell>LSD [26]</cell><cell>9.5</cell><cell>9.3</cell><cell>17.2</cell><cell>9.4</cell><cell>9.4</cell><cell>15.4 50.9</cell></row><row><cell>DWP [21]</cell><cell>6.8</cell><cell>6.6</cell><cell>38.6</cell><cell>2.7</cell><cell>2.7</cell><cell>23.4 2.3</cell></row><row><cell>AFM [27]</cell><cell cols="3">24.3 23.4 24.3</cell><cell>9.1</cell><cell>8.9</cell><cell>12.5 14.3</cell></row><row><cell>L-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>(ours) 61.2 60.2 56.3 30.2 29.6 32.6 36.8 ULSD 3 (ours) 60.3 59.3 56.1 28.6 28.0 31.5 36.5 ULSD 4 (ours) 59.9 59.0 56.3 30.1 29.6 33.1 36.3</figDesc><table><row><cell></cell><cell cols="7">Quantitative results and comparisons on the F-</cell></row><row><cell cols="5">Wireframe dataset and F-YorkUrban dataset.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="7">F-Wireframe Dataset F-YorkUrban Dataset FPS sAP 10 msAP mAP J sAP 10 msAP mAP J</cell></row><row><cell>LSD [26]</cell><cell>4.3</cell><cell>4.3</cell><cell>11.1</cell><cell>5.2</cell><cell>5.1</cell><cell cols="2">10.7 47.9</cell></row><row><cell cols="8">L-CNN [29] 43.4 42.9 44.2 19.9 19.6 26.4 14.3</cell></row><row><cell cols="8">HAWP [30] 46.3 45.6 43.8 21.5 21.2 26.4 31.5</cell></row><row><cell>HAWP +</cell><cell cols="2">56.4 55.4</cell><cell>-</cell><cell cols="2">25.8 25.4</cell><cell>-</cell><cell>31.5</cell></row><row><cell>ULSD 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Quantitative results and comparisons on the SUN360 dataset. The input image for L-CNN and HAWP is resized to 512? 512, but ULSD remains the original size 512?1024, thus the speed of ULSD is a little slower than HAWP's. CNN<ref type="bibr" target="#b31">[29]</ref> 39.8 42.5 43.6 42.0 34.8 12.6 HAWP [30] 41.7 44.7 45.8 44.1 33.1 25.4 ULSD 2 (ours) 61.9 67.6 69.8 66.4 47.3 24.8 ULSD 3 (ours) 60.9 66.7 68.7 65.4 47.0 24.6 ULSD 4 (ours) 60.3 66.1 68.0 64.8 47.3 24.4</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="5">SUN360 Dataset sAP 5 sAP 10 sAP 15 msAP mAP J</cell><cell>FPS</cell></row><row><cell></cell><cell>SHT</cell><cell>0.9</cell><cell>1.7</cell><cell></cell><cell>2.5</cell><cell>1.7</cell><cell>3.4 0.05</cell></row><row><cell>Precision</cell><cell cols="3">Recall PR Curve for sAP 10 L-0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 0.1 0.2 f=0.2 f=0.3 0.3 0.4 0.5 0.6 0.7 0.8 0.9 f=0.4 f=0.5 f=0.6 f=0.7 f=0.8 f=0.9 [sAP 10 = 4.3] LSD [sAP 10 =43.4] L-CNN [sAP 10 =46.3] HAWP [sAP 10 =56.4] HAWP + [sAP 10 =61.2] ULSD 2 (ours) [sAP 10 =60.3] ULSD 3 (ours) [sAP 10 =59.9] ULSD 4 (ours)</cell><cell>Precision</cell><cell cols="2">0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Recall 0.0 0.1 0.2 f=0.2 f=0.3 PR Curve for sAP 10 f=0.4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 f=0.5 f=0.6 f=0.7 f=0.8 f=0.9 [sAP 10 = 5.2] LSD [sAP 10 =19.9] L-CNN [sAP 10 =21.5] HAWP [sAP 10 =25.8] HAWP + [sAP 10 =30.2] ULSD 2 (ours) [sAP 10 =28.6] ULSD 3 (ours) [sAP 10 =30.1] ULSD 4 (ours)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anisotropicscale junction detection and matching for indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="91" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to calibrate straight lines for fisheye image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1643" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Line-based multi-label energy optimization for fisheye image rectification and calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4137" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure from motion with line segments under relaxed endpoint constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wildenauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 2nd International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Line-based structure from motion for urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="846" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structvio: Visual-inertial odometry with structural regularity of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="999" to="1013" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular-vision based slam using line segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2007 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>2007 IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2791" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structslam: Visual slam with building structure lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular camera localization in prior lidar maps with 2d-3d line correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spherical camera localization in man-made environment using 3d-2d matching of line information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Advanced Image Technology 2017 (IWAIT2017)</title>
		<meeting>the International Workshop on Advanced Image Technology 2017 (IWAIT2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linebased global localization of a spherical camera in manhattan worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2296" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Corners for layout: End-to-end layout recovery from 360 images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Labrador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Facil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fisheyedistancenet: Self-supervised scale-aware distance estimation using monocular fisheye camera for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Hiremath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>M?der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="574" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End to end vehicle lateral control using a single fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vejarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3613" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time dense mapping for self-driving vehicles using fisheye cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6087" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fisheye image correction based on straightline detection and preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1793" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Line-based geometric consensus rectification and calibration from single distorted manhattan image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Abcnet: Realtime scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9806" to="9815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing scene viewpoint using panoramic place representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2695" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust detection of lines using the progressive probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grompone Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1595" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ppgnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7098" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2785" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
