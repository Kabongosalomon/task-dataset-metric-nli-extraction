<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-25">25 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
							<email>zhenhai@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Radu Soricut Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Radu Soricut Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-25">25 Jul 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linearly combining information using contentbased weights, a method generically known as attention, is a key building block in many deep neural networks such as recurrent neural networks (RNN) <ref type="bibr" target="#b26">(Luong et al., 2015)</ref>, convolutional neural networks (CNN)  and graph convolutional networks (GCN) <ref type="bibr" target="#b44">(Velickovic et al., 2018)</ref>. One particular type of such attention, called multi-head scaled dot-product attention, is one of the main components of the Transformer architecture proposed by <ref type="bibr">Vaswani et al. (2017)</ref>, which has been shown to push the state-of-the-art (SOTA) performance for various understanding and generation tasks.</p><p>These include standard natural language processing (NLP) tasks such as machine translation, document classification, entailment, summarization and question answering <ref type="bibr" target="#b46">(Zaheer et al., 2020;</ref><ref type="bibr" target="#b12">Dai et al., 2019;</ref><ref type="bibr" target="#b2">Baevski and Auli, 2019)</ref>, as well as music generation <ref type="bibr" target="#b21">(Huang et al., 2018)</ref>, image generation <ref type="bibr" target="#b9">Chen et al., 2020)</ref> and genomics <ref type="bibr" target="#b46">(Zaheer et al., 2020;</ref><ref type="bibr" target="#b11">Choromanski et al., 2020)</ref>. The Transformer is also the backbone architecture for models such as BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> (and its numerous relatives) and GPT3 <ref type="bibr">(Brown et al., 2020)</ref>, which have delivered impressive performance across many NLP tasks. However, the standard attention mechanism of the Transformer has a run time and memory usage that scales quadratically with sequence length. Therefore, this quadratic complexity has become a critical bottleneck in processing long sequences (over 1,000 tokens), and has since motivated many new attention algorithms, see <ref type="bibr" target="#b40">(Tay et al., 2020d)</ref> for a survey of such work.</p><p>In this paper, we draw inspiration from two branches in numerical analysis: Hierarchical Matrix (H-Matrix) <ref type="bibr" target="#b18">(Hackbusch, 1999</ref><ref type="bibr" target="#b19">(Hackbusch, , 2000</ref> and Multigrid method <ref type="bibr" target="#b6">(Briggs et al., 2000)</ref>. We propose a hierarchical attention that has linear complexity in run time and memory, and only utilizes dense linear algebra operations optimized for GPUs or TPUs.</p><p>We hypothesize that the inductive bias embodied by the proposed hierarchical structure for the attention matrix is effective in capturing the hierarchical structure in the sequences typically seen in many natural language processing and computer vision tasks. The main benchmark we use in this paper is the Long Range Arena (LRA) benchmark <ref type="bibr" target="#b39">(Tay et al., 2020c)</ref>, which has been specifically designed to evaluate and compare various sub-quadratic attention algorithms. Our new hierarchical attention mechanism achieves best average performance to-date on the LRA benchmark by more than 6 points over the previous-best BigBird algorithm <ref type="bibr" target="#b46">(Zaheer et al., 2020)</ref>, while pushing SOTA performance higher in 4 of the 5 successful tasks. Furthermore, using this new attention, a Transformer-based language model trained on the One-Billion Word dataset <ref type="bibr" target="#b8">(Chelba et al., 2014)</ref> sets a new SOTA performance record by reducing the test perplexity by 1.55 points comparing to the previousbest Transformer-XL <ref type="bibr" target="#b12">(Dai et al., 2019)</ref> with 5x more parameters. Overall, these empirical results both validate the soundness of our approximation method for computing attention weights, as well as the the appropriateness of the inductive bias present in the proposed hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>It is well established in the NLP literature that the embeddings of nearby tokens tend to be more similar than the distant ones <ref type="bibr" target="#b27">(Manning and Sch?tze, 1999)</ref>. This leads to the intuition that token similarity and hence the attention should decrease with the sequence distance between a query token and a key token 1 . This motivates the sliding-window local attention <ref type="bibr" target="#b33">Ramachandran et al., 2019;</ref><ref type="bibr" target="#b32">Qiu et al., 2019)</ref> which amounts to truncating off-diagonal entries in the attention matrix beyond a user-specified sequence distance. A second approach is to keep O(1) number of nonzeros per row in the attention matrix.</p><p>The nonzero entry selection is either content-based <ref type="bibr" target="#b24">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b34">Roy et al., 2020;</ref><ref type="bibr" target="#b38">Tay et al., 2020b;</ref><ref type="bibr" target="#b47">Zhou et al., 2020)</ref>, handcrafted <ref type="bibr" target="#b4">(Beltagy et al., 2020;</ref><ref type="bibr">Brown et al., 2020;</ref><ref type="bibr" target="#b10">Child et al., 2019;</ref><ref type="bibr" target="#b20">Ho et al., 2019)</ref> or simply random <ref type="bibr" target="#b46">(Zaheer et al., 2020)</ref>. It is also well known in the NLP literature that long-range contextual information is necessary for many NLP tasks <ref type="bibr" target="#b23">(Khandelwal et al., 2018;</ref><ref type="bibr" target="#b25">Liu and Lapata, 2019)</ref>.</p><p>So a set of global tokens are also considered. This adds O(1) number of dense rows and columns to the attention matrix <ref type="bibr" target="#b46">(Zaheer et al., 2020;</ref><ref type="bibr" target="#b4">Beltagy et al., 2020)</ref>. A third approach is to approximate the attention matrix with a lowrank factored form <ref type="bibr" target="#b11">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b37">Tay et al., 2020a)</ref>.</p><p>The first two approaches are based on the premise that one needs to explicitly zero out entries in the attention matrix in order to reduce the quadratic complexity.</p><p>Decades of 1 Eq. (11) and (12) offer a simple illustration of this intuition.</p><p>research by the scientific computing and numerical analysis community has resulted in more sophisticated algorithms to sparsify matrices. A small set of samples of these algorithms and their engineering applications include Fast Multipole Method <ref type="bibr" target="#b17">(Greengard and Rokhlin, 1987;</ref><ref type="bibr" target="#b16">Greengard, 1994;</ref><ref type="bibr" target="#b29">Nabors et al., 1994;</ref><ref type="bibr" target="#b36">Shi et al., 1998)</ref>, Pre-corrected FFT <ref type="bibr" target="#b31">(Phillips and White, 1997;</ref>, Hierarchical Singular Value Decomposition (SVD) <ref type="bibr" target="#b22">(Kapur and Long, 1997)</ref> and Hierarchical Matrix (H-Matrix) <ref type="bibr" target="#b18">(Hackbusch, 1999</ref><ref type="bibr" target="#b19">(Hackbusch, , 2000</ref>. These are generally called Multilevel Methods <ref type="bibr" target="#b5">(Brandt and Lubrecht, 1990)</ref>. The hierarchical attention proposed in this paper is inspired by these Multilevel Methods in general and the H-Matrix in particular. The hierarchical matrix structure allows a linear complexity in both constructing and applying the attention matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definition and Notation</head><p>Given matrices Q, K and V , with rows representing sequences of token embedding or feature vectors for query, key and value respectively, the output weighted by the scaled dot-product attention in the Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> is defined as</p><formula xml:id="formula_0">Z = softmax( QK T ? d )V<label>(1)</label></formula><p>where Z, Q, K, V ? R L?d , L is the length of the sequences, and d is the embedding or feature size. In a more compact matrix form, Eq. (1) can be written as</p><formula xml:id="formula_1">Z = D ?1 AV (2) where A = e S<label>(3)</label></formula><formula xml:id="formula_2">S i,j = Q i K T j ? d (4) D = diag{A ? 1 L } (5) 1 L = [1, 1, ..., 1] T .<label>(6)</label></formula><p>Here, A, S ? R L?L , 1 L ? R L is a vector with all ones, and S i,j represents the unnormalized cosine similarity between query embedding Q i (the i-th row in Q) and key embedding K j (the j-th row in K).</p><p>For the sake of clarity, we focus on the singlehead attention in the exposition of the proposed algorithm. Extension to the multi-head case is straightforward since each attention head is computed independently <ref type="bibr">(Vaswani et al., 2017)</ref>.</p><p>Computing the similarity matrix S in Eq. (4) and the attention matrix A in Eq. (3) takes O(L 2 d) time and O(L 2 ) memory. Similarly, computing AV in Eq. (2) takes O(L 2 d) time, and computing A ? 1 L in Eq. (5) takes O(L 2 ) time. The O(L 2 d) and O(L 2 ) complexities are the bottlenecks for applying the attention mechanism over very long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Introduction on H-Matrix and</head><p>Multigrid Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">H-Matrix</head><p>The singular-value decomposition of the attention matrix A in Eq. <ref type="formula" target="#formula_1">(3)</ref> is</p><formula xml:id="formula_3">A = U ?V T<label>(7)</label></formula><p>where ? = diag{? 1 , ? 2 , ..., ? L } and ? i is the ith singular value. The numerical rank of matrix A is r if L i=r+1 ? i &lt; ? for a given tolerance ? <ref type="bibr" target="#b41">(Trefethen and Bau, 1997)</ref>. The standard rankr approximation to matrix A is</p><formula xml:id="formula_4">A ???V T =?? T<label>(8)</label></formula><p>where? = diag{? 1 , ? 2 , ..., ? r },? ,V ? R L?r have the first r columns of U and V , and? = V?. This is the low-rank approximation used in <ref type="bibr" target="#b11">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b37">Tay et al., 2020a)</ref>. This approximation compresses L 2 entries in A to 2rL entries in? and V T . So the compression rate is L 2r . The H-Matrix generalizes this low-rank approximation by using matrix block hierarchy. Consider a two-level H-Matrix with 4 ? 4 and 2 ? 2 block partition at level-0 and level-1, respectively. Matrix A is partitioned as</p><formula xml:id="formula_5">A = ? ? ? ? ? ? A (0) 11 A (0) 12 A (0) 21 A (0) 22 A (1) 12 A (1) 21 A (0) 33 A (0) 34 A (0) 43 A (0) 44 ? ? ? ? ? ? .<label>(9)</label></formula><p>The low-rank approximation in Eq. (8) is applied to the off-diagonal blocks at each level. For example,</p><formula xml:id="formula_6">A (l) 12 ?? (l) 12 (? (l) 12 ) T<label>(10)</label></formula><p>where l = 0, 1. To give a concrete example, suppose each entry in matrix A has the analytical form</p><formula xml:id="formula_7">A i,j = e S i,j (11) S i,j = 2e ?(i?j) 2 ? 1<label>(12)</label></formula><p>where i, j = 0, 1, 2, ..., 15 2 . With the block hierarchy defined in Eq. (9), the size of the matrix block at level-1 and level-0 is 8 ? 8 and 4 ? 4, respectively. For tolerance ? = 10 ?3 , one can verify that the numerical rank map of matrix A is</p><formula xml:id="formula_8">? ? ? ? 4 2 2 4 2 2 4 2 2 4 ? ? ? ?<label>(13)</label></formula><p>where the number in each block is the numerical rank of the corresponding block in Eq. (9). Note that matrix A still has full numerical rank of 16 at a looser tolerance 10 ?1 . So the standard lowrank approximation is ineffective in this case. But even this simple two-level H-matrix already offers a compression rate of 4 3 since storing an H-matrix with the rank map in Eq. (13) takes 192 entries 3 . In addition, one can verify that no entry A i,j in Eq. (11) is very small, since S i,j ? [?1, 1] in Eq. (12). Therefore, truncating off-diagonal entries of matrix A, as proposed in , would produce a poor approximation. In practice, the number of levels is adapted to the underlining governing equations that result in matrix A and it can easily be over 10 ( <ref type="bibr" target="#b22">Kapur and Long, 1997;</ref><ref type="bibr" target="#b19">Hackbusch, 2000;</ref>. In turn, this can substantially increase the compression rate. In general, the computation complexity of the H-Matrix is either O(L) or O(L log L), depending on the underlining physics <ref type="bibr" target="#b18">(Hackbusch, 1999</ref><ref type="bibr" target="#b19">(Hackbusch, , 2000</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Elements of the Multigrid Method</head><p>Multigrid Method is a multi-level nested iterative method for solving large-scale sparse matrices resulting from discretized partialdifferential equations (PDEs) <ref type="bibr" target="#b6">(Briggs et al., 2000;</ref><ref type="bibr" target="#b42">Trottenberg et al., 2000)</ref>. At its core are two simple but powerfully complementary ideas: relaxation and correction. Our proposed hierarchical attention only uses the correction scheme as a building block since there is no sparse matrix to relax on.</p><p>The correction scheme has two components: restriction or coarsening, and interpolation or prolongation. Consider a vectorv h of scalar values defined on a set of N grids with uniform interval h. The simplest coarsening is to take the average of the scalar values on each pair of grids, i.e.,</p><formula xml:id="formula_9">v 2h j = 1 2 (v h 2j +v h 2j+1 )<label>(14)</label></formula><p>where j = 0, 1, 2, ...N/2 ? 1. The superscript in Eq. <ref type="formula" target="#formula_0">(14)</ref> indicates that the grid interval at these two levels is h and 2h, respectively. The simplest interpolation is to duplicate the value on each coarse grid to values on a pair of fine grids, i.e.,</p><formula xml:id="formula_10">v h 2j =v 2h j ,v h 2j+1 =v 2h j<label>(15)</label></formula><p>where j = 0, 1, 2, ...N/2 ? 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Intuition for Hierarchical Attention</head><p>The hierarchical low-rank structure like Eq. (13) turns out to be pervasive in many if not all physics phenomena. Much of the theoretical analysis by <ref type="bibr" target="#b17">(Greengard and Rokhlin, 1987;</ref><ref type="bibr" target="#b18">Hackbusch, 1999)</ref> is concerned with quantifying such aspects.</p><p>The key insight into these Multilevel Methods can be summarized as follows: perform no approximation for near interactions, and apply progressively lower-precision approximation for progressively longer distance interactions. The simple case shown in Eq. (9)-(13) is a good example. To satisfy the tolerance of 10 ?3 , we need full rank (no approximation) for the diagonal blocks (near interactions), higher precision approximation (rank-2 vs full-rank of 4) for the 4 ? 4 off-diagonal blocks at level-0 (mid-distance) and lower precision approximation (rank-2 vs full-rank of 8) for the 8 ? 8 off-diagonal blocks at level-1 (long-distance).</p><p>In this section, we present some intuition to answer two important questions: 1) Does the hierarchical low-rank structure hold for the attention matrix A in Eq. (3)? 2) What is the algorithm to efficiently compute the hierarchical low-rank structure? We only give an informal exposition of the hierarchical attention. The formal mathematical derivation is deferred to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hierarchical Structure As Inductive Bias</head><p>The error analysis in <ref type="bibr" target="#b17">(Greengard and Rokhlin, 1987;</ref><ref type="bibr" target="#b18">Hackbusch, 1999)</ref> offers little direct insight since the attention matrix A in Eq. (3) is data dependent by definition and hence its analytical form like Eq. <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref> is generally unknown. So gathering empirical evidences seems the only viable path to answer the first question listed above.</p><p>The ablation studies by <ref type="bibr" target="#b23">(Khandelwal et al., 2018)</ref> examine the effect of context words on a language model. Within the context range of about 200 tokens, word order is only relevant within the 20 most recent tokens or about a sentence. In the long-range context, order has almost no effect on performance, suggesting that the model maintains a high-level, rough semantic representation of faraway words. The observation is succinctly summarized by the title of the paper "sharp nearby, fuzzy far away". Remarkably, this is in spirit very close to the key insight into the Multilevel Methods.</p><p>A few recent attention-related studies have explored this direction with some success, such as word-level and sentence-level attentions in <ref type="bibr" target="#b28">(Miculicich et al., 2018;</ref><ref type="bibr" target="#b0">Abreu et al., 2019)</ref>, and sentence-level and paragraph-level attentions in <ref type="bibr" target="#b25">(Liu and Lapata, 2019)</ref>. Even though the proposed hierarchical attention in these studies only has two levels, as opposed to ten or more levels typically used by the Multilevel Methods, the reported positive results are quite suggestive.</p><p>We therefore hypothesize that the same hierarchical low-rank structure as shown in Eq (13) might also hold for the attention matrix in many NLP tasks. And we treat it as the inductive bias in the hierarchical attention mechanism proposed in this paper. As pointed out in <ref type="bibr" target="#b15">(Goyal and Bengio, 2020)</ref>, inductive biases encourage the learning algorithm to prioritise solutions with certain properties. Hence good benchmark performance delivered by a Transformer-based model with proposed hierarchical attention can be regarded as a positive evidence to support the hierarchical low-rank structure hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Informal Exposition of Hierarchical Attention</head><p>In the standard definition of attention in Eq. <ref type="formula" target="#formula_1">(3)</ref> and <ref type="formula">(4)</ref>, there is no preference given to any keys based on the sequence distance between a query and keys. The observation in <ref type="bibr" target="#b23">(Khandelwal et al., 2018)</ref> clearly suggests that a distance-dependent attention mechanism should be a better alternative.</p><p>We will take three steps to informally explain the hierarchical attention mechanism. First, the attention matrix blocks for nearby, mid-distance and long-distance attention are separated in section 5.2.1. This is the first step toward the distance-dependent attention mentioned above. Second, a token hierarchy is established in section 5.2.2. Third, the hierarchical attention is constructed in section 5.2.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Attention Partition</head><p>Consider a 16-word sentence in <ref type="figure" target="#fig_0">Fig. 1</ref>. The sentence is partitioned at three segment granularity. This induces a three-level partition of the attention matrix A for the original sequence:</p><formula xml:id="formula_11">A = A (2) + A (1) + A (0)<label>(16)</label></formula><p>where</p><formula xml:id="formula_12">A (2) = 0 A (2) 12 A (2) 21 0 (17) A (1) = ? ? ? ? ? ? A (1) 12 A (1) 21 A (1) 23 A (1) 32 A (1) 34 A (1) 43 ? ? ? ? ? ?<label>(18)</label></formula><formula xml:id="formula_13">A (0) = ? ? ? ? ? ? A (0) 11 A (0) 12 A (0) 21 A (0) 22 A (0) 23 . . . . . . . . . A (0) 87 A (0) 88 ? ? ? ? ? ? .</formula><p>(19) Note that the nonzero entries in A (0) , A (1) and A (2) are the same as the corresponding entries of</p><formula xml:id="formula_14">matrix A in Eq. (3). Matrix block size of A (0) ij , A (1) ij and A (2)</formula><p>ij is 2?2, 4?4 and 8?8, respectively. Following the key insight into Multilevel Methods, we perform no approximation to any level-0 matrix block A (0) ij and apply a low-rank approximation to off-diagonal matrix blocks in A (1) and A (2) . If we set the numerical rank of all these blocks to 2, then we can assemble the three rank  maps into a single rank map as 4</p><formula xml:id="formula_15">? ? ? ? ? ? ? ? ? ? ? ? ? 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ? ? ? ? ? ? ? ? ? ? ? ? ? .<label>(20)</label></formula><p>The hierarchical structure embodied by the predetermined rank map in Eq. <ref type="formula" target="#formula_15">(20)</ref> represents the inductive bias for the attention matrix A in Eq. <ref type="formula" target="#formula_0">(16)</ref>. But this construction step is inefficient because we need to form the original attention matrix and then perform SVD to discover the low-rank approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Token Hierarchy</head><p>To illustrate the notion of token hierarchy, consider the same 16-word sentence in <ref type="figure" target="#fig_1">Fig. 2</ref>. A simple 3-level binary-tree hierarchy can be set up by following the simple coarsening defined in Eq. <ref type="formula" target="#formula_0">(14)</ref>: 1) At level-0, each one of the 16 words is mapped to its word embedding; 2) At level-1, each token (parent node) corresponds to a pair of adjacent words at level-0 (child nodes), which are shown inside each box. The embedding of each parent token is simply the average of its child token embeddings; 3) At level-2, each token (parent node) corresponds to one pair of adjacent tokens at level-1 (child nodes) or 4 adjacent words at level-0 (grand child nodes), which are shown inside each box. The embedding of each parent token is simply the average of its child token embeddings. In general, the height of the binary tree is O(log 2 (L) and the total number of tree nodes is O(2L), where L is the sequence length. We only need word embeddings for the leaf nodes since the embeddings of all other tree nodes can be recursively computed. The formal definition and notations of the recursion for query and key are detailed in section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Informal Construction of Hierarchical</head><p>Attention It is clear from <ref type="figure" target="#fig_1">Fig. 2</ref> that the embeddings of higher level tokens represent a coarser level representation of a larger chunk of the text. The tokens at different levels can be understood as multi-scale snapshots of the original token sequence at level-0. Hence this token hierarchy naturally induces a set of multi-scale attention matrices. Let? (i) be the attention matrix induced by the tokens at level-i. It is clear from <ref type="figure" target="#fig_1">Fig. 2</ref> that the size of? (0) ,? (1) and A (2) is 16 ? 16, 8 ? 8 and 4 ? 4, respectively. This multi-scale viewpoint does not directly lead to a useful algorithm since matrix? (0) contains all the information and there is little additional information from? (1) and? <ref type="bibr">(2)</ref> .</p><p>A key step to arrive at the hierarchical attention is to apply the contextual sliding window at each hierarchy level. The tokens at each level are partitioned into segments of size 2 in <ref type="figure" target="#fig_1">Fig. 2</ref>. One way to implement the local attention is to allow each query token segment to attend only two adjacent key token segments, one to its left and another to its right. At level-0, each query token segment also attends to the collocated key token segment. The token segment partition and local attention lead to a tri-diagonal block sparse matrix structure for A (0) and bi-diagonal block sparse matrix structure for? (1) and? <ref type="bibr">(2)</ref> . Their sparsity patterns ar?</p><formula xml:id="formula_16">A (0) ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ? ? ? ? ? ? ? ? ? ? ? ? ? (21) A (1) ? ? ? ? ? 2 2 2 2 2 2 ? ? ? ? (22) A (2) ? 2 2<label>(23)</label></formula><p>where the 2 in the nonzero blocks indicates that these are dense blocks of size 2 ? 2.  It is clear that? (0) is identical to A (0) in Eq. <ref type="formula" target="#formula_0">(19)</ref>. The efficiency gain comes from? (2) and A (1) . Each nonzero entry in? (2) and? (1) captures the aggregated or coarse attention between two disjoint chunk of four and two tokens, respectively. Progressively larger token chunks lead to progressively lower-precision approximation to the original attention blocks. This is precisely the intention of the rank map in Eq. (20). We can now see that? (2) and? (1) provide an efficient way to approximate A (2) in Eq. (17) and A (1) in Eq. <ref type="formula" target="#formula_0">(18)</ref>, respectively.</p><p>6 Key Components in Hierarchical Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Constructing Hierarchical Attention</head><p>The simple example in <ref type="figure" target="#fig_1">Fig. 2</ref> can be easily generalized. Eq. (14) is used to coarsen or merge rows in matrices Q, K and V in Eq. (1). For sequence length L = 2 M +1 , the coarsening establishes a binary tree of depth M for Q, K and V , respectively. Each tree node represents a matrix row and there are 2 M +1?l nodes or rows at level-l. To facilitate the discussion, we define a few hierarchy related notations here. LetQ (l) ,K (l) and? (l) be coarsened versions of Q, K and V at level-l in the binary tree. We note that l = 0 is a special case, which is defined as</p><formula xml:id="formula_17">Q (0) = Q,K (0) = K,? (0) = V.<label>(24)</label></formula><p>Following Eq. <ref type="formula" target="#formula_0">(14)</ref>, the recursion to coarsen Q, K and V is:Q</p><formula xml:id="formula_18">(l+1) j = 1 2 (Q (l) 2j +Q (l) 2j+1 )<label>(25)</label></formula><formula xml:id="formula_19">K (l+1) j = 1 2 (K (l) 2j +K (l) 2j+1 )<label>(26)</label></formula><formula xml:id="formula_20">V (l+1) j = (? (l) 2j +? (l) 2j+1 )<label>(27)</label></formula><p>where l = 0, 1, ..., M ? 2 and j = 0, 1, 2, ..., 2 M ?l . It should be noted that the coarsening of V in Eq. <ref type="formula" target="#formula_3">(27)</ref> does not have the averaging factor 1 2 . We defer more details on coarsening to Appendix Section A.1. Now we are ready to compute the nonzero entries in Eq. <ref type="formula" target="#formula_0">(21)</ref>, <ref type="formula">(22)</ref> and <ref type="formula" target="#formula_1">(23)</ref> and construct hierarchical attention matrix? <ref type="bibr">(l)</ref> . Substituting Eq. <ref type="formula" target="#formula_18">(25)</ref> and <ref type="formula" target="#formula_2">(26)</ref> into <ref type="formula">(4)</ref> and then into (3), we obtain?</p><formula xml:id="formula_21">(l) ij = eS (l) ij = eQ (l) i (K (l) j ) T ? d<label>(28)</label></formula><p>Again, we note that l = 0 is a special case becaus? A</p><formula xml:id="formula_22">(0) ij = A ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Applying Hierarchical Attention</head><p>The hierarchical matrix structure in Eq. <ref type="formula" target="#formula_0">(17)</ref>, <ref type="formula" target="#formula_0">(18)</ref> and <ref type="formula" target="#formula_0">(19)</ref> naturally leads to a hierarchical approach to the matrix-matrix multiplication in Eq.</p><p>(2) and the matrix-vector multiplication in Eq. (5). We use the matrix-matrix multiplication as an example since matrix-vector multiplication is just a special case of the matrix-matrix multiplication.</p><p>In view of Eq. <ref type="formula" target="#formula_0">(17)</ref>, <ref type="formula" target="#formula_0">(18)</ref> and <ref type="formula" target="#formula_0">(19)</ref>, we write the matrix-matrix multiplication in Eq. (2) as</p><formula xml:id="formula_23">Y = AV = Y (0) + P (0) ? (1) + P (1)? (2) (29) where Y (0) = A (0) V (0) ,? (l) =? (l)? (l) , l = 1, 2</formula><p>(30) We defer the detailed derivation of Eq. (29) to Appendix Section A.5 and A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Algorithm And Computational Complexity</head><p>To facilitate the description and the complexity analysis of the algorithm, we define a few more hierarchy-related notations. In addition to sequence length L, number of hierarchy levels M and embedding or feature size d in Eq.</p><p>(1), the new notations include: 1) N r : numerical rank of the off-diagonal blocks (for instance, 2 in Eq. <ref type="formula" target="#formula_15">(20)</ref>). This is also the diagonal block size at level-0; 2) N (l) b : number of blocks at level-l. Note that L and d are usually data-dependent hyper-parameters, while N r is the only model hyper-parameter responsible for our method's inductive bias. In turn, N (l) b and M are derived parameters, computed as:</p><formula xml:id="formula_24">N (0) b = L N r , N (l+1) b = N (l) b 2 (31) M = log 2 (N (0) b ).<label>(32)</label></formula><p>It is easy to verify that</p><formula xml:id="formula_25">M ?1 l=0 N (l) b = M ?1 l=0 N (0) b 2 l ? 2N (0) b .<label>(33)</label></formula><p>It is important to note that only the diagonal blocks at level-0 and the super-diagonal and subdiagonal blocks at level-l are needed in applying the hierarchical attention matrix. This is clearly shown in Eq. (21)-(23). This means that only N (l) b ? 1 super-diagonal and sub-diagonal blocks are computed at level-l. This is crucial to the overall linear complexity in run time and memory.</p><p>We should also note that all matrix blocks in coarse attention matrix? (l) have the same size N r ? N r . This is due to the rank map in Eq. <ref type="formula" target="#formula_15">(20)</ref>. This is crucial for efficiency reason since the single-instruction-multiple-data (SIMD) programming style supported by the dense linear algebra libraries for GPU and TPU encourages uniform tensor shapes.</p><p>We summarize the main steps to construct and apply the hierarchical attention in Algorithm 1. So the overall run time complexity of the hierarchical attention algorithm is O(dL). Likewise, the memory complexity can be shown to be O(dL) as well. We defer the detailed analysis to appendix Section A.5 and A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments And Results</head><p>We have implemented the proposed hierarchical attention using Jax, an open source library 5 for automatic gradient computation and linear algebra operations on GPUs and TPUs. All numerical operations in our algorithm use the Numpy native linear algebra functions supported by Jax. In all our experiments in this section, we use the standard Transformer architecture described in <ref type="bibr">(Vaswani et al., 2017)</ref> as the backbone for our H-Transformer-1D model. Unless specified otherwise, the model parameters are: number of layers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implementation 6 so that we can perform a simple drop-in replacement of the standard multihead attention with our hierarchical attention implementation. This allows for an easy and fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Long-Range Arena</head><p>The open-source Long-Range Arena (LRA) benchmark 7 has been proposed as a standard way to probe and quantify the capabilities of various xformer (long-range Transformer) architectures <ref type="bibr" target="#b39">(Tay et al., 2020c)</ref>. In our case, it also serves to highlight the effectiveness of the inductive bias inspired by the H-Matrix method, as well as the capability of our hierarchical attention to handle long sequences. The LRA has several desirable qualities that made us focus on it as a primary evaluation benchmark: generality (restricted to encoder-only tasks to accommodate most proposals); simplicity (no pretraining, no data augmentation allowed); difficulty (large headroom with existing approaches); long-input focus (so that modeling improvements in this area are visible); diverse (6 tasks, covering math, language, image, and spatial modeling); and lightweight (so that modeling improvements are measurable independently of the ability to train and run high-capacity models).</p><p>The tasks that comprise LRA are: ListOps (sequences of arithmetical expressions of lengths of up to 2K that tests the ability to reason hierarchically while handling long context); Text (byte/character-level text classification at document level, which both simulates longer input sequences -max length 4K -and increases the difficulty level); Retrieval (byte/character-level document retrieval, which simulates the ability to model document similarity as a score between two independently-encoded long input sequences -max length 4K + 4K = 8K); Image (image classification based on the CIFAR-10 dataset, where an NxN image is flattened to a sequence of length N 2 pixels); Pathfinder (long-range spatial dependency task, with images consisting of two small circles and dash-line paths that either connect the two circles or not -image dimensions of 32x32 for a pixel sequence of length 1,024); Path-X (same as Pathfinder, but for image dimensions of 128x128 for a total pixel sequence of length 16,384). The default Transformer model parameters such as number of layers and number of heads etc are pre-determined by the benchmark configuration for each task.</p><p>The results obtained by our H-Transformer-1D model on the LRA benchmark are given in <ref type="table">Table 1</ref>. Overall, the H-Transformer-1D model achieves 61.41 average accuracy, a +6.4 points improvement over the previous-best average performance from BigBird <ref type="bibr" target="#b46">(Zaheer et al., 2020)</ref>. We want to highlight ListOps, Text and Retrieval because they all involve long sequences and H-Transformer-1D model improves SOTA performance by relatively large margins. These should be strong evidences to support our hypothesis in section 5.1 and validate the inductive bias due to the hierarchical attention.  <ref type="table">Table 1</ref>: Experimental results on long-range arena benchmark. Best model is in boldface and second best is underlined. All models do not learn anything on Path-X task, contrary to the Pathfinder task and this is denoted by FAIL. Path-X is not counted toward the Average score as it has no impact on relative performance.</p><p>Model perplexity parameters <ref type="bibr" target="#b12">(Dai et al., 2019)</ref> 21.8 800M <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref> 23.02 1000M <ref type="bibr" target="#b12">(Dai et al., 2019)</ref> 23.5 465M <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref> 23.91 465M  24.0 4900M</p><p>Transformer baseline 30.04 53M Transformer baseline 24.8 144M H-Transformer-1D N r = 16 23.95 53M H-Transformer-1D N r = 16 20.25 144M <ref type="table">Table 2</ref>: Experimental results on one-billion word benchmark. We compare previous SOTA results obtained with models of size 465M-4900M parameters against the performance of the quadratic attention baseline and the H-Transformer-1D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Language Models Trained on One-Billion Words</head><p>We have used Flax, an open-source library 8 to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implementation in Flax as the backbone. Only the attention is replaced with our hierarchical attention. We trained both the Transformer baseline and H-Transformer-1D on the One-Billion Word benchmark <ref type="bibr" target="#b8">(Chelba et al., 2014)</ref>. We tried different N r (numerical rank) in our H-Transformer-1D model. These represent different inductive bias. We found that H-Transformer-1D with N r = 16 generated 8 https://github.com/google/flax text with quality comparable to that of the baseline Transformer. For both Transformer baseline and H-Transformer-1D, we also tried two sets of model parameters: 1) embedding size is 512 and feed-forward module size is 2048 and hence the parameter count is 53M; 2) embedding size is 1024 and feed-forward module size is 4096 and hence the parameter count is 144M. The test perplexity results of these four models and various SOTA models are shown in table 2. H-Transformer-1D delivers the lowest perplexity to-date while using 5? smaller model capacity than that of the previous SOTA model Transformer-XL <ref type="bibr" target="#b12">(Dai et al., 2019)</ref>. This is another strong evidence to support our hypothesis in section 5.1 and validate the inductive bias due to the hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Future Work</head><p>We have proposed a new Transformer attention using the inductive bias inspired by the H-Matrix. The new algorithm has linear complexity in run time and memory usage and is fully compatible with dense linear algebra libraries on GPU and TPU. The effectiveness of this new attention is demonstrated by the empirical evidences from long-range arena benchmark and One-Billion word language modeling. Future work include applying the new attention to music and genomics, developing proper inductive bias for cross-attention and extending to 2D images.</p><p>For sequence length L = 2 M +1 , the coarsening establishes a binary tree of depth M for Q, K and V , respectively. The root of the binary tree at level-(M ? 1) has two nodes which correspond to the two matrix rows coarsened from four matrix rows at level-(M ? 2). The piecewise constant restriction matrix at level-(M ? 2) is </p><formula xml:id="formula_26">R (M ?2) = 1 1 0 0 0 0 1 1 2?4 .<label>(34)</label></formula><formula xml:id="formula_27">? ? 4?8 = R (M ?2) 0 0 R (M ?2) .<label>(35)</label></formula><p>In general, the restriction matrices follow the recursion</p><formula xml:id="formula_28">R (l?1) = R (l) 0 0 R (l)<label>(36)</label></formula><p>which starts from R (M ?2) of size 2 ? 4 and goes backward to R (0) of size L 2 ? L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Interpolation Matrices</head><p>Given Y (l) at level-l, the interpolated Y (l?1) at level-(l ? 1) can be written as</p><formula xml:id="formula_29">Y (l?1) = P (l) Y (l)<label>(37)</label></formula><p>where l = 1, 2, ..., M ? 1, sparse matrix P (l) has size L (l?1) ? L (l) , and L (l) = 2 M ?l is the node count at level-l of the binary tree. This recursion also follows the binary tree hierarchy. The four matrix rows at level-(M ? 2) are interpolated from the two matrix rows at level-(M ? 1). Specifically, the piecewise constant interpolation matrix at level-(M ? 1) is</p><formula xml:id="formula_30">P (M ?1) = ? ? ? ? 1 0 1 0 0 1 0 1 ? ? ? ? 4?2 .<label>(38)</label></formula><p>Likewise, the piecewise constant interpolation matrix at level-(M ? 2) is</p><formula xml:id="formula_31">P (M ?2) = ? ? ? ? ? ? ? ? ? ? ? ? 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 ? ? ? ? ? ? ? ? ? ? ? ? 8?4 = P (M ?1) 0 0 P (M ?1) . (39)</formula><p>In general, the interpolation matrices follow the recursion</p><formula xml:id="formula_32">P (l?1) = P (l) 0 0 P (l)<label>(40)</label></formula><p>which starts from P (M ?1) of size 4 ? 2 and goes backward to P (0) of size L? L 2 . In view of Eq. <ref type="formula" target="#formula_1">(34)</ref> and <ref type="formula" target="#formula_1">(38)</ref>, it is obvious that</p><formula xml:id="formula_33">P (M ?1) = (R (M ?2) ) T .<label>(41)</label></formula><p>In view of the recursions in Eq. <ref type="formula" target="#formula_1">(36)</ref> and <ref type="formula" target="#formula_32">(40)</ref>, it is easy to prove by induction that</p><formula xml:id="formula_34">P (l) = (R (l?1) ) T .<label>(42)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Expansion Matrices</head><p>For the purpose of factored low-rank approximation for the off-diagonal attention matrix blocks, we design a series of so-called expansion matrices. The first two expansion matrices in this series are</p><formula xml:id="formula_35">T (M ?1) = P (M ?1) = ? ? ? ? 1 0 1 0 0 1 0 1 ? ? ? ? 4?2 = 1 2 0 0 1 2<label>(43)</label></formula><p>and</p><formula xml:id="formula_36">T (M ?2) = P (M ?2) P (M ?1) = ? ? ? ? ? ? ? ? ? ? ? ? 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ? ? ? ? ? ? ? ? ? ? ? ? 8?2 = 1 4 0 0 1 4<label>(44)</label></formula><p>where 1 N is a length-N vector of ones. The general form of matrix T (l) is defined as</p><formula xml:id="formula_37">T (l) = ? M ?1 i=l P (i)<label>(45)</label></formula><p>where l = 1, 2, ..., M ? 1. In view of Eq. <ref type="formula" target="#formula_1">(43)</ref>, <ref type="formula" target="#formula_37">(45)</ref> and <ref type="formula" target="#formula_32">(40)</ref>, it is easy to prove by induction that</p><formula xml:id="formula_38">T (l) = 1 2 M ?l 0 0 1 2 M ?l<label>(46)</label></formula><p>and it has size 2 M ?l+1 ? 2. Further more, in view of Eq. <ref type="formula" target="#formula_37">(45)</ref> and <ref type="formula" target="#formula_34">(42)</ref>, we have</p><formula xml:id="formula_39">(T (l) ) T = ? l i=M ?1 R (i?1) .<label>(47)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Low-Rank Factored Form</head><p>Matrix T (l) plays a pivotal role in constructing the low-rank approximation to the off-diagonal attention matrix blocks. Let the ij-th block in the coarsened attention matrix at level-1 b?</p><formula xml:id="formula_40">A (1) ij = a 11 a 12 a 21 a 22<label>(48)</label></formula><p>where a ij is the entry resulted from the inner product between a row inQ (1) andK (1) . The rank-2 approximation to the corresponding ij-th block in the original attention matrix A at level-1 can be written as</p><formula xml:id="formula_41">A (1) ij ? T (M ?1)? (1) ij (T (M ?1) ) T (49) = ? ? ? ? 1 0 1 0 0 1 0 1 ? ? ? ?</formula><p>a 11 a 12 a 21 a 22 1 1 0 0 0 0 1 1 = ? ? ? ? a 11 a 11 a 12 a 12 a 11 a 11 a 12 a 12 a 21 a 21 a 22 a 22 a 21 a 21 a 22 a 22 ? ? ? ? .</p><p>It is clear that the resulting 4 ? 4 matrix A</p><p>ij is essentially the piecewise constant interpolation of the 2 ? 2 matrix? </p><p>ij necessarily has rank 2. One can also view a ij as being similar to the average value at the ij-th cluster center in the K-mean method. The role of matrix T (M ?1) is to expand from these 2 ? 2 clusters to the 4 ? 4 grid and hence the name expansion matrix.</p><p>Since we maintain the same numerical rank 2 for all super-and sub-diagonal attention matrix blocks, the rank-2 approximation to the ij-th block in the original attention matrix A at level-l is</p><formula xml:id="formula_45">A (l) ij ? T (M ?l)? (l) ij (T (M ?l) ) T = ? M ?1 i=M ?l P (i)? (l) ij ? M ?l i=M ?1 R (i?1) (51)</formula><p>where the last equality is due to Eq. <ref type="formula" target="#formula_37">(45)</ref> and <ref type="formula" target="#formula_3">(47)</ref>. We note that matrix T (l) has full column rank 2 by design and this can be easily shown from Eq. (46). We have used this fact to construct the rank-2 approximation in Eq. (51).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Construct Hierarchical Attention Matrix</head><p>To see how Eq. (51) can be used, consider a simple three-level partition of the attention matrix A for sequence length L = 16</p><formula xml:id="formula_46">A = A (2) 11 A (2) 12 A (2) 21 A (2) 22 (52) A (2) 11 = ? ? ? ? ? ? A (0) 11 A (0) 12 A (0) 21 A (0) 22 A (1) 12 A (1) 21 A (0) 33 A (0) 34 A (0) 43 A (0) 44 ? ? ? ? ? ? (53) A (2) 22 = ? ? ? ? ? ? A (0) 55 A (0) 56 A (0) 65 A (0) 66 A (1) 34 A (1) 43 A (0) 77 A (0) 78 A (0) 87 A (0) 88 ? ? ? ? ? ?<label>(54)</label></formula><p>where the size of level-0, level-1 and level-2 matrix blocks is 2 ? 2, 4 ? 4 and 8 ? 8, respectively. Note that the number of levels is M = log 2 (L/2) = 3. We use this simple three-level example to illustrate the key steps in both constructing and applying the hierarchical attention matrix. In view of Eq. (51), we have</p><formula xml:id="formula_47">A ? ? (2) 11 T (1)? (2) 12 (T (1) ) T T (1)? (2) 21 (T (1) ) T? (2) 22 (55) A (2) 11 = ? ? ? ? ? ? A (0) 11 A (0) 12 A (0) 21 A (0) 22 T (2)? (1) 12 (T (2) ) T T (2)? (1) 21 (T (2) ) T A (0) 33 A (0) 34 A (0) 43 A (0) 44 ? ? ? ? ? ?<label>(56)</label></formula><formula xml:id="formula_48">? (2) 22 = ? ? ? ? ? ? A (0) 55 A (0) 56 A (0) 65 A (0) 66 T (2)? (1) 34 (T (2) ) T T (2)? (1) 43 (T (2) ) T A (0) 77 A (0) 78 A (0) 87 A (0) 88 ? ? ? ? ? ? .</formula><p>(57) We note that matrices T (l) , l = 1, 2 are never explicitly formed and are only implicitly used, as shown in next section. So only the diagonal blocks at level-0 and super-and sub-diagonal blocks of the coarsened matrix? at level-l need to be explicitly computed. By design, all these blocks have the same size 2 ? 2 if we set the numerical rank to N r = 2. The total number of superand sub-diagonal blocks in the binary tree hierarchy is upper bounded by twice the number of super-and sub-diagonal blocks at level-0, which is 2N</p><formula xml:id="formula_49">(0) b .</formula><p>Hence the total number of entries is 5N</p><formula xml:id="formula_50">(0) b N 2 r = 5LN r = O(LN r )</formula><p>. Each entry is equal to the inner product betweenQ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Apply Hierarchical Attention Matrix</head><p>Computing matrix-matrix product AV follows the hierarchical structure of matrix A in Eq. (55), (56) and (57). We first partition matrix V according to the three-level binary tree established by the coarsening process, i.e.,</p><formula xml:id="formula_51">V = ? ? ? ? ? ? ? ? V (0) 1 V (0) 2 . . . V (0) 7 V (0) 8 ? ? ? ? ? ? ? ? = ? ? ? ? ? V (1) 1 V (1) 2 V (1) 3 V (1) 4 ? ? ? ? ? = V (2) 1 V (2) 2 .</formula><p>(58) Note that these are partitions of the same matrix V at 3 different levels. For sequence length L = 16, matrix V has size 16 ? d, and the size of the partitioned blocks V (2) k are 2 ? d, 4 ? d and 8 ? d, respectively. In the derivation to come, we may exchange partitions at different levels. For instance, in view of Eq. (58), we have</p><formula xml:id="formula_52">V (2) 1 = V (1) 1 V (1) 2 .<label>(59)</label></formula><p>So we may replace V</p><p>(2) 1 with the right-hand side in Eq. (59).</p><p>In view of Eq. <ref type="formula">(52)</ref> and <ref type="formula" target="#formula_4">(58)</ref>, matrix-matrix product AV can be written as</p><formula xml:id="formula_53">Y = AV = A (2) 11 V (2) 1 A (2) 22 V (2) 2 + A (2) 12 V (2) 2 A (2) 21 V (2) 1 = A (2) 11 V (2) 1 A (2) 22 V (2) 2 + Y (2) .<label>(60)</label></formula><p>In view of Eq. (55), we have</p><formula xml:id="formula_54">Y (2) = A (2) 12 V (2) 2 A (2) 21 V (2) 1 ? T (1)? (2) 12 (T (1) ) T V (2) 2 T (1)? (2) 21 (T (1) ) T V (2) 1 = P (1) P (2)? (2) 12 R (1) R (0) V (2) 2 P (1) P (2)? (2) 21 R (1) R (0) V (2) 1 = P (0) P (1) ? (2) 12? (2) 2 A (2) 21? (2) 1 = P (0) P (1) ? (2) 1 Y (2) 2<label>(61)</label></formula><p>where</p><formula xml:id="formula_55">? (2) 1 V (2) 2 = R (1) R (0) V (2) 1 R (1) R (0) V (2) 2 .<label>(62)</label></formula><p>The third equality in Eq. (61) is due to Eq. <ref type="formula" target="#formula_37">(45)</ref> and <ref type="formula" target="#formula_3">(47)</ref>  </p><p>where Y (1) 1 = T (2)? (1) 12 (T (2) ) T V (1) 2 T (2)? (1) 21 (T (2) ) T V</p><p>(1) 1 = P (2)? (1) 12 R (1) V</p><p>(1) 2 P (2)? (1) 21 R (1) V</p><p>(1) 1 = P (1) ? (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12?</head><p>(1) 2 A (1) 21?</p><p>(1) 1</p><formula xml:id="formula_57">= P (1) ? (1) 1 Y (1) 2<label>(64)</label></formula><p>and</p><formula xml:id="formula_58">? (1) 1 V (1) 2 = R (1) V (1) 1 R (1) V (1) 2 .<label>(65)</label></formula><p>The second equality in Eq. (64) is due to Eq. <ref type="formula" target="#formula_37">(45)</ref> and <ref type="formula" target="#formula_3">(47)</ref> where l = 2. The third equality in Eq. <ref type="formula" target="#formula_2">(64)</ref>  </p><p>where Y (1) 2 = P (2)? (1) 34 R (1) V</p><p>(1) 4 P (2)? (1) 43 R (1) V (1) 3 = P (1) ? (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34?</head><p>(1) 4 A (1) 43?</p><p>(1) 3</p><formula xml:id="formula_60">= P (1) ? (1) 3 Y (1) 4<label>(67)</label></formula><p>and </p><formula xml:id="formula_61">? (1) 3 V (1) 4 = R (1) V (1) 3 R (1) V (1) 4 .<label>(68)</label></formula><formula xml:id="formula_62">Y (1) = ? ? ? ? ?? (1) 1 Y (1) 2 Y (1) 3 Y (1) 4 ? ? ? ? ? = ? ? ? ? ?? (1) 12? (1) 2 A (1) 21? (1) 1 A (1) 34? (1) 4 A (1) 43? (1) 3 ? ? ? ? ? (71) Y (2) = ? (2) 1 Y (2) 2 = ? (2) 12?</formula><p>(2) 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>(2) 21?</p><p>(2) 1</p><p>To summarize, matrix-matrix product computation includes the following steps:</p><p>1. Compute? (1) in Eq. <ref type="formula" target="#formula_2">(65)</ref> and <ref type="formula" target="#formula_2">(68)</ref>, and com-pute? (2) in Eq. <ref type="formula" target="#formula_2">(62);</ref> 2. Compute Y (0) in Eq. <ref type="formula" target="#formula_3">(70)</ref>,? (1) in Eq. <ref type="formula" target="#formula_0">(71)</ref> and? <ref type="bibr">(2)</ref> in Eq. <ref type="formula" target="#formula_3">(72);</ref> 3. Interpolate and cumulative sum in Eq. <ref type="formula" target="#formula_2">(69)</ref>;</p><p>Note that all operations in step-2 are dense matrixmatrix product, well suited for dense linear algebra libraries optimized for GPU and TPU. The total number of super-and sub-diagonal blocks is upper bounded by twice the number of super-and sub-diagonal blocks at level-0, which is 2N The coarsening in step-1 and interpolation in step-3 all use sparse matrices with fixed sparsity patterns. Hence matrices P (l) and R (l) are never explicitly formed and applying them can be easily done with standard library functions. Take Jax Numpy library as an example, coarsening can be done with sum() along row axis and interpolation can be done with repeat() along row axis. For this reason, step-1 and step-3 only have dense matrix operations as well.</p><p>The formulation of the matrix-matrix product for the general level-M case is Y = AV = Y (0) + P (0) (? (1) + P (1) (? (2) + P (2) (? ? ? + P (M ?2)? (M ?1) ) ? ? ? )). <ref type="formula" target="#formula_1">(73)</ref> This formulation is a direct consequence of the nested attention matrix structure and can be derived similarly as Eq. (69).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Token sequence partitions in three segment granularity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A three-level token hierarchy. Dashed boxes represent segmentation and solid boxes represents tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Algorithm 1 H-Transformer-1D Input: Q(query), K(key), V (value) Output: Z Coarsen Q using Eq. (25) and coarsen K using Eq. (26) Compute diagonal blocks in? (0) and superdiagonal and sub-diagonal blocks in? (l) using Eq. (28) Coarsen V using Eq. (27) Compute Y = AV in Eq. (2) using Eq. (29) Compute D in Eq. (5) using Eq. (29) Compute Z = D ?1 Y The computational cost for Algorithm 1 has two parts: 1. Computing the hierarchical attention matrix: (a) diagonal blocks at level-0: dN 2 r N (0) b (b) Super-and sub-diagonal blocks at levell: 4dN 2 r (N (l) b ? 1) (c) total: 5dLN r = O(dL) 2. Computing matrix-matrix (MM) multiplication in Eq. (2) and matrix-vector (MV) multiplication in Eq. (5): (a) MM: 5dLN r (b) MV: 5LN r (c) total: 5(d + 1)LN r = O(dL)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Likewise</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>run time cost per entry is O(d), where d is the embedding size. So the final total run time cost is O(Ld) and memory foot print is O(L). Here we leave out N r since it is a constant model hyper parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>where l = 1. The fourth equality in Eq.(61)is due to Eq. (40).In view of Eq. (56), we have A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>is due to Eq. (40). In view of Eq.(57), we have A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Substituting Eq. (61), (63) and (66) into (60), we obtain the final result for the matrix-matrix productY = AV ? Y (0) + P (0) ? (1) + P (1)? (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>of each dense matrix-matrix product is O(N 2 r d). So the total run time is 5N (0) b N 2 r d = 5LN r d = O(Ld). Here we leave out N r since it is a constant model hyper-parameter.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Matrix A in Eq.(11) is a symmetric Toeplitz matrix<ref type="bibr" target="#b14">(Golub and Loan, 1996)</ref> and hence only has 16 unique entries. But we ignore this fact and treat A as a general matrix here.3 Each one of four diagonal blocks at level-0 takes 16 entries. Each one of four off-diagonal blocks at level-0 takes 16 entries. Each one of two off-diagonal blocks at level-1 takes 32 entries.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We omit some of implementation details to handle the overlapping entries between adjacent levels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/google/jax 6 https://github.com/google/flax/blob/master/flax/nn 7 https://github.com/google-research/long-range-arena</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Restriction or Coarsening Matrices</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical attentional hybrid neural networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jader</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mac?do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zanchettin</surname></persName>
		</author>
		<idno>abs/1901.06610</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Kenneth</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1809.10853</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3285" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multilevel matrix multiplication and fast solution of integral equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Lubrecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="348" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Multigrid Tutorial. SIAM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Benjamin Pickman Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<idno>abs/1312.3005</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR 119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1904.10509</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masked language modeling for proteins via linearly scalable long-context transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<idno>abs/2006.03555</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Matrix Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The John Hopkins University Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive biases for deep learning of higher-level cognition. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast algorithms for classical physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greengard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page" from="909" to="914" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A fast algorithm for particle simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Greengard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rokhlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="325" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sparse matrix arithmetic based on h-matrices. part I: Introduction to H-matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="89" to="108" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sparse matrix arithmetic based on H-matrices. part II: Application to multidimensional problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hackbusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="21" to="47" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno>abs/1912.12180</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Music transformer. arXiv: Learning</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">IES3: A fast integral equation solver for efficient 3-dimensional extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided-Design</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sharp nearby, fuzzy far away: How neural language models use context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>abs/1805.04623</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno>abs/2001.04451</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1508.04025</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Document-level neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multipole accelerated preconditioned iterative methods for three-dimensional potential integral equations of the first kind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nabors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Korsmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. and Stat. Comp</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>abs/1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A precorrected-FFT method for electrostatic analysis of complicated 3D structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="page" from="1059" to="1072" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1911.02972</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/1906.05909</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno>abs/2003.05997</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meshtensorflow: Deep learning for supercomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A fast hierarchical algorithm for 3-d capacitance extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kakani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Design Automation Conference</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno>abs/2005.00743</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno>abs/2011.04006</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno>abs/2009.06732</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Numerical linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Trottenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><forename type="middle">W</forename><surname>Oosterlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Multigrid. Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. ArXiv, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2006.04768</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Onta??n</surname></persName>
		</author>
		<editor>Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Big bird: Transformers for longer sequences</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wancai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2012.07436</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Algorithms in FastImp: A fast and wideband impedance extraction program for complicated 3D geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fastsies: a fast stochastic integral equation solver for modeling the rough surface effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided-Design</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
