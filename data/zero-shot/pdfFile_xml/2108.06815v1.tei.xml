<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymmetric Bilateral Motion Estimation for Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
							<email>jhpark@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
							<email>chullee@dongguk.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Dongguk University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
							<email>changsukim@korea.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Asymmetric Bilateral Motion Estimation for Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel video frame interpolation algorithm based on asymmetric bilateral motion estimation (ABME), which synthesizes an intermediate frame between two input frames. First, we predict symmetric bilateral motion fields to interpolate an anchor frame. Second, we estimate asymmetric bilateral motions fields from the anchor frame to the input frames. Third, we use the asymmetric fields to warp the input frames backward and reconstruct the intermediate frame. Last, to refine the intermediate frame, we develop a new synthesis network that generates a set of dynamic filters and a residual frame using local and global information. Experimental results show that the proposed algorithm achieves excellent performance on various datasets. The source codes and pretrained models are available at https://github.com/JunHeum/ABME.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation is a low-level vision task to increase the frame rate of a video sequence by interpolating intermediate frames between successive input frames. It is widely used in applications, including video enhancement <ref type="bibr" target="#b41">[42]</ref>, video compression <ref type="bibr" target="#b24">[25]</ref>, slow-motion generation <ref type="bibr" target="#b15">[16]</ref>, and view synthesis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. Due to its practical importance, various algorithms have been proposed to increase video frame rates <ref type="bibr">[2, 3, 6-8, 13, 15, 16, 20, 22-24, 30-34]</ref>.</p><p>These algorithms can be classified into three categories: kernel-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, phase-based <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, and motion-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. With the recent advances in optical flow estimation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>, motion-based algorithms have been developed most actively. They use optical flows to predict an intermediate frame by warping two successive frames forward or backward. For example, Niklaus and Liu <ref type="bibr" target="#b29">[30]</ref> predict bidirectional optical flows between two frames and halve them to generate intermediate frames based on forward warping. However, forward warping may cause interpolation artifacts in holes and overlapped regions <ref type="bibr" target="#b40">[41]</ref>. To overcome the hole issue, they develop a synthesis network that  <ref type="figure">Figure 1</ref>. Symmetric vs. asymmetric bilateral motion models. In (a), the asymmetric model represents bilateral motion vectors from an intermediate frame I0.5 to two input frames I0 and I1 accurately, where the symmetric one fails, by loosening the linear constraint. In (b), when I0.5 is interpolated from I0 and I1, the asymmetric model provides more faithful reconstruction with less artifacts, especially around the head, than the symmetric one does.</p><p>learns to fill in holes. The overlapping issue, however, remains, so they propose softmax-splatting <ref type="bibr" target="#b30">[31]</ref> to combine overlapping pixel information adaptively and render the target pixel more faithfully.</p><p>On the other hand, many algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref> are based on backward warping, which is free from the hole and overlapping issues. Backward warping needs motion vectors from intermediate frames to input frames, but intermediate frames, which should be interpolated, are unavailable at the time of motion estimation. Thus, conventional algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref> approximate those intermediate motion vectors using optical flows between input frames. However, the approximation errors may degrade frame interpolation performance. Park et al. <ref type="bibr" target="#b33">[34]</ref> employ the symmetric bilateral motion estimation to improve the accuracy of the intermediate motion, assuming that motion trajectories between input frames are linear. The linear motion constraint, however, may cause inaccurate motion estimation in regions where the constraint is invalid, such as occluded regions around motion boundaries, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>In this paper, we propose a novel video frame interpola-arXiv:2108.06815v1 [cs.CV] 15 Aug 2021 <ref type="figure">Figure 2</ref>. Illustration of various motion fields: Each column represents a frame, and a dot corresponds to a pixel in the frame. I0 and I1 are input frames, and It is an unavailable intermediate frame at time instance t = 0.5. Orange dots depict background pixels without any movement, while green dots depict moving objects. The motion field V0?1 in (a) is halved to approximate the motion field Vt?1 in (b). The symmetric bilateral motion fields in (c) can be estimated to interpolate It based on backward warping. To improve the video frame interpolation performance, we propose the ABME algorithm, illustrated in (d).</p><formula xml:id="formula_0">0 t 1 (a) Motion field V 0?1 0 t 1 (b) Approximation V t?1 = 0.5V 0?1 0 t 1 (c) Symmetric bilateral motion 0 t 1 (d) Asymmetric bilateral motion</formula><p>tion algorithm based on backward warping, composed of the asymmetric bilateral motion estimation (ABME) and the frame synthesis network. In ABME, we predict symmetric bilateral motion fields and refine them by loosening the linear motion constraint. Specifically, we interpolate a temporary intermediate frame, called an anchor frame, using the symmetric fields. Then, we estimate asymmetric bilateral motion fields from the anchor frame to the two input frames, as illustrated by the red arrows in <ref type="figure">Figure 1</ref>. In the frame synthesis, the input frames are warped using the bilateral motion fields. To aggregate these warped frames, we develop a synthesis network composed of two subnetworks: FilterNet and RefineNet. FilterNet generates dynamic filters to exploit local information, while RefineNet reconstructs a residual frame using global information. Experimental results demonstrate that the proposed ABME algorithm outperforms the state-of-the-art video interpolators <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> meaningfully on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motion-Based Frame Warping</head><p>Let us review motion-based frame warping techniques for video frame interpolation, and introduce the notations and concepts necessary to describe how the proposed ABME is different from the conventional techniques.</p><p>Forward and backward warping: Let V S?T denote a pixel-wise motion field (or optical flow) from a source frame I S to a target frame I T . Then, the target frame can be approximated by forward warping the source frame <ref type="bibr" target="#b9">[10]</ref>,</p><formula xml:id="formula_1">I T = ? F (I S , V S?T )<label>(1)</label></formula><p>where ? F is the forward warping operator. On the contrary, the source frame can be approximated by backward warping the target frame <ref type="bibr" target="#b39">[40]</ref>,</p><formula xml:id="formula_2">I S = ? B (V S?T , I T )<label>(2)</label></formula><p>where ? B is the backward warping operator.</p><p>Given two input frames I 0 and I 1 at adjacent time instances 0 and 1, video frame interpolation aims to synthesize an intermediate frame I t , where 0 &lt; t &lt; 1. This can be achieved, using the forward warping, b?</p><formula xml:id="formula_3">I t,F = (1 ? t) ? ? F (I 0 , V 0?t ) + t ? ? F (I 1 , V 1?t ).<label>(3)</label></formula><p>Here, the required motion fields are often obtained by scaling the motion fields V 0?1 and V 1?0 between the input frames <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, which are given by</p><formula xml:id="formula_4">V 0?t = t ? V 0?1 (4) V 1?t = (1 ? t) ? V 1?0 .<label>(5)</label></formula><p>However, as illustrated in <ref type="figure">Figure 2</ref>(a), the scaled V 0?t does not pass through pixels in I t exactly in general. Moreover, no flow vector may pass near a certain pixel, or multiple vectors may pass near the same pixel, causing hole or occlusion problems, respectively. Softmax splatting <ref type="bibr" target="#b30">[31]</ref> alleviates these problems in the forward warping. On the other hand, a majority of video frame interpolation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>, as well as the proposed algorithm, instead use the backward warping,</p><formula xml:id="formula_5">I t,B = (1 ? t) ? ? B (V t?0 , I 0 ) + t ? ? B (V t?1 , I 1 ). (6)</formula><p>However, unlike (4) and <ref type="formula" target="#formula_4">(5)</ref>, it is not straightforward to obtain the motion fields V t?0 and V t?1 because the intermediate frame I t is unavailable.</p><p>Motion approximation for backward warping: Conventional algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> approximate the motion fields V t?0 and V t?1 in <ref type="bibr" target="#b5">(6)</ref>. For example, the flow projection in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> approximates V t?0 and V t?1 by aggregating multiple flow vectors between I 0 and I 1 , which pass near each pixel in I t . Alternatively, some algorithms simply borrow flow vectors from V 0?1 and V 1?0 to approximate V t?0 and V t?1 <ref type="bibr" target="#b33">[34]</ref>,</p><formula xml:id="formula_6">V t?0 = ?t ? V 0?1 or t ? V 1?0<label>(7)</label></formula><formula xml:id="formula_7">V t?1 = (1 ? t) ? V 0?1 or ? (1 ? t) ? V 1?0 . (8) 0 1 Symmetric Bilateral Motion Estimation ABMR-Net ?0 S ?1 S Anchor Frame Interpolation ?0 A ?1 A ?</formula><p>Frame Synthesis Network ABME <ref type="figure">Figure 3</ref>. An overview of the proposed algorithm. ABMR-Net is detailed in <ref type="figure" target="#fig_3">Figure 5</ref>, and the frame synthesis network in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref>, the motion fields are approximated by combining the candidates in <ref type="bibr" target="#b6">(7)</ref> and <ref type="formula">(8)</ref>, given by</p><formula xml:id="formula_8">V t?0 = ?(1 ? t)t ? V 0?1 + t 2 ? V 1?0 (9) V t?1 = (1 ? t) 2 ? V 0?1 ? t(1 ? t) ? V 1?0 .<label>(10)</label></formula><p>These approximations in (7)?(10) assume that neighboring pixels have similar motion vectors. However, as in <ref type="figure">Fig</ref> Symmetric bilateral motion estimation: Instead of approximating the bilateral motion fields V t?0 and V t?1 using the fields V 0?1 and V 1?0 between the input frames, Park et al. <ref type="bibr" target="#b33">[34]</ref> proposed the symmetric bilateral motion estimation algorithm, assuming that a motion trajectory between I 0 and I 1 is linear. Under the linear assumption, the bilateral motion fields V t?0 and V t?1 are symmetric with respect to I t , as in <ref type="figure">Figure 2</ref>(c). More specifically,</p><formula xml:id="formula_9">V t?0 = ? t 1 ? t V t?1 .<label>(11)</label></formula><p>Therefore, roughly speaking, they obtained V t?1 to minimize the frame difference</p><formula xml:id="formula_10">? B (V t?0 , I 0 ) ? ? B (V t?1 , I 1 ) = ? B (? t 1?t V t?1 , I 0 ) ? ? B (V t?1 , I 1 ) . (12)</formula><p>To this end, they developed the bilateral motion network with the bilateral cost volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>The proposed algorithm is composed of two procedures: ABME and frame synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ABME</head><p>In <ref type="figure">Figure 2</ref>(c), bilateral motion vectors convey valid motion information between symmetrically matched pixel pairs in input frames. However, when a pixel in I t is occluded in either I 0 or I 1 , the symmetry does not hold. Nonlinear object motions due to acceleration also break the symmetry, as in <ref type="figure">Figure 1</ref>(a). To overcome these issues, we develop the ABME technique, which refines symmetric bilateral motion vectors so that they become asymmetric and represent motion more reliably and more accurately. <ref type="figure">Figure 3</ref> presents an overview of the proposed algorithm. We first obtain symmetric bilateral motion fields V S t?0 and V S t?1 by employing the motion estimator of BMBC <ref type="bibr" target="#b33">[34]</ref>. Using V S t?0 and V S t?1 , we interpolate an anchor frame? t , which is then used as a source frame for the asymmetric bilateral motion refinement (ABMR). Finally, we obtain asymmetric bilateral motion fields V A t?0 and V A t?1 . Anchor frame interpolation: The motion estimation from a source frame I t to a target frame I 0 or I 1 is challenging, since I t is unavailable and should be synthesized in the video frame interpolation. Hence, we generate a temporary source frame? t , called an anchor frame, using the symmetric bilateral motion fields V S t?0 and V S t?1 . Based on the backward warping in (6), we may generat?</p><formula xml:id="formula_11">I t = (1 ? t) ? ? B (V S t?0 , I 0 ) + t ? ? B (V S t?1 , I 1 ). (13)</formula><p>This simple blending, however, may cause errors due to occlusion, especially in boundary regions of the anchor frame in the case of camera panning. To reduce such errors, we exploit masks to reveal occluded regions, given by</p><formula xml:id="formula_12">M S t?0 = ? B (V S t?0 , 1) and M S t?1 = ? B (V S t?1 , 1) (14)</formula><p>where 1 is a binary image of all ones. Note that a mask value 0 means that the corresponding pixel in I t moves outside the frame at time instance 0 or 1. We then reconstruct the anchor frame in an occlusion-aware manner,</p><formula xml:id="formula_13">I t = (1 ? t) ? (1 ? M S t?1 + M S t?0 ) ? ? B (V S t?0 , I 0 ) + t ? (1 ? M S t?0 + M S t?1 ) ? ? B (V S t?1 , I 1 ) (15)</formula><p>where ? is the Hadamard product.</p><p>Asymmetric bilateral motion refinement: To perform the backward warping in (6), conventional algorithms approximate bilateral motion fields V t?0 and V t?1 using the motion fields between I 0 and I 1 . In contrast, we estimate the motion field from I t to I 0 or I 1 directly, after approximating I t with the anchor frame? t . Let us describe the asymmetric motion estimation from I t to I 1 . Note that the estimation from? t to I 0 is performed  similarly but independently. We develop ABMR-Net for asymmetric bilateral motion refinement in <ref type="figure" target="#fig_3">Figure 5</ref> to refine the motion field from the source? t to the target I 1 . ABMR-Net hierarchically obtains the motion field, as done in PWC-Net <ref type="bibr" target="#b38">[39]</ref>. At level l, the motion field V l?1 t?1 at the previous level (l ? 1) is up-sampled to warp the target feature map F l 1 . Also, we multiply the anchor feature map F l t with a reliability mask Z l?1 t and compensate for masked-out features by adding an offset map O l?1 t . Then, the warped target feature map and the compensated anchor feature map are input to the correlation layer to compute matching costs. The cost volume is used to generate the residual field ?V l t?1 , which is added to the up-sampled V l?1 t?1 to yield the motion field V l t?1 . As mentioned previously, the symmetric field V S t?1 is estimated using the motion estimator of BMBC <ref type="bibr" target="#b33">[34]</ref>, which is at the quarter resolution. It is used as the up-sampled V 0 t?1 at level l = 1 in <ref type="figure" target="#fig_3">Figure 5</ref>. Then, the refinement is performed for two levels, and the half resolution V 2 t?1 becomes the final result V A t?1 . Since V A t?0 and V A t?1 are refined separately, they become asymmetric. As shown in <ref type="figure">Figure 1</ref>(a) and <ref type="figure">Figure 2(d)</ref>, the asymmetric fields may represent motion information more faithfully than the symmetric ones.</p><formula xml:id="formula_14">? ? ? 2 ? 3 0 1 0 3 Feature Extractor 0 2 0 1 1 3 1 2 1 1 Warping Layer B Warping Layer B FilterNet ? ? 1 ? 3 ? 2 ? 1 ? RefineNet ? ? ? ?0 S ?1 S ?0 A ?1 A Feature Extractor ? 2 ? 3 ? 1 ? 3 ? 2 ? 1 DLC DLC : Output : Input</formula><p>Recently, Zhao et al. <ref type="bibr" target="#b42">[43]</ref> improved the matching performance of PWC-Net with a learnable occlusion mask.</p><p>Similarly, we employ a mask in ABMR-Net to improve the refinement performance. The source frame? t is an approximation of I t , so it may contain errors around motion boundaries or on complicated texture. Such errors make the matching process unreliable. Hence, we adopt the reliability mask Z l?1 t in <ref type="figure" target="#fig_3">Figure 5</ref> to suppress errors in anchor features. Notice that a source frame is masked in this work, while a target frame is masked in <ref type="bibr" target="#b42">[43]</ref>. At level 1, O 0 t is initialized to zero, and the up-sampled Z 0 t is set to</p><formula xml:id="formula_15">? Z 0 t = exp ? ? ? ? B (V S t?0 , I 0 ) ? ? B (V S t?1 , I 1 )<label>(16)</label></formula><p>where ? = 20. If a certain pixel in? t has a large symmetric matching error in |? B (V S t?0 , I 0 )?? B (V S t?1 , I 1 )|, its feature is suppressed by the reliability mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Frame Synthesis</head><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we synthesize an intermediate frame I t using two input frames I 0 and I 1 . We use four motion fields generated by the proposed ABME in <ref type="figure">Figure 3</ref>: two symmetric fields V S t?0 and V S t?1 and two asymmetric fields V A t?0 and V A t?1 . Using a feature extractor for frame synthesis, we extract multi-scale feature maps C l 0 from I 0 , where there are three levels l ? {1, 2, 3}. Note that the extraction from I 1 to C l 1 is performed similarly with shared parameters. The highest level maps C 3 0 and C 3 1 have the same spatial resolution as the input frames. We backward warp the input images and their feature pyramids. In <ref type="figure" target="#fig_2">Figure 4</ref>, I 0 is warped by V S  Given multiple candidates, we can synthesize I t by blending them with weights. This simple blending, however, may cause blurry artifacts and reconstruction errors in occluded regions. Recent algorithms hence employ synthesis networks, which process warped frames to generate the intermediate frame in various ways: direct frame generation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, residual frame generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, or dynamic local blending <ref type="bibr" target="#b33">[34]</ref>. While the dynamic local blending synthesizes each pixel using local neighbors, the other two approaches use global contexts. To exploit both local and global information, we propose a novel synthesis network composed of two subnetworks: FilterNet and RefineNet.</p><p>FilterNet: It learns to generate dynamic filters for combining the four candidates, denoted by? c t , 1 ? c ? 4. We employ the modified version <ref type="bibr" target="#b29">[30]</ref> of GridNet <ref type="bibr" target="#b11">[12]</ref> as the backbone of FilterNet, which is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Filter-Net takes the four candidates with the corresponding feature pyramids as input, by employing the lateral blocks. For each pixel (x, y), the rightmost lateral block generates filter coefficients dynamically to fuse the 3 ? 3 local neighboring pixels in each candidate. The coefficients are denoted by</p><formula xml:id="formula_16">H x,y (i, j, c), ?1 ? i, j ? 1, 1 ? c ? 4<label>(17)</label></formula><p>where (i, j) are local coordinates around (x, y), and c is the candidate index. The filter coefficients are normalized, c i j H x,y (i, j, c) = 1. Then, we obtain the filtered frame via the dynamic local convolution (DLC),</p><formula xml:id="formula_17">I t (x, y) = 4 c=1 1 i=?1 1 j=?1 H x,y (i, j, c)? c t (x + i, y + j).<label>(18)</label></formula><p>Furthermore, by applying the same dynamic filters to the four warped feature candidates C l t at the highest level l = 3, we obtain the filtered feature map C t .</p><p>RefineNet: The dynamic filters consider local neighbors only. Thus, if the local neighbors do not contain proper information on a certain pixel due to motion errors or severe occlusion, its filtered result becomes also erroneous. To overcome this limitation using global information, Re-fineNet generates a residual frame ?I t to refine the filtered frame? t . It has the same network architecture as FilterNet, but it takes the filtered frame? t , the filtered feature map C t , and the warped feature pyramids as input. In particular, the feature map C t , which is obtained using the same dynamic filters for? t , meaningfully increases the refinement performance, as will be discussed in Section 4. After generating the residual ?I t , the final reconstructed frame is given by</p><formula xml:id="formula_18">I t =? t + ?I t .<label>(19)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>We first train the symmetric bilateral motion estimator and then, after fixing it, train ABMR-Net. Finally, we endto-end train the frame synthesis network with those two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABME:</head><p>We adopt the motion estimator of BMBC <ref type="bibr" target="#b33">[34]</ref> for the symmetric bilateral motion estimation in <ref type="figure">Figure 3</ref>, but we retrain it by setting the stride of the first convolution layer to 2 for efficiency. To train the motion estimators of both BMBC and ABMR-Net, we define the photometric loss between a ground-truth I GT t and two warped frames as</p><formula xml:id="formula_19">L pho = ?(I GT t ? ? B (V A t?0 , I 0 )) + ?(I GT t ? ? B (V A t?1 , I 1 ) + L cen (I GT t , ? B (V A t?0 , I 0 )) + L cen (I GT t , ? B (V A t?1 , I 1 ))</formula><p>where ?(x) = (x 2 + 2 ) ? is the Charbonnier function <ref type="bibr" target="#b4">[5]</ref> and L cen is the census loss <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> defined as the soft Hamming distance between census-transformed image patches <ref type="bibr" target="#b25">[26]</ref> of size 7?7. The parameters are set to ? = 0.5 and = 10 ?6 .</p><p>To train the motion estimator of BMBC, we use the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate of ? = 10 ?4 until 0.1M iterations and halve ? after every 0.04M iterations. We use a batch size of 24 for 0.2M iterations. For ABMR-Net, we also use the Adam optimizer with ? = 10 ?4 until 0.12M iterations and halve ? after every 0.06M iterations. We use a batch size of 16 for 0.3M iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame synthesis network:</head><p>We define the synthesis loss L syn as the sum of the Charbonnier loss and the census loss between I GT t and its synthesized version I t , given by</p><formula xml:id="formula_20">L syn = ?(I GT t ? I t ) + L cen (I GT t , I t ).<label>(20)</label></formula><p>We use the Adam optimizer with ? = 10 ?4 until 0.35M iterations and halve ? after every 0.15M iterations. We use a batch size of 6 for 0.8M iterations in total.</p><p>Training dataset: We use only the Vimeo90K training set <ref type="bibr" target="#b41">[42]</ref> to train the proposed networks. It is composed of 51,312 triplets with a resolution of 448?256. This training set does not contain motion ground-truth. We augment the dataset by randomly flipping, rotating, reversing the sequence order, and cropping 256?256 patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>While we use strictly a single training dataset, we test the proposed ABME algorithm on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101 [38]:</head><p>We use the test set constructed by Liu et al. <ref type="bibr" target="#b22">[23]</ref>, which contains 379 triplets of resolution 256?256.</p><p>Vimeo90K <ref type="bibr" target="#b41">[42]</ref>: The test set in Vimeo90K contains 3,782 triplets of spatial resolution 448 ? 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNU-FILM [8]:</head><p>It contains 1,240 triplets of videos of resolutions up to 1280 ? 720. It has four different settings -Easy, Medium, Hard, and Extreme.</p><p>Xiph <ref type="bibr" target="#b28">[29]</ref>: It contains 30 raw video sequences for testing video codecs that have HD (1280 ? 720) or FHD (1920 ? 1080) resolutions. For triplets from the FHD sequences, we crop the central HD part of each frame without resizing. Thus, we extract 2,000 triplets of the HD resolution in total.</p><p>We divide these triplets into 5 classes (D1 ? D5) according to the difficulty levels of interpolation. To quantify the difficulty, we compress each triplet using the recent video coding standard VVC <ref type="bibr" target="#b3">[4]</ref> with a fixed QP. The middle frame is encoded in the B-frame mode, while the other two frames in the I-frame mode. The B-frame is motion-compensated from the others, and the motion vectors and compensation errors are encoded. The number of bits for the B-frame, hence, represents how difficult it is to interpolate it using the adjacent frames. The 2,000 triplets are sorted according to these numbers of bits and then grouped into the five classes so that each class contains 400 triplets. D1 is the easiest class, while D5 is the most difficult one.</p><p>X4K1000FPS <ref type="bibr" target="#b36">[37]</ref>: Concurrently with this paper in ICCV 2021, Sim et al. present a high-quality, extensive dataset of 4K resolution. They provide the training set X-TRAIN and the test set X-TEST. We evaluate the proposed algorithm directly on X-TEST without retraining it on X-TRAIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-Arts</head><p>We compare the proposed algorithm with conventional algorithms: ToFlow <ref type="bibr" target="#b41">[42]</ref>, SepConv <ref type="bibr" target="#b32">[33]</ref>, CyclicGen <ref type="bibr" target="#b21">[22]</ref>, DAIN <ref type="bibr" target="#b1">[2]</ref>, CAIN <ref type="bibr" target="#b7">[8]</ref>, AdaCoF <ref type="bibr" target="#b19">[20]</ref>, and BMBC <ref type="bibr" target="#b33">[34]</ref>. In videos with faster motions, more regions are occluded around motion boundaries, making it more difficult to interpolate frames. <ref type="table" target="#tab_0">Table 1</ref> confirms that the proposed ABME handles these occluded regions effectively and provides excellent interpolation results. <ref type="table" target="#tab_0">Table 1</ref> also lists the actual runtime for interpolating an intermediate frame in the "Urban" sequence in the Middlebury benchmark <ref type="bibr" target="#b0">[1]</ref> using an RTX 2080 Ti GPU. The proposed algorithm is more than three times faster than BMBC. <ref type="table" target="#tab_1">Table 2</ref> compares the average PSNRs on the Xiph dataset, which contains diverse sequences with challenging factors, such as complicated texture, fast motion, and severe occlusion. ABME outperforms all conventional algorithms in all difficulty classes. DAIN achieves the second-highest PSNRs in classes D2, D3, and D4. However, its motion and depth estimators were pre-trained using additional datasets, whereas ABME was trained strictly using Vimeo90K only. Nonetheless, in D2 and D3, ABME outperforms DAIN by    <ref type="bibr" target="#b19">[20]</ref> 23.90 0.7271 XVFI <ref type="bibr" target="#b36">[37]</ref> 30.12 0.8704 ABME (Proposed) 30.16 0.8793 0.94 dB and 0.84 dB, respectively. These results indicate that ABME interpolates challenging videos faithfully. <ref type="table" target="#tab_2">Table 3</ref> compares the performances on the recent X4K1000FPS dataset. Its training set, X-TRAIN, is not used to retrain ABME. In contrast, X-TRAIN is used to train XVFI <ref type="bibr" target="#b36">[37]</ref> that is designed for 4K sequences with extreme motions. Nevertheless, ABME performs slightly better than XVFI. It is a future research issue to tailor ABME for 4K sequences. For this purpose, the Vimeo90K training set might not be adequate, and X-TRAIN would be useful. <ref type="figure" target="#fig_6">Figure 7</ref> shows interpolation results of two frames in FILM (Extreme). Because of large movements and extreme deformation, the conventional algorithms fail to reconstruct details within the green and red squares reliably. In contrast, the proposed algorithm reconstructs them faithfully without any noticeable artifacts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Analysis</head><p>Let us analyze the contributions of key components in the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion fields:</head><p>We compare the motion fields in <ref type="figure">Figure 2</ref> quantitatively. Specifically, we test the approximate motion field in <ref type="bibr" target="#b6">(7)</ref>, called Approx1, and the approximate motion field in (9), called Approx2, and the symmetric bilateral motion field (SBMF), and the proposed asymmetric bilateral motion field (ABMF). PWC-Net <ref type="bibr" target="#b38">[39]</ref> is used as the optical flow estimator for Approx1 and Approx2, while BMBC <ref type="bibr" target="#b33">[34]</ref> for SBMF. These four types of motion fields are used, respectively, to warp input frames backward to approximate the intermediate frame, and then the PSNRs of the warped frames are computed. <ref type="figure" target="#fig_7">Figure 8</ref> compares the average PSNRs on Vimeo90K and SNU-FILM. Approx1 produces the worst performance. Approx2, which improves the approximation via (9), slightly increases the PSNRs. SBMF in <ref type="bibr" target="#b33">[34]</ref> yields considerably higher PSNRs than Approx1 or Approx2. However, the proposed ABMF outperforms SBMF significantly by more than 2 dB on Vimeo90K, Easy, and Medium and at least 1 dB on Hard and Extreme. <ref type="figure" target="#fig_8">Figure 9</ref> compares warped frames with error maps. The car moves quickly, causing severe occlusion. Hence, Ap- prox1 and Approx2 distort the warped frames severely. SBMF estimates the motion of the car accurately but causes errors around the boundary of the car due to disocclusion. In contrast, the proposed ABMF reduces these errors effectively by employing asymmetric motion vectors.</p><p>Reliability mask and offset map: We analyze the effectiveness of ABMR-Net in <ref type="figure" target="#fig_3">Figure 5</ref>. It uses a learnable reliability mask Z t and a learnable offset map O t . <ref type="table">Table 4</ref> shows that both components contribute to the overall performance of the asymmetric bilateral motion refinement.</p><p>Candidate warped frames: In the proposed frame synthesis in <ref type="figure" target="#fig_2">Figure 4</ref>, four candidate warped frames are used in the default mode. Specifically, two candidates are obtained using the symmetric fields V S t?0 and V S t?1 , and the other two from the asymmetric fields V A t?0 and V A t?1 . We test the following three combinations.</p><p>? Symmetric only: Two candidates using the symmetric fields are used. ? Asymmetric only: Two candidates using the asymmetric fields are used. ? Both: All four candidates are used. <ref type="table">Table 5</ref> compares the average PSNRs of these three settings. First, 'Symmetric only' yields the worst performance, while it exceeds the performance of BMBC <ref type="bibr" target="#b33">[34]</ref>. This means that the proposed synthesis network is more effective than that of BMBC. Second, 'Asymmetric only' performs better than 'Symmetric only' by loosening the linear motion constraint. Last, the best PSNRs are achieved by employing all four candidates, which complement each other to improve the interpolation performance.</p><p>Dynamic filters: <ref type="figure" target="#fig_9">Figure 10</ref> shows the four candidate warped frames, their filter coefficient maps, and their error maps. To visualize dynamic filters, we compute the absolute sum of coefficients for each pixel and render the sum in a gray level. The top two candidates are obtained by the symmetric fields, while the others by the asymmetric ones. The former are more reliable in the background without motion, while the latter are more effective around motion boundaries. Hence, the filter coefficients are determined accordingly. Moreover, to the left and right sides of the man, the third and fourth candidates are mainly used for the dynamic filtering, respectively. This is because different sides are occluded in different frames I 0 and I 1 . In such occluded regions, the linear motion constraint is invalid and the symmetric fields are not reliable.</p><p>Filtered feature maps: In <ref type="figure" target="#fig_2">Figure 4</ref>, RefineNet takes the filtered feature map C t , as well as the filtered frame? t , as input. If C t is removed and only? t is used to synthesize I t , the average PSNR is reduced by 0.1 dB on Vimeo90K.</p><p>Since C t conveys contextual information in? t , it helps Re-fineNet to remove noise in? t and restore I t more faithfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a novel, effective video frame interpolation algorithm. First, we developed the ABME technique to refine symmetric bilateral motions by loosening the linear motion constraint. Second, we designed the novel synthesis network that generates a set of dynamic filters and a residual frame using local and global information. Extensive experiments demonstrated that the proposed algorithm achieves state-of-the-art performance on various datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ure 2(b), the assumption is invalid around motion boundaries. In such cases, the qualities of the approximate motion fields are degraded, resulting in poorly interpolated frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Architecture of the proposed frame synthesis network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Architecture of ABMR-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t?0 and V A t?0 to obtain the estimate? t of I t , while I 1 by V S t?1 and V A t?1 . Thus, there are four candidate warped frame? I t in total. Similarly, at each level l, there are four candidate warped features C l t . These candidates are combined in a complementary manner to reconstruct the intermediate frame I t more faithfully.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Modified GridNet for FilterNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison of interpolated frames. Triplets in SNU-FILM (Extreme) are used in this test. The proposed ABME algorithm in (i) reconstructs rapid objects faithfully to the ground truth in (a) without noticeable artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of different motion fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of warped frames and their error maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of dynamic filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison (PSNR/SSIM) of video interpolation results on the UCF101, Vimeo90K, and SNU-FILM datasets. In each test, the best result is boldfaced, while the second best is underlined. All results are obtained by executing available source codes.</figDesc><table><row><cell></cell><cell>UCF101</cell><cell>Vimeo90K</cell><cell>Easy</cell><cell cols="2">SNU-FILM Medium</cell><cell>Hard</cell><cell>Extreme</cell><cell>Runtime (seconds)</cell><cell>#Parameters (millions)</cell></row><row><cell>ToFlow [42]</cell><cell>34.58/0.9667</cell><cell>33.73/0.9682</cell><cell>39.08/0.9890</cell><cell>34.39/0.9740</cell><cell cols="2">28.44/0.9180</cell><cell>23.39/0.8310</cell><cell>0.43</cell><cell>1.1</cell></row><row><cell>SepConv [33]</cell><cell>34.78/0.9669</cell><cell>33.79/0.9702</cell><cell>39.41/0.9900</cell><cell>34.97/0.9762</cell><cell cols="2">29.36/0.9253</cell><cell>24.31/0.8448</cell><cell>0.20</cell><cell>21.6</cell></row><row><cell>CyclicGen [22]</cell><cell>35.11/0.9684</cell><cell>32.09/0.9490</cell><cell>37.72/0.9840</cell><cell>32.47/0.9554</cell><cell cols="2">26.95/0.8871</cell><cell>22.70/0.8083</cell><cell>0.09</cell><cell>19.8</cell></row><row><cell>DAIN [2]</cell><cell>34.99/0.9683</cell><cell>34.71/0.9756</cell><cell>39.73/0.9902</cell><cell>35.46/0.9780</cell><cell cols="2">30.17/0.9335</cell><cell>25.09/0.8584</cell><cell>0.13</cell><cell>24.0</cell></row><row><cell>CAIN [8]</cell><cell>34.91/0.9690</cell><cell>34.65/0.9730</cell><cell>39.89/0.9900</cell><cell>35.61/0.9776</cell><cell cols="2">29.90/0.9292</cell><cell>24.78/0.8507</cell><cell>0.04</cell><cell>42.8</cell></row><row><cell>AdaCoF [20]</cell><cell>34.90/0.9680</cell><cell>34.47/0.9730</cell><cell>39.80/0.9900</cell><cell>35.05/0.9754</cell><cell cols="2">29.46/0.9244</cell><cell>24.31/0.8439</cell><cell>0.03</cell><cell>22.9</cell></row><row><cell>BMBC [34]</cell><cell>35.15/0.9689</cell><cell>35.01/0.9764</cell><cell>39.90/0.9902</cell><cell>35.31/0.9774</cell><cell cols="2">29.33/0.9270</cell><cell>23.92/0.8432</cell><cell>0.77</cell><cell>11.0</cell></row><row><cell>ABME (Proposed)</cell><cell>35.38/0.9698</cell><cell>36.18/0.9805</cell><cell>39.59/0.9901</cell><cell>35.77/0.9789</cell><cell cols="2">30.58/0.9364</cell><cell>25.42/0.8639</cell><cell>0.22</cell><cell>18.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>PSNRs on the Xiph dataset according to the difficulty levels. D1 is the easiest class, while D5 is the most difficult one.</figDesc><table><row><cell></cell><cell>D1</cell><cell>D2</cell><cell>D3</cell><cell>D4</cell><cell>D5</cell></row><row><cell>DAIN [2]</cell><cell>34.65</cell><cell>33.21</cell><cell>29.42</cell><cell>25.41</cell><cell>22.61</cell></row><row><cell>CAIN [8]</cell><cell>34.67</cell><cell>32.68</cell><cell>27.97</cell><cell>24.98</cell><cell>22.66</cell></row><row><cell>AdaCoF [20]</cell><cell>34.23</cell><cell>32.16</cell><cell>27.53</cell><cell>24.84</cell><cell>22.28</cell></row><row><cell>BMBC [34]</cell><cell>34.47</cell><cell>32.13</cell><cell>27.21</cell><cell>24.52</cell><cell>22.39</cell></row><row><cell>ABME (Proposed)</cell><cell>35.21</cell><cell>34.15</cell><cell>30.26</cell><cell>25.77</cell><cell>23.02</cell></row><row><cell cols="6">Table 1 compares the average PSNR/SSIM scores. All</cell></row><row><cell cols="6">results are obtained by executing available source codes.</cell></row><row><cell cols="6">? UCF101, Vimeo90K, and FILM (Medium) are easier</cell></row><row><cell cols="6">to interpolate than FILM (Hard, Extreme). Also, on</cell></row><row><cell cols="6">the easiest FILM (Easy), most algorithms interpolate</cell></row><row><cell cols="6">high quality frames, and visual differences between the</cell></row><row><cell cols="2">results are negligible.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">? On all datasets except FILM (Easy), the proposed</cell></row><row><cell cols="6">ABME algorithm provides the best results. Especially,</cell></row><row><cell cols="6">on Vimeo90K, in comparison with the second best</cell></row><row><cell cols="6">BMBC, ABME yields about 1 dB higher PSNR.</cell></row><row><cell cols="6">? On FILM (Medium, Hard, Extreme), DAIN achieves</cell></row><row><cell cols="6">the second best results in general; on FILM (Hard),</cell></row><row><cell cols="6">ABME yields 30.58 dB, while DAIN 30.17 dB.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison on the X4K1000FPS dataset.</figDesc><table><row><cell></cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>DAIN [2]</cell><cell>26.78</cell><cell>0.8065</cell></row><row><cell>AdaCoF</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Impacts of the reliability mask Zt and the offset map Ot on the asymmetric bilateral motion refinement. Average PSNRs of warped frames are reported. Impacts of candidate warped frames on the frame synthesis. Average PSNRs of interpolated frames are reported.</figDesc><table><row><cell>Component</cell><cell></cell><cell>Vimeo90K</cell><cell>Extreme</cell></row><row><cell>Zt</cell><cell>Ot</cell><cell>PSNR</cell><cell>PSNR</cell></row><row><cell></cell><cell></cell><cell>27.81</cell><cell>22.75</cell></row><row><cell></cell><cell></cell><cell>28.32</cell><cell>22.96</cell></row><row><cell></cell><cell></cell><cell>28.40</cell><cell>22.98</cell></row><row><cell></cell><cell></cell><cell>28.80</cell><cell>23.07</cell></row><row><cell>Candidates</cell><cell></cell><cell>Vimeo90K</cell><cell>Extreme</cell></row><row><cell></cell><cell></cell><cell>PSNR</cell><cell>PSNR</cell></row><row><cell cols="2">Symmetric only</cell><cell>35.91</cell><cell>25.28</cell></row><row><cell cols="2">Asymmetric only</cell><cell>36.05</cell><cell>25.34</cell></row><row><cell>Both</cell><cell></cell><cell>36.18</cell><cell>25.42</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Research Foundation of Korea (NRF) grants funded by the Korea government (MSIT) (Nos. NRF-2018R1A2B3003896, NRF-2019R1A2C4069806, and NRF-2021R1A4A1031864).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MEMC-Net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="933" to="948" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Developments in international video coding standardization after AVC, with an overview of Versatile Video Coding (VVC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianle</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens-Rainer</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Kui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="1994-11" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video frame interpolation via deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10607" to="10614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motion-compensated frame interpolation using bilateral motion estimation and adaptive overlapped block motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Doo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Woo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuit Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="416" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10663" to="10671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A nonaliasing, real-time spatial transform technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">M</forename><surname>Fant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="1986-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepStereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Residual convdeconv grid network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?lisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Trem?au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="181" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FeatureFlow: Robust video interpolation via structure-totexture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="14004" to="14013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motioncompensated frame interpolation based on multihypothesis motion estimation and texture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Seong-Gyun Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4497" to="4509" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Super SloMo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="557" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning-based view synthesis for light field cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Nima Khademi Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AdaCoF: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="6489" to="6498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8794" to="8802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Novel integration of frame rate up conversion and HEVC coding based on rate-distortion optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="678" to="691" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Alexander Sorkine-Hornung, Markus Gross, and Christopher Schroers. PhaseNet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Xiph.org Video Test Media (derf&apos;s collection), the Xiph Open Source Community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Montgomery</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf" />
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5437" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BMBC: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="109" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised video interpolation using cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">XVFI: Extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16206v2</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Digital Image Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">One-dimensional resampling with inverse and forward mapping functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Sueyllam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Graph. Tools</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MaskFlowNet: Asymmetric feature matching with learnable occlusion mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised deep epipolar flow for stationary or dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="12095" to="12104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
