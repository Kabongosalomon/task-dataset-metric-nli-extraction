<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weisfeiler and Lehman Go Cellular: CW Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
							<email>ffrasca@twitter.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
							<email>montufar@math.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
							<email>mbronstein@twitter.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Imperial College London &amp; Twitter</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">MPI-MIS, SJTU &amp; UNSW</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">MPI-MIS &amp; UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Imperial College London &amp; Twitter</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weisfeiler and Lehman Go Cellular: CW Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph "lifting" transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higherorder signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.</p><p>Definition 2. The k-skeleton of a cell complex X, denoted X (k) , is the subcomplex of X consisting of cells of dimension at most k.</p><p>This definition is useful for referring for certain parts of the complex. For instance, X (0) contains the vertices in the complex, while X (1) contains the vertices and the edges (i.e. the underlying graph).</p><p>The combinatorial structure of the complex can be more compactly described by an incidence relation we call the boundary relation, whose reflexive and transitive closure gives the partial order defined above. The boundary relation describes what cells are on the boundary of other cells. For instance, the edges of the sphere in <ref type="figure">Figure 3</ref> are on the boundary of the 2-cells forming the two hemispheres.</p><p>Definition 3. We have the boundary relation ? ? ? iff ? &lt; ? and there is no cell ? such that ? &lt; ? &lt; ? .</p><p>We can use this to define the four types of (local) adjacencies present in cell complexes. These adjacencies will be the fundamental building block of our message passing procedure. To explain these in more familiar terms, for each adjacency, we exemplify how it shows up in graphs.</p><p>Definition 4 (Cell complex adjacencies). For a cell complex X and a cell ? ? P X , we define: 1. The boundary adjacent cells B(?) = {? | ? ? ?}. These are the lower-dimensional cells on the boundary of ?. For instance, the boundary cells of an edge are its vertices. 2. The co-boundary adjacent cell C(?) = {? | ? ? ? }. These are the higher-dimensional cells with</p><p>? on their boundary. For instance, the co-boundary cells of a vertex are the edges it is part of. 3. The lower adjacent cells N ? (?) = {? | ?? such that ? ? ? and ? ? ? }. These are the cells of the same dimension as ? that share a lower dimensional cell on their boundary. The line graph adjacencies between the edges are a classic example of this. 4. The upper adjacent cells N ? (?) = {? | ?? such that ? ? ? and ? ? ?}. These are the cells of the same dimension as ? that are on the boundary of the same higher-dimensional cell as ?. The typical graph adjacencies between vertices are the canonical example here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The operations performed by message passing Graph Neural Networks (GNNs) emulate the structure of the input graph. While this property has clear computational advantages, it brings with it a series of fundamental limitations. As observed by Xu et al. <ref type="bibr" target="#b73">[74]</ref> and Morris et al. <ref type="bibr" target="#b54">[55]</ref> the local neighbourhood aggregations used by GNNs are at most as powerful as the Weisfeiler-Lehman (WL) test <ref type="bibr" target="#b70">[71]</ref> in distinguishing non-isomorphic graphs. Therefore, GNNs fail to detect certain higer-order meso-scale structures such as cliques or (induced) cycles <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, which are particularly important in applications dealing with social and biological networks or molecular graphs. At the same time, many such layers have to be stacked to make long-range interactions in the graph possible. Besides the computational burden this incurs, deep GNNs typically come with additional problems such as over-smoothing <ref type="bibr" target="#b50">[51]</ref> and over-squashing <ref type="bibr" target="#b0">[1]</ref> of the node representations.</p><p>To address these problems, we propose a novel message passing procedure based on (regular) cell complexes, also known as CW complexes <ref type="bibr" target="#b1">2</ref> , topological objects that form the building block of algebraic topology <ref type="bibr" target="#b37">[38]</ref>. When paired with a theoretically-justified "lifting" transformation augmenting the graph with higher-dimensional constructs called "cells", our method results in a multi-dimensional and hierarchical message passing procedure over the input graph. Our approach generalises and subsumes the recently proposed Message Passing Simplicial Networks (MPSNs) <ref type="bibr" target="#b7">[8]</ref>, which operate on simplicial complexes (SCs), topological generalisations of graphs. However, SCs have a rigid combinatorial structure that significantly limits the range of lifting transformations one could use to meaningfully modulate the message passing procedure. In contrast, we show that cell complexes, which in turn generalise simplicial complexes and come with additional flexibility, allow one to construct new and better ways of decoupling the input and computational graphs.</p><p>Main Contributions To summarise, we propose a message passing scheme operating on regular cell complexes. We call this family of models CW Networks (CWNs) and study their expressive power using a cellular version of the WL test. We show that for an entire class of "lifting" transformations CWNs are at least as powerful as the WL test. Furthermore, we prove that for some of the maps in this class, CWNs can be strictly more powerful than WL, Simplicial WL (SWL) and also not less powerful than 3-WL. We also express the fundamental symmetries of these models and show how they can be seen as generalised convolutional operators on cell complexes. Experimentally, we focus our attention on a particular "lifting" map based on induced cycles. When applied to molecular graphs, it leads to an intuitive hierarchical message passing procedure involving the atoms, the bonds between them and the chemical rings of the molecules. We demonstrate that this provably powerful approach obtains state-of-the-art results on popular large-scale molecular graph datasets and other related tasks. To the best of our knowledge, this is the first work proposing a cell complex representation for molecules. Our code is available at https://github.com/twitter-research/cwn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Definition 1 (Hansen and Ghrist <ref type="bibr" target="#b35">[36]</ref>). A regular cell complex <ref type="figure">(Figure 1</ref>) is a topological space X together with a partition {X ? } ??P X of subspaces X ? of X called cells, and such that 1. For each x ? X there exists an open neighbordhood of x that intersects finitely many cells. 2. For all ?, ? we have that X ? ? X ? = ? iff X ? ? X ? , where X ? is the closure of a cell. <ref type="bibr" target="#b2">3</ref>. Every cell is homeomorphic to R n for some n. 4. (Regularity) For every ? ? P X there is a homeomorphism ? of a closed ball in R n? to X ? such that the restriction of ? to the interior of the ball is a homeomorphism onto X ? . <ref type="figure">Figure 1</ref>: A cell complex X and the corresponding homeomorphisms to the closed balls for three cells of different dimensions in the complex.</p><p>We note that by condition (2) the indexing set P X has a poset structure ? ? ? ? X ? ? X ? , while condition (4) guarantees that this poset structure encodes all the topological information about X. Thus, we can identify a regular cell complex X with this poset, called face poset of X. We also use ? &lt; ? for the strict version of this partial order.</p><p>Intuitively, one constructs a cell complex through a hierarchical gluing procedure. One starts with a set of vertices (0-cells). Then edges (1-cells) are attached to these by gluing the endpoints of closed line segments to them. We have now only described a (multi) graph. However, one can generalise this even further by taking a two-dimensional closed disk and glue its boundary (i.e. a circle) to any simple cycle in the (multi) graph previously built as in <ref type="figure" target="#fig_0">Figure 2</ref>. While we are generally not concerned with dimensions above two, this can be further generalised by gluing the boundary of n-dimensional balls to certain (n ? 1)?cells in the complex.</p><p>Consider the examples in <ref type="figure" target="#fig_1">Figure 3</ref>. The shown sphere is a cell complex obtained from two 0-cells (i.e. vertices), to which two 1-cells (i.e. edges), which form the equator, were attached. The boundary  of two 2-dimensional disks (i.e. the two hemispheres) were glued to the equator to form a sphere. The second example is a tetrahedron with empty interior. It is a particular type of cell complex called a simplicial complex (SC). The only 2-cells it allows are triangle-shaped. More generally, the n-dimensional cells of SCs are n-simplices, which makes them slightly more rigid structures. <ref type="figure">Figure 4</ref>: The CWL colouring procedure for the yellow edge of the cell complex. All cells have been assigned unique colours to aid the visualisation of the adjacencies. Note that the yellow edge aggregates long-range information from the light green edge.</p><p>Note that unlike in graphs and simplicial complexes, the sets B(?, ? ) and C(?, ? ) can have more than one element. For instance, two (closed) 2-cells might intersect in more than one edge (e.g. the two hemispheres in <ref type="figure" target="#fig_1">Figure 3</ref>), and conversely, two edges might be on the boundary of the same two 2-cells. This illustrates the more flexible combinatorial structure of cell complexes.</p><p>Cellular WL (CWL) We consider CWL, a colour refinement scheme for cell complexes that generalises the Simplicial WL <ref type="bibr" target="#b7">[8]</ref> and WL <ref type="bibr" target="#b70">[71]</ref> tests. We use c t ? to refer to the colour assigned by CWL to cell ? at iteration t of the algorithm. When the input is a simplicial complex, this recovers the SWL algorithm. A step of the algorithm is graphically depicted in <ref type="figure">Figure 4</ref> for a single cell. <ref type="bibr" target="#b0">1</ref>. Given a regular cell complex X, all the cells ? are initialised with the same colour. 2. Given the colour c t ? of cell ? at iteration t, we compute the colour of cell ? at the next iteration c t+1 ? by injectively mapping the multi-sets of colours belonging to the adjacent cells of ? using a perfect HASH function:</p><formula xml:id="formula_0">c t+1 ? = HASH c t ? , c t B (?), c t C (?), c t ? (?), c t ? (?) . 3.</formula><p>The algorithm stops when a stable colouring is reached. Two cell complexes are considered non-isomorphic if their colour histograms are different. Otherwise, the test is inconclusive.</p><p>First, we state the following theorem from Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> involving SWL and simplicial complexes. This theorem shows that on simplicial complexes, certain adjacencies can be pruned without affecting the non-isomorphic SCs that can be distinguished. This has important computational implications. Theorem 6. SWL without coboundary and lower-adjacencies has the same expressive power in distinguishing non-isomorphic simplicial complexes as SWL with the complete set of adjacencies.</p><p>It is not immediately clear whether an equivalent theorem would also hold for cell complexes. This is because cells, unlike simplices, can have widely different shapes and, as described above, the adjacencies between them take more complicated forms. Nevertheless, we show that a positive result can be obtained. Theorem 7. CWL without coboundary and lower-adjacencies has the same expressive power in distinguishing non-isomorphic cell complexes as CWL with the complete set of adjacencies.</p><p>We note this does not mean that the removed adjacencies are completely redundant in practice. Even if they are not needed from a (theoretical) colour refinment perspective, they might still include important inductive biases that make them suitable for certain tasks.</p><p>We are now interested in examining various procedures for mapping, or "lifting", graphs into the space of regular cell complexes. Such a procedure can be used to test the isomorphism of two graphs by performing colour refinement on the cell complexes they are mapped to. The hope is that CWL applied to these cell complexes is more powerful than WL applied to the initial graphs. We will later show that for a wide range of transformations, this is indeed the case. We start by rigorously defining what we mean by a "lifting". Definition 8. A cellular lifting map is a function f : G ? X from the space of graphs G to the space of regular cell complexes X with the property that two graphs G 1 , G 2 are isomorphic iff the cell complexes f (G 1 ), f (G 2 ) are isomorphic.</p><p>This property ensures that testing the isomorphism of the two cell complexes is equivalent to testing the isomorphism in the input graphs. This would not be the case if two non-isomoprhic graphs were mapped to the same cell complex.</p><p>Example 9. It can be verified that the function mapping each graph to its clique complex (i.e. every (k + 1)-clique in the graph becomes a k-simplex) is a cellular lifting map.</p><p>The clique complex lifting map from Example 9 has been used by Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> to show that SWL is strictly more powerful than WL. We restate this result: Theorem 10. SWL with clique complex lifting is strictly more powerful than WL.</p><p>A natural question is what other lifting transformations make CWL strictly more powerful than WL?</p><p>We first describe a space of lifting transformations that make CWL at least as powerful as WL. Definition 11. A lifting map is skeleton-preserving if for any graph G, the 1-skeleton of f (G) and G are isomorphic as (multi) graphs.</p><p>Intuitively, skeleton-preserving liftings ensure that the additional structure added by the lifting map comes from attaching cells of dimension at least two to the graph. These mappings keep the 0-cells and 1-cells intact and are, therefore, restricted from making modifications to the input graph structure. An important remark is that for simplicial complexes, attaching simplices based on cliques present in the graph is the only possible skeleton preserving transformation. Once again, this illustrates the limitations of simplicial complexes for adding useful higher-dimensional structures to the graph. Example 12. The function from Example 9 is also skeleton-preserving because the 1-skeleton of the clique complex of a graph is trivially isomorphic to the graph. A lifting function mapping each graph to a multi-graph where each edge is doubled by a parallel edge is not skeleton-preserving ( <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5:</head><p>A graph, its clique complex and the graph with duplicated edges. The first map is skeletonpreserving, while the second is not.</p><p>We now show that all the maps in the skeleton-preserving class have the following desirable property: Theorem 13. Let f be a skeleton-preserving lifting map. Then CWL(f ) (i.e. CWL using lifting f ) is at least as powerful as WL in distinguishing non-isomorphic graphs.</p><p>To prove that some of these make CWL strictly more powerful than WL, it is sufficient to find a pair of graphs that cannot be distinguished by WL, but can be distinguished by CWL. The following result gives examples of such maps. Definition 14. Let k-CL, k-IC, k-C be the lifting maps attaching cells to all the cliques, induced cycles and simple cycles, respectively, of size at most k. Corollary 15. For all k ? 3, CWL(k-CL), CWL(k-IC) and CWL(k-C) are strictly more powerful than WL.</p><p>We note that this is not a complete list. For instance, the result can also be extended to combinations of the above or other transformations. We can also relate CWL to the higher-order 3-WL test. Theorem 16. There exists a pair of graphs indistinguishable by 3-WL but distinguishable by CWL(k-CL) with k ? 4, CWL(k-IC) with k ? 4 and CWL(k-C) with k ? 8.</p><p>Finally, we conclude this section by showing how CWL can achieve a superior expressive power compared to SWL. This result is proven by Corollary 31 in the Appendix. Theorem 17. Let k-CL ? k-IC and k-CL ? k-C denote combined liftings attaching cells to the union of the specified substructures. CWL(k 1 -CL ? k 2 -IC) and CWL(k 1 -CL ? k 2 -C) are strictly more powerful than SWL(k 1 -CL) for all k 2 ? 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Molecular Message Passing with CW Networks</head><p>We now describe CW Networks with an applied focus on molecular graphs to ground the discussion. Therefore, from now on we assume the use of the skeleton-preserving lifting transformation that attaches 2-cells to all the induced cycles (i.e. chordless cycles) in the graph as in <ref type="figure" target="#fig_0">Figure 2</ref>. This leads to a message passing procedure involving atoms (vertices / 0-cells), the bonds between atoms (edges / 1-cells) and chemical rings (induced cycles / 2-cells). Additionally, in virtue of Theorem 7, we consider only the boundary and upper adjacencies between these cells without sacrificing the expressive power. The equations for the other adjacencies, which we do not use, can be found in Appendix A. We note however, that the theoretical results in this section are general and not particular to these specific choices of adjacencies and lifting transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecular Message Passing</head><p>The cells in our CW Network receive two types of messages:</p><formula xml:id="formula_1">m t+1 B (?) = AGG ? ?B(?) M B h t ? , h t ? m t+1 ? (?) = AGG ? ?N ? (?),??C(?,? ) M ? h t ? , h t ? , h t ? .</formula><p>The first specifies messages from atoms to bonds and from bonds to rings. The second type of message, specifies messages between atoms connected by a bond and messages between bonds that are part of the same ring ( <ref type="figure" target="#fig_2">Figure 6</ref>). Note that for the second type of adjacency, when two atoms communicate, we include the features of the bond between them. Similarly, when two bonds communicate, we include the features of the ring they communicate through. The update operation takes into account these two types of incoming messages and updates the features of the cells:</p><formula xml:id="formula_2">h t+1 ? = U h t ? , m t B (?), m t+1 ? (?) .<label>(1)</label></formula><p>To obtain a global embedding for a cell complex X from a model with L layers, the readout function takes as input the separate multi-sets of features corresponding to the atoms, bonds and the rings: Expressivity Naturally, the ability of CWNs to distinguish non-isomorphic regular cell complexes is bounded by CWL. Similarly to GNNs and WL, CWNs can also be shown to be as powerful as CWL as long as they are equipped with a sufficient number of layers and the parametric local aggregators they use can learn to be injective. Multiple such multi-set aggregators <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b73">74]</ref> are known to exist and can be directly employed in our model. Theorem 18. CW Networks are at most as powerful as CWL. Additionally, when using injective neighbourhood aggregators and a sufficient number of layers, CWNs are as powerful as CWL.</p><formula xml:id="formula_3">h X = READOUT({{h L ? }} dim(?)=0 , {{h L ? }} dim(?)=1 , {{h L ? }} dim(?)=2 ).<label>(2)</label></formula><p>Corollary 15 states that CWL is strictly more powerful than the standard WL when the lifting procedure attaches 2-cells to induced cycles of size k ? 3. As a consequence of Theorem 18, this result also holds for molecular message passing CWNs equipped with injective aggregators. In practice, k is to be considered as a standard hyperparameter, and its choice can either be driven by validation set performance, or by domain knowledge (if available).</p><p>Symmetries Given a graph G with adjacency matrix A and feature matrix X, a function f is (node) permutation equivariant if P f (A, X) = f (P AP T , P X), for any permutation matrix P . GNN layers respect this equation, which ensures they compute the same functions up to a permutation (i.e. relabeling) of the nodes. Similarly, it can be shown that CW Networks are equivariant with respect to permutations of the cells and corresponding permutations of the boundary relations ? ? ? between cells. We define this notion of equivariance more formally in Appendix C. Theorem 19. CW Network layers are cell permutation equivariant.</p><p>Long-Range Interactions Several graph-related tasks require the ability to capture long-range interactions between nodes. For instance, certain molecular properties depend on atoms placed on the opposite sides of a ring <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b59">60]</ref>. As a consequence of the coupling between the input and computational graphs, L message passing operations are necessary in GNNs to let a node receive information from an L-hops distant node. In contrast, our hierarchical message passing scheme requires at most L layers since 2-cells create shortcuts. For example, a constant number of CWN layers (3) is enough to capture dependencies between atoms on the opposite sides of a ring, independently of the ring size. In Section 5.1 we verify this in a controlled scenario. Additional experiments on real world graphs in Section 5.2 confirm that it can achieve state-of-the-art performance with a limited number of layers.</p><p>Anisotropic Filters Due to the lack of a canonical ordering between neighbours, many common GNNs use symmetric convolutional kernels, resulting in isotropic filters treating neighbours equally. Recent works have proposed to address this limitation by employing additional structural information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. CWNs also implicitly achieve this form of anisotropy by integrating information from (b) Failure rates on the SR isomorphism task, the smaller the better (mean and std-error over 5 runs). In parantheses, for each model, the maximum size k of rings lifted to 2-cells. <ref type="figure">Figure 7</ref>: Results on the RingTransfer and SR synthetic benchmarks.</p><p>the higher-order cells and their associated substructures into the message passing procedure. For instance, bond features can learn to encode their membership to a ring and also communicate directly with other bonds present in the ring. Consequently, the messages between atoms connected through these bonds are modulated by the presence of the ring as well as by the presence of other nodes and bonds part of that ring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CWNs as Generalised Convolutions</head><p>Our message passing scheme can be seen a (non-linear) generalisation of linear diffusion operators on cell complexes. Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> have introduced convolutional operators on SCs by employing the Hodge Laplacian <ref type="bibr" target="#b62">[63]</ref>, a generalisation of the graph Laplacian. By leveraging on the cellular Sheaf Laplacian <ref type="bibr" target="#b35">[36]</ref>, a similar construction can be extended to cell complexes to define cellular convolutional operators. In Appendix D we discuss this approach and show that our cellular message passing scheme subsumes it. This represents a promising avenue for studying CWNs from a spectral perspective, an endeavour we leave for future work.</p><p>Computational Complexity When considering cells of a constant maximum dimension and boundary size, the computational complexity of the message passing scheme is linear in the size of the input complex. For the molecular applications we are interested in, the average number of rings per molecule is upper bounded by a small constant (e.g. three for MOLHIV), so the size of the complex is approximately the same as the size of the graph. Therefore, in this setting, the computational complexity of the model is similar to that of message passing GNNs. Separately of this, the one-time preprocessing step of computing the lifting of the graphs should also be considered. The C induced cycles in a graph can be listed in O (|E| + |V |C) polylog |V | time <ref type="bibr" target="#b25">[26]</ref>. Again, given that C is upper bounded by a small constant for the molecular datasets of interest in this work, the complexity of the lifting procedure is also almost linear in the size of the graph. A more detailed analysis backed up by wall-clock time experiments is given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we validate the theoretical and empirical properties of our proposed message passing scheme in controlled scenarios as well as in real-world graph classification problems, with a focus on large scale molecular benchmarks. For simplicity, in all experiments we employ a model which stacks CWN layers with local aggregators as in GIN <ref type="bibr" target="#b73">[74]</ref>. We name our architecture "Cell Isomorphism Network" (CIN). 0-cells are always endowed with the original node features; higher-dimensional cells are populated in a benchmark specific manner. See Appendix E for details on feature initialisation, message passing and readout operations, hyperparameters, implementation and benchmark statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Benchmarks</head><p>CSL Circular Skip Link dataset was first introduced in <ref type="bibr" target="#b56">[57]</ref> and has been recently adopted as a reference benchmark to test the expressivity of GNNs <ref type="bibr" target="#b23">[24]</ref>. It consists of 150 4-regular graphs from 10 different isomorphism classes, which we need to predict. Unsolvable by the WL test and message passing approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">57]</ref>, we use it to validate the expressive power of CWNs. We follow the same evaluation setting as Dwivedi et al. <ref type="bibr" target="#b23">[24]</ref>: 5-fold cross validation procedure and 20 different random weight initialisations. For our model, we set the maximum ring size k = 8. In <ref type="table" target="#tab_0">Table 1</ref> we follow the common practice on this dataset and report the mean, minimum and maximum test accuracy obtained by CIN over the 100 runs, along with the results by the baselines presented in Dwivedi et al. <ref type="bibr" target="#b23">[24]</ref>. MP-GNNs, that is classic message passing GNNs (GAT <ref type="bibr" target="#b68">[69]</ref>, MoNet <ref type="bibr" target="#b53">[54]</ref>, GIN <ref type="bibr" target="#b73">[74]</ref>, etc.), and RingGNN <ref type="bibr" target="#b13">[14]</ref> perform as random guessers. In contrast, our model is able to identify the isomorphism class of each test graph in every run while featuring only a fraction of the computational complexity of 3WLGNN, the best performing reference baseline <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>SR Similarly to Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b9">[10]</ref>, we consider Strongly Regular graphs within the same family as hard examples of non-isomorphic graphs we seek to distinguish. Any pair of graphs within the same family cannot provably be distinguished by 3-WL test <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. We reproduce the same experimental setting of Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>. In particular, we consider 9 distinct SR families 3 and run our model untrained on the cell complex lifting of each graph, with k = 4, 5, 6. 0-cells (nodes) are initialised with a constant unitary signal, while 1and 2-cells are initialised with the sum of the contained 0-cells. We additionally run an MLP baseline with sum readout to appreciate the contribution of message passing. We report the percentage of non-distinguished pairs in <ref type="figure">Figure 7b</ref>. Contrary to 3-WL, both CIN and the MLP baseline are able to distinguish many pairs across all families, with better performance attained for larger k. For k = 6, we observed CIN to disambiguate all pairs in all families (0.0% failure rate). Despite the strong results achieved by the baseline, we found CIN to always distinguish a larger number of non-isomorphic pairs for the same values of k, this confirming the importance of cellular message passing.</p><p>RingTransfer In order to empirically validate the ability of CIN to capture long-range node dependencies, we additionally design a third synthetic benchmark dubbed as 'RingTransfer'. Graphs in this dataset are chordless cycles (rings) of size k. In each graph we mark two special nodes as target and source, always placed at distance k 2 . The task is for target to output the one-hot encoded label assigned to source. All other nodes in the ring are assigned a unitary constant feature vector. A model has to learn to transfer the information contained in source to the opposite side of the ring, where target resides. We initialise 1and 2-dimensional cells with a null signal. In <ref type="figure">Figure 7a</ref> we show the performance of a 3-layer CIN as a function of the ring size k, along with that of GIN <ref type="bibr" target="#b73">[74]</ref> baselines equipped with k 2 stacked layers. We observe that our model learns to solve the task with only 3 computational steps, independent of k. As for GIN, we observed degradation in the performance for k ? 24, up to complete failure. We hypothesise this to be due to the difficulties of training such a deep GNN (? 12 layers). We further verify the (theoretically expected) failure of GIN (not included) when endowed with less than k 2 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-World Graph Benchmarks</head><p>TUD We test our model on 8 TUDataset benchmarks <ref type="bibr" target="#b55">[56]</ref> with small and medium sizes from biology (PROTEINS <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>), chemistry (i.e. molecules -MUTAG <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b60">61]</ref>, PTC, NCI1 and NCI109 <ref type="bibr" target="#b69">[70]</ref>) to social networks (IMDB-B, IMDB-M, RDT-B). We consider induced cycle of size up to k = 6 for our graph lifting procedure. We initialise node (and 0-cell) features as described in Xu et al. <ref type="bibr" target="#b73">[74]</ref>, and higher dimensional cells by averaging or summing the features of the included 0-cells. The training setting and evaluation procedure follow those in Xu et al. <ref type="bibr" target="#b73">[74]</ref>. We report the results in <ref type="table" target="#tab_1">Table 2</ref>. CIN compares more than favourably with the baselines, displaying strong empirical performance on all benchmarks. The mean accuracy of CIN ranks top on four out of eight datasets.  On the remaining datasets, CIN achieves the second place. We observe that the best results are on datasets from the biological and chemical domains, where rings play a relevant role.</p><p>ZINC We study the effectiveness of cellular message passing on larger scale molecular benchmarks from the ZINC database <ref type="bibr" target="#b67">[68]</ref>. ZINC (12k graphs) and ZINC-FULL (250k graphs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b74">75]</ref> are two graph regression task datasets for drug constrained solubility prediction. In these experiments, we consider rings up to size k = 18. We follow the training and evaluation procedures in <ref type="bibr" target="#b23">[24]</ref>. Our experiments encompass different scenarios, examine the impact of ablating edge features and of constraining the parameter budget of the architecture to 100k. All results are illustrated in <ref type="table" target="#tab_2">Table 3</ref> where we also include the results for ZINC-FULL obtained by the same exact architectures. Our model exhibits particularly strong performance on these benchmarks: it attains state-of-the-art results on both the two dataset variants, outperforming other models by a significant margin. CIN attains strong results even when constrained by the parameter budget. It still achieves state-of-the-art performance on ZINC and is on-par with the best unconstrained baseline under edge-feature ablation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mol-HIV</head><p>We additionally test our model on the molecular ogbg-molhiv dataset from the Open Graph Benchmark <ref type="bibr" target="#b39">[40]</ref> (41k graphs). The task is to predict the capacity of compounds to inhibit HIV replication. Rings of size up to k = 6 are considered as 2-cells. We take the architecture in <ref type="bibr" target="#b26">[27]</ref> as reference and replicate the same hyperparameter setting in our model, including the use of only 2 message passing layers. We report the mean of test ROC-AUC metrics at the epoch of best validation performance for 10 random weight initialisations. Similarly to ZINC, we experiment with a "small" model whose number of parameters is constrained in the order of 100k. <ref type="table" target="#tab_2">Table 3</ref> displays the results. CIN significantly outperforms other strong GNN baselines, even when constrained by the parameter budget. Consistently with <ref type="bibr" target="#b26">[27]</ref>, we observe that only two layers are sufficient when performing hierarchical message passing across meso-scale structures such as rings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work, Discussion and Conclusion</head><p>Cell complex models Recent works have proposed the generalisation of GNNs to simplicial complexes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. All these simplicial methods are subsumed by the model in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>, which CWNs in turn subsume. To the best of our knowledge, Hajij et al. <ref type="bibr" target="#b33">[34]</ref> is the only other example of message passing on cell complexes, but this work does not study the expressive power of the proposed scheme, neither it experimentally validates its performance. In contrast, our work comprehensively characterises the expressiveness of cellular message passing, and introduces a theoretically grounded and empirically effective framework to apply it on graph structured data in a way to address several limitations of standard Graph Neural Networks.</p><p>Molecular substructures A few other works have extended GNNs to account for molecular substructures. Junction Trees (JT), which conveniently represent singletons, bonds and rings as supernodes in a tree, have been used in molecular graph generation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. JTs are also used in the recent work of Fey et al. <ref type="bibr" target="#b26">[27]</ref>, who employs them to design a hierarchical message passing scheme based on the tree structure. However, this hierarchy has a different configuration than the one cell complexes provide. Information about cycles is also used in GSNs <ref type="bibr" target="#b9">[10]</ref> to augment the node features, but the model retains the usual message passing procedure of GNNs. These last two models are of particular relevance to the present work, since they utilise information about chemical rings. It is important to remark that CWNs compare favourably with both of them in all our benchmarks.</p><p>Higher-order GNNs A related line of work has studied lifting graphs into k-dimensional tensor representations that can be processed by provably expressive k-GNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. With higher values of k, these models achieve higher-expressivity, but due to the computational complexity this incurs, values of k ? 3 are of little use in practice. Therefore, unlike CWNs, these models cannot explicitly represent in practice chemical rings of common sizes (e.g. five or six). Furthermore, by being upper-bounded by 3-WL, the 2-GNN models cannot count the number of induced cycles of size greater than four (see Appendix A for details). In contrast, CWNs can easily count these important chemical substructures through the readout operation it performs on the 2-cells.</p><p>Limitations The main limitations of the model are of computational nature. While the computational complexity of the message passing procedure and its preprocessing step is suitable for molecular and geometric graphs, the number of rings (and more generally simple cycles) in general graphs can be exponential in the number of nodes. In that case, one has to resort to smaller 2-cells like triangles, which can be found efficiently in general graphs. Moreover, one has to typically use weights specific for each dimension of the cell complex, increasing the number of parameters compared to GNNs. However, we have shown that our model can compensate this increase with a reduced number of layers and still achieve state-of-the-art results on some of the molecular benchmarks.</p><p>From a theoretical point of view, this work is concerned only with regular cell complexes. Adopting this restriction is useful from multiple perspectives: regular cell complexes are easier to analyse, their combinatorial structure completely describes their topology and convolutions can be defined on them through the Sheaf Laplacian (see Appendix D). Nonetheless, some of our theoretical results could be extended to non-regular complexes, which could be obtained by lifting transformations not studied in this work, such as attaching 2-cells to paths in the graph. We leave addressing non-regular complexes and their trade-offs to future developments of this work.</p><p>Societal Impacts Most of our paper is theoretical in nature and we do not see immediate direct negative societal impacts. Within the scope of social network applications, we do not yet have sufficient evidence of performance improvement on related benchmarks to justify obvious adoption in such a domain. In contrast, the empirical performance on molecular benchmarks suggests it may have a positive impact on applications of immediate interest in pharmaceutics, such as drug discovery <ref type="bibr" target="#b29">[30]</ref>.</p><p>Conclusion We have proposed a provably powerful message passing procedure on cell complexes motivated by a novel colour refinement algorithm to test their isomorphism. This allows us to consider flexible lifting operations on graphs to implement more expressive architectures which benefit from decoupling the computational and input graphs. Our methods show excellent performance on diverse synthetic and real-world molecular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding and Acknowledgements</head><p>YW and GM acknowledge support from the ERC under the EU's Horizon 2020 programme (grant agreement n o 757983). MB is supported in part by ERC Consolidator grant n o 724228 (LEMAN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Cellular WL Results</head><p>In this section, we assume basic familiarity with the WL test and its higher-order variants. For an introduction to these topics, we refer the reader to the survey of Sato <ref type="bibr" target="#b61">[62]</ref>. We begin by introducing a few useful concepts. Definition 20. A cellular colouring is a map c that maps a cell complex X and one of its cells ? to a colour from a fixed colour palette. We denote this colour by c X ? . Definition 21. Let X, Y be two regular cell complexes and c a cellular colouring. We say that X, Y are c-similar, denoted by c X = c Y , if the number of cells in X coloured with a given colour equals the number of cells in Y with the same colour. Otherwise, we have c X = c Y .</p><p>We emphasise that in this paper we are interested only in colourings c with the property that any two isomorphic cell complexes are c-similar. </p><formula xml:id="formula_4">? ? P Y , c X ? = c Y ? implies d X ? = d Y ? .</formula><p>Additionally, if d c, we say the two colourings are equivalent and we represent it by c ? d.</p><p>We state the following result from Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> about simplicial colourings, which we translate here directly to cell complexes. The proof is however, identical, and we refer the reader to their work for that. Proposition 23. Let X, Y be any regular cellular complexes with A ? P X and B ? P Y . Consider two cellular colourings c, d such that c d.</p><formula xml:id="formula_5">If {{d X ? | ? ? A}} = {{d Y ? | ? ? B}}, then {{c X ? | ? ? A}} = {{c Y ? | ? ? B}}. Corollary 24. Consider two cellular colourings c, d such that c d. For all cell complexes X and Y , if d X = d Y , then c X = c Y .</formula><p>This last result implies that if c refines d, then c can distinguish all the non-isomorphic cell complexes that d can distinguish. We say that the colouring c is at least as powerful as the colouring d.</p><p>In contrast to simplicial complexes, cell complexes have a more flexible structure. The main complication compared to the proofs in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> is that cells can have a variable number of lower-dimensional cells on their boundary. It is therefore useful in many proofs, to separate the cells into buckets containing cells with the same boundary size. The following result helps us do that. Proposition 25. Let c t be the CWL colouring at iteration t. For all cells ?, ? in any cell complexes X and Y , if |B(?)| = |B(? )|, then for any t &gt; 0 we have c t ? = c t ? .</p><p>Proof. If ? and ? have boundaries of different sizes, then c 1 B (?) = c 1 B (? ), which immediately implies c t ? = c t ? for all t &gt; 0.</p><p>Next, we show that one can drop the co-boundary adjacencies without sacrificing expressive power.</p><formula xml:id="formula_6">Lemma 26. CWL with HASH c t ? , c t B (?), c t ? (?), c t ? (?) is as powerful as CWL with the generalised update rule HASH c t ? , c t B (?), c t C (?), c t ? (?), c t ? (?) .</formula><p>Proof. Let a t denote the colouring produced by CWL using the general version and b t the colouring produced using the restricted version at iteration t. It can be verified that a t b t because it considers the additional c t B (?) colours in the refinement rule. We now prove b t+1 a t by induction. Note that to take advantage of Proposition 25, we shift the time-step by one (i.e. we use b t+1 as opposed to b t ).</p><p>The base case holds since a 0 assigns the same colour to all the cells. Suppose b t+2 ? = b t+2 ? for any two cells ? and ? from any cell complexes X and Y , respectively. Then we know that b t+1</p><formula xml:id="formula_7">? = b t+1 ? , b t+1 B (?) = b t+1 B (? ), b t+1 ? (?) = b t+1 ? (? ) and b t+1 ? (?) = b t+1 ? (? ).</formula><p>The goal is to show that this also implies that b t+1</p><formula xml:id="formula_8">C (?) = b t+1 C (? ). Given b t+1 ? (?) = b t+1 ? (? ), by definition {{b t+1 ?? | (?, b t+1 ?? ) ? b t+1 ? (?)}} = {{b t+1 ?? | (?, b t+1 ?? ) ? b t+1 ? (? )}}.</formula><p>By Proposition 25, cells with different boundary sizes have different colours. Therefore, we can partition these two multi-sets by the size of the cell boundaries, while preserving the equality between these sub-multisets. Therefore, for each n ? N:</p><formula xml:id="formula_9">{{b t+1 ?? | (?, b t+1 ?? ) ? b t+1 ? (?) and |B(? ? )| = n}} = {{b t+1 ?? | (?, b t+1 ?? ) ? b t+1 ? (? )</formula><p>and |B(? ? )| = n}}. Let ? be an arbitrary cell. Then for each cell ? ? ? C(?), ? exchanges messages with all the other boundary cells of ? ? . Therefore, the colour of each ? ? with |B(? ? )| = n shows up with a multiplicity of n ? 1 in the tuples of b t+1 ? (?). Eliminating n ? 2 of these repeated colours for all ? ? and ? ? :</p><p>{{b t+1 ?? | ? ? ? C(?) and |B(? ? )| = n}} = {{b t+1 ?? | ? ? ? C(? ) and |B(? ? )| = n}}. Merging these in a single multi-set gives the colours of the co-boundary cells:</p><formula xml:id="formula_10">b t+1 C (?) = {{b t+1 ?? | ? ? ? C(?)}} = {{b t+1 ?? | ? ? ? C(? )}} = b t+1 C (? ). By the induction hypothesis, a t ? = a t ? , a t B (?) = a t B (? ), a t C (?) = a t C (? ), a t ? (?) = a t ? (? ) and a t ? (?) = a t ? (? ). This implies a t+1 ? = a t+1 ? .</formula><p>The following theorem shows that we can further prune the CWL update rule by removing the colours associated with the lower adjacencies. The structure of the proof is similar to the one in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>, with the main difference being in the proof of Proposition 27.</p><p>Proof of Theorem 7. Let b t denote the colouring of CWL using</p><formula xml:id="formula_11">HASH b t ? , b t B (?), b t ? (?)</formula><p>and a t the colouring of CWL using the rule HASH a t ? , a t B (?), a t ? (?), a t ? (?) from Lemma 26. Trivially a t b t because of the additional argument c t ? (?) in the update rule. We prove b 2t+1 a t by induction. As before, the addition by one in 2t + 1 is to allow us to apply Proposition 25 in the induction step. The multiplication by 2 is due to the fact that the information transmitted through the lower adjacencies in one step is propagated in two steps through the boundary adjacencies.</p><p>As before, the base case trivially holds since a 0 assigns the same colour to all cells. For all cell complexes X and all cells ? in P X , consider the collection of multi-sets A X indexed by ?:</p><formula xml:id="formula_12">A X (?) = {{(b 2t+1 ? = C 0 , b 2t+1 ? = C 1 ) | ? ? C(?)}}.</formula><p>We are interested in the size of these multi-sets for some specific cells ?. To that end, for each cell ? ? P X , we define the multi-set:</p><formula xml:id="formula_13">C X (?) = {{|A X (?)| | ? ? B(?)}}.</formula><p>We know that C X (?) = C Y (? ) since the sum of the elements of C X (?), which gives the number of tuples (C 0 , C 1 ) in b 2t+1 ? (?), is greater than the sum of the elements of C Y (? ), which gives the number of tuples (C 0 , C 1 ) in b 2t+1 ? (? ). We prove this contradicts our hypothesis that b 2t+3</p><formula xml:id="formula_14">? = b 2t+3 ? .</formula><p>Proposition 27. For all regular cell complexes X, Y and all ? ? P</p><formula xml:id="formula_15">X , ? ? P Y , if C X (?) = C Y (? ), then b 2t+3 ? = b 2t+3 ? .</formula><p>Proof. Given a cell complex X and a cell ? ? P X , consider the cellular colouring c X ? = |A X (?)|. The idea of the proof is to show that b 2t+2 c, which allows us to use Proposition 23 for the multi-sets C X (?) and C Y (? ).</p><p>Let ? 1 , ? 2 be two arbitrary cells from any regular cell complexes X, Y such that c X ?1 = c Y ?2 . Assume without loss of generality that |A X (? 1 )| &gt; |A Y (? 2 )|. Two cases can be distinguished for this inequality. In the first case, b 2t+1 ?2 = C 1 , which implies |A X (? 1 )| &gt; |A Y (? 2 )| = 0 and, therefore,  . Additionally, note that since the cell complex is regular, self-loops are not allowed and, therefore, n &gt; 1.</p><p>Applying this to ? 1 and ? 2 , C 0 shows up |A X (? 1 )|?(n?1) times in b 2t+1</p><formula xml:id="formula_16">? (? 1 ) and |A Y (? 2 )|?(n?1) times in b 2t+1 ? (? 2 ). Therefore, b 2t+1 ? (? 1 ) = b 2t+1 ? (? 2 ) and, similarly to the first case, b 2t+2 ?1 = b 2t+2 ?2 .</formula><p>The results obtained for the two cases prove b 2t+2 c.</p><p>Applying Proposition 23 for the multi-sets C X (?) and C Y (? ), we obtain two non-equal multi-sets:</p><formula xml:id="formula_17">b 2t+2 B (?) = {{b 2t+2 ?1 | ? 1 ? B(?)}} = {{b 2t+2 ?2 | ? 2 ? B(? )}} = b 2t+2 B (? )</formula><p>Since these two multi-sets are used in the colour updating rule, b 2t+3 := c G,t v ) at the same time step t.</p><formula xml:id="formula_18">? = b 2t+3 ? . Therefore, b 2t+1 ? (?) = b 2t+1 ? (? ).</formula><p>Because f (G) <ref type="bibr" target="#b0">(1)</ref> and G are isomorphic as graphs and WL is invariant under isomorphism, a f (G) <ref type="bibr" target="#b0">(1)</ref> ,t = c G,t . It follows that for all graphs G 1 , G 2 ? G, if c G1,t = c G2,t then a f (G1) <ref type="bibr" target="#b0">(1)</ref> ,t = f (G2) <ref type="bibr" target="#b0">(1)</ref> ,t . Let b t be the CWL colouring of the 0-cells at iteration t. The goal is to show that for all regular cell complexes X, Y ? f (G), b t a t . By transitivity and combined with</p><formula xml:id="formula_19">Corollary 24, it follows that if c G1,t = c G2,t , then b f (G1),t = b f (G2),t .</formula><p>The base case trivially holds. Let ?, ? be two 0-cells in X ? f (G) and Y ? f (G), respectively such that b t+1 (?) = b t+1 (? ). Since 0-cells have only upper adjacencies, the equality implies</p><formula xml:id="formula_20">that b t ? = b t ? and b t ? (?) = b t ? (? ).</formula><p>The latter multi-set equality further implies</p><formula xml:id="formula_21">{{b t ?? | (b t ?? , ?) ? b ? (?)}} = {{b t ?? | (b t ?? , ?) ? b ? (? )}}.</formula><p>Equivalently, for 0-cells of a cell complex whose 1-skeleton is a graph (i.e. not a multi-graph), this can be rewritten as {{b t ?? | ? ? ? N ? (?)}} = {{b t ?? | ? ? ? N ? (? )}}. By the induction hypothesis we have a t ? = a t ? and {{a t ?? | ? ? ? N ? (?)}} = {{a t ?? | ? ? ? N ? (? )}}.</p><p>These equalities imply a t+1 (?) = a t+1 (? ). Proof of Corollary 15. Due to Theorem 13, it is sufficient to find some examples of non-isomorphic graph pairs that WL cannot distinguish, but CWL can with the given lifting transformations. <ref type="figure" target="#fig_7">Figure 8</ref> includes such examples. Based on Proposition 25, CWL can distinguish these graphs since it can count the number of substructures (e.g. triangles, rings, cycles) that the lifting is based on.</p><p>The next proposition shows that CWL can identify cells that are n-simplices. Proposition 28 (Simplex Identification). Let X, Y be regular cell complexes and ? ? P X , ? ? P Y two cells. Denote by c t the CWL colouring at iteration t. Suppose ? is an n-simplex and ? is not. Then c t (?) = c t (? ) for all t ? n + 1.</p><p>Proof. The base case holds since c 1 ? = c 1 ? if ? is a vertex and ? is a cell of another dimension. This is because ? has no boundary adjacencies, while ? does.</p><p>Suppose the statement holds for n-simplices. Then, an (n + 1)-simplex can be identified by having n + 2 n-simplices on its boundary. By Proposition 25, the colour of ? encodes the boundary size. Furthermore, by the induction hypothesis c n+1 B (?) encodes the fact that the boundary cells are n-simplices.</p><p>Proof of Theorem 16. The sub-results of the theorem can be proven by finding pairs of graphs from the same family of Strongly Regular Graphs that can be distinguished by CWL with the corresponding lifting transformations. Graphs in this family are provably indistinguishable by the higher-order 3-WL test <ref type="bibr" target="#b7">[8]</ref>.</p><p>Ring-based lifting We can show that there is a pair of SR graphs in the same family with a different number of induced cycles of a certain size. We include such an example in <ref type="figure" target="#fig_9">Figure 9</ref>. The two graphs differ in the number of 4-, 5-, 6and 8-rings (see <ref type="table" target="#tab_3">Table 4</ref>), which indirectly proves 3-WL cannot count induced cycles of these sizes. It is also natural to conjecture that 3-WL cannot count induced cycles of size strictly larger than 3. In contrast, CWL(4-IC) is sufficient to distinguish these two graphs.</p><p>Clique complex lifting We can leverage on the same example: the graph on the right does not possess 4-cliques, contrary to the graph on the left (one such example is marked in blue). This proves that 3-WL cannot count cliques of size 4. As shown by Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>, this result immediately implies that SWL (and consequently CWL) with a clique complex lifting is not less powerful than 3-WL.</p><p>Cycle-based lifting To prove the result for this lifting transformation we leverage on a result by Arvind et al. <ref type="bibr" target="#b1">[2]</ref>, who show that 2-Folklore WL (which is equivalent to 3-WL <ref type="bibr" target="#b61">[62]</ref>) cannot count subgraph cycles of size strictly larger than 7. <ref type="table" target="#tab_3">Table 4</ref> illustrates this for the same example as above. Since CWL can count the number of 8-cycles when the lifting transformation k-C with k ? 8 is used (see <ref type="bibr">Proposition 25)</ref>, this proves the result.</p><p>We note that while the proof above is purely based on substructure counts, the superior expressive power of CWL is very likely not limited to counting the substructures involved in the lifting transformation. We have seen evidence in favour of this claim in the SR experiment in Section 5, where message passing layers reduced the failure rate.</p><p>Next, we prove a statement comparing Simplicial WL and CWL. This will later be used to show that CWNs are strictly more powerful than MPSNs when a lifting transformation based on the clique complex and rings is used. Definition 29. A subset L of a cell complex X is called a subcomplex if it is a union of cells of X containing the closures of these cells.</p><p>Theorem 30. Let f : G ? X be a skeleton-preserving transformation such that for any graph G, the clique complex of G is a subcomplex of f (G). Then CWL(f ) is at least as powerful as SWL using the clique complex lifting at distinguishing non-isomorphic graphs.</p><p>Proof. Let c t be the simplicial colouring performed by SWL. We can extend it into a cellular colouring a t defined as follows:</p><formula xml:id="formula_22">a X,t ? := c L,t ? if X ? ? L otherwise</formula><p>where L is the maximal simplicial complex that is a subcomplex of X and is a special colour assigned to the cells that are not simplices. Let h : G ? X be the clique-complex lifting map. Then, it is easy to see that for all graphs G 1 , G 2 ? G, if c h(G1),t = c h(G2),t , then a f (G1),t = a f (G2),t . Let b t be the CWL colouring map at iteration t. We aim to show that b t+n+1 a t by using Proposition 28. Then, by transitivity and using Corollary 23, if c h(G1),t = c h(G2),t , then b f (G1),t+n+1 = b f (G2),t+n+1 .</p><p>Let n be the maximum dimension of the cells used by the lifting transformation f . As usual, the base case holds at initialisation since a 0 assigns the same colour to all the cells. Let ?, ? be two cells from the regular cell complexes X, Y ? f (G). When ? and ? are not simplices, then a t ? = a t ? = . Suppose ? and ? are both simplices and b t+n+2</p><formula xml:id="formula_23">? = b t+n+2 ? . Then we know that b t+n+1 ? = b t+n+1 ? , b t+n+1 B (?) = b t+n+1 B (? ) and b t+n+1 ? (?) = b t+n+1 ? (? ).</formula><p>Since ? and ? are simplices, their boundary cells are also lower-dimensional simplices, so by induction hypothesis, a t B (?) = a t B (? ). Let us consider the equality between the colours involving the upper adjacent cells. By expanding the definition we have:</p><formula xml:id="formula_24">{{(b t+n+1 ?1 , b t+n+1 ?2 ) | ? 1 ? N ? (?), ? 2 ? C(?, ? 1 )}} = {{(b t+n+1 ?1 , b t+n+1 ?2 ) | ? 1 ? N ? (? ), ? 2 ? C(?, ? 1 )}}.</formula><p>Generally, not all of these adjacencies involve simplices. For instance, a 2-simplex could incident to a general 3-cell. However, by Proposition 28 this equality must still hold if we restrict the multi-sets to the colour of those cells that are simplices: ) | ? 1 ? N ? (? ), ? 2 ? C(?, ? 1 ), and ? 1 , ? 2 are simplices}}.</p><formula xml:id="formula_25">{{(b t+n+1 ?1 , b t+n+1 ?2 ) | ? 1 ? N ? (?), ? 2 ? C(?,<label>?</label></formula><p>These multi-sets, give exactly the upper adjacencies used by SWL for computing its colouring map c t . Therefore, by the induction hypothesis, a t ? (?) = a t ? (? ). Finally, this proves a t+1 ? = a t+1 ? . Corollary 31. CWL(k 1 -CL ? k 2 -IC) and CWL(k 1 -CL ? k 2 -C) are strictly more powerful than SWL(k 1 -CL) for all k 2 ? 5.</p><p>Proof. The second pair of graphs from <ref type="figure" target="#fig_7">Figure 8</ref> cannot be distinguished by SWL(k 1 -CL) because it has no cliques greater than two, but it can be distinguished by CWL with the liftings above because of the different number of (induced) cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CW Network Proof</head><p>Proof of Theorem 18. Let c t denote the colouring of CWL at iteration t and h t the colouring (i.e. features) produced by a CW-Network as described in Section 4. Without loss of generality (Theorem 7), we use only boundary and upper adjacencies for both methods.</p><p>To show CWNs are at most as powerful as CWL, we must show c t h t . Again, we show this by induction. For a CWN with L layers we assume h t = h L for all t &gt; L. Let ?, ? be two cells with</p><formula xml:id="formula_26">c t+1 ? = c t+1 ? . Then, c t ? = c t ? , c t B (?) = c t B (? ) and c t ? (?) = c t ? (? ). By the induction hypothesis, h t ? = h t ? , h t B (?) = h t B (? ) and h t ? (?) = h t ? (? ). If t + 1 &gt; L, then h t+1 ? = h t ? = h t ? = h t+1 ? .</formula><p>Otherwise, h t+1 is given by Equation 1 involving the update function U , the aggregate function AGG and the message functions M B , M ? . Given that the inputs passed to these functions are equal for ? and ? , h t+1 ? = h t+1 ? . We now prove that CWNs can be as powerful as CWL. Suppose the aggregation from Equation 1 is injective and the model is equipped with a number of layers L sufficient to guarantee the convergence of the colouring. Then, we show that h t c t . Let ?, ? be two cells with h t+1</p><formula xml:id="formula_27">? = h t+1 ? . Then, since the local aggregation is injective h t ? = h t ? , h t B (?) = h t B (? ) and h t ? (?) = h t ? (? ). By the induction hypothesis, c t ? = c t ? , c t B (?) = c t B (? ) and c t ? (?) = c t ? (? ). Finally, c t+1 ? = c t+1 ? .</formula><p>The consequence of this result is that CWNs inherit all the properties of CWL. We summarise these in the following Corollary. Corollary 32. CWNs have the following properties:</p><p>1. They are at least as powerful as the WL test when using skeleton-preserving lifting transformations.</p><p>2. They are strictly more powerful than the WL test when using the lifting maps from Corollary 15.</p><p>3. They are not less powerful than 3-WL when using the lifting transformations from Theorem 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>They are at least as powerful as MPSNs using the clique complex lifting <ref type="bibr" target="#b7">[8]</ref> when using a lifting transformation whose output complexes have the clique complex as a subcomplex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>They are strictly more powerful than MPSNs when using a transformation attaching cells to cliques and rings/cycles. In particular, CWNs using rings are strictly more powerful than MPSNs using a lifting based on triangles (i.e. 2-simplices), since triangles are rings of size 3.</p><p>The latter point regarding triangles is important because Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> do not use simplices of dimension higher than two in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Equations for Other Adjacencies</head><p>For completeness, we include in this section the equations for the co-boundary and lower adjacent messages.</p><formula xml:id="formula_28">m t+1 C (?) = AGG ? ?C(?) M C h t ? , h t ? , m t+1 ? (?) = AGG ? ?N ? (?),??B(?,? ) M ? h t ? , h t ? , h t ? .</formula><p>Together with the adjacencies described in the main text, the update rule takes the form</p><formula xml:id="formula_29">h t+1 ? = U h t ? , m t B (?), m t C (?), m t+1 ? (?), m t+1 ? (?) .</formula><p>As mentioned before, even though these adjacencies are redundant from a colour refinement perspective when the others are used, they might still be employed in other combinations that preserve the expressive power of the test. Additionally, for certain applications, they might still encode important inductive biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computational Analysis</head><p>Let X be a d-dimensional regular cell complex. For an arbitrary p-cell ? with boundary size k, the number of ?-messages between the (p ? 1)-cells on its boundary is 2 * k 2 and the number of B-messages it receives is k. Let B p be the maximum boundary size of a p-cell in X and S p the number of p-cells. The computational complexity of our message passing scheme is thus O d p=1 B p S p + 2 * Bp 2 S p . For instance, consider the skeleton-preserving lifting based on induced cycles. There, the dimension of the complex is d = 2 and we have B 0 = 0, B 1 = 2, and B 2 equals the size of the maximum induced cycle considered. For all practical purposes, we can consider d and B p as fixed constants. Then the complexity can be rewritten as ? d p=1 S p . This is optimal because the complexity is linear in the size of the cell complex and a linear time is required to read the cell complex.  In practice, we observed the empirical training runtimes to be contained, even on the largest benchmarks. We performed timing analyses on ZINC and ZINC-FULL, measuring the time required to complete one training epoch and a full performance evaluation on train, validation and test sets. We report the runtimes in <ref type="table" target="#tab_4">Tables 5 and 6</ref> the runtimes measured for our best performing CIN and CIN-small models and by GIN baselines with, approximately, the same number of parameters. We observe that the evaluation runtimes are relatively comparable to those of GIN models and that the difference decreases significantly at inference time (i.e. no backprop). The training runtimes are significantly reduced on CIN-small architectures, which always perform on-par or even better than state-of-the-art baselines, regardless of the imposed parameter budget (see <ref type="table" target="#tab_2">Table 3</ref>). These experiments where run over an NVIDIA ? Tesla V100 GPU device on an Amazon Web Services (AWS) Elastic Cloud (EC) 2 p3.16xlarge instance.</p><p>Other than the computational complexity of message passing we need to consider the (one-off) complexity pertaining the graph lifting procedures. Lifting procedures that are more likely to find immediate practical applications involve clique, cycle and induced cycle listing. For cliques, we refer readers to Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>, where the authors report theoretical results regarding clique-listing complexity and the practical impact of employing specialised topological data analysis libraries.</p><p>As for cycle-based liftings, specialised cycle-listing algorithms exist. The algorithm in Birmel? et al. <ref type="bibr" target="#b6">[7]</ref> is able to list all simple cycles in a graph in O(m + c?C(G) |c|), where m is the number of edges, C(G) is the set of simple cycles in graph G and |c| is the size of the cycle. As for induced cycles, the algorithm presented in Ferreira et al. <ref type="bibr" target="#b25">[26]</ref> has a listing time of?(m + nC), with n and C being the number of nodes and induced cycles, respectively. In certain types of graphs, a better complexity can be obtained. In the case of planar graphs, Chiba and Nishizeki <ref type="bibr" target="#b15">[16]</ref> show linear time complexity to list triangles and quadratic complexity for 4-rings. This is very important because almost all molecules are planar in a graph-theoretic sense <ref type="bibr" target="#b65">[66]</ref> as a direct consequence of the chemical implications of Kuratowski's theorem <ref type="bibr" target="#b48">[49]</ref>. However, we are not aware of any improved bounds for finding general induced cycles in planar graphs. Finally, we remind the reader that molecular rings can also be listed from the junction tree representation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref>, obtained by specialised molecular libraries such as RDKit <ref type="bibr" target="#b49">[50]</ref>. In our experiments, we implemented a lifting procedure based on the generic substructure matching algorithm exposed by the graph-tool Python library, which internally employs VF2 <ref type="bibr" target="#b18">[19]</ref> to perform subgraph isomorphism. Noticing that the lifting procedure is embarrassingly parallel w.r.t. the independent graphs in a dataset, we easily parallelised the procedure via Python's Joblib library.</p><p>On molecular benchmarks we observed the effective time required by preprocessing routines to always be modest compared to the training times. In <ref type="table" target="#tab_6">Table 7</ref> we report the wall clock runtimes, averaged over 5 runs, to lift all the graphs in the largest datasets amongst our benchmarks: ZINC, Mol-HIV and ZINC-FULL. The analysis has been conducted considering rings up to size 18 and by varying the number of parallel processing jobs on a server with an Intel ? Xeon E5-2686 v4 processor with 64 vCPUs. It is possible to observe that the empirical lifting runtime scales linearly with the number of jobs in the range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, and that such a simple parallelisation scheme dramatically reduces the preprocessing time on all datasets. When employing 32 parallel jobs, less than 19 seconds are required to preproceess the whole ZINC dataset, only 1 minute is required for Mol-HIV, and we needed slightly more than 6 minutes to lift all the 250k graphs in ZINC-FULL. We remark once more that these experiments have been conducted with a generic subgraph matching algorithm, and that even more parsimonious computation would be possible by using optimised ring-listing routines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Symmetries</head><p>In line with a recent effort in Geometric Deep Learning to understand different models through the lens of symmetry <ref type="bibr" target="#b11">[12]</ref>, we aim here to give a description of the underlying equivariance properties of CW Networks.</p><p>First, let us define the following matrix representation of the boundary relation from Definition 3. Definition 33. Let X be a regular cell complex with S k denoting the number of cells in dimension k.</p><p>The k-th unsigned boundary matrix B k ? R S k?1 ?S k of X is given by B k (i, j) = 1 if ? i ? ? j and 0, otherwise.</p><p>Let X be a regular cell complex of dimension n with boundary matrices B = (B 1 , . . . , B n ) and feature matrices X = (X 0 , X 1 , . . . , X n ) for the cells of different dimensions. Additionally, consider a sequence of permutation matrices P = (P 0 , . . . , P n ). Denote by PX = (P 0 X 0 , . . . , P n X n ) and PBP T = (P 0 B 1 P T 1 , . . . , P n?1 B n P T n ). Definition 34. A function f mapping (X, B) ? X = (X 0 , . . . , X n ) with the property that Pf (X, B) = f (PX, PBP T ) for any P is called cell permutation equivariant.</p><p>Proof of Theorem 19. Definition 34 is similar to the (simplex) permutation equivariance definition from Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>, with the subtle difference that the boundary matrices now have a more flexible structure in the case of cell complexes. The high-level idea is to see that all the adjacency matrices used by CWNs (i.e. B k , B k+1 , B k B k , B k+1 B k+1 ) are permuted accordingly by the permutation matrices in P. Therefore, CWNs layers computes the same function up to a permutation of the cells. The proof follows a similar logic to to the one in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> for simplicial networks, and we refer the reader to their work for a detailed proof.</p><p>It is common in algebraic topology and differential geometry to equip the incidence relation ? ? ? with additional structure that makes it a signed incidence relation. Definition 35 <ref type="bibr">(Hansen and Ghrist [36]</ref>). A signed incidence relation on P X is a map [? : ?] : P X ? P X ? {0, ?1} with the properties:</p><p>1. If [? : ? ] = 0, then ? ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For any</head><formula xml:id="formula_30">? ? ? , ??Px [? : ?][? : ? ] = 0.</formula><p>This signed incidence relation can be be encoded by the signed incidence (boundary) matrices of X. We define these below: Definition 36. Let X be a regular cell complex with a signed incidence relation [? : ?]. Let S k denote the number of cells in dimension k. The k-th signed boundary matrix</p><formula xml:id="formula_31">B k ? R S k?1 ?S k of X is given by B k (i, j) = [? i : ? j ].</formula><p>The difference with respect to the unsigned boundary matrices is that the non-zero values of the matrix can be ?1, not just 1. This can be used to define a notion of orientation equivariance for CW Networks. This ensures that when changing the orientation of the cell complex X (i.e. changing [? : ?]) one computes the same function up to that change in orientation.</p><p>Let X be a regular cell complex of dimension n described by the signed boundary matrices B = (B 1 , . . . , B n ) and feature matrices X = (X 0 , X 1 , . . . , X n ) for the cells of different dimensions. Additionally, consider a sequence of diagonal matrices T = (T 0 , . . . , T n ) with values in ?1. Additionally, let T 0 = I. Denote by TX = (T 0 X 0 , . . . , T n X n ) and TBT = (T 0 B 1 T 1 , . . . , T n?1 B n T n ). Definition 37. A function f mapping (X, B) ? X = (X 0 , . . . , X n ) with the property that Tf (X, B) = f (TX, TBT) for any T is called orientation equivariant.</p><p>Making CWNs orientation equivariant requires imposing additional constraints on the layers of the model. This proceeds similarly to MPSNs <ref type="bibr" target="#b7">[8]</ref>. Since applications involving oriented simplicial complexes are out of the scope of this work, we refer the reader to Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> for an intuition of how this can be extended to cell complexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Sheaves, Laplacians and Convolutions</head><p>It is useful on cell complexes to derive a Laplacian operator based on cellular sheaves <ref type="bibr" target="#b35">[36]</ref>, since many interesting Laplacians, such as the (normalised) graph Laplacian <ref type="bibr" target="#b16">[17]</ref>, the Hodge Laplacian <ref type="bibr" target="#b62">[63]</ref> and the connection Laplacian <ref type="bibr" target="#b66">[67]</ref> can be obtained as particular cases. Intuitively, a cellular sheaf is a construction that assigns a vector space to each cell in the complex and a (linear) map for each face relation in the complex ? ? ? . Additionally, these linear maps must satisfy some compositionality constraints imposed by the structure of P X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Sheaf Laplacian</head><p>Definition 38. Let (X, P X ) be a regular cell complex, and denote by Hilb K the class of Hilbert spaces over a field K. A weighted cellular sheaf F is given by the assignment</p><formula xml:id="formula_32">F : P X ? Hilb K ? ? F(?)</formula><p>together with a bounded linear map F ??? : F(?) ? F(? ) for any ? ? ? .</p><p>This data satisfies that F ??? = id for all ? ? P X and F ??? = F ? ?? ? F ??? whenever ? ? ? ? ?.</p><p>Given a weighted cellular sheaf F, we define a chain complex as follows. For each k = 0, 1, . . . we set</p><formula xml:id="formula_33">C k (X; F) = dim(?)=k F(?) .</formula><p>Further, we define coboundary maps ? k : C k (X; F) ? C k+1 (X; F) by</p><formula xml:id="formula_34">? k (x) ? = dim(?)=k [? : ? ]F ??? (x ? ),</formula><p>where [? : ?] : P X ? P X ? {0, ?1} is a signed incidence relation (see <ref type="bibr">Definition 35)</ref>.</p><p>Given Hilbert spaces V and W and a bounded linear map T : V ? W , the adjoint of T is the unique bounded linear map T : W ? V satisfying that for all v ? V and all w ? W :</p><formula xml:id="formula_35">w, T v = T w, v . Definition 39. Let C ? = C 0 ? C 1 ? . .</formula><p>. be a chain complex of Hilbert spaces. The Hodge Laplacian is the graded linear map defined in degree k as ? k :</p><formula xml:id="formula_36">C k ? C k with ? k = (? k ) ? k + ? k?1 (? k?1 ) . When C ? = C 0 ? C 1 ? . .</formula><p>. is the complex of cochains of a weighted cellular sheaf F, the Hodge Laplacian is called the sheaf Laplacian of X.</p><p>In particular, the Hodge Laplacian of a cell complex can be obtained by considering the constant weighted cellular sheaf with a standard inner product. That is the cellular sheaf where F(?) = R and the restriction maps F ??? = id. A normalised version of it can also be obtained by carefully adjusting the inner products associated with each F(?). This normalisation is always possible for finite cell complexes (see <ref type="bibr">Hansen and Ghrist [36]</ref> for details). This is very useful because finding normalised versions of Hodge Laplacians is not trivial and even on simplicial complexes <ref type="bibr" target="#b62">[63]</ref>, the process of constructing one can be quite involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Convolutional Operators</head><p>One can use the general sheaf Laplacian to define linear, local diffusion operators which, in the GNN literature, are broadly addressed as 'convolutional'. Diffusion operators built from the standard graph Laplacian have been employed in several graph neural network architectures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>. Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> have introduced convolutional operators on SCs by employing the Hodge Laplacian <ref type="bibr" target="#b62">[63]</ref>, interpreted as a generalisation of the graph Laplacian. As for cell complexes, here we focus, for simplicity, on the case of a constant sheaf with a standard inner product in R n . Then, the matrix representations of ? k and (? k ) * are the signed incidence matrices B T k and B k , respectively. Therefore, the Hodge Laplacian can be written in matrix form as</p><formula xml:id="formula_37">L k = B T k B k + B k+1 B T k+1</formula><p>. A convenient way to define a convolutional operator on cochains is by designing a learnable filter parameterised as a polynomial of the Hodge Laplacian. This approach has been already adopted on graphs using the standard graph Laplacian <ref type="bibr" target="#b21">[22]</ref> or more general sheaf Laplacians <ref type="bibr" target="#b36">[37]</ref>, and also on simplicial complexes <ref type="bibr" target="#b24">[25]</ref>. The advantage of this approach is that of retaining a connection with spectral constructions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> while not requiring any explicit diagonalisation of the operator itself. A polynomial convolutional filter of this kind, when applied to the p-cells of a d-cell complex, would take the form</p><formula xml:id="formula_38">H t+1 = ? R r=0 L r p H t W t+1 r = ? H t W t+1 0 + R r=1 L r p H t W t+1 r .<label>(3)</label></formula><p>where H t is a matrix gathering p-cell representations at layer t, W t+1 r are learnable parameters, and ? summarises the application of a bias term and a non-linearity.</p><p>Proof of Theorem 19. While the structure of the boundary matrices is more flexible in a cell complex than in a simplicial complex, algebraically, the proof is very similar to the proof showing MPSNs generalise simplicial convolutions in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref>. We offer here a high-level view of the proof and refer the reader to Appendix C of their paper for a detailed version.</p><p>For a generic p-cell ?, and r &gt; 0, the application of the r-power of the Hodge Laplacian effectively induces an information flow from a generalised notion of r-upper and r-lower adjacent p-cells, i.e. p-cells ? such that there exists a sequence of upper-(respectively, lower-) adjacent p-cells [? 0 , ? 1 , . . . , ? r ] such that ? 0 = ?, ? r = ? . Therefore, the convolution described above is easily interpreted in terms of a cellular message passing scheme which only exchanges ?and ?-messages. Intuitively, the upper-and lower-message functions would share their parameters W t+1 r and compute messages by linearly projecting the representations of upper-and lower-adjacent cells (ignoring any information in shared (co)boudaries). Such messages would then be aggregated by summation into an overall message, taken as input by an update function parameterised by ? and W t+1 0 . A formal derivation of how the equation <ref type="formula" target="#formula_38">(3)</ref> is rewritten in terms of cellular message passing would closely follow the one provided in Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> for SCs, and we therefore refer readers to Section C of such work.</p><p>Normalised versions of the aforementioned Hodge Laplacian can be used to design a model in the spirit of the popular Graph Convolutional Network of Kipf and Welling <ref type="bibr" target="#b46">[47]</ref>. To this aim, one could resort to normalised sheaves as suggested in <ref type="bibr" target="#b35">[36]</ref>. Additionally, one could explicitly make use of the (co)boundary operators defined in Section C to let information flow from lower-and higherdimensional cells contained in cell (co)boundaries, effectively extending the Simplicial Convolutional Networks recently introduced in Bunch et al. <ref type="bibr" target="#b12">[13]</ref>. We defer these research directions to future developments of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental details and additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Used Code Assets</head><p>The model has been implemented in PyTorch <ref type="bibr" target="#b58">[59]</ref> and by building on top of the PyTorch Geometric library <ref type="bibr" target="#b27">[28]</ref>. Lifting operations use the graph-tool <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Used Computer Resources</head><p>All experiments were run on NVIDIA ? GPUs. Experiments on SR, Mol-HIV and molecular TUDatasets were run on Tesla V100 GPUs with 5,120 CUDA cores and 16GB GPU memory on a p3.16xlarge Amazon Web Services (AWS) Elastic Cloud (EC) 2 instance. Experiments on the social TUDatasets were run on the same GPU devices but with 32GB HBM2 memory mounted on an HPC cluster. All remaining experiments, that is CSL, RingTransfer and ZINC, were run on a machine with TITAN Xp GPUs with 12GB GPU memory and an Intel ? Xeon ? CPU E5-2630 v4 @ 2.20GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Model</head><p>In all cases, we apply our model to the 2-dimensional cell complexes obtained by ring-lifting the original graphs, i.e. we consider nodes and edges as 0and 1-cells, and each induced cycle of size up to k as a 2-cell. 0-cell are always endowed with the original node features or learnt node embeddings, if the benchmark prescribes so. The way higher dimensional cells are assigned features depend on the specific benchmark.</p><p>Throughout all experiments, we employ cellular message passing layers which update the representation of p-cell ? as follows: consist of a dense layer followed by a non-linearity. We neglect messages from cofaces and downadjacent cells consistently with Theorem 7. We name an architecture which stacks L layers of this form as 'Cell Isomorphism Network' (CIN). Readout operations are performed as follows. First, for p ? 0, 1, 2, we compute the joint representation h p of the cells at dimension p by applying a mean or sum readout operation. Then, for complex K, we compute an overall representation h K = p=0,1,2 MLP R,p h p , where each MLP R,p is parameterised as a single dense layer followed by a non-linearity. Complex-wise predictions are obtained by a final dense layer preceded by dropout. All MLP layers internally apply Batch Normalization <ref type="bibr" target="#b41">[42]</ref> and ReLU activations, unless otherwise specified. All training procedures are performed with the Adam optimiser <ref type="bibr" target="#b45">[46]</ref>.</p><formula xml:id="formula_39">h t+1 ? = MLP t U,p MLP t B,p (1 + B )h t ? + ? ?B(?) h t ? MLP t ?,p (1 + ? )h t ? + ? ?N ? (?),??C(?,? ) MLP t M,p h t ? h t ?<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Additional experimental details</head><p>CSL Each of the 150 4-regular graphs in the CSL dataset comprises N = 48 nodes and is characterized by skip number parameter C ? C = {2, <ref type="bibr">3, 4, 5, 6, 9, 11, 12, 13, 16}</ref>. Parameters N and C determine the isomorphism class G N,C of each graph, which we seek to predict. The number of possible classes is |C| = 10. We employ the same stratified dataset folds in Dwivedi et al. <ref type="bibr" target="#b23">[24]</ref>. Consistently with the adopted reference procedure, 0-cells share the same learnt embedding, while 1and 2-cells are endowed with the sum of the embeddings of the included 0-cells. As for the optimisation procedure, we set the batch size to 12 and the initial learning rate of 5e-4. which is halved whenever the validation performance does not improve after a patience value of 20. The training is early stopped as soon as it falls below 1e-6, at which step we measure the model test accuracy. The size of hidden layers in our model is set to 160 and we stack 3 cellular message passing layers. In this benchmark, we replace Batch Normalisation with Layer Normalization <ref type="bibr" target="#b4">[5]</ref>, as the former wsa observed to produce instabilities in the optimisation procedure. At each dimension, cell embeddings are readout via averaging.</p><p>SR These experiments are run in double floating point precision and with untrained models. We initialise the cell complexes associated with SR graph by populating 0-cells with constant, scalar, unitary signal, and 1and 2-dimensional cells with the sum of the contained 0-cells. Complexes are embedded in a 16-dimensional space and, coherently with Bodnar et al. <ref type="bibr" target="#b7">[8]</ref> and Bouritsas et al. <ref type="bibr" target="#b9">[10]</ref>, if the L 2 -distance between the embeddings of two complexes is larger than = 0.01, we deem the corresponding graphs to be non-isomorphic. We confirmed the validity of the chosen threshold by numerically verifying that, under the described experimental setting, each SR graph in our datasets is deemed isomorphic w.r.t. a counterpart obtained by randomly permuting its nodes. We run a CIN model with 3 cellular message passing layers, whose hidden layers comprise 16 units. At each dimension, cell embeddings are readout via summation. As the number of induced cycles of a certain size may be enough to tell apart non-isomorphic SR graphs (see <ref type="table" target="#tab_3">Table 4</ref>), an MLP with sum readouts represents a strong baseline, which we additionally run. Such a model applies non-linear dense layers at each cell dimension, and then performs readout operations as in CIN. We set the size of hidden layers to 256, while the final complex embeddings are embedded in a 16-dimensional space as in our model. Both approaches are equipped with ELU nonlinearities <ref type="bibr" target="#b17">[18]</ref>.</p><p>RingTransfer This benchmark dataset comprises 5, 000 training graphs. Each graph is randomly associated with one of the 5 independent labels, which are also assigned as node features to source nodes. Labels are unifomly represented. On this benchmark we run a CIN model with 3 stacked message passing layers, independently on the ring size. The hidden size of the layers is set to 64 and we do not apply Batch Normalisation. Differently than in the other benchmarks, we do not need to perform readout operations to compute complex-wise embeddings; instead, we simply take the representation of the 0-cell corresponding to node target at the last layer of the architecture and use it to predict the label of source. GIN models have always k 2 standard message passing layers with hidden size 64. The models are trained with an initial learning rate of 10 ?3 , decayed by a factor of 0.5 and a patience of 5 epochs. The training is stopped when the learning rate drops below 10 ?5 .</p><p>TUD Amongst the datasets from this benchmarking suite: the task in MUTAG is to recognise mutagenic molecular compounds for potentially marketable drug <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b60">61]</ref>; the one in PTC is to recognise the chemical compounds according to carcinogenicity on rodents <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48]</ref>; PROTEINS is about to categorising proteins into enzyme and non-enzyme structures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>; NCI1 and NCI109 deal with identifying chemical compounds against the activity of non-small lung cancer and ovarian cancer cells, respectively <ref type="bibr" target="#b69">[70]</ref>; REDDIT-BINARY or RDT-B is a social network dataset where the task is to predict whether a graph belongs to a question-answer-based community or a discussionbased community. On these datasets, we followed the approach in Xu et al. <ref type="bibr" target="#b73">[74]</ref>, which prescribes to run a 10-fold cross-validation procedure and report the maximum of the average validation accuracy across folds. Consistently with such work, we train our model starting from an initial learning rate which is decayed after a fixed amount of epochs and we apply cell-readout operations on the multiscale representations obtained by a Jumping Knowledge scheme <ref type="bibr" target="#b72">[73]</ref> by performing averaging or summation depending on the dataset, still in accordance with Xu et al. <ref type="bibr" target="#b73">[74]</ref>. We ran a grid-search to tune batch size, hidden dimension, dropout rate, initial learning rate along with its decay steps and strengths, feature initialisation strategy of higher-dimensional cells (mean vs. sum), inclusion of coboundary features in ?-messages, number of layers and the dropout position (immediately after readout on cells ("cell read.") or the final readout on the complex ("comp read.")). We report the hyperparameter configurations in <ref type="table" target="#tab_8">Table 8</ref>. We finally report that we did not employ Batch Normalization layers in RDT-B since they were observed to produce severe instabilities in the training procedure.</p><p>ZINC The ZINC benchmarks dataset have been constructed by the ZINC database provided by the Irwin and Shoichet Laboratories in the Department of Pharmaceutical Chemistry at the University of California, San Francisco (UCSF) <ref type="bibr" target="#b67">[68]</ref>. Each graph represents a molecule, with node features indicating the atom type and edge features the type of chemical bond between two atoms. Graph targets correspond to the penalised water-octanol partition coefficient -logP <ref type="bibr" target="#b32">[33]</ref>. In these experiments, rings up to size k = 18 are mapped to 2-cells, and are assigned feature values as the sum of the learnable atom embeddings for the included 0-cells (nodes). 1-cells are assigned learnable bond embeddings if edge-features are considered, otherwise we apply the same policy employed for 2-cells. We employ the same predefined training, validation and test splits as in Dwivedi et al. <ref type="bibr" target="#b23">[24]</ref>, and train our model by minimising the the Mean Absolute Error (MAE) loss on the train targets. As prescribed by the benchmark, the optimisation procedure employs a batch size of 128 and a dynamic learning rate which starts from 10 ?3 and is halved whenever the validation loss does not improve after a patience value we set to 20. The training is early stopped as soon as it falls below 10 ?5 . We repeat the training with 10 different weight initialisations and report the mean of the test MAEs at the time of early stopping. In accordance with the best performing baselines, our CIN model does not use any dropout, and stacks 4 message passing layers with hidden size 128. In order to enforce the parameter budget we reduce the size of hidden layers to 48 and only perform 2 message passing layers. At each dimension, cell embeddings are readout via summation.</p><p>Mol-HIV This dataset comprises 41127 molecular graphs associated with a binary label representing their capacity to inhibit HIV replication. The benchmark provides predefined train, validation and test sets based on the "scaffold splitting" procedure, which separates molecules based on their two-dimensional structural frameworks <ref type="bibr" target="#b39">[40]</ref>. As in ZINC, graphs are attributed at the level of nodes and edges, and we directly employ the atom and bond embedding layers provided by the benchmarking platform 6 to populate 0and 1-dimensional cells. Rings of size up to k = 6 are considered as 2-cells, and are endowed with feature vectors with the same procedure as in ZINC. The value k = 6 has been chosen from the pool of values {6, 8, 18} as it yielded the highest validation performance. The architecture hyperparameters are directly replicated from the HIMP model in Fey et al. <ref type="bibr" target="#b26">[27]</ref>: 2 message passing layers, dropout rate of 0.5 applied after each layer, 64 as size of hidden layers, constant learning rate of 10 ?4 , batch size of 128. We train our model for 150 epochs. The small CIN model is obtained by simply reducing the size of hidden layers to 48. At each dimension, cell embeddings are readout via averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Ablation study on ZINC</head><p>We end this section by reporting the results of an ablation study we conducted on the ZINC dataset to appreciate the contribution of including rings. In <ref type="table">Table 9</ref> we show the average test MAE for two additional CIN models: "CIN No-Rings small" and "CIN No-Rings", which differ from their original counterparts in that they neglect 2-cells when performing message passing. In these experiments we always make use of edge features and use the same hyperparameters as our original CIN model. <ref type="table">Table 9</ref>: ZINC Ablation with edge features. The ablation shows the benefits of integrating rings into the message passing procedure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Closed two-dimensional disks are glued to the boundary of the rings present in the graph (left). The result is a 2D regular cell complex (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>A sphere and an empty tetrahedron. The latter is also a simplicial complex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Hierarchical depiction of the message passing procedure. Orange arrows indicate boundary messages received by cells ? and ? , while blue ones show upper messages received by cells ? and ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>RingTransfer Results. Accuracy is over 5 balanced classes. A score of 0.2 is equivalent to a random guess. Error bars show the min and max. Our model obtains high-scores in average even for large rings despite using only three layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 22 .</head><label>22</label><figDesc>A cellular colouring c refines a cellular colouring d, denoted by c d, if for all cell complexes X and Y and all ? ? P X and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>.</head><label></label><figDesc>By unrolling the hash function two steps in time, we obtain b 2t+1 ? We need to prove that b 2t+1 ? (?) = b 2t+1 ? (? ) also holds. For the sake of contradiction, assume b 2t+1 ? (?) = b 2t+1 ? (? ). Then there exists a pair of colours (C 0 , C 1 ) that shows up (without loss of generality) more times in b 2t+1 ? (?) than in b 2t+1 ? (? ). For simplicity, we also assume b 2t+1 ? = C 0 = b 2t+1 ? as this special case can be easily treated separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>b 2t+1 ?1 = C 1 .</head><label>?11</label><figDesc>Then b 2t+2 ?1 = b 2t+2 ?2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>(Left) A pair of non-isomorphic graphs indistinguishable by WL, but distinguishable by CWL with a clique complex, ring or cycle-based lifting. (Right) A pair of non-isomorphic molecular graphs (Decalin and Bicyclopentyl) indistinguishable by WL but distinguishable by CWL with a ring-based or cycle-based lifting. The node colours show the stable colouring reached by WL. In the second case, b 2t+1 ?2 = C 1 , which implies |A X (? 1 )| &gt; |A Y (? 2 )| ? 0 and b 2t+1 ?1 = C 1 . Then, the difference in the size of the multi-sets is made by the number of times C 0 shows up in A X (? 1 ) and A Y (? 2 ), respectively. By Proposition 25, all k-cells ? with k &gt; 0 and b 2t+1 ? = C 0 must have a fixed boundary size |B(?)| = n. Because each cell ? ? B(?) is upper adjacent with every other cell in B(?), b 2t+1 ? appears n ? 1 times in the tuples inside b 2t+1 ? (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Finally, applying the induction hypothesis, we have that a t ? = a t ? , a t B (?) = a t B (? ), a t ? (?) = a t ? (? ) and a t ? (?) = a t ? (? ). Then a t+1 ? = a t+1 ? .Proof of Theorem 13. Consider the map f : G ? X , a skeleton-preserving lifting transformation from the space of graphs G, to the space of regular cell complexes X . Let g G : V G ? P f (G) (0) be the graph isomorphism associated to f between the vertices of G and the 0-cells of f (G) for all G ? G. Let c G,t be the WL colouring of graph G at iteration t and a f (G),t the colouring of f (G)<ref type="bibr" target="#b0">(1)</ref> induced by the isomorphism g G (i.e a f (G)<ref type="bibr" target="#b0">(1)</ref> ,t g(v)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>The two SR graphs in family SR<ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2)</ref>: Rook's 4?4 (left) and Shrikhande (right). The 3-WL test is not able to deem them as non-isomorphic. Contrary to the Shrikhande graph, Rook's graph possesses 4-cliques. The Shrikhande graph, however, features 5-rings, not present in Rook's. Instances of these substructures are marked in blue. With appropriate lifting procedures, CWL can distinguish between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>1 ), and ? 1 , ? 2 are simplices}} = {{(b t+n+1 ?1 , b t+n+1 ?2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Python library and are parallelised via Joblib 5 . PyTorch, NumPy, SciPy and Joblib are made available under the BSD license, Matplotlib under the PSF license, graph-tool under the GNU LGPL v3 license. PyTorch Geometric is made available under the MIT license.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy on CSL.</figDesc><table><row><cell>Method</cell><cell>Mean</cell><cell>Min</cell><cell>Max</cell></row><row><cell>MP-GNNs</cell><cell>10.000?0.000</cell><cell>10.000</cell><cell>10.000</cell></row><row><cell>RingGNN</cell><cell>10.000?0.000</cell><cell>10.000</cell><cell>10.000</cell></row><row><cell>3WLGNN</cell><cell cols="3">97.800?10.916 30.000 100.000</cell></row><row><cell cols="4">CIN (Ours) 100.000?0.000 100.000 100.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>TUDatasets. The first section of the table includes the accuracy of graph kernel methods, while the second includes GNNs. The top three are highlighted by First, Second, Third. ? 6.1 68.2 ? 5.6 77.0 ? 4.3 83.6 ? 1.4 84.0 ? 1.6 75.6 ? 3.7 52.7 ? 3.1 92.4 ? 2.1</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell cols="2">PROTEINS NCI1</cell><cell>NCI109</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>RDT-B</cell></row><row><cell>RWK [29]</cell><cell>79.2?2.1</cell><cell>55.9?0.3</cell><cell>59.6?0.1</cell><cell>&gt;3 days</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">GK (k = 3) [64] 81.4?1.7</cell><cell>55.7?0.5</cell><cell>71.4?0.3</cell><cell>62.5?0.3</cell><cell>62.4?0.3</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PK [58]</cell><cell>76.0?2.7</cell><cell>59.5?2.4</cell><cell>73.7?0.7</cell><cell>82.5?0.5</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>WL kernel [65]</cell><cell>90.4?5.7</cell><cell>59.9?4.3</cell><cell>75.0?3.1</cell><cell>86.0?1.8</cell><cell>N/A</cell><cell>73.8?3.9</cell><cell>50.9?3.8</cell><cell>81.0?3.1</cell></row><row><cell>DCNN [3]</cell><cell>N/A</cell><cell>N/A</cell><cell>61.3?1.6</cell><cell>56.6?1.0</cell><cell>N/A</cell><cell>49.1?1.4</cell><cell>33.5?1.4</cell><cell>N/A</cell></row><row><cell>DGCNN [76]</cell><cell>85.8?1.8</cell><cell>58.6?2.5</cell><cell>75.5?0.9</cell><cell>74.4?0.5</cell><cell>N/A</cell><cell>70.0?0.9</cell><cell>47.8?0.9</cell><cell>N/A</cell></row><row><cell>IGN [52]</cell><cell cols="2">83.9?13.0 58.5?6.9</cell><cell>76.6?5.5</cell><cell>74.3?2.7</cell><cell>72.8?1.5</cell><cell>72.0?5.5</cell><cell>48.7?3.4</cell><cell>N/A</cell></row><row><cell>GIN [74]</cell><cell>89.4?5.6</cell><cell>64.6?7.0</cell><cell>76.2?2.8</cell><cell>82.7?1.7</cell><cell>N/A</cell><cell>75.1?5.1</cell><cell>52.3?2.8</cell><cell>92.4?2.5</cell></row><row><cell>PPGNs [53]</cell><cell>90.6?8.7</cell><cell>66.2?6.6</cell><cell>77.2?4.7</cell><cell>83.2?1.1</cell><cell>82.2?1.4</cell><cell>73.0?5.8</cell><cell>50.5?3.6</cell><cell>N/A</cell></row><row><cell cols="2">Natural GN [21] 89.4?1.6</cell><cell>66.8?1.7</cell><cell>71.7?1.0</cell><cell>82.4?1.3</cell><cell>N/A</cell><cell>73.5?2.0</cell><cell>51.3?1.5</cell><cell>N/A</cell></row><row><cell>GSN [10]</cell><cell cols="3">92.2 ? 7.5 68.2 ? 7.2 76.6 ? 5.0</cell><cell cols="2">83.5 ? 2.0 N/A</cell><cell cols="3">77.8 ? 3.3 54.3 ? 3.3 N/A</cell></row><row><cell>SIN [8]</cell><cell>N/A</cell><cell>N/A</cell><cell>76.4 ? 3.3</cell><cell cols="2">82.7 ? 2.1 N/A</cell><cell cols="3">75.6 ? 3.2 52.4 ? 2.9 92.2 ? 1.0</cell></row><row><cell>CIN (Ours)</cell><cell>92.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>All methods</cell><cell>All methods</cell></row></table><note>ZINC (MAE), ZINC-FULL (MAE) and Mol-HIV (ROC-AUC).Method ZINC ? ZINC-FULL ? MOLHIV ? No Edge Feat. With Edge Feat.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Number of cycles and induced cycles (rings) on the SR graphs in family SR<ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2)</ref>.</figDesc><table><row><cell>Graph ? / Size ?</cell><cell cols="2">3 (Tri.) 4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>Rook's 4?4 (cycles)</cell><cell>32</cell><cell cols="5">60 288 1,248 4,032 11,952</cell></row><row><cell>Shrikhande (cycles)</cell><cell>32</cell><cell cols="5">60 288 1,248 4,032 11,688</cell></row><row><cell>Rook's 4?4 (rings)</cell><cell>32</cell><cell>36</cell><cell>0</cell><cell>96</cell><cell>0</cell><cell>72</cell></row><row><cell>Shrikhande (rings)</cell><cell>32</cell><cell cols="2">12 96</cell><cell>64</cell><cell>0</cell><cell>36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Wall-clock training and evaluation times on ZINC; mean, std over 10 runs (seconds).</figDesc><table><row><cell>Model</cell><cell>Training (Epoch)</cell><cell>Eval (Train)</cell><cell>Eval (Val)</cell><cell>Eval (Test)</cell></row><row><cell>GIN</cell><cell>4.582 ? 0.012</cell><cell cols="3">3.138 ? 0.071 0.310 ? 0.002 0.309 ? 0.001</cell></row><row><cell>GIN-small</cell><cell>3.737 ? 0.012</cell><cell cols="3">3.070 ? 0.058 0.304 ? 0.002 0.303 ? 0.003</cell></row><row><cell>CIN</cell><cell>10.828 ? 0.059</cell><cell cols="3">4.679 ? 0.051 0.470 ? 0.002 0.471 ? 0.003</cell></row><row><cell>CIN-small</cell><cell>7.082 ? 0.041</cell><cell cols="3">3.682 ? 0.056 0.365 ? 0.002 0.373 ? 0.030</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Wall-clock training and evaluation times on ZINC-FULL; mean, std over 10 runs (seconds).</figDesc><table><row><cell>Model</cell><cell>Training (Epoch)</cell><cell>Eval (Train)</cell><cell>Eval (Val)</cell><cell>Eval (Test)</cell></row><row><cell>GIN</cell><cell>106.268 ? 1.991</cell><cell>73.051 ? 1.742</cell><cell cols="2">7.874 ? 0.174 1.618 ? 0.039</cell></row><row><cell>GIN-small</cell><cell>87.581 ? 2.343</cell><cell>71.160 ? 1.865</cell><cell cols="2">7.714 ? 0.206 1.583 ? 0.037</cell></row><row><cell>CIN</cell><cell cols="4">249.334 ? 17.927 107.510 ? 1.637 11.759 ? 0.642 2.398 ? 0.028</cell></row><row><cell>CIN-small</cell><cell>163.282 ? 8.016</cell><cell>85.342 ? 2.637</cell><cell cols="2">9.251 ? 0.431 1.876 ? 0.044</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Wall-clock lifting times, mean and std over 5 runs (seconds). ? 16.50 3549.16 ? 7.73 1782.41 ? 3.84 918.38 ? 3.46 492.77 ? 6.13 383.92 ? 3.30</figDesc><table><row><cell>Dataset ? / Processes ?</cell><cell>Seq.</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>ZINC (12k)</cell><cell>320.27 ? 0.54</cell><cell>169.95 ? 0.32</cell><cell>84.90 ? 0.21</cell><cell>43.38 ? 0.07</cell><cell>23.17 ? 0.68</cell><cell>18.59 ? 0.68</cell></row><row><cell>Mol-HIV (41k)</cell><cell>1178.98 ? 3.90</cell><cell>635.58 ? 0.83</cell><cell cols="3">319.01 ? 0.40 164.26 ? 0.52 86.92 ? 0.77</cell><cell>60.62 ? 2.05</cell></row><row><cell>ZINC-FULL (250k)</cell><cell>6805.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter configurations on TUDatasets. Drop. Pos. cell read. comp read. comp read. comp read. comp read. comp read. comp read. comp read.</figDesc><table><row><cell>Hyperparameter</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>RDT-B</cell></row><row><cell>Batch Size</cell><cell>32</cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>32</cell><cell>128</cell><cell>128</cell><cell>32</cell></row><row><cell>Initial LR</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.001</cell></row><row><cell>LR Dec. Steps</cell><cell>20</cell><cell>50</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>50</cell><cell>20</cell><cell>50</cell></row><row><cell>LR Dec. Strength</cell><cell>0.5</cell><cell>0.9</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Hidden Dim.</cell><cell>64</cell><cell>16</cell><cell>32</cell><cell>16</cell><cell>64</cell><cell>16</cell><cell>64</cell><cell>64</cell></row><row><cell>Drop. Rate</cell><cell>0.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.5</cell><cell>0.0</cell></row><row><cell>Initialisation</cell><cell>sum</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell><cell>mean</cell></row><row><cell>Cobound. in ?-msg</cell><cell>N</cell><cell>N</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell></row><row><cell>Num. Layers</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use these terms interchangeably. For the latter, the C stands for "closure-finite", and the W for "weak" topology. The term was coined by Whitehead<ref type="bibr" target="#b71">[72]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Data available at: http://users.cecs.anu.edu.au/~bdm/data/graphs.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://graph-tool.skewed.de/ 5 https://joblib.readthedocs.io/en/latest/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/snap-stanford/ogb/blob/master/ogb/graphproppred/mol_encoder. py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors declare no competing interests. We are also grateful to Ben Day, Gabriele Corso and Nikola Simidjievski for their helpful feedback. We would also like to thank Vijay P. Dwivedi and Chaitanya K. Joshi for clarifying certain aspects of their Benchmarking GNNs <ref type="bibr" target="#b23">[24]</ref> work, and to Muhammet Balcilar for signalling a numerical precision issue in early SR graphs experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MAE</head><p>GatedGCN <ref type="bibr" target="#b10">[11]</ref> 0.363?0.009 GIN <ref type="bibr" target="#b73">[74]</ref> 0.252?0.014 PNA <ref type="bibr" target="#b19">[20]</ref> 0.188?0.004 DGN <ref type="bibr" target="#b5">[6]</ref> 0.168?0.003 HIMP <ref type="bibr" target="#b26">[27]</ref> 0.151?0.006 GSN <ref type="bibr" target="#b9">[10]</ref> 0.108?0.018 In line with our expectations, we observe a decrease in the overall performance of both versions. They are outperformed by the GSN <ref type="bibr" target="#b9">[10]</ref> and HIMP <ref type="bibr" target="#b26">[27]</ref> models, which either include structural information from cycle isomorphism counting (GSN) or additionally perform message passing on the Junction Tree representation of molecules (where rings are considered as nodes). At the same time, we observe "CIN No-Rings" still outperforms all other ring-agnostic baselines. We attribute such strong performance to the more natural and richer modelling of edge signals (1-cells): this model updates edge representations at each layer as a function of the present representations and those of the incident nodes (0-cells). As an additional confirmation of this hypothesis, we implemented an architecture which replicates the same structure as "CIN No-Rings", but replaces cellular message passing with GIN-E layers <ref type="bibr" target="#b40">[41]</ref>. These layers extend the message passing scheme in GIN by accounting for edge features. We refer to this model as "GIN-E Custom". Contrary to CIN, it does not update edge representations and performs readout only at the node level. As expected, we observed that "GIN-E Custom" is outperformed by all our models, including, in particular, "CIN No-Rings".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIN</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On weisfeiler-leman invariance: Subgraph counts and related graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fuhlbr?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>K?bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Verbitsky</surname></persName>
		</author>
		<idno>0022-0000</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expressive power of invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waiss</forename><surname>Azizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lelarge</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lxHgXYN4bwl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li?</surname></persName>
		</author>
		<title level="m">Directional graph networks. ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal Listing of Cycles and st-Paths in Undirected Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><forename type="middle">E</forename><surname>Birmel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Grossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Pisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romeo</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Sacomoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting><address><addrLine>New Orleans, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weisfeiler and lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng Soon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<title level="m">Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simplicial 2-complex convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bunch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Topological Data Analysis and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arboricity and subgraph listing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takao</forename><surname>Nishizeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="210" to="223" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A (sub)graph isomorphism algorithm for matching large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1367" to="1372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simplicial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Ebli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gard</forename><surname>Spreemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Topological Data Analysis and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amortized ?(|V|) -Delay Algorithm for Listing Chordless Cycles in Undirected Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Grossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romeo</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Sacomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-France</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22th Annual European Symposium on Algorithms</title>
		<meeting><address><addrLine>Wroclaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="volume">8737</biblScope>
			<biblScope unit="page" from="418" to="429" />
		</imprint>
	</monogr>
	<note>Algorithms ESA</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Graph Representation Learning and Beyond (GRL+) Workhop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Utilizing graph machine learning within drug discovery and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothish</forename><surname>Jamasb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrude</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake P Taylor-King</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>1477-4054</idno>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Principled simplicial neural networks for trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Glaze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Mitchell</forename><surname>Roddenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Segarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjam?n</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cell complex neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Istvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghada</forename><surname>Zamzmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Topological Data Analysis and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Simplicial complex representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghada</forename><surname>Zamzmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanting</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04046</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Toward a spectral theory of cellular sheaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied and Computational Topology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="315" to="358" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sheaf neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gebhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020 Workshop on Topological Data Analysis and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Algebraic topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Hatcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge Univ. Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multimodal graph-to-graph translation for molecular optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Derivation and validation of toxicophores for mutagenicity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Bursi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Subgraph matching kernels for attributed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6483</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Sur le probleme des courbes gauches en topologie. Fundamenta mathematicae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casimir</forename><surname>Kuratowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="271" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">RDKit: Open-source cheminformatics software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Graph Representation Learning and Beyond (GRL+) Workhop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Iam graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A survey on the expressive power of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Random walks on simplicial complexes and the normalized Hodge 1-Laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Lippner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="391" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTAT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Synthesis of the first topologically non-planar molecule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><forename type="middle">E</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Maggio</surname></persName>
		</author>
		<idno>0040-4039</idno>
	</analytic>
	<monogr>
		<title level="j">Tetrahedron Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="287" to="290" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Vector diffusion maps and the connection laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hau-Tieng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">ZINC 15 -ligand discovery for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.5b00559</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.5b00559" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTI Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H C</forename><surname>Whitehead</surname></persName>
		</author>
		<idno>doi: bams/1183513543</idno>
	</analytic>
	<monogr>
		<title level="j">Combinatorial homotopy. I. Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="245" />
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
