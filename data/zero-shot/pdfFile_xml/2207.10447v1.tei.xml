<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Bai</surname></persName>
							<email>haotianwhite@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">The Chinese Univeristy of Hong Kong (Shenzhen)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">The Chinese Univeristy of Hong Kong (Shenzhen)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Weakly Supervised Object Localization</term>
					<term>Image Context Mod- eling</term>
					<term>Class Activation Mapping</term>
					<term>Transformer</term>
					<term>Semantic Propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Recent studies leverage the advantage of self-attention in visual Transformer for long-range dependency to re-active semantic regions, aiming to avoid partial activation in traditional class activation mapping (CAM). However, the long-range modeling in Transformer neglects the inherent spatial coherence of the object, and it usually diffuses the semantic-aware regions far from the object boundary, making localization results significantly larger or far smaller. To address such an issue, we introduce a simple yet effective Spatial Calibration Module (SCM) for accurate WSOL, incorporating semantic similarities of patch tokens and their spatial relationships into a unified diffusion model. Specifically, we introduce a learnable parameter to dynamically adjust the semantic correlations and spatial context intensities for effective information propagation. In practice, SCM is designed as an external module of Transformer, and can be removed during inference to reduce the computation cost. The object-sensitive localization ability is implicitly embedded into the Transformer encoder through optimization in the training phase. It enables the generated attention maps to capture the sharper object boundaries and filter the object-irrelevant background area. Extensive experimental results demonstrate the effectiveness of the proposed method, which significantly outperforms its counterpart TS-CAM on both CUB-200 and ImageNet-1K benchmarks. The code is available at https://github.com/164140757/SCM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Weakly supervised object localization (WSOL), which learns to localize objects by only using image-level labels, has attracted much attention recently for its low annotation cost. The representative study of WSOL, Class Activation Map (CAM) <ref type="bibr" target="#b38">[39]</ref> generates localization results using features from the last convolutional layer. However, the model trained for classification usually focuses on the discriminative regions, resulting insufficient activation for object localization.</p><p>To solve such an issue, there are many CNN-based methods have been proposed in the literature, including regularization <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>, adversarial training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, and divergent activation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, but the CNN's inherent limitation of local activation dampens their performance. Although discriminative activation is optimal for minimizing image classification loss, it suffers from the inability to capture object boundaries precisely. Recently, visual Transformer has succeeded in computer vision due to its superior ability to capture long-range feature dependency. Vision Transformer <ref type="bibr" target="#b26">[27]</ref> splits an input image into patches with the positional embedding, then constructs a sequence of tokens as its visual representation. The self-attention mechanism enables Transformer to learn long-range semantic correlations, which is pivotal for object localization. A representative study is Token Semantic Coupled Attention Map (TS-CAM) <ref type="bibr" target="#b8">[9]</ref> which replaces traditional CNN with Transformer and takes full advantage of long-range dependencies to solve the partial activation problem. It localizes objects by semantic-awarded attention maps from patch tokens. However, we argue that only using a Transformer is not an optimal choice in practice. Firstly, Transformer attends to long-range global dependency while inevitably it cannot capture local structure well, which is critical in describing the boundaries of objects. In addition, Transformer splits images into discrete patches. Thus it may not attend to the inherent spatial coherence of objects, which makes it unable to predict the complete activation. As shown in <ref type="figure" target="#fig_0">Fig.1(d)</ref>, the activation map obtained from TS-CAM captures the global structure. Still, it concentrates in a small semantic-rich region like the bird's upper body, failing to solve partial activation completely. Furthermore, we observe that the fur has no abrupt change in neighboring space, and its semantic context may favor propagating the activated regions to provide a more accurate result covering the whole body.</p><p>Inspired by this potential continuity, we propose a novel external module named Spatial Calibration Module (SCM), tailored for Transformers to produce activation maps with sharper boundaries. As shown in <ref type="figure" target="#fig_0">Fig.1</ref>(a)-(b), instead of directly applying Global Average Pooling (GAP) on semantic maps to calculate loss as TS-CAM <ref type="bibr" target="#b8">[9]</ref>, we insert an external SCM to refine both semantic and attention maps and then use the calibrated features to calculate the semantic loss. Precisely, it implicitly calibrates attention representation of Transformer and produces more meaningful activation maps to cover functional areas based on spatial and contextual coherence. Our core design, a unified diffusion model, is introduced to incorporate semantic similarities of patch tokens and their local spatial relations during training. While in the inference phase, SCM can be dropped out to maintain the model's simplicity, as shown in <ref type="figure" target="#fig_0">Fig.1(c)</ref>. Then, we use the calibrated Transformer backbone to predict the localization results by coupling SM and AM. The main contributions of this paper are as follows:</p><p>1. We propose a novel spatial calibration module (SCM) as an external Transformer module to solve the partial activation problem in WSOL by leveraging the spatial correlation. Specifically, SCM is designed to optimize Transformers implicitly and will be dropped out during inference. 2. We propose a novel information propagation methodology that provides a flexible way to integrate spatial and semantic relationships to enlarge the semantic-rich regions and cover objects completely. In practice, we introduce learnable parameters to adjust the diffusion range and filter the noise dynamically for flexible control and better adaptability. 3. Extensive experiments demonstrate that the proposed framework outperforms its counterparts in the two challenging WSOL benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weakly Supervised Object Localization.</head><p>The weakly supervised object localization aims to localize objects by solely image-level labels. The seminar work CAM <ref type="bibr" target="#b38">[39]</ref> demonstrates the effectiveness of localizing objects using feature maps from CNNs trained initially for classification. Despite its simplicity, CAM-based methods suffer from limited discriminative regions, which cannot cover objects completely. The field has focused on how to expand the activation with various attempts. Firstly, the dropout strategy is proposed to guide the model to attend to more significant regions. For instance, HaS <ref type="bibr" target="#b27">[28]</ref> hides patches in training images randomly to force the network to seek other relevant parts; CutMix <ref type="bibr" target="#b33">[34]</ref> adopts the same way to drop out patches but further augment the area of the patches with ground-truth labels to reduce information loss. Similarly, ADL <ref type="bibr" target="#b5">[6]</ref> adopts an importance map to maintain the informative regions' classification power. Instead of dropping out patches, people leverage the pixels correlations to fulfill objects as they often share similar patterns. SPG <ref type="bibr" target="#b36">[37]</ref> learns to sense more areas with similar distribution and expand the attention scope. I 2 C <ref type="bibr" target="#b37">[38]</ref> exploits inter-and-cross images' pixel-level consistency to improve the quality of localization maps. Furthermore, the predicted masks can be enhanced to become complete. GC-Net <ref type="bibr" target="#b17">[18]</ref> highlights tight geometric shapes to fit the masks. SPOL <ref type="bibr" target="#b29">[30]</ref> fuses shallow features and deep features from CNN that filter the background noise and generates sharp boundaries.</p><p>Instead of applying only CNN as the backbone for WSOL, Transformer can be another candidate to alleviate the problem of partial activation as it captures long-range feature dependency. A recent study TS-CAM <ref type="bibr" target="#b8">[9]</ref> utilizes attention maps from patches coupled with reallocated semantics to predict localization maps, surpassing most of its CNN counterparts in WSOL. Recent work LCTR [2] adopted a similar framework with Transformer while inserting their tailored module in each Transformer block to strengthen the global features. However, we observe that using Transformer alone cannot solve the partial activation completely as it fails to capture the local structure and ignores spatial coherence. What is more, it is cumbersome to insert a module for each Transformer block like LCTR <ref type="bibr" target="#b1">[2]</ref>. To address the issue, we propose a simple external module termed spatial calibration module (SCM) that calibrates Transformer by incorporating spatial and semantic relations to provide more complete feature maps and erase background noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Diffusion.</head><p>Pixels in natural images generally exhibit strong correlation, and constructing graph structure to capture such relationships has attracted much attention. In semantic segmentation, studies like <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> build graphs on images to obtain contextual information and long-term dependencies to model label distribution jointly. In image preprossessing, Gene et.al <ref type="bibr" target="#b2">[3]</ref> analyses graphs constructed from 2D images in spectral and succeeds in many traditional processing areas, including image compression, restoration filtering, and segmentation. The graph structure enables many classic graph algorithms and leads to new insights and understanding of image properties.</p><p>Similarly, in WSOL, the limited activation regions share semantic coherence with neighboring locations, making it possible to expand the area by information flow to cover objects precisely. In our study, we revise the classic Graph Diffusion Kernel (GDK) algorithm <ref type="bibr" target="#b12">[13]</ref> to infer complete pseudo masks based on partial activation results. GDK is initially adopted in graph analysis like social networks <ref type="bibr" target="#b0">[1]</ref>, search engines <ref type="bibr" target="#b18">[19]</ref>, and biology <ref type="bibr" target="#b24">[25]</ref> to inference pathway membership in genetic interaction networks. GDK's strategy to explore graphs via random walk inspires us to modify it to incorporate information from the image context, enabling dynamical adjustment by semantic similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section describes the Spatial Calibration Module (SCM), which is built by stacking multiple activation diffusion blocks (ADB). ADB consists of several submodules, including semantic similarity estimation, activation diffusion, diffuse matrix approximation, and dynamic filtering. At the end of the section, we show how to predict the final localization results by using the proposed framework during the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>In WSOL, the attention maps from models trained on image-level labels mainly concentrate on discriminative parts, which fail to cover the whole objects. Our proposed SCM aims to diffuse activation at small areas outwards to alleviate the partial activation problem in WSOL. In a broad view, the whole framework is supervised by image-level labels during training. As shown in <ref type="figure" target="#fig_0">Fig.1(b)</ref>, Transformer learns to calibrate both attention maps and semantic maps through the semantic loss from SCM implicitly. To infer the prediction, as described in <ref type="figure" target="#fig_0">Fig.1</ref>(c), we drop SCM and use the element-wise product of revised maps to localize objects.</p><p>As shown in <ref type="figure" target="#fig_1">Fig.2</ref>, an input image is split into N = H ? W patches with each represented as a token, where (H, W ) is the patch resolution. After grouping these patch tokens and CLS token into a sequence, we send it into I cascaded Transformer blocks for further representation learning. Similar as TS-CAM <ref type="bibr" target="#b8">[9]</ref>, to build the initial attention map F 0 ? R H?W , the self-attention matrix W i ? R (N +1)?(N +1) at i th layer is averaged over the multiple self-attention heads. Denote M i ? R H?W as attention weights that corresponds to the class token in W i , we average {M i } I i=1 across all intermediate layers to get the attention map F 0 of Transformer.</p><formula xml:id="formula_0">F 0 = 1 I I i=1 M i<label>(1)</label></formula><p>To obtain the semantic map S 0 ? R H?W ?C , where C denotes the number of categories, we extract all spatial tokens {t n } N n=1 from the last Transformer layer and then encode them by a convolution head,</p><formula xml:id="formula_1">S 0 = reshape(t 1 ...t N ) * k (2)</formula><p>where * is the convolution operation, k is a 3 ? 3 convolution kernel, and reshape(?) is an operation that converts a sequence of tokens into 2D feature maps. Then we send both F 0 and S 0 into SCM to refine them. As illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>, for the l th ADB, denote S l and F l as the inputs, and S l+1 and F l+1 as the outputs. Firstly, to guide the propagation, we estimate embedding similarity E between pairs of patches in S l . To enlarge activation F l , we apply E to diffuse F l towards the equilibrium status indicated by the inverse of Laplacian matrix L l . In practice, we re-activate F l by approximating (L l ) ?1 with Newton Shulz Iteration. Afterward, a dynamic filtering module is applied to remove over-diffused parts. Finally, the refined F l updates S l via an element-wise multiplication.</p><p>In general, by stacking multiple ADBs, the intensity of both maps is dynamically adjusted to balance semantic and spatial features. In the training phase, we apply GAP to S L to get classification logits and calculate semantic loss with the ground truth. During inference, SCM will be dropped out, and the element-wise product of newly extracted F 0 and S 0 is used to obtain the localization result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Activation Diffusion Block</head><p>In this subsection, we dive into Activation Diffusion Block (ADB). Under the assumption of continuity of visual content, we calculate the semantic and spatial relationships of patches in S L , then diffuse it outwards dynamically to alleviate the partial activation problem in WSOL.</p><p>Semantic Similarity Estimation. Within the l th activation diffusion block, l ? {1, 2, ..., L}, we need semantic and spatial relationships between any pair of patches for propagation. To achieve it, we construct an undirected graph with each v l i connected with its first-order neighbors. Please refer to <ref type="figure" target="#fig_4">Fig.5</ref> at the Appendix for details. Given token representation of S l , we build an N -node graph G l . Denote the i th node as v l i ? R Q . Then, we can infer the semantic similarity E l , where the specific element E l i,j is defined as the cosine distance between v l i and v l j :</p><formula xml:id="formula_2">E l i,j = v l i (v l j ) ? ||v i l ||||v j l ||<label>(3)</label></formula><p>where v l i and v l j are flattened vectors, and the larger value E l i,j denotes the higher similarity shared by v l i and v l j .</p><p>Activation Diffusion. To present spatial relationship, we define a binary adjacency matrix A l ? R N ?N , whose element A l i,j indicates whether v l i and v l j are connected. We further introduce a diagonal degree matrix D l ? R N ?N , where D l i,i corresponds to the summation of all the degrees related to v l i . Then, we obtain Laplacian matrixL l = D l ? A l , with each element (L l ) ?1 i,j describes the correlation of v l i and v l j at the equilibrium status. Recent studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref> on graph representation inspire us that the inverse of the Laplacian matrix leads to the global diffusion, which allows each unit to communicate with the rest. To enhance the diffusion with semantic relationships, we incorporateL l with node contextual information E l . Intuitively, we take advantage of the spatial connectivity and semantic coherence to split the tokens into the semantic-awarded foreground objects and the background environment. In practice, we use a learnable parameter ? to dynamically adjust the semantic intensity, which makes the diffusion process more flexible and easier to fit various situations. The Laplacian matrix L l with semantics is defined as,</p><formula xml:id="formula_3">L l = (D l ? A l ) ? (?E l ? 1)<label>(4)</label></formula><p>where ? represents element-wise multiplication, and 1 denotes the information flow exchange with neighboring vertexes. (D l ? A l ) denotes the spatial connectivity, (?E l ? 1) represents the semantic coherence, and ? incorporates them for diffusion. Please refer to Appendix for full details of Eqn. <ref type="bibr" target="#b3">(4)</ref>. After the global propagation, the reallocated activation score map can be calculated as follows,</p><formula xml:id="formula_4">F l+1 = (L l ) ?1 ? (F l )<label>(5)</label></formula><p>where F l+1 is the output re-allocated attention map and ? is a flattening operation that reshapes F l into a patch sequence.</p><p>Diffuse Matrix Approximation. In practice, directly using (L l ) ?1 may be impractical since L l is not guaranteed to be positive-definite and its inverse may not exist. Meanwhile, as observed in our initial experiments, directly applying the inverse produced unwanted artifacts. To deal with the problems, we exploit Newton Schulz Iteration <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> to solve (L l ) ?1 to approximate the global diffusion result,</p><formula xml:id="formula_5">X 0 = ?(L l ) ? X p+1 = X p (2I ? L l X p ),<label>(6)</label></formula><p>where X 0 is initialized as (L l ) ? multiplied by a small constant value ?. The subscript p denotes the number of iterations, and I is the identity matrix. As discussed above, we only need (L l ) ?1 to thrust propagation instead of obtaining the equilibrium result, so we just iterate the Eqn.(6) for p times then take the approximated (L l ) ?1 back to Eqn. <ref type="bibr" target="#b4">(5)</ref>. Then we obtain the diffused activation of F l , which is visualized in <ref type="figure" target="#fig_2">Fig.3</ref>(c). We can see that diffusion has redistributed the averaged attention map with more boundary details, such as the ear and the mouth, which are beneficial for final object localization.</p><p>Dynamic Filtering. As depicted in <ref type="figure" target="#fig_2">Fig.3</ref>(c), we found that the reallocated score map F l+1 provides a sharper boundary, but there is a side-effect that it diffuses the activation out of object boundaries, which may make the unnecessary background context back into S l+1 or result in over-estimation of bounding box. Therefore, we propose a soft-threshold filter, depicted as Eqn. <ref type="formula" target="#formula_6">(7)</ref>, to increase density contrast between the objects and the surrounding background to depress the outside noise.</p><formula xml:id="formula_6">T (F l , ?) = ? ? tanhShrink( F l ? )<label>(7)</label></formula><p>where ? ? (0, 1) is a threshold parameter for more flexible control. T denotes a soft-threshold function, and tanhShrink(x) = x ? tanh(x) is used to depress activation under ?. Then S l+1 = S l ? T (F l , ?). As shown in <ref type="figure" target="#fig_2">Fig.3(d)</ref>, the filter operation removes noise and provides sharper contrast. Here we use three rows for each method to show activation maps, binary map predictions, and bounding box predictions, respectively. The threshold value ? is set to be the optimal values proposed in TS-CAM and SCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction</head><p>After optimizing the model through backpropagation, the calibrated Transformer can generate the object-boundary-aware activation maps. Thus, we drop SCM during inference to obtain the final bounding box. Specifically, the bounding box prediction is generated by coupling S 0 and F 0 as depicted in <ref type="figure" target="#fig_1">Fig.2</ref>. As S 0 ? R H?W ?C is a C-channel 2D semantic map, each channel represents an activation map for a specific class c. To obtain the prediction from score maps, we carry out the following procedures: (1) Pass S 0 through a GAP to calculate classification scores. (2) Select i th map S 0 i ? R H?W corresponding to the highest classification score from S 0 . (3) Calculate the element-wise product F 0 ? S 0 i . The coupled result is then up-sampled to the same size as the input for bounding box prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>Datasets. We evaluate SCM on two commonly used benchmarks, CUB-200-2011 <ref type="bibr" target="#b31">[32]</ref> and ILSVRC2012 <ref type="bibr" target="#b25">[26]</ref>. CUB-200-2011 is an image dataset with photos of 200 bird species, containing a training set of 5,994 images and a test set of 5,794 images. ILSVRC contains about 1.2 million images with 1,000 categories for training and 50,000 images for validation. Our SCM is trained on the training set and evaluated on the validation set from which we only use the bounding box annotations for evaluation.</p><p>Evaluation Metrics. We evaluate the performance by the commonly used metric GT-Known and save models with the best performance. For GT-Known, a bounding box prediction is positive if its Intersection-over-Union (IoU) ? with at least one of the ground truth boxes is over 50% . Furthermore, for a fair comparison with previous works, we apply the commonly reported Top1/5 Localization Accuracy(Loc Acc) and Classification Accuracy(Cls Acc). Compared with GT-Known, Loc Acc requires the correct classification result besides the condition of GT-Known. Please refer to the appendix for more strict measures like MaxboxAccV1 and MaxboxAccV2 as recommended by <ref type="bibr" target="#b3">[4]</ref> to evaluate localization performance only.</p><p>Implementation details. The Transformer module is built upon the Deit <ref type="bibr" target="#b28">[29]</ref> pretrained on ILSVRC. In detail, we initialize ?, ? in ABDs to constant values (1 and 0.5 respectively), and choose p = 4 and ? = 0.002 in Eqn. <ref type="bibr" target="#b5">(6)</ref>. For input images, each sample is re-scaled to a size of 256?256, then randomly cropped to 224?224. The MLP head in the pretrained Transformer is replaced by a 2D convolution head with kernel size of 3, stride of 1, and padding of 1 to encode feature maps into semantic maps S 0 (200 output units for CUB-200-2011, and 1000 for ILSVRC). The new head is initialized with He's approach <ref type="bibr" target="#b10">[11]</ref>. During training, we use AdamW <ref type="bibr" target="#b16">[17]</ref> with ? = 1e ?8 , ? 1 = 0.9, ? 2 = 0.99 and weight decay of 5e-4. On CUB-200-2011, the training lasts 30 epochs with an initial learning rate of 5e-5 and batch size of 256. On ILSVRC, the training procedure carries out 20 epochs with a learning rate of 1e-6 and batch size of 512. We measure model performance on the validation set after every epoch. At last, we save the parameters with the best GT-Known performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance</head><p>To demonstrate the effectiveness of the proposed SCM, we compare it against previous methods on CUB-200-2011 and ILSVRC2012 in <ref type="table">Table.</ref>1. From GT-Known in CUB, SCM outperforms baseline method TS-CAM <ref type="bibr" target="#b8">[9]</ref> with a large margin, yielding GT-known 96.6% with a performance gain of 8.9%. Compared with other CNN counterparts, SCM is competitive and outperforms the stateof-the-art SPOL <ref type="bibr" target="#b29">[30]</ref> using only about 24% parameters. As for ILSVRC, SCM surpasses TS-CAM by 1.2% on GT-Known and 5.1% on Top-1 Loc Acc and is competitive against SPOL built on the multi-stage CNN models. Compared with SPOL, SCM has the following advantages, (1)Simple: SPOL produces semantic maps and attention maps on two different modules separately, while SCM is only <ref type="table">Table 1</ref>: Comparison of SCM with state-of-the-art methods in both classification and localization on CUB <ref type="bibr" target="#b31">[32]</ref> and ILSVRC <ref type="bibr" target="#b25">[26]</ref> test set. The column Params indicates the number of parameters in backbone on which models are built. Values in bracket show improvement of our method compared with TS-CAM <ref type="bibr" target="#b8">[9]</ref>. GT-K. stands for ground truth known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Backbone * CNN-based models are listed above. Transformer-based models are given at the center. Both PSOL <ref type="bibr" target="#b34">[35]</ref> and SPOL <ref type="bibr" target="#b29">[30]</ref> are composed of multiple-stage models are listed below. The best performance is shown as bold for CNN-based, Transformer, and multi-stage models, respectively.</p><p>finetuned on a single backbone. (2) Light-weighted: SPOL is built on a multistage model with huge parameters, while SCM is built on a small Transformer with only about 24% parameters of the former. (3) Convenient: SPOL has to infer the prediction with the complex network design, but SCM is dropped out during the inference stage. Furthermore, compared with the recent Transformerbased works like LCTR <ref type="bibr" target="#b1">[2]</ref>, with the same backbone Deit-S, we surpass it by a large margin 4.2% in terms of GT-Known in CUB and obtain comparable performance on Loc Acc for both CUB and ISVRC. We achieve this without additional parameters during inference, while other recent proposed methods add carefully designed modules or processes to improve the performance. The models are saved with the best GT-Known performance and achieve satisfactory Loc Acc and Cls Acc. Please refer to Sec.4.3 for more details.</p><p>The visual comparison of SCM and TS-CAM is shown in <ref type="figure" target="#fig_3">Fig.4</ref>. We observe that TS-CAM preserves the global structure but still suffers from the partial activation problem that degrades its localization ability. Specifically, it cannot predict a complete component from the activation map. We notice that minor and sporadic artifacts appear on the binary threshold maps, and most of them include half parts of the objects. After adding SCM as a simple external adaptor, the masks become integral and accurate, so we believe that SCM is necessary for Transformers to find their niche in WSOL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we first illustrate the trade-off between localization and classification given the pre-determined backbone. Then we explore why SCM can reallocate and enlarge activation from two perspectives. Specifically, we show the visual results of both semantic maps S l and attention maps F l across all layers, and analyze them with the learnable parameters' trend during training. Next, we illustrate the influence of module scale by stacking a different number of ADBs. At last, we apply SCM to other Transformers like ViT <ref type="bibr" target="#b26">[27]</ref>, and Conformer <ref type="bibr" target="#b9">[10]</ref> to prove SCM's adaptability. If not mentioned specifically, We carry Trade-off between Classification &amp; localization. SCM is an external module and will be dropped out during inference, adding no additional computational burden. Thus there is a trade-off between performance of localization and classification when the backbone is pre-determined. As shown in <ref type="figure" target="#fig_4">Fig.5(a)</ref>, SCM aims to calibrate the raw attention to localize the bird. Specifically, Transformer trained with SCM localizes objects well while suffers from sub-optimal CLS Acc in <ref type="figure" target="#fig_4">Fig.5(b)</ref>. In contrast, as training process continues, it classifies objects better but only focuses on the discriminant part of the whole object, resulting in worse localization result in <ref type="figure" target="#fig_4">Fig.5(c)</ref>. To clearly show the advantage of SCM for localization, we saved the model with the highest GT-Known as depicted in <ref type="figure" target="#fig_4">Fig.5(b)</ref>. Visualization Result of S l and F l . Implicit attention of models trained on image-level labels is blessed with remarkable localization ability as shown in CAM <ref type="bibr" target="#b38">[39]</ref>. However, due to the effect of label-wise semantic loss, the models would finally be driven to gather around semantic-rich regions, causing the problem of partial activation. TS-CAM <ref type="bibr" target="#b8">[9]</ref> suffers from a similar issue despite improving the localization performance by Transformer's long-range feature dependency. In <ref type="figure" target="#fig_5">Fig.6</ref>, we display both S l and F l at each layer of SCM. We observe that F 0 and S 0 have already covered the object completely, demonstrating that SCM can calibrate Transformer to cover objects. As the layer gets deeper, S l and F l concentrate more on semantic-rich regions, and S L at the last layer is further used to calculate the loss. It explains why we drop out SCM instead of appending it to Transformer, as sharper boundaries are provided at S 0 and F 0 . Propagating and filtering. To understand the effect of propagating and filtering, we analyze parameters ? and ? in each layer of SCM. As shown in <ref type="figure" target="#fig_6">Fig.7</ref>, the training record tells that ? in deeper layers increases, while ? in shallow layers is reduced. It indicates that SCM learns to diffuse activation at front layers while concentrating it in latter layers, verifying that SCM can enlarge partially activated regions with label-wise supervision. On the other hand, ? at all layers  drops at the beginning, possibly because the activation provided by Transformer is sparse. It takes time for the model to shift its focus from classification to localization, as Transformer is pretrained for classification. Then it starts climbing and goes down again, indicating that attention becomes more concentrated at beginning and then turns sparse to fit the demand across layers. For instance, the front layer prefers a higher filtering threshold to reduce noise, while other layers prefer a smaller threshold to get more semantic context.</p><p>Stacking ADBs. We further investigate the effect of module scale by stacking different numbers of ADBs. As shown in <ref type="figure" target="#fig_6">Fig.7(c)</ref>, we find out that the trend of GT-known and the optimal threshold almost fits the bell curve. It indicates that setting the suitable scale for SCM is essential, as when SCM becomes too deep, it fails to classify and localize objects precisely. On the other hand, the classification accuracy drops as the number of ADBs increases, while the localization performance increases first and drops later. It tells us that classification and localization are two different tasks, and we cannot obtain the optimal for both.</p><p>Adapting SCM to more situations. To evaluate SCM's performance with other Transformers, we select ViT <ref type="bibr" target="#b26">[27]</ref>, Conformer <ref type="bibr" target="#b9">[10]</ref> to testify SCM. Next, we compare SCM on various model scales on Deit. As shown in <ref type="figure" target="#fig_4">Fig.5(d)</ref>, we record the localization performance with the optimal epoch at which the best model is saved. It turns out that SCM is successfully adapted to ViT and Conformer, which achieves satisfactory performance 91.8% and 96.1% on CUB-200-2011 respectively. On the other hand, we test SCM on Deit with different scales. Surprisingly the larger models don't perform as well as Deit-small. It turns out that increasing model parameter size may not be optimal for SCM to obtain better performance, and the dropped optimal epoch number indicates that it may need a lower learning rate in training for better result.</p><p>Discussions. Our study presents a novel way to calibrate the Transformer for WSOL. Although we prove its adaptability to ViT <ref type="bibr" target="#b26">[27]</ref>, Conformer <ref type="bibr" target="#b9">[10]</ref>, we cannot calibrate Transformers without CLS token such as Swin <ref type="bibr" target="#b13">[14]</ref>, since CLS token is required to obtain F 0 . Furthermore, it's heuristic to choose the number of iterations used in Eqn. <ref type="bibr" target="#b5">(6)</ref>, and we simplify it as a constant number. Future research may explore methods such as Deep Reinforcement Learning to search the parameter space for the optimal diffusion policy. Furthermore, the equilibrium status Eqn.(4) is a patch-wise correlation like the self-attention matrix. It may indicate a new way to find the regions of interest by diffusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a simple external spatial calibration module (SCM) to refine attention and semantic representations of Vision Transformer for weakly supervised object localization (WSOL). SCM exploits the spatial and semantic coherence in images and calibrates Transformers to address the issue of partial activation.</p><p>To dynamically incorporate semantic similarities and local spatial relationships of patch tokens, we propose a unified diffusion model to capture sharper object boundaries and inhibit irrelevant background activation. SCM is designed to be removed during the inference phase, and we use Transformers' calibrated attention and semantic representations to predict localization results. Experiments on CUB-200-2011 and ILSVRC2012 datasets prove that SCM effectively covers the full objects and significantly outperforms its counterpart TS-CAM. As the first Transformer external calibration module on WSOL, we hope SCM could shed light on refining Transformers for the more challenging WSOL scenarios.  <ref type="figure">Fig. 8</ref>: Illustration of approximation by Newton Schulz Iteration. Each map denotes the redistributed F with corresponding number of iterations p below. In the main paper, we use iteration number p = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional ablation study</head><p>We give more experimental results and analysis on Spatial Calibration Module (SCM) proposed in the main paper. Firstly, we conduct more ablation studies on the activation diffusion module, especially on the Newton Schulz Approximation iteration. Next, we study various strategies of combing S l and F l and testify its influence on localization. To test SCM on more challenging measures, we validate it on MaxboxAcc. Furthermore, we provide the complete proof of the semantics-coupled Laplacian matrix L l at Eqn.(4) in the main paper, followed by a theoretical analysis of the semantic flow redistribution.</p><p>In this section, we conduct experiments on the influence of the iteration number in Newton Schulz Iteration. We also test the methodology to build up the final prediction score map using maps from various layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Selecting different iteration numbers</head><p>As shown in <ref type="figure">Fig.8</ref>, we observe that the approximation of L ?1 by Newton Schulz can be accelerated with the increasing number of iterations. It raises the question of what its impact on the localization performance is. To answer this question, we train several models with four ADB layers following the same setting as the main paper, except that the number of iterations varies.</p><p>As depicted in <ref type="figure" target="#fig_0">Fig.10</ref>, we plot GT-Known and the hyperparameter threshold ? above which we generate the binary map. It turns out that iteration p = 4 is still the optimal choice that exceeds other settings over 5% in GT-Known. To explore the reason, we plot ? and observe that the iteration p = 4 yields a much larger region of interest than others. It indicates that SCM may need a relatively small number of iterations in each block, or semantic information would be over-diffused, resulting in degraded performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Strategies on combining maps</head><p>We design SCM as an external module that calibrates Transformer trained on the classification to weakly supervised localization scenarios. During inference, SCM will be dropped out, so we only use S 0 and F 0 for prediction. We further explore whether combing maps from other blocks would yield a different result.</p><p>As shown in <ref type="figure" target="#fig_8">Fig.9</ref>, we produce the activation by combining S l and F l and depict it in a pair-wise way. We find out that S l tends to concentrate more on semantic-rich regions as the number of layers increases. On the other hand, F l shows a similar pattern as the layer goes deeper. The reason is that the semantic token maps S l are supervised by the label loss that drives the model to focus on discriminative parts. However, different from the naive transformer implementation (TS-CAM), the Transformer with SCM learns to calibrate semantic and attention maps through backpropagation, as we can observe that it revises the coupled activation with more spatial details and clear boundaries in upper layers. At last, the refined coupled score map S 0 and F 0 becomes a promising candidate for localization. <ref type="figure" target="#fig_0">Fig. 10</ref>: Illustration of the GT-Known performance and the optimal filtering threshold ? with various number of Newton Schulz Iterations p in validation. ? determines the threshold above which the bounding box is predicted from the score maps, which means ? is proportional to the activated region. <ref type="figure" target="#fig_0">Fig. 11</ref>: Architecture of (l + 1) th Activation Diffusion Block (ADB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation result on other metrics</head><p>MaxboxAcc is reformulated to further GT-Known (the same fixed ? 50%) with the optimal threshold ? in generating the binary map. Compared with GT-Known, MaxboxAcc precludes misleading hyperparameter ? that depends heavily on the data and model architecture. MaxboxAccV2 with optimal ?, is a more strict measure than the former MaxboxAccV1. (1) It averages the performance across ? ? {0.3, 0.5, 0.7} to address diverse demand for localization fitness. <ref type="formula">(2)</ref> It considers the best match between the set of all estimated boxes and the set of all ground-truth boxes as prediction, instead of only one box prediction from the largest connected component of the score map in prior methods. Towards a well-posed setup on WSOL, which is trained without any localization supervision, we shift the evaluation on a held-out set CUBV2 <ref type="bibr" target="#b3">[4]</ref> not overlapping with the available validation set (now the test set). Then we evaluate both SCM and TS-CAM on it with the metrics MaxboxAccV1 and MaxboxAccV2 in the experiment shown in <ref type="table">Table.</ref>2 for the reason that selecting hyperparameter ? with full supervision in the test set violates the principle of WSOL. To make a fair comparison with the evaluation results given in <ref type="bibr" target="#b3">[4]</ref>, we keep the same training budget with fixed training epochs to 50 and a fixed batch size of 32 and save the models, including SCM and TS-CAM with the best MaxboxAccV1 or MaxboxAccV2 on CUBV2.</p><p>In <ref type="table">Table.</ref>2, we compare TS-CAM and SCM on CUBV2 <ref type="bibr" target="#b3">[4]</ref> on the same computational budget as previous methods. It turns out that both TS-CAM and SCM have achieved satisfactory performance, but SCM surpasses TS-CAM by 7.7% and 10.3% on MaxboxAccV1 and MaxboxAccV2, respectively. Furthermore, a higher MaxboxAccV2 score proves that SCM has great adaptability and attends to various levels of localization fitness demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More details on activation diffusion</head><p>Over the past decades, Transformer has had tremendous success, largely attributed to its efficient attention mechanism to capture the long-range dependency. However, its limitations cannot be ignored. Studies have found that the transformer has a natural limitation on local context modeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, which is critical for the object localization task. We further extend its ability by introducing SCM that calibrates the Transformer to embrace spatial and semantic coherence to solve this issue.</p><p>As shown in <ref type="figure" target="#fig_0">Fig.11</ref>, we apply Activation Diffusion Block in SCM to reallocate the activation region F . Here, we give the detailed stpdf to get the Laplacian matrix L which denotes "equilibrium status" at Eqn.(4) in our main paper. This section will describe the activation diffusion behind a physic evolution model on a network structure in detail. We start with an introduction to the diffusion process that enables the exchange of information among vertexes. Next, we further analyze diffusion behavior with semantics on a global scale. At last, we show how to get the re-allocated attention map. Firstly, we build a graph G ?V, E?, where V and E represent the set of vertexes and edges, respectively. Also, v i denotes a vertex in V , and e i,j in E denotes an edge between v i and v j , and we define the information flow as I ? R N , where N is the number of patches. G is shown in <ref type="figure" target="#fig_0">Fig.12</ref>, where we display the flow exchange between v i and its first-order</p><formula xml:id="formula_7">neighbors v i?1 , v i+1 , v i?W , v i+W , where (H, W )</formula><p>is the 2D patch resolution and we use the token sequence indexes to denote the spatially connected four neighbors.</p><p>To make diffusion semantic-aware in G, as shown in <ref type="figure" target="#fig_0">Fig.12</ref>, we design a model to describe both the flow influx and the outflux on v i . Firstly, the flow input is based on the initial activation maps, where the activation score is proportional to the input rate; another source is the neighbor nodes as v i will share flow with them. On the other hand, the flow will go outwards to nearby ones simultaneously, and to make it semantics-aware, we introduce the 'semantic flow' that escapes from the nodes. Thus, The rate of fluid change in v i at time t could be <ref type="figure" target="#fig_0">Fig. 12</ref>: Illustration of diffusion for v i and its first-order neighbors, where (H, W ) is the reshaped 2D graph resolution, where H denotes the number of nodes per column, and W denotes the number of nodes per row. Each circle represents a patch in this graph, and we denote the patch sequence indexes on top of them. The arrows represent flow change with the horizontal direction that denotes exchange with neighbor vertexes, and the vertical represents input and output for G. We further specify types of exchange by different colors, where (Green) F i u(t) is the initial input rate; (Blue) The communication rate with neighbors; (Red) The rate of semantic flow which is related to the embedding similarity and the amount of flow.  <ref type="bibr" target="#b7">(8)</ref> where ? is a learnable parameter for flexible control over the scale of diffusion. Specifically, for each v i , the input for G exists if v i is one of the source nodes, then the input rate is F i u(t), i.e. the score maps F i &gt; 0 then v i can be treated as the source. Next the input from the direct neighbors should also be considered, given j A i,j I i (t). On the other hand, for output, when propagating from v i to v j , there exists the semantic flow which penalizes the flow exchange with low semantic similarity, i.e. the cosine distance E i,j . Thus, ? j A i,j (I i (t) ? I j (t))E i,j ) describes the escaped semantic flows for the propagation from v i to its neighbor, denoted as red arrows in <ref type="figure" target="#fig_0">Fig.12</ref>. ? is a hyperparameters to adjust the overall contribution of semantic flow. Next, the outflux into the direct neighbors is j A j,i I j (t).</p><p>Then we can study the dynamic change of flow regarding G ?V, E? and describe the graph's response to the flow dynamics. Eqn.(8) could be further extended to the global scale,? (t) = LI(t) + u(t)? (F )</p><p>where L = (D ? A) * (1 ? ?E)</p><p>Eqn. <ref type="formula" target="#formula_0">(10)</ref> is the shifted Laplacian matrix and ? is a flatten operator used to reshape F into a sequence. Eqn.(8) tells the flow at v i that changes with time. Next, we could further take the integral to accumulate the total changes within a certain amount of time, which could be used to describe the trend of flow at G. Thus, from Eqn. <ref type="bibr" target="#b8">(9)</ref>, we obtain the expression of the amount of flow in G by,</p><formula xml:id="formula_10">I(t) = t t ? =0 e ?L(t?t ? )? (F )u(t ? )dt ?<label>(11)</label></formula><p>Eqn.(11) tells us that the graph is dynamically adjusted by semantic embedding similarity E with spatial relationship. Denote a special time t 0 when I(t 0 ) = 0, we consider the 'equilibrium' status is reached as the influx rate equals the outflux rate for v i . As t 0 ? [0, ?], when t ? ?, the total amount of flow in G will not change and we obtain,</p><formula xml:id="formula_11">lim t?? I(t) = L ?1 ? (F )<label>(12)</label></formula><p>Eqn.(12) implies the fully-diffused activation, however, as discussed in our main paper, L is not guaranteed to be positive-definite, and its inverse may not exist. Meanwhile, as observed in our initial experiments in <ref type="figure">Fig.8</ref>, directly applying the inverse has produced unwanted artifacts that may downgrade localization quality. Thanks to the Newton Schulz method, we exploit its great convergence ability that approximates L ?1 with a few numbers of iterations. As shown in <ref type="figure" target="#fig_0">Fig.11</ref>, we couple the approximated L ?1 to incorporate spatial and semantic correlation into F in the end, which is shown in Eqn. <ref type="bibr" target="#b11">(12)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Transformer-based localization pipelines in WSOL. The dashed arrows indicate the module parameters update during backpropagation. (a) TS-CAM [9]: the training pipeline encodes the feature maps into semantic maps (SM) through a convolution head, then applies a GAP to receive gradients from the image-label supervision. (b) SCM(Ours): our training pipeline incorporates external SCM to produce new semantic maps SM refined with the learned spatial and semantic correlation. Then it updates the Transformer backbone through backpropagation to obtain better attention maps and semantic representations for WOLS. (c) Inference: SCM is dropped out, and we couple attention maps (AM) and SM just like TS-CAM for final localization prediction. (d) Comparison of AM, SM, and final activation maps of TS-CAM and proposed SCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall framework consists of two parts. (Left) Vision Transformer provides the original attention map F 0 and semantic map S 0 , (Right) They are dynamically adjusted by stacked activation diffusion blocks (ADBs). The detail of the layer design is shown at the bottom-right corner (the residual connections for F l and S l are omitted for simplicity). Once model optimized, F 0 and S 0 are directly element-wise multiplied for final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of activation diffusion pipeline with a hand-crafted example. (a) Input image. (b) Original Transformer's attention map. (c) Diffused attention map. (d) Filtered attention map. As the spatial coherence is embedded into the attention map via our SCM, the obtained attention map by using proposed method captures a complete object boundary with less noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Visual comparison of TS-CAM and SCM on 4 samples from CUB-200-2011 and ISVRC2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>(a) The overview of the activation scores propagation, which is a process that evolves from the raw attention regions to the semantic rich regions. (b) Status with the best Loc Acc at the relatively early training stage. (c) Status with the best CLS Acc at the later training stage. (d) The comparison between SCM on different Transformers and various scales. We record GT-known and the epoch number at which the best GT-known performance is obtained. out all the experiments on Deit-small with SCM consisting of four ADBs and all the experiments share the same implementation discussed above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization of both semantic maps S l (upper) and attention maps F l (lower) input to the l th ADB block for a sample from CUB-200-2011 test set.(a) Diffusion Scale Control Parameter (b) Filtering Threshold (c) Number of ADBs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>The learnable parameters update when trained on Deit-small. The layer number l is shown below. (a) ? is used for the diffusion scale control, and lower ? means the wilder scale of diffusion. (b) ? determines the threshold under which the activation maps should be filtered. (c) Evaluation of GT-known, Cls Acc (top-1) for different numbers of ADBs. ? (here in percentage format) denotes the threshold above which the bounding box is predicted from the score maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Illustration of coupling semantic maps and attention maps across layers. Sources of each image are indicated at corresponding row and column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(t) + ? j Ai,j(Ii(t) ? Ij(t))Ei,j) outf lux</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of SCM by MaxboxAcc<ref type="bibr" target="#b3">[4]</ref> on CUB<ref type="bibr" target="#b31">[32]</ref>. Values in bracket shows improvement of our method compared with TS-CAM<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>MaxboxAccV1</cell><cell>MaxboxAccV2</cell></row><row><cell>CAM[39]</cell><cell>VGG16</cell><cell>71.1</cell><cell>63.7</cell></row><row><cell>ACoL[36]</cell><cell>VGG16</cell><cell>72.3</cell><cell>57.4</cell></row><row><cell>ADL[6]</cell><cell>VGG16</cell><cell>75.7</cell><cell>66.3</cell></row><row><cell>CutMix[34]</cell><cell>VGG16</cell><cell>71.9</cell><cell>62.3</cell></row><row><cell>SPG[37]</cell><cell>InceptionV3</cell><cell>62.7</cell><cell>55.9</cell></row><row><cell>ADL[6]</cell><cell>InceptionV3</cell><cell>63.4</cell><cell>58.8</cell></row><row><cell>PDM[21]</cell><cell>Resnet50</cell><cell>-</cell><cell>70.7</cell></row><row><cell>BGC[12]</cell><cell>Resnet50</cell><cell>-</cell><cell>80.1</cell></row><row><cell>TS-CAM[9]</cell><cell>Deit-S</cell><cell>88.9</cell><cell>79.6</cell></row><row><cell>SCM(ours)</cell><cell>Deit-S</cell><cell>96.6 (7.7?)</cell><cell>89.9 (10.3?)</cell></row><row><cell cols="4">* The experiment is iteratively trained one epoch on CUB train set and evaluated one</cell></row><row><cell cols="4">epoch on CUBV2 [4]. The annotation mapping in the counterpart ISLVRCV2 [4] is</cell></row><row><cell cols="4">currently not available, so we evaluate TS-CAM and SCM only on CUB-200-2011.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning social network embeddings for predicting information diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bourigault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lagnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On awakening the local continuity of transformer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph spectral image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3133" to="3142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Laplacian sparse coding, hypergraph laplacian sparse coding, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ts-cam: Token semantic coupled attention map for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2886" to="2895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<title level="m">Conformer: Convolution-augmented transformer for speech recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bridging the gap between classification and localization for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00220</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on machine learning</title>
		<meeting>the 19th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1814" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometry constrained weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="481" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining web graphs for recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1051" to="1064" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Erasing integrated learning: A simple yet effective approach for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8766" to="8775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diverse complementary part mining for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1774" to="1788" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast and efficient parallel algorithms for the exact inversion of integer matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Foundations of Software Technology and Theoretical Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="504" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient parallel solution of linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth annual ACM symposium on Theory of computing</title>
		<meeting>the seventeenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On the integration of self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14556</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding friends and enemies in an enemies-only network: a graph diffusion kernel for predicting novel genetic interactions and co-complex membership from yeast genetic interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Boeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1991" to="2004" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shallow feature matters for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5993" to="6001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization through inter-class feature similarity and intra-class appearance consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Danet: Divergent activation for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6589" to="6598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the route towards weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13460" to="13469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inter-image communication for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="271" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
