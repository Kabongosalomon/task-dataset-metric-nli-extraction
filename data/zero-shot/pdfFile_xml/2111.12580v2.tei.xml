<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyeop</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Uk</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Choe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaist</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ukcheol Shin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to estimate object pose often requires groundtruth (GT) labels, such as CAD model and absolute-scale object pose, which is expensive and laborious to obtain in the real world. To tackle this problem, we propose an unsupervised domain adaptation (UDA) for category-level object pose estimation, called UDA-COPE. Inspired by recent multi-modal UDA techniques, the proposed method exploits a teacher-student self-supervised learning scheme to train a pose estimation network without using target domain pose labels. We also introduce a bidirectional filtering method between the predicted normalized object coordinate space (NOCS) map and observed point cloud, to not only make our teacher network more robust to the target domain but also to provide more reliable pseudo labels for the student network training. Extensive experimental results demonstrate the effectiveness of our proposed method both quantitatively and qualitatively. Notably, without leveraging target-domain GT labels, our proposed method achieved comparable or sometimes superior performance to existing methods that depend on the GT labels.</p><p>Recent category-level object pose estimation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref> take an RGB-D input to extract the 2D/3D features. We designed separate 2D/3D branches to extract features from both modalities. We use PSPNet <ref type="bibr" target="#b40">[41]</ref> with ResNet34 [11]  for 2D feature extraction and the Mink16UNet34 [7] for 3D feature extraction. At this time, the 2D feature is extracted by sampling features that are validly matched with point cloud P from the feature vol-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object pose estimation is one of the crucial tasks used in various robotics and computer vision applications for robot manipulation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> and augmented reality (AR) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. Using sensor data such as images or point clouds, this task aims to estimate the poses of target objects including 3D orientation, 3D location, and size information.</p><p>Previous 6D object pose estimation methods follow the instance-level pose estimation schemes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref> that rely on given 3D CAD model information (e.g., keypoints, geometry) and the size of known objects. However, these methods typically have difficulty estimating the pose of unknown objects since they do not yet have 3D CAD models as priors.</p><p>In contrast to the instance-level scheme, category-level object pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> approaches are more efficient in that a single network can infer multiple classes at once. In particular, Wang et al. <ref type="bibr" target="#b34">[35]</ref> introduced a pioneering representation called Normalized Object Coordinate Space (NOCS), to align different object instances within one category in a shared 3D orientation. By estimating per-category NOCS maps, it is able to estimate the 6D pose of unseen objects without prior 3D CAD models. Its strengths have led to the use of NOCS representation in the following studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>However, current object pose estimation research mostly relies on supervised learning, which requires expensive GT labels such as 3D object CAD models and absolute object pose. These labels are not only difficult to obtain in the real world but are also unreliable due to the human-annotation. Because of this difficulty, most of the training depends on synthetic datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> and is usually not feasible in real-world applications due to domain gaps.</p><p>To cope with the real-world data scarcity problem, we take a look at unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>. UDA approaches often consider two types of datasets, the source domain (i.e. synthetic dataset) and the target domain (i.e. real-world dataset) dataset. The main goal of the UDA methods is to successfully make deep learning networks robust to the target domain using only the GT labels of the source domain. Various techniques exist, such as pseudo label generation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, teacher and student networks with momentum updates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref>, adversarial learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref>, and etc.</p><p>In this paper, we propose an Unsupervised Domain Adaptation for Category-level Object Pose Estimation (UDA-COPE). The proposed method effectively transfers task knowledge from a synthetic domain to a real domain by exploiting a multi-modal self-supervised learning scheme using pseudo labels. Our UDA-COPE concentrates on how to make high-quality pseudo-labels that are efficiently tar-geted for the category-level pose estimation task. To this end, we designed bidirectional point filtering to remove noisy and inaccurate points based on pose optimization. Extensive experiments demonstrate that our UDA-COPE and bidirectional point filtering successfully can reduce the domain gap between synthetic and real datasets. Moreover, our framework achieved better performance than the previous supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. The contributions of our method are summarized as:</p><p>? We propose an RGB-D based Unsupervised Domain Adaptation for Category-level Object Pose Estimation (UDA-COPE) framework that addresses the problem of data deficiency in real-world scenarios.</p><p>? We design a teacher-student framework where highquality pose-aware pseudo labels can be obtained via the proposed bidirectional point filtering.</p><p>? Our method shows comparable or sometimes better results than supervised pose estimation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Category-level object pose estimation. This task <ref type="bibr">[4-6, 20,30,35]</ref> deals with objects of unseen instances but known categories. The most recent category-level object pose and size methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> use the dense Normalized Object Coordinate Space (NOCS) representation as a basic way to estimate pose. It aligns different object instances within one category in a shared 3D orientation. Shape Prior <ref type="bibr" target="#b29">[30]</ref> improves the quality of NOCS maps by generating a representative shape prior and a deformation NOCS map for each category. CR-Net <ref type="bibr" target="#b35">[36]</ref> extends the shape-prior method <ref type="bibr" target="#b29">[30]</ref> by using the cascade relation and recurrent reconstruction methods. Similarly, SGPA <ref type="bibr" target="#b4">[5]</ref> proposes a prior adaptation method. Despite notable improvements, these studies rely on fully supervised learning, and as such, they require a large amount of manually labeled data, such as the 6D pose of objects, 3D CAD models, and NOCS maps. It is also timeconsuming and expensive to create accurate GT information in real-world scenarios. To the best of our knowledge, CPS++ <ref type="bibr" target="#b21">[22]</ref> is the only existing approach that addresses this data dependency problem. It aims to predict 3D shape using an RGB image to optimize pose and utilizes an unsupervised learning scheme by computing consistency between the observed depth map and the rendered depth. The rendered depth is obtained by projecting an estimated 3D shape with the predicted pose. However, 3D shape reconstruction from a single image is challenging and makes their unsupervised guidance unreliable. Recently, Li et al. <ref type="bibr" target="#b17">[18]</ref> suggested a self-supervised method that leverages SE(3) equivalent representation for category-level pose estimation. However, <ref type="figure">Figure 1</ref>. Network for multi-modal NOCS map estimation. they do not fully consider pose parameters at the categorylevel that consist of 3D orientation, 3d location, and size information.</p><p>Multi-modal UDA. xMUDA <ref type="bibr" target="#b13">[14]</ref> is a pioneer approach for unsupervised domain adaptation (UDA) in 2D/3D semantic segmentation for multi-modal scenarios. A few methods have considered an extra modality (i.e., depth) during training time and leveraged such privileged information to boost adaptation performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>. Reza et al. <ref type="bibr" target="#b20">[21]</ref> proposed a multi-modal UDA in instance-level object pose estimation, but it only considered relative pose on a 2D image level. However, multi-modal UDA approaches have not yet been explored in the category-level object pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an RGB image I, point cloud P , and segmentation labels S, our architecture aims to regress the 6D pose and size s?R 3 of objects. The 6D pose is defined as the rigid transformation of [R|t]: rotation R?SO(3), and translation t?R 3 . Following previous studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>, the segmentation labels S are used to crop the RGB images and point clouds. We leverage the NOCS representation to align different object instances within one category in a shared orientation. The categorical object pose [R|t] and size s are estimated by Umeyama algorithm <ref type="bibr" target="#b31">[32]</ref> with RANSAC <ref type="bibr" target="#b8">[9]</ref>, which optimizes [R|t] and s by minimizing the distances between point cloud P and an estimated NOCS map N .</p><p>We first illustrate our network architecture (Sec. 3.1). Then, we introduce training methods of supervised learning using synthetic dataset (Sec. 3.2) and unsupervised domain adaption using real-world dataset (Sec. 3.3). <ref type="figure">Figure 2</ref>. Overview of unsupervised domain adaptation for category-level object pose estimation (UDA-COPE). UDA-COPE utilizes pseudo label based teacher/student training scheme. Our proposed bidirectional point filtering method removes the noisy pseudo labels and gives reliable guidance to the student network. At the same time, filtered depth points gives additional self-supervision to the teacher network so that it can be robust to the domain gap between the synthetic and real dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>ume. Finally, we have a fused branch that combines each feature from both branches. Every branch estimates a NOCS map (N ) with a separate NOCS header, which consists of three multi-layer perceptrons (MLP) layers. Our multi-modal NOCS map prediction network is illustrated in <ref type="figure">Fig. 1</ref>. We designate the NOCS map estimation of each branch as N 2D , N 3D , N Fused , according to respective feature property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-Training with Synthetic Data</head><p>Inspired by pseudo label (PL) based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, our method consists of a teacher and a student model. <ref type="figure">Fig. 2</ref> shows the overview of our teacher and student model. The initial prediction of teacher model M T becomes a pseudo label N PL for a student model, and student model M S learns from the pseudo label as a GT. Our teacher and student model have the same structure as was described in Sec. 3.1.</p><p>We first train our teacher model in a supervised manner using the labeled synthetic dataset. For the NOCS map prediction using the GT information, we utilize cross-entropy loss, as in, H(N gt , N T ), where the supervision is given to all predictions from three branches. Additionally, to make our teacher network more robust, we apply the 2D image and 3D points augmentation and use consistency loss L C so that each modality can output consistent results. Total loss for the teacher network on the synthetic dataset is formulated as follows:</p><formula xml:id="formula_0">L MT = ? N H(N gt , N T ) + ? C L C , L C = H(N T , N T Aug )<label>(1)</label></formula><p>where N T Aug is the NOCS map prediction from the augmented input, and ? N and ? C are weighting parameters. Notation for the modality of the predictions is discarded for better readability. Given pseudo labels and depth points (a), we estimate the pose and size using the Umeyama <ref type="bibr" target="#b31">[32]</ref> algorithm and RANSAC <ref type="bibr" target="#b8">[9]</ref>, and align the depth points to normalized object coordinate (b). The pseudo label (red) and aligned depth points (blue) have noisy and inaccurate points. After our bidirectional point filtering, the noisy points are removed to give more reliable supervision for both teacher and student (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pose-Aware Unsupervised Domain Adaptation</head><p>After training from the synthetic dataset, the most straight-forward yet naive approach is to train the student network using the prediction of the teacher network. However, using the initial prediction from the teacher model as a pseudo label can be risky. The risk is due to the lack of robustness of the teacher model itself, or more importantly, because of the insufficient knowledge that the teacher model holds with respect to real-world scenarios, due to the domain gap between the simulated and real worlds. Techniques such as data augmentation and momentum update might help the feasibility but are still restricted. Therefore, we need additional guidance for the teacher model to estimate high-quality predictions, and more reliable pseudo labels for our student model to learn from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Bidirectional Point Filtering</head><p>To solve these problems, we propose the bidirectional point filtering method which simultaneously removes the noise of the pseudo labels for the student and filters noisy depth points P for a teacher network. <ref type="figure" target="#fig_0">Fig. 3</ref> shows an overview of the proposed bidirectional filtering method. Our bidirectional filtering method uses the P and N PL as input and initially estimates the pose [R|t] and size s using the Umeyama algorithm <ref type="bibr" target="#b31">[32]</ref> with RANSAC <ref type="bibr" target="#b8">[9]</ref>. Then it aligns the depth points P to the NOCS coordinate by applying the inverse of the estimated pose, as in multiplying the matrix [sR|t] ?1 . We denote aligned depth points as P ? . And then we calculate the point-wise 3D distance d between the aligned depth points P ? and pseudo label N PL to filter out noisy points from both sides using ? as the threshold. Finally, we get the refined pseudo labelN PL and filtered aligned depthP . Our bidirectional point filtering can be expressed as: <ref type="figure" target="#fig_0">Fig. 3</ref> shows that our bidirectional filtering method removes outliers of pseudo label N PL and depth P , and results in refined pseudo labelN PL and filtered depth pointsP .</p><formula xml:id="formula_1">d(n) = ?P ? (n) ? N PL (n)? where ?n ? [1|P ? ], N PL = {N PL (n) : d(n) &lt; ?}, P = {P ? (n) : d(n) &lt; ?},<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Self-Supervised Learning</head><p>After the bidirectional filtering, we jointly train the teacher network and student network using the filtered pseudo la-belsN PL and filtered aligned depth pointsP . Noted that we only use the filtered pointsP for the teacher training, which may be a smaller subset of an original P . We use crossentropy loss to train a student model using clean pseudo labelsN PL . The student model loss is defined as:</p><formula xml:id="formula_2">L MS = ? 1 |N PL | |NPL| n=1 H(N PL (n), N S (n)),<label>(3)</label></formula><p>where N S is the predictions of our student network. At the same time, the teacher learns real data knowledge from observation. We use cross-entropy loss by utilizing geometric consistency between our filtered aligned depthP and estimated pseudo labels N T . The teacher model loss is defined as:</p><formula xml:id="formula_3">L MT = ? 1 |P | |P | n=1 H(P (n), N T (n)).<label>(4)</label></formula><p>We train our teacher model with a small learning rate for stable teacher network training. For both teacher and student models, we compute the loss for all estimations, N 2D , N 3D , N Fused , which shows better result than applying the loss to only N Fused . We denote all estimation losses as all modality (AM) loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We employ two commonly used category-level pose estimation datasets, a synthetic dataset and a real dataset. The synthetic dataset (source domain) is the Context-Aware MixedEd ReAlity (CAMERA) dataset <ref type="bibr" target="#b34">[35]</ref>, generated by rendering and compositing synthetic objects into real images in a context-aware manner. It comprises 275k synthetic images for training. The dataset contains 1085 object instances selected from 6 different categories -bottle, bowl, camera, can, laptop and mug. We use the REAL dataset <ref type="bibr" target="#b34">[35]</ref> as a real dataset (target domain). Compared to previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>, our method trains without object pose, 3D CAD, and NOCS map GT labels of the target domain. It consists of 43,000 real world images of 7 scenes for training and 2,750 real world images of 6 scenes for evaluation. The REAL evaluation set was designated REAL275. We evaluated our method on a standard REAL275 benchmark for the task of category-level object pose estimation.</p><p>Metrics. We followed the previous pose and size evaluation metric from Wang et al. <ref type="bibr" target="#b34">[35]</ref>, which evaluated the performance of 3D object detection and 6D pose estimation. We report the average precision at different Intersection-Over-Union (IoU) thresholds for 3D object detection. Threshold values of 25%, 50%, and 75% were used to evaluate the results. For 6D object pose evaluation, the average precision was computed considering rotation and translation errors. For example, the 10 ? , 10cm metric denotes the percentage of object instances where the prediction error was less than 10 ? and 10cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implemented our method with PyTorch <ref type="bibr" target="#b25">[26]</ref>. To obtain object regions from the real-world dataset, we used the GT segmentation labels during training and the off-the-shelf object segmentation method, Mask R-CNN <ref type="bibr" target="#b9">[10]</ref>, during inference. For a detected instance, we resized the image patch to 192 x 192, and randomly sampled 1024 point clouds. The 2D and 3D features had 64 dimensions and the fused feature had 128 dimension features, due to concatenation. We trained our teacher network for 50 epochs on the synthetic dataset. We used the Adam optimizer with an initial learning rate of 0.0001 and a batch size of 16, where the learning rate was decreased by a factor of 0.6, 0.3, 0.1, 0.01 at 15k, 30k, 45k, 60k iterations, respectively. We set ? N = 1.0, ? C = 1e?6 for the teacher network. The ? was set to 0.05 for our proposed bidirectional filtering. On the unlabeled real dataset, both the teacher and student networks were trained for 1 epoch. The learning parameters for our student network were the same as those for the teacher training scheme on the synthetic dataset. We set the learning rate for realworld self-supervised learning for the teacher network to be 1e?7.  <ref type="table">Table 2</ref>. Quantitative comparison of unsupervised pose estimation approaches on the REAL275 dataset. Empty entries are either not able to be evaluated or not reported in the original paper. Performance margins are calculated compared to the synthetic-only results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art</head><p>We compared our methods with state-of-the-art methods that were trained on different datasets and labels: 1) labeled synthetic dataset, 2) labeled synthetic and real datasets, 3) labeled synthetic and unlabeled real datasets. All methods were evaluated on the REAL275 dataset. Note that only the approaches with the ability to perform multi-class categorylevel pose estimation using a single network were considered. RGB, Depth and RGB-D denotes the modality of the network input, and most of the RGB based approaches utilize depth information in the pose optimization or refinement process. Supervised Pose Estimation methods. <ref type="table">Table 1</ref> summarizes the results of the state-of-the-art category-level object pose estimation methods. Obviously, supervised training with the real data annotation significantly improved the overall performance, as are revealed by comparing the results of NOCS <ref type="bibr" target="#b34">[35]</ref> and SPD <ref type="bibr" target="#b29">[30]</ref> on different training dataset conditions. However, our unsupervised method showed results superior to NOCS <ref type="bibr" target="#b34">[35]</ref>, SPD <ref type="bibr" target="#b29">[30]</ref>, CASS <ref type="bibr" target="#b3">[4]</ref>, and CR-Net <ref type="bibr" target="#b35">[36]</ref>. Compared to two of the strongest previous approaches, SGPA <ref type="bibr" target="#b4">[5]</ref> and DualPoseNet <ref type="bibr" target="#b19">[20]</ref>, ours still showed comparable performance. This indicates that our proposed filtered pseudo label based UDA-COPE is robust when estimating object pose in unseen real-world instances.</p><p>Unsupervised Pose Estimation methods. <ref type="table">Table 2</ref> summarizes the results of CPS++ and our method on source only, and source with unlabeled target training conditions. CPS++ <ref type="bibr" target="#b21">[22]</ref> provides self-supervision by computing the consistency between the observed depth map and the rendered depth. The rendered depth is obtained by projecting an estimated 3D shape with the predicted pose. The results from row 1 and row 2 in <ref type="table">Table 2</ref> show that for CPS++, using unlabeled real data marginally improved performance, and sometimes even worsened it, as in 5 ? , 5cm metric. We believe that their self-supervision is unreliable because of ambiguous 3D shape reconstruction using only a single-view RGB image.</p><p>Comparing row 3 and row 4, it can be seen that our proposed method shows improved results for every metrics, with some metrics showing notable margins such as an 8.7 mAP (48%) increase in 5 ? , 5cm. Also, in the last row, our RGB-D result had better performance than the single modality based outputs. Therefore, we claim that our proposed algorithm is more effective by utilizing a pseudo label based learning scheme with modality and pose-aware self-supervision. The effectiveness of each components will be ablated thoroughly in the following sections.  <ref type="table">Table 3</ref>. Ablation studies on UDA components. Lower Bound: trained with labeled source only, Upper Bound: trained with both labeled source and target, PL: Pseudo Label, MU: Momentum Update, AM: All Modality loss, PL-F: Pseudo Label Filtering, TSL: Teacher Self-supervised Learning <ref type="bibr" target="#b3">(4)</ref>. Performance margins are calculated compared to the Lower Bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Our ablation studies were conducted using the predictions from all branches: RGB, Depth, and RGB-D. The lower bound and upper bound in <ref type="table">Table 3</ref> are the results from using a single network described in Sec. 3.1. The lower bound was trained source only, while the upper bound utilized both source and target data with their labels. <ref type="table">Table 3</ref> summarizes the results of the ablation studies. Naive Teacher/Student UDA for Pose Estimation. To verify the effectiveness of our proposed pose-aware UDA methods, we first applied a very naive version of teacherstudent learning scheme with the momentum update. When applying the momentum update, we updated our teacher network every 100 iterations with a ? of 0.999.</p><p>However, it can be seen that neither methods were not very helpful. The possible reasons are: 1) directly using the initial predictions of our teacher network as a pseudo label was unreliable due to the domain gap and noisy prediction, 2) providing supervision only to the fused branch may restrict effective training of the 2D and 3D branches, which disrupt the fused branch after all, and 3) the momentum update using the gradient information retrieved from the unreliable pseudo label and thereby unreliable student network training were insufficient for the teacher network to learn real world adaptation. Modality-Aware UDA methods. We applied all modality (AM) loss which computes cross-entropy loss between a pseudo label and all outputs from the three branches. Comparing (PL) and (PL + AM) in <ref type="table">Table 3</ref>, shows that the performances of the RGB-based prediction and Depth-based prediction improved by large margins. Accordingly, we achieved improved accuracy from the RGB-D-based fused branch as well.</p><p>To compare our methods with the previous multi-modal unsupervised domain adaptation method, we also applied xMUDA <ref type="bibr" target="#b13">[14]</ref> which constraints the 2D feature and 3D feature for consistency. xMUDA consistency was given during the student network training. xMUDA showed a similar amount of performance boost as our AM loss. This is because both AM and xMUDA help our network to output consistent predictions between N 2D , N 3D , and N Fused . We chose AM since it can provide more straight-forward supervision to each branch.</p><p>Pose-Aware UDA methods. We compared our two major components, which were specifically designed for pose estimation. As shown in <ref type="table">Table 3</ref>, pseudo label filtering (PL-F + AM) resulted in significant improvements compared with naive pseudo label (PL + AM), which indicates that providing more reliable and confident pseudo labels to our student network is important. More detailed comparisons of various approaches of selecting confident pseudo label are explained in the next session. Using AM and PL-F, our student network was more robustly trained on real world data, and now holds meaningful knowledge which can be passed back to the teacher network. Utilizing teacher selfsupervised learning (TSL) at this stage results in a notable increase in performance, compared to how basic momentum update technique performed from (PL) to (PL + MU).</p><p>Pseudo Label Filtering. To show the effectiveness of our pseudo label filtering based on the proposed bidirectional point filtering, we experimented with well-known pseudo label filtering techniques. The candidates were, Top k (conf), Top k (conf, class-wise), Entropy, SoftMax Max, SoftMax Avg., and ArgMax Match. Top k filterings use the top k% predictions of the pseudo label N PL based on the softmax values, where class-wise does the sorting/filtering within all instances in each class. Entropy filtering uses the bottom k% of the predictions regarding its entropy. For SoftMax Max, SoftMax Avg., and ArgMax Match, we considered all three outputs from the teacher network, N T 2D , N T 3D , N T Fused . Among the three predictions, the filtered pseudo label was generated by selecting or compositing values that were either the maximum of the softmax, the average of the softmax, or having indices match of the argmax operation. We set k to be 50, regarding the filtering approximate ratio of bidirectional point filtering.  <ref type="table">Table 4</ref>. Ablation study of the pseudo label filtering methods. Performance margins were calculated as compared to the results without using any pseudo label filtering. Compared to other filtering methods, our filtering method achieved the best performance overall, as can be seen in <ref type="table">Table 4</ref>. This suggests that our pose-aware bidirectional point filtering generates more reliable pseudo label. This is because while other filter methods only concentrate on the predicted logit itself, while ours consider the relationship between estimated NOCS map and observed depth points. Therefore, our approach can efficiently remove pose and depth-aware outliers.</p><p>Reliability of our Pseudo label. <ref type="figure" target="#fig_1">Fig. 4</ref> visualizes some examples of the real training set with GT labels and our pseudo labels. 6D poses were obtained and visualized using the Umeyama algorithm <ref type="bibr" target="#b31">[32]</ref>, using GT NOCS map and our pseudo label NOCS map. The real data annotations were mainly performed automatically using aruco markers. For some of the failure cases, additional ICP or manual human annotations were needed. Therefore, frames with inaccurate labels exist, which might disrupt supervised training.</p><p>However, there were cases where our method generated more accurate annotations than the GT provided by the dataset. For example, the GT image of the first column shows that all objects have relatively wrong poses due to the occluded aruco markers in the image, while our prediction seems much more reliable. It shows that our pseudo label is not only reliable but also sometimes more accurate than the GT, which means that our proposed approach successfully addresses both the real-world data scarcity and unreliability issues. <ref type="figure">Fig. 5</ref> shows qualitative results on the REAL275 dataset. We compare our results with some of the supervised methods, NOCS <ref type="bibr" target="#b34">[35]</ref>, SPD <ref type="bibr" target="#b29">[30]</ref> and DualPoseNet <ref type="bibr" target="#b19">[20]</ref>. Our method estimated pose and sizes more accurately than NOCS and SPD, especially on cameras and laptops. Compared to the state-of-the-art approach, DualPoseNet, ours exhibited comparable predictions, although it was not trained with the GT labels of the real dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>(a) NOCS <ref type="bibr" target="#b34">[35]</ref> (b) SPD <ref type="bibr" target="#b29">[30]</ref> (c) DualPoseNet <ref type="bibr" target="#b19">[20]</ref> (d) UDA-COPE (Ours) (e) Ground Truth (GT) <ref type="figure">Figure 5</ref>. Qualitative comparison on the REAL275 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and Future Work</head><p>To the best of our knowledge, ours is the first approach that tries to solve unsupervised domain adaption for category-level 6D pose estimation using unlabeled RGB-D data. Therefore, there may exist some issues or future directions to be addressed. For example, our pose estimation depends on object classification, detection, and segmentation to produce appropriately cropped images and sampled depth points, thus sensitive to the performance of the offthe-shelf segmentation pipeline. Also, while the proposed method utilizes a single frame RGB-D image, we may utilize geometric constraints from the video input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose UDA-COPE, unsupervised domain adaptation for category-level object pose estimation which ad-dresses the real-world lack-of-label problem using multimodality (RGB-D). Specifically, we designed a bidirectional point filtering method to filter noisy pseudo labels, and observed depth points, where the filtered depth points improve the robustness of the teacher network, and the filtered pseudo label helps efficient student network training. Both provide for better domain adaptation with real-world pose estimation. Experiments showed that our proposed pipeline and pose-aware point filtering results were comparable to or sometimes better than the performance of fully supervised approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Overview of bidirectional point filtering method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Noisy GT label examples of the Real training dataset. Human-annotated GT pose labels on the real dataset (top row) are sometimes more inaccurate than our predicted pseudo labels (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3D 50 3D 75 5 ? 2cm 5 ? 5cm 10 ? 2cm 10 ? 5cm</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Input</cell><cell cols="2">Syn</cell><cell>w/ Label Real</cell><cell>w/o Label Real</cell><cell></cell><cell></cell><cell></cell><cell>mAP (?)</cell></row><row><cell cols="2">CPS++ [22]</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>72.6</cell><cell></cell><cell>-</cell><cell>-</cell><cell>25.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Metric Scale [17]</cell><cell>RGB</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>68.1</cell><cell cols="2">32.9</cell><cell>2.2</cell><cell>5.3</cell><cell>10.0</cell><cell>24.7</cell></row><row><cell cols="2">NOCS [35]</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>36.7</cell><cell></cell><cell>3.4</cell><cell>-</cell><cell>3.4</cell><cell>-</cell><cell>20.4</cell></row><row><cell cols="2">SPD [30]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell></cell><cell></cell><cell>71.0</cell><cell cols="2">43.1</cell><cell>11.4</cell><cell>12.0</cell><cell>33.5</cell><cell>37.8</cell></row><row><cell cols="2">NOCS [35]</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>78.0</cell><cell cols="2">30.1</cell><cell>7.2</cell><cell>10.0</cell><cell>13.8</cell><cell>25.2</cell></row><row><cell cols="2">SPD [30]</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>75.2</cell><cell cols="2">46.5</cell><cell>15.7</cell><cell>18.8</cell><cell>33.7</cell><cell>47.4</cell></row><row><cell cols="2">SPD [30]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell>?</cell><cell></cell><cell>77.4</cell><cell cols="2">53.5</cell><cell>19.5</cell><cell>21.6</cell><cell>43.5</cell><cell>54.0</cell></row><row><cell cols="2">CASS [4]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell>?</cell><cell></cell><cell>77.7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>23.5</cell><cell>-</cell><cell>58.0</cell></row><row><cell cols="2">CR-Net [36]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell>?</cell><cell></cell><cell>79.3</cell><cell cols="2">55.9</cell><cell>27.8</cell><cell>34.3</cell><cell>47.2</cell><cell>60.8</cell></row><row><cell cols="5">DualPoseNet [20] RGB-D</cell><cell>?</cell><cell>?</cell><cell></cell><cell>79.8</cell><cell cols="2">62.2</cell><cell>29.3</cell><cell>35.9</cell><cell>50.0</cell><cell>66.8</cell></row><row><cell cols="2">SGPA [5]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell>?</cell><cell></cell><cell>80.1</cell><cell cols="2">61.9</cell><cell>35.9</cell><cell>39.6</cell><cell>61.3</cell><cell>70.7</cell></row><row><cell cols="2">CPS++ [22]</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>72.8</cell><cell></cell><cell>-</cell><cell>-</cell><cell>25.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell>RGB</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>82.0</cell><cell cols="2">59.0</cell><cell>24.4</cell><cell>27.0</cell><cell>49.3</cell><cell>54.8</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell>D</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>79.6</cell><cell cols="2">57.8</cell><cell>21.2</cell><cell>29.1</cell><cell>48.7</cell><cell>65.9</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell cols="2">RGB-D</cell><cell>?</cell><cell></cell><cell>?</cell><cell>82.6</cell><cell cols="2">62.5</cell><cell>30.4</cell><cell>34.8</cell><cell>56.9</cell><cell>66.0</cell></row><row><cell>Method</cell><cell>Syn</cell><cell cols="2">Real w/o Label</cell><cell cols="2">3D 25</cell><cell>3D 50</cell><cell>3D 75</cell><cell cols="2">5 ? 2cm</cell><cell cols="2">mAP (?) 5 ? 5cm</cell><cell>10 ? 2cm</cell><cell>10 ? 5cm</cell><cell>10 ? 10cm</cell></row><row><cell>CPS++ (RGB)</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">84.5</cell><cell>72.6</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>25.8</cell><cell>-</cell><cell>-</cell><cell>55.4</cell></row><row><cell>CPS++ (RGB)</cell><cell>?</cell><cell></cell><cell>?</cell><cell cols="3">84.6 (+0.1) 72.8 (+0.2)</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>25.2 (-0.6)</cell><cell>-</cell><cell>-</cell><cell>58.6 (+3.2)</cell></row><row><cell>Ours (RGB)</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">83.3</cell><cell>79.9</cell><cell>49.7</cell><cell>15.4</cell><cell></cell><cell></cell><cell>18.3</cell><cell>37.6</cell><cell>46.7</cell><cell>48.9</cell></row><row><cell>Ours (RGB)</cell><cell>?</cell><cell></cell><cell>?</cell><cell cols="4">83.8 (+0.5) 82.0 (+2.1) 59.0 (+9.3)</cell><cell cols="3">24.4 (+9.0)</cell><cell>27.0 (+8.7) 49.3 (+11.7) 54.8 (+8.1)</cell><cell>56.9 (+8.0)</cell></row><row><cell>Ours (RGB-D)</cell><cell>?</cell><cell></cell><cell>?</cell><cell>84.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 1. Quantitative comparison with state-of-the art methods on the REAL275 dataset. Empty entries either could not be evaluated or were not reported in the original paper.(+0.7) 82.6 (+2.7) 62.5 (+12.8) 30.4 (+15.0) 34.8 (+16.5) 56.9 (+19.3) 66.0 (+19.3) 68.3 (+19.4)</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was conducted by Center for Applied Research in Artificial Intelligence(CARAI) grant funded by Defense Acquisition Program Administration(DAPA) and Agency for Defense Development(ADD) (UD190031RD).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15384" to="15394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4243" to="4250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixellevel domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="11973" to="11982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2773" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Category level object pose estimation via neural analysis-by-synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguo</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1677" to="1734" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ffb6d: A full flow bidirectional fusion network for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3003" to="3013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">xmuda: Cross-modal unsupervised domain adaptation for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12605" to="12614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Myungchul Kim, and In So Kweon. Category-level metric scale object shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Uk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging se(3) equivariance for self-supervised category-level object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3560" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation through inter-modal rotation for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Mohammad Reza Loghmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Robbiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Planamente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6631" to="6638" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cps++: Improving class-level 6d pose and shape estimation from monocular images with selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05848</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Project tango</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Marder-Eppstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2016 Real-Time Live!</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7668" to="7677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Runz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Buffier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shape prior deformation for categorical 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakumar</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7364" to="7373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Category-level 6d object pose estimation via cascaded relation and recurrent reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integrated deep semantic segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syler</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian-Luca</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Mariottini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Chipalkatty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS)</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS)</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS)</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS)</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1386" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
